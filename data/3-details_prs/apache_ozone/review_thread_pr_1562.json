{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE3NjkxNzQw", "number": 1562, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMDoxNDowOFrOE5ytkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxODozMzo1OVrOFf_fJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDM1MTUzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/docs/content/design/s3-performance.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMDoxNDowOFrOH0e3tw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMDoxNDowOFrOH0e3tw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc5Mzc4Mw==", "bodyText": "OzoneClientProduced -> OzoneClientProducer", "url": "https://github.com/apache/ozone/pull/1562#discussion_r524793783", "createdAt": "2020-11-17T00:14:08Z", "author": {"login": "bharatviswa504"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDQxOTI1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/docs/content/design/s3-performance.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMDoyODo1MFrOH0fnAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwODoyNTowNFrOH0pWPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgwNTg5MA==", "bodyText": "Just to clarify my understanding, we don't have invalidate token method for s3G right?\nAs this token is generated from Client auth header fields. Means token is generated per request.\nWe are sending the required info to validate the auth header with the secret which OM has.", "url": "https://github.com/apache/ozone/pull/1562#discussion_r524805890", "createdAt": "2020-11-17T00:28:50Z", "author": {"login": "bharatviswa504"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk2NTQzOA==", "bodyText": "Just to clarify my understanding, we don't have invalidate token method for s3G right?\n\nYou are right. This example is independent of s3g just explains how the cache works. I tried to describe the problem with the simple delegation token. (I can add it as a note)", "url": "https://github.com/apache/ozone/pull/1562#discussion_r524965438", "createdAt": "2020-11-17T08:25:04Z", "author": {"login": "elek"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgwNTg5MA=="}, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDQyNTkwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/docs/content/design/s3-performance.md", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMDozMTo0MlrOH0fq7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QwNzozNDowM1rOH4Bs7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgwNjg5Mw==", "bodyText": "Does that mean with this approach we need one ozone Client instantiated, as token is part of OMRequest.", "url": "https://github.com/apache/ozone/pull/1562#discussion_r524806893", "createdAt": "2020-11-17T00:31:42Z", "author": {"login": "bharatviswa504"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.\n+\n+```\n+public class UserGroupInformation {\n+  \n+  ...\n+  \n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == this) {\n+      return true;\n+    } else if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    } else {\n+      return subject == ((UserGroupInformation) o).subject;\n+    }\n+  }\n+}\n+```\n+\n+ But the UGI initialization of Ozone always creates a new `Subject` instance for each request (even if the subject name is the same). In `OzoneClientProducer`:\n+\n+```\n+  UserGroupInformation remoteUser =\n+          UserGroupInformation.createRemoteUser(awsAccessId); // <-- new Subject is created\n+      \n+      if (OzoneSecurityUtil.isSecurityEnabled(config)) {\n+        try {\n+          OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();\n+          //setup identifier\n+          \n+          Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),\n+              identifier.getSignature().getBytes(UTF_8),\n+              identifier.getKind(),\n+              omService);\n+          remoteUser.addToken(token);\n+          ....\n+```\n+\n+**As a result Hadoop RPC caching doesn't apply to Ozone S3g**. It's a good news because it's secure, but bad news as the performance is bad.\n+\n+# Proposed change\n+\n+We need an RPC mechanism between the Ozone S3g service and Ozone Manager service which can support per-request authentication and accepts\n+\n+The  Ozone Manager client already has a pluggable transport interface: `OmTransport` is a simple interface which can deliver `OMRequest` messages:\n+\n+```\n+public interface OmTransport {\n+\n+  /**\n+   * The main method to send out the request on the defined transport.\n+   */\n+  OMResponse submitRequest(OMRequest payload) throws IOException;\n+  ...\n+```\n+\n+ The proposal is to create a new **additional** transport, based on GRPC, which can do the per-request authentication. **Existing Hadoop clients will use the well-known Hadoop RPC client**, but S3g can start to use this specific transport to achieve better performance.\n+\n+As this is nothing more, just a transport: exactly the same messages (`OmRequest`) will be used, it's not a new RPC interface.\n+\n+Only one modification is required in the RPC interface: a new per-request`token` field should be introduced in `OMRequest` which is optional.\n+\n+A new GRPC service should be started in Ozone Manager, which receives `OMRequest` and for each request, the Hadoop `UserGroupInformation` is set based on the new token field (after authentication).\n+\n+`OzoneToken` identifier can be simplified (after deprecation period) with removing the S3 specific part, as it won't be required any more.\n+\n+With this approach the `OzoneClient` instances can be cached on S3g side (with persistent GRPC connections) as the authentication information is not part of the OzoneClient any more (added by the `OmTransport` implementation per request (in case of GRPC) or per connection (in case of HadoopRPC)).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgwNzk4NA==", "bodyText": "Few questions:\nThis means S3G does not use hadoop Rpc Client, it will be use GrpcClient\nSo how OM HA will be handled retry handling logic, so all that logic need to be implemented in this new GrpcClient?\nAnd once the token is validated, will it go with the normal flow of execution in OzoneManager?\nFew minor questions, as I don't have much expertise on Grpc Implementation.\n\nDoes GrpcServer also will have RPC handler threads where requests can be handled parallel on OM.", "url": "https://github.com/apache/ozone/pull/1562#discussion_r524807984", "createdAt": "2020-11-17T00:35:15Z", "author": {"login": "bharatviswa504"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.\n+\n+```\n+public class UserGroupInformation {\n+  \n+  ...\n+  \n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == this) {\n+      return true;\n+    } else if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    } else {\n+      return subject == ((UserGroupInformation) o).subject;\n+    }\n+  }\n+}\n+```\n+\n+ But the UGI initialization of Ozone always creates a new `Subject` instance for each request (even if the subject name is the same). In `OzoneClientProducer`:\n+\n+```\n+  UserGroupInformation remoteUser =\n+          UserGroupInformation.createRemoteUser(awsAccessId); // <-- new Subject is created\n+      \n+      if (OzoneSecurityUtil.isSecurityEnabled(config)) {\n+        try {\n+          OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();\n+          //setup identifier\n+          \n+          Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),\n+              identifier.getSignature().getBytes(UTF_8),\n+              identifier.getKind(),\n+              omService);\n+          remoteUser.addToken(token);\n+          ....\n+```\n+\n+**As a result Hadoop RPC caching doesn't apply to Ozone S3g**. It's a good news because it's secure, but bad news as the performance is bad.\n+\n+# Proposed change\n+\n+We need an RPC mechanism between the Ozone S3g service and Ozone Manager service which can support per-request authentication and accepts\n+\n+The  Ozone Manager client already has a pluggable transport interface: `OmTransport` is a simple interface which can deliver `OMRequest` messages:\n+\n+```\n+public interface OmTransport {\n+\n+  /**\n+   * The main method to send out the request on the defined transport.\n+   */\n+  OMResponse submitRequest(OMRequest payload) throws IOException;\n+  ...\n+```\n+\n+ The proposal is to create a new **additional** transport, based on GRPC, which can do the per-request authentication. **Existing Hadoop clients will use the well-known Hadoop RPC client**, but S3g can start to use this specific transport to achieve better performance.\n+\n+As this is nothing more, just a transport: exactly the same messages (`OmRequest`) will be used, it's not a new RPC interface.\n+\n+Only one modification is required in the RPC interface: a new per-request`token` field should be introduced in `OMRequest` which is optional.\n+\n+A new GRPC service should be started in Ozone Manager, which receives `OMRequest` and for each request, the Hadoop `UserGroupInformation` is set based on the new token field (after authentication).\n+\n+`OzoneToken` identifier can be simplified (after deprecation period) with removing the S3 specific part, as it won't be required any more.\n+\n+With this approach the `OzoneClient` instances can be cached on S3g side (with persistent GRPC connections) as the authentication information is not part of the OzoneClient any more (added by the `OmTransport` implementation per request (in case of GRPC) or per connection (in case of HadoopRPC)).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgwNjg5Mw=="}, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk2NDUyMg==", "bodyText": "Does that mean with this approach we need one ozone Client instantiated, as token is part of OMRequest.\n\nYes.\n\nAnd once the token is validated, will it go with the normal flow of execution in OzoneManager?\n\nYes, exactly the same logic.\n\nDoes GrpcServer also will have RPC handler threads where requests can be handled parallel on OM.\n\nYes. As far as I understood from the documentation the new thread is created by the async IO handler thread.\nBut if we need more freedom, we can always introduce a simple Executor.\nBut it's a very good question. Thinking about this, I have new ideas: with separating S3g and client side traffic we can monitor the two in different way (for example compare queue time of client and s3g calls, or set priorities). Not in this step, but something which will be possible.\n\nHow OM HA will be handled retry handling logic, so all that logic need to be implemented in this new GrpcClient?\n\nYes. We need to take care about the retry logic. My initial suggestion is to create 3, persistent connection to all the OM, and in case of not leader exception try to send the message on a different connection.\nIn case of client it can be expensive (always create 3 different connection to 3 different OM HA instance), but in case of S3g and persistent connections it seems to more effective as the connections are persistent.", "url": "https://github.com/apache/ozone/pull/1562#discussion_r524964522", "createdAt": "2020-11-17T08:23:36Z", "author": {"login": "elek"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.\n+\n+```\n+public class UserGroupInformation {\n+  \n+  ...\n+  \n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == this) {\n+      return true;\n+    } else if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    } else {\n+      return subject == ((UserGroupInformation) o).subject;\n+    }\n+  }\n+}\n+```\n+\n+ But the UGI initialization of Ozone always creates a new `Subject` instance for each request (even if the subject name is the same). In `OzoneClientProducer`:\n+\n+```\n+  UserGroupInformation remoteUser =\n+          UserGroupInformation.createRemoteUser(awsAccessId); // <-- new Subject is created\n+      \n+      if (OzoneSecurityUtil.isSecurityEnabled(config)) {\n+        try {\n+          OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();\n+          //setup identifier\n+          \n+          Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),\n+              identifier.getSignature().getBytes(UTF_8),\n+              identifier.getKind(),\n+              omService);\n+          remoteUser.addToken(token);\n+          ....\n+```\n+\n+**As a result Hadoop RPC caching doesn't apply to Ozone S3g**. It's a good news because it's secure, but bad news as the performance is bad.\n+\n+# Proposed change\n+\n+We need an RPC mechanism between the Ozone S3g service and Ozone Manager service which can support per-request authentication and accepts\n+\n+The  Ozone Manager client already has a pluggable transport interface: `OmTransport` is a simple interface which can deliver `OMRequest` messages:\n+\n+```\n+public interface OmTransport {\n+\n+  /**\n+   * The main method to send out the request on the defined transport.\n+   */\n+  OMResponse submitRequest(OMRequest payload) throws IOException;\n+  ...\n+```\n+\n+ The proposal is to create a new **additional** transport, based on GRPC, which can do the per-request authentication. **Existing Hadoop clients will use the well-known Hadoop RPC client**, but S3g can start to use this specific transport to achieve better performance.\n+\n+As this is nothing more, just a transport: exactly the same messages (`OmRequest`) will be used, it's not a new RPC interface.\n+\n+Only one modification is required in the RPC interface: a new per-request`token` field should be introduced in `OMRequest` which is optional.\n+\n+A new GRPC service should be started in Ozone Manager, which receives `OMRequest` and for each request, the Hadoop `UserGroupInformation` is set based on the new token field (after authentication).\n+\n+`OzoneToken` identifier can be simplified (after deprecation period) with removing the S3 specific part, as it won't be required any more.\n+\n+With this approach the `OzoneClient` instances can be cached on S3g side (with persistent GRPC connections) as the authentication information is not part of the OzoneClient any more (added by the `OmTransport` implementation per request (in case of GRPC) or per connection (in case of HadoopRPC)).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgwNjg5Mw=="}, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYwNDI2Mg==", "bodyText": "So, it is like a new retry logic should be implemented for GrpcClient.", "url": "https://github.com/apache/ozone/pull/1562#discussion_r525604262", "createdAt": "2020-11-17T23:57:30Z", "author": {"login": "bharatviswa504"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.\n+\n+```\n+public class UserGroupInformation {\n+  \n+  ...\n+  \n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == this) {\n+      return true;\n+    } else if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    } else {\n+      return subject == ((UserGroupInformation) o).subject;\n+    }\n+  }\n+}\n+```\n+\n+ But the UGI initialization of Ozone always creates a new `Subject` instance for each request (even if the subject name is the same). In `OzoneClientProducer`:\n+\n+```\n+  UserGroupInformation remoteUser =\n+          UserGroupInformation.createRemoteUser(awsAccessId); // <-- new Subject is created\n+      \n+      if (OzoneSecurityUtil.isSecurityEnabled(config)) {\n+        try {\n+          OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();\n+          //setup identifier\n+          \n+          Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),\n+              identifier.getSignature().getBytes(UTF_8),\n+              identifier.getKind(),\n+              omService);\n+          remoteUser.addToken(token);\n+          ....\n+```\n+\n+**As a result Hadoop RPC caching doesn't apply to Ozone S3g**. It's a good news because it's secure, but bad news as the performance is bad.\n+\n+# Proposed change\n+\n+We need an RPC mechanism between the Ozone S3g service and Ozone Manager service which can support per-request authentication and accepts\n+\n+The  Ozone Manager client already has a pluggable transport interface: `OmTransport` is a simple interface which can deliver `OMRequest` messages:\n+\n+```\n+public interface OmTransport {\n+\n+  /**\n+   * The main method to send out the request on the defined transport.\n+   */\n+  OMResponse submitRequest(OMRequest payload) throws IOException;\n+  ...\n+```\n+\n+ The proposal is to create a new **additional** transport, based on GRPC, which can do the per-request authentication. **Existing Hadoop clients will use the well-known Hadoop RPC client**, but S3g can start to use this specific transport to achieve better performance.\n+\n+As this is nothing more, just a transport: exactly the same messages (`OmRequest`) will be used, it's not a new RPC interface.\n+\n+Only one modification is required in the RPC interface: a new per-request`token` field should be introduced in `OMRequest` which is optional.\n+\n+A new GRPC service should be started in Ozone Manager, which receives `OMRequest` and for each request, the Hadoop `UserGroupInformation` is set based on the new token field (after authentication).\n+\n+`OzoneToken` identifier can be simplified (after deprecation period) with removing the S3 specific part, as it won't be required any more.\n+\n+With this approach the `OzoneClient` instances can be cached on S3g side (with persistent GRPC connections) as the authentication information is not part of the OzoneClient any more (added by the `OmTransport` implementation per request (in case of GRPC) or per connection (in case of HadoopRPC)).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgwNjg5Mw=="}, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYwNTAyNQ==", "bodyText": "Thank You for detailed answers for other points.\n\nI have new ideas: with separating S3g and client side traffic we can monitor the two in different way (for example >compare queue time of client and s3g calls, or set priorities). Not in this step, but something which will be possible.\n\nThis idea looks interesting, but at end, both are coming from end clients, so getting additional metrics helps to understand better calls from S3/other interface, but not sure in which scenarios this will help.", "url": "https://github.com/apache/ozone/pull/1562#discussion_r525605025", "createdAt": "2020-11-17T23:59:41Z", "author": {"login": "bharatviswa504"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.\n+\n+```\n+public class UserGroupInformation {\n+  \n+  ...\n+  \n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == this) {\n+      return true;\n+    } else if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    } else {\n+      return subject == ((UserGroupInformation) o).subject;\n+    }\n+  }\n+}\n+```\n+\n+ But the UGI initialization of Ozone always creates a new `Subject` instance for each request (even if the subject name is the same). In `OzoneClientProducer`:\n+\n+```\n+  UserGroupInformation remoteUser =\n+          UserGroupInformation.createRemoteUser(awsAccessId); // <-- new Subject is created\n+      \n+      if (OzoneSecurityUtil.isSecurityEnabled(config)) {\n+        try {\n+          OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();\n+          //setup identifier\n+          \n+          Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),\n+              identifier.getSignature().getBytes(UTF_8),\n+              identifier.getKind(),\n+              omService);\n+          remoteUser.addToken(token);\n+          ....\n+```\n+\n+**As a result Hadoop RPC caching doesn't apply to Ozone S3g**. It's a good news because it's secure, but bad news as the performance is bad.\n+\n+# Proposed change\n+\n+We need an RPC mechanism between the Ozone S3g service and Ozone Manager service which can support per-request authentication and accepts\n+\n+The  Ozone Manager client already has a pluggable transport interface: `OmTransport` is a simple interface which can deliver `OMRequest` messages:\n+\n+```\n+public interface OmTransport {\n+\n+  /**\n+   * The main method to send out the request on the defined transport.\n+   */\n+  OMResponse submitRequest(OMRequest payload) throws IOException;\n+  ...\n+```\n+\n+ The proposal is to create a new **additional** transport, based on GRPC, which can do the per-request authentication. **Existing Hadoop clients will use the well-known Hadoop RPC client**, but S3g can start to use this specific transport to achieve better performance.\n+\n+As this is nothing more, just a transport: exactly the same messages (`OmRequest`) will be used, it's not a new RPC interface.\n+\n+Only one modification is required in the RPC interface: a new per-request`token` field should be introduced in `OMRequest` which is optional.\n+\n+A new GRPC service should be started in Ozone Manager, which receives `OMRequest` and for each request, the Hadoop `UserGroupInformation` is set based on the new token field (after authentication).\n+\n+`OzoneToken` identifier can be simplified (after deprecation period) with removing the S3 specific part, as it won't be required any more.\n+\n+With this approach the `OzoneClient` instances can be cached on S3g side (with persistent GRPC connections) as the authentication information is not part of the OzoneClient any more (added by the `OmTransport` implementation per request (in case of GRPC) or per connection (in case of HadoopRPC)).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgwNjg5Mw=="}, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODUwOTI2Mw==", "bodyText": "For example to understand / compare the cluster usage. Which part is used more HCFS or s3? What is the source of small files s3 or HCFS?\nThis (different metrics) is not something to do right now but an interesting option to think forward if this approach is accepted.", "url": "https://github.com/apache/ozone/pull/1562#discussion_r528509263", "createdAt": "2020-11-23T07:31:12Z", "author": {"login": "elek"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.\n+\n+```\n+public class UserGroupInformation {\n+  \n+  ...\n+  \n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == this) {\n+      return true;\n+    } else if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    } else {\n+      return subject == ((UserGroupInformation) o).subject;\n+    }\n+  }\n+}\n+```\n+\n+ But the UGI initialization of Ozone always creates a new `Subject` instance for each request (even if the subject name is the same). In `OzoneClientProducer`:\n+\n+```\n+  UserGroupInformation remoteUser =\n+          UserGroupInformation.createRemoteUser(awsAccessId); // <-- new Subject is created\n+      \n+      if (OzoneSecurityUtil.isSecurityEnabled(config)) {\n+        try {\n+          OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();\n+          //setup identifier\n+          \n+          Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),\n+              identifier.getSignature().getBytes(UTF_8),\n+              identifier.getKind(),\n+              omService);\n+          remoteUser.addToken(token);\n+          ....\n+```\n+\n+**As a result Hadoop RPC caching doesn't apply to Ozone S3g**. It's a good news because it's secure, but bad news as the performance is bad.\n+\n+# Proposed change\n+\n+We need an RPC mechanism between the Ozone S3g service and Ozone Manager service which can support per-request authentication and accepts\n+\n+The  Ozone Manager client already has a pluggable transport interface: `OmTransport` is a simple interface which can deliver `OMRequest` messages:\n+\n+```\n+public interface OmTransport {\n+\n+  /**\n+   * The main method to send out the request on the defined transport.\n+   */\n+  OMResponse submitRequest(OMRequest payload) throws IOException;\n+  ...\n+```\n+\n+ The proposal is to create a new **additional** transport, based on GRPC, which can do the per-request authentication. **Existing Hadoop clients will use the well-known Hadoop RPC client**, but S3g can start to use this specific transport to achieve better performance.\n+\n+As this is nothing more, just a transport: exactly the same messages (`OmRequest`) will be used, it's not a new RPC interface.\n+\n+Only one modification is required in the RPC interface: a new per-request`token` field should be introduced in `OMRequest` which is optional.\n+\n+A new GRPC service should be started in Ozone Manager, which receives `OMRequest` and for each request, the Hadoop `UserGroupInformation` is set based on the new token field (after authentication).\n+\n+`OzoneToken` identifier can be simplified (after deprecation period) with removing the S3 specific part, as it won't be required any more.\n+\n+With this approach the `OzoneClient` instances can be cached on S3g side (with persistent GRPC connections) as the authentication information is not part of the OzoneClient any more (added by the `OmTransport` implementation per request (in case of GRPC) or per connection (in case of HadoopRPC)).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgwNjg5Mw=="}, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 197}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODUxMDE5MQ==", "bodyText": "So, it is like a new retry logic should be implemented for GrpcClient.\n\nYes. And I argue that this logic can be optimized for servers (connections to different OM instances can be cached long-term) and not only optimized for client (open second connections to the right OM only in case of leader election)", "url": "https://github.com/apache/ozone/pull/1562#discussion_r528510191", "createdAt": "2020-11-23T07:34:03Z", "author": {"login": "elek"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.\n+\n+```\n+public class UserGroupInformation {\n+  \n+  ...\n+  \n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == this) {\n+      return true;\n+    } else if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    } else {\n+      return subject == ((UserGroupInformation) o).subject;\n+    }\n+  }\n+}\n+```\n+\n+ But the UGI initialization of Ozone always creates a new `Subject` instance for each request (even if the subject name is the same). In `OzoneClientProducer`:\n+\n+```\n+  UserGroupInformation remoteUser =\n+          UserGroupInformation.createRemoteUser(awsAccessId); // <-- new Subject is created\n+      \n+      if (OzoneSecurityUtil.isSecurityEnabled(config)) {\n+        try {\n+          OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();\n+          //setup identifier\n+          \n+          Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),\n+              identifier.getSignature().getBytes(UTF_8),\n+              identifier.getKind(),\n+              omService);\n+          remoteUser.addToken(token);\n+          ....\n+```\n+\n+**As a result Hadoop RPC caching doesn't apply to Ozone S3g**. It's a good news because it's secure, but bad news as the performance is bad.\n+\n+# Proposed change\n+\n+We need an RPC mechanism between the Ozone S3g service and Ozone Manager service which can support per-request authentication and accepts\n+\n+The  Ozone Manager client already has a pluggable transport interface: `OmTransport` is a simple interface which can deliver `OMRequest` messages:\n+\n+```\n+public interface OmTransport {\n+\n+  /**\n+   * The main method to send out the request on the defined transport.\n+   */\n+  OMResponse submitRequest(OMRequest payload) throws IOException;\n+  ...\n+```\n+\n+ The proposal is to create a new **additional** transport, based on GRPC, which can do the per-request authentication. **Existing Hadoop clients will use the well-known Hadoop RPC client**, but S3g can start to use this specific transport to achieve better performance.\n+\n+As this is nothing more, just a transport: exactly the same messages (`OmRequest`) will be used, it's not a new RPC interface.\n+\n+Only one modification is required in the RPC interface: a new per-request`token` field should be introduced in `OMRequest` which is optional.\n+\n+A new GRPC service should be started in Ozone Manager, which receives `OMRequest` and for each request, the Hadoop `UserGroupInformation` is set based on the new token field (after authentication).\n+\n+`OzoneToken` identifier can be simplified (after deprecation period) with removing the S3 specific part, as it won't be required any more.\n+\n+With this approach the `OzoneClient` instances can be cached on S3g side (with persistent GRPC connections) as the authentication information is not part of the OzoneClient any more (added by the `OmTransport` implementation per request (in case of GRPC) or per connection (in case of HadoopRPC)).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgwNjg5Mw=="}, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 197}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDQ0NTYyOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/docs/content/design/s3-performance.md", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMDo0MTowMFrOH0f2Xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QwNzozNjo0NVrOH4BwYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgwOTgyMg==", "bodyText": "Not understood this point, what is meant by service discovery call is not required and also not using ozone client may have own challenge.\nSo, can we use one single client even with Hadoop RPC? More information on this point will help what is meant by this alternative.", "url": "https://github.com/apache/ozone/pull/1562#discussion_r524809822", "createdAt": "2020-11-17T00:41:00Z", "author": {"login": "bharatviswa504"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.\n+\n+```\n+public class UserGroupInformation {\n+  \n+  ...\n+  \n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == this) {\n+      return true;\n+    } else if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    } else {\n+      return subject == ((UserGroupInformation) o).subject;\n+    }\n+  }\n+}\n+```\n+\n+ But the UGI initialization of Ozone always creates a new `Subject` instance for each request (even if the subject name is the same). In `OzoneClientProducer`:\n+\n+```\n+  UserGroupInformation remoteUser =\n+          UserGroupInformation.createRemoteUser(awsAccessId); // <-- new Subject is created\n+      \n+      if (OzoneSecurityUtil.isSecurityEnabled(config)) {\n+        try {\n+          OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();\n+          //setup identifier\n+          \n+          Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),\n+              identifier.getSignature().getBytes(UTF_8),\n+              identifier.getKind(),\n+              omService);\n+          remoteUser.addToken(token);\n+          ....\n+```\n+\n+**As a result Hadoop RPC caching doesn't apply to Ozone S3g**. It's a good news because it's secure, but bad news as the performance is bad.\n+\n+# Proposed change\n+\n+We need an RPC mechanism between the Ozone S3g service and Ozone Manager service which can support per-request authentication and accepts\n+\n+The  Ozone Manager client already has a pluggable transport interface: `OmTransport` is a simple interface which can deliver `OMRequest` messages:\n+\n+```\n+public interface OmTransport {\n+\n+  /**\n+   * The main method to send out the request on the defined transport.\n+   */\n+  OMResponse submitRequest(OMRequest payload) throws IOException;\n+  ...\n+```\n+\n+ The proposal is to create a new **additional** transport, based on GRPC, which can do the per-request authentication. **Existing Hadoop clients will use the well-known Hadoop RPC client**, but S3g can start to use this specific transport to achieve better performance.\n+\n+As this is nothing more, just a transport: exactly the same messages (`OmRequest`) will be used, it's not a new RPC interface.\n+\n+Only one modification is required in the RPC interface: a new per-request`token` field should be introduced in `OMRequest` which is optional.\n+\n+A new GRPC service should be started in Ozone Manager, which receives `OMRequest` and for each request, the Hadoop `UserGroupInformation` is set based on the new token field (after authentication).\n+\n+`OzoneToken` identifier can be simplified (after deprecation period) with removing the S3 specific part, as it won't be required any more.\n+\n+With this approach the `OzoneClient` instances can be cached on S3g side (with persistent GRPC connections) as the authentication information is not part of the OzoneClient any more (added by the `OmTransport` implementation per request (in case of GRPC) or per connection (in case of HadoopRPC)).\n+\n+# Possible alternatives\n+\n+* It's possible to use pure Hadoop RPC client instead of Ozone Client which would make the client connection slightly cheaper (service discovery call is not required) but it's still require to create new connections for each requests (and downloading data without OzoneClient may have own challenges).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 201}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk1ODIyMg==", "bodyText": "Not understood this point, what is meant by service discovery call is not required and also not using ozone client may have own challenge.\n\nWhen you use OzoneClient an initial service discovery call will be executed at the beginning. But after that you can use it easily. Both OM client connection and datanode connections are managed by OzoneClient.\nWe can try to use pure OM Client call (without using OzoneClient just to use Hadoop RPC client API) to avoid service discovery, but in that case we couldn't use OzoneClient. As OzoneClient contains the client logic for datanode, without OzoneClient the OM Client calls can be more simple, but at the end the solution can be more complex as we should use a lower level datanode client api, too.", "url": "https://github.com/apache/ozone/pull/1562#discussion_r524958222", "createdAt": "2020-11-17T08:12:20Z", "author": {"login": "elek"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.\n+\n+```\n+public class UserGroupInformation {\n+  \n+  ...\n+  \n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == this) {\n+      return true;\n+    } else if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    } else {\n+      return subject == ((UserGroupInformation) o).subject;\n+    }\n+  }\n+}\n+```\n+\n+ But the UGI initialization of Ozone always creates a new `Subject` instance for each request (even if the subject name is the same). In `OzoneClientProducer`:\n+\n+```\n+  UserGroupInformation remoteUser =\n+          UserGroupInformation.createRemoteUser(awsAccessId); // <-- new Subject is created\n+      \n+      if (OzoneSecurityUtil.isSecurityEnabled(config)) {\n+        try {\n+          OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();\n+          //setup identifier\n+          \n+          Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),\n+              identifier.getSignature().getBytes(UTF_8),\n+              identifier.getKind(),\n+              omService);\n+          remoteUser.addToken(token);\n+          ....\n+```\n+\n+**As a result Hadoop RPC caching doesn't apply to Ozone S3g**. It's a good news because it's secure, but bad news as the performance is bad.\n+\n+# Proposed change\n+\n+We need an RPC mechanism between the Ozone S3g service and Ozone Manager service which can support per-request authentication and accepts\n+\n+The  Ozone Manager client already has a pluggable transport interface: `OmTransport` is a simple interface which can deliver `OMRequest` messages:\n+\n+```\n+public interface OmTransport {\n+\n+  /**\n+   * The main method to send out the request on the defined transport.\n+   */\n+  OMResponse submitRequest(OMRequest payload) throws IOException;\n+  ...\n+```\n+\n+ The proposal is to create a new **additional** transport, based on GRPC, which can do the per-request authentication. **Existing Hadoop clients will use the well-known Hadoop RPC client**, but S3g can start to use this specific transport to achieve better performance.\n+\n+As this is nothing more, just a transport: exactly the same messages (`OmRequest`) will be used, it's not a new RPC interface.\n+\n+Only one modification is required in the RPC interface: a new per-request`token` field should be introduced in `OMRequest` which is optional.\n+\n+A new GRPC service should be started in Ozone Manager, which receives `OMRequest` and for each request, the Hadoop `UserGroupInformation` is set based on the new token field (after authentication).\n+\n+`OzoneToken` identifier can be simplified (after deprecation period) with removing the S3 specific part, as it won't be required any more.\n+\n+With this approach the `OzoneClient` instances can be cached on S3g side (with persistent GRPC connections) as the authentication information is not part of the OzoneClient any more (added by the `OmTransport` implementation per request (in case of GRPC) or per connection (in case of HadoopRPC)).\n+\n+# Possible alternatives\n+\n+* It's possible to use pure Hadoop RPC client instead of Ozone Client which would make the client connection slightly cheaper (service discovery call is not required) but it's still require to create new connections for each requests (and downloading data without OzoneClient may have own challenges).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgwOTgyMg=="}, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 201}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYwMzk0Nw==", "bodyText": "Understood, so for OM API's we want to use direct omClient instead of coming via ozone client to save service discovery, but for dn we need still ozone client.\nBut how token authentication will happen?", "url": "https://github.com/apache/ozone/pull/1562#discussion_r525603947", "createdAt": "2020-11-17T23:56:37Z", "author": {"login": "bharatviswa504"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.\n+\n+```\n+public class UserGroupInformation {\n+  \n+  ...\n+  \n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == this) {\n+      return true;\n+    } else if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    } else {\n+      return subject == ((UserGroupInformation) o).subject;\n+    }\n+  }\n+}\n+```\n+\n+ But the UGI initialization of Ozone always creates a new `Subject` instance for each request (even if the subject name is the same). In `OzoneClientProducer`:\n+\n+```\n+  UserGroupInformation remoteUser =\n+          UserGroupInformation.createRemoteUser(awsAccessId); // <-- new Subject is created\n+      \n+      if (OzoneSecurityUtil.isSecurityEnabled(config)) {\n+        try {\n+          OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();\n+          //setup identifier\n+          \n+          Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),\n+              identifier.getSignature().getBytes(UTF_8),\n+              identifier.getKind(),\n+              omService);\n+          remoteUser.addToken(token);\n+          ....\n+```\n+\n+**As a result Hadoop RPC caching doesn't apply to Ozone S3g**. It's a good news because it's secure, but bad news as the performance is bad.\n+\n+# Proposed change\n+\n+We need an RPC mechanism between the Ozone S3g service and Ozone Manager service which can support per-request authentication and accepts\n+\n+The  Ozone Manager client already has a pluggable transport interface: `OmTransport` is a simple interface which can deliver `OMRequest` messages:\n+\n+```\n+public interface OmTransport {\n+\n+  /**\n+   * The main method to send out the request on the defined transport.\n+   */\n+  OMResponse submitRequest(OMRequest payload) throws IOException;\n+  ...\n+```\n+\n+ The proposal is to create a new **additional** transport, based on GRPC, which can do the per-request authentication. **Existing Hadoop clients will use the well-known Hadoop RPC client**, but S3g can start to use this specific transport to achieve better performance.\n+\n+As this is nothing more, just a transport: exactly the same messages (`OmRequest`) will be used, it's not a new RPC interface.\n+\n+Only one modification is required in the RPC interface: a new per-request`token` field should be introduced in `OMRequest` which is optional.\n+\n+A new GRPC service should be started in Ozone Manager, which receives `OMRequest` and for each request, the Hadoop `UserGroupInformation` is set based on the new token field (after authentication).\n+\n+`OzoneToken` identifier can be simplified (after deprecation period) with removing the S3 specific part, as it won't be required any more.\n+\n+With this approach the `OzoneClient` instances can be cached on S3g side (with persistent GRPC connections) as the authentication information is not part of the OzoneClient any more (added by the `OmTransport` implementation per request (in case of GRPC) or per connection (in case of HadoopRPC)).\n+\n+# Possible alternatives\n+\n+* It's possible to use pure Hadoop RPC client instead of Ozone Client which would make the client connection slightly cheaper (service discovery call is not required) but it's still require to create new connections for each requests (and downloading data without OzoneClient may have own challenges).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgwOTgyMg=="}, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 201}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODUxMTA3Mw==", "bodyText": "Understood, so for OM API's we want to use direct omClient instead of coming via ozone client to save service discovery, but for dn we need still ozone client.\n\nYes, that is added as possible alternative (using pure om client API + OzoneClient for datanode), but I don't like it:\n\nAuthentication (as you asked) is not solved here, still you need per request connection\nUsing OM client + OzoneClient for datanode is not straightforward, requires more work.\n\nTherefore, I suggested to use a different approach. (use OzoneClient but create a new OMTransport implementation based on GRPC).", "url": "https://github.com/apache/ozone/pull/1562#discussion_r528511073", "createdAt": "2020-11-23T07:36:45Z", "author": {"login": "elek"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.\n+\n+```\n+public class UserGroupInformation {\n+  \n+  ...\n+  \n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == this) {\n+      return true;\n+    } else if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    } else {\n+      return subject == ((UserGroupInformation) o).subject;\n+    }\n+  }\n+}\n+```\n+\n+ But the UGI initialization of Ozone always creates a new `Subject` instance for each request (even if the subject name is the same). In `OzoneClientProducer`:\n+\n+```\n+  UserGroupInformation remoteUser =\n+          UserGroupInformation.createRemoteUser(awsAccessId); // <-- new Subject is created\n+      \n+      if (OzoneSecurityUtil.isSecurityEnabled(config)) {\n+        try {\n+          OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();\n+          //setup identifier\n+          \n+          Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),\n+              identifier.getSignature().getBytes(UTF_8),\n+              identifier.getKind(),\n+              omService);\n+          remoteUser.addToken(token);\n+          ....\n+```\n+\n+**As a result Hadoop RPC caching doesn't apply to Ozone S3g**. It's a good news because it's secure, but bad news as the performance is bad.\n+\n+# Proposed change\n+\n+We need an RPC mechanism between the Ozone S3g service and Ozone Manager service which can support per-request authentication and accepts\n+\n+The  Ozone Manager client already has a pluggable transport interface: `OmTransport` is a simple interface which can deliver `OMRequest` messages:\n+\n+```\n+public interface OmTransport {\n+\n+  /**\n+   * The main method to send out the request on the defined transport.\n+   */\n+  OMResponse submitRequest(OMRequest payload) throws IOException;\n+  ...\n+```\n+\n+ The proposal is to create a new **additional** transport, based on GRPC, which can do the per-request authentication. **Existing Hadoop clients will use the well-known Hadoop RPC client**, but S3g can start to use this specific transport to achieve better performance.\n+\n+As this is nothing more, just a transport: exactly the same messages (`OmRequest`) will be used, it's not a new RPC interface.\n+\n+Only one modification is required in the RPC interface: a new per-request`token` field should be introduced in `OMRequest` which is optional.\n+\n+A new GRPC service should be started in Ozone Manager, which receives `OMRequest` and for each request, the Hadoop `UserGroupInformation` is set based on the new token field (after authentication).\n+\n+`OzoneToken` identifier can be simplified (after deprecation period) with removing the S3 specific part, as it won't be required any more.\n+\n+With this approach the `OzoneClient` instances can be cached on S3g side (with persistent GRPC connections) as the authentication information is not part of the OzoneClient any more (added by the `OmTransport` implementation per request (in case of GRPC) or per connection (in case of HadoopRPC)).\n+\n+# Possible alternatives\n+\n+* It's possible to use pure Hadoop RPC client instead of Ozone Client which would make the client connection slightly cheaper (service discovery call is not required) but it's still require to create new connections for each requests (and downloading data without OzoneClient may have own challenges).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgwOTgyMg=="}, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 201}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NTI4Mzg4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/docs/content/design/s3-performance.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QyMjozMTo1MlrOH1OLVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QwNzozOTowMFrOH4BzZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU2ODg1Mg==", "bodyText": "To protect the token from being stolen, TLS must be enabled for GRPC.\nTo set up TLS for GRPC, the client must get the CA cert via service discovery.", "url": "https://github.com/apache/ozone/pull/1562#discussion_r525568852", "createdAt": "2020-11-17T22:31:52Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.\n+\n+```\n+public class UserGroupInformation {\n+  \n+  ...\n+  \n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == this) {\n+      return true;\n+    } else if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    } else {\n+      return subject == ((UserGroupInformation) o).subject;\n+    }\n+  }\n+}\n+```\n+\n+ But the UGI initialization of Ozone always creates a new `Subject` instance for each request (even if the subject name is the same). In `OzoneClientProducer`:\n+\n+```\n+  UserGroupInformation remoteUser =\n+          UserGroupInformation.createRemoteUser(awsAccessId); // <-- new Subject is created\n+      \n+      if (OzoneSecurityUtil.isSecurityEnabled(config)) {\n+        try {\n+          OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();\n+          //setup identifier\n+          \n+          Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),\n+              identifier.getSignature().getBytes(UTF_8),\n+              identifier.getKind(),\n+              omService);\n+          remoteUser.addToken(token);\n+          ....\n+```\n+\n+**As a result Hadoop RPC caching doesn't apply to Ozone S3g**. It's a good news because it's secure, but bad news as the performance is bad.\n+\n+# Proposed change\n+\n+We need an RPC mechanism between the Ozone S3g service and Ozone Manager service which can support per-request authentication and accepts\n+\n+The  Ozone Manager client already has a pluggable transport interface: `OmTransport` is a simple interface which can deliver `OMRequest` messages:\n+\n+```\n+public interface OmTransport {\n+\n+  /**\n+   * The main method to send out the request on the defined transport.\n+   */\n+  OMResponse submitRequest(OMRequest payload) throws IOException;\n+  ...\n+```\n+\n+ The proposal is to create a new **additional** transport, based on GRPC, which can do the per-request authentication. **Existing Hadoop clients will use the well-known Hadoop RPC client**, but S3g can start to use this specific transport to achieve better performance.\n+\n+As this is nothing more, just a transport: exactly the same messages (`OmRequest`) will be used, it's not a new RPC interface.\n+\n+Only one modification is required in the RPC interface: a new per-request`token` field should be introduced in `OMRequest` which is optional.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODUxMTg0Nw==", "bodyText": "To protect the token from being stolen, TLS must be enabled for GRPC.\n\nYes, 100% agree.\n\nTo set up TLS for GRPC, the client must get the CA cert via service discovery.\n\nFix me If I am wrong, but CA certificate is also downloaded during the datanode initialization and can be used.\nBut anyway: as I suggest to use OzoneClient (but with new OM transport), serviceDiscovery call will be executed as before, but instead of calling once for each S3 HTTP request, it will be called only once one connection is added to the connection pool.", "url": "https://github.com/apache/ozone/pull/1562#discussion_r528511847", "createdAt": "2020-11-23T07:39:00Z", "author": {"login": "elek"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.\n+\n+```\n+public class UserGroupInformation {\n+  \n+  ...\n+  \n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == this) {\n+      return true;\n+    } else if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    } else {\n+      return subject == ((UserGroupInformation) o).subject;\n+    }\n+  }\n+}\n+```\n+\n+ But the UGI initialization of Ozone always creates a new `Subject` instance for each request (even if the subject name is the same). In `OzoneClientProducer`:\n+\n+```\n+  UserGroupInformation remoteUser =\n+          UserGroupInformation.createRemoteUser(awsAccessId); // <-- new Subject is created\n+      \n+      if (OzoneSecurityUtil.isSecurityEnabled(config)) {\n+        try {\n+          OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();\n+          //setup identifier\n+          \n+          Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),\n+              identifier.getSignature().getBytes(UTF_8),\n+              identifier.getKind(),\n+              omService);\n+          remoteUser.addToken(token);\n+          ....\n+```\n+\n+**As a result Hadoop RPC caching doesn't apply to Ozone S3g**. It's a good news because it's secure, but bad news as the performance is bad.\n+\n+# Proposed change\n+\n+We need an RPC mechanism between the Ozone S3g service and Ozone Manager service which can support per-request authentication and accepts\n+\n+The  Ozone Manager client already has a pluggable transport interface: `OmTransport` is a simple interface which can deliver `OMRequest` messages:\n+\n+```\n+public interface OmTransport {\n+\n+  /**\n+   * The main method to send out the request on the defined transport.\n+   */\n+  OMResponse submitRequest(OMRequest payload) throws IOException;\n+  ...\n+```\n+\n+ The proposal is to create a new **additional** transport, based on GRPC, which can do the per-request authentication. **Existing Hadoop clients will use the well-known Hadoop RPC client**, but S3g can start to use this specific transport to achieve better performance.\n+\n+As this is nothing more, just a transport: exactly the same messages (`OmRequest`) will be used, it's not a new RPC interface.\n+\n+Only one modification is required in the RPC interface: a new per-request`token` field should be introduced in `OMRequest` which is optional.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU2ODg1Mg=="}, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 191}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NTMxODY3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/docs/content/design/s3-performance.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QyMjo0MjozOFrOH1Ofrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QwODoyNjo1MlrOH4DEVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU3NDA2Mw==", "bodyText": "I still feel we can reuse the Hadoop Rpc connection here.\nDon't remember exactly why we have to use a token user and do the token validation at OM. But another solution I would like to propose is to use Proxy user at S3g:\nInstead of wrap the token to create a new Hadoop RPC connection per call. S3g can validate OM token similar to the way DN validate OM block token. After validation succeeds, S3g can create a proxy user to connect to OM. If it is the same client, the proxy user can be reused.", "url": "https://github.com/apache/ozone/pull/1562#discussion_r525574063", "createdAt": "2020-11-17T22:42:38Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.\n+\n+```\n+public class UserGroupInformation {\n+  \n+  ...\n+  \n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == this) {\n+      return true;\n+    } else if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    } else {\n+      return subject == ((UserGroupInformation) o).subject;\n+    }\n+  }\n+}\n+```\n+\n+ But the UGI initialization of Ozone always creates a new `Subject` instance for each request (even if the subject name is the same). In `OzoneClientProducer`:\n+\n+```\n+  UserGroupInformation remoteUser =\n+          UserGroupInformation.createRemoteUser(awsAccessId); // <-- new Subject is created\n+      \n+      if (OzoneSecurityUtil.isSecurityEnabled(config)) {\n+        try {\n+          OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();\n+          //setup identifier\n+          \n+          Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),\n+              identifier.getSignature().getBytes(UTF_8),\n+              identifier.getKind(),\n+              omService);\n+          remoteUser.addToken(token);\n+          ....\n+```\n+\n+**As a result Hadoop RPC caching doesn't apply to Ozone S3g**. It's a good news because it's secure, but bad news as the performance is bad.\n+\n+# Proposed change\n+\n+We need an RPC mechanism between the Ozone S3g service and Ozone Manager service which can support per-request authentication and accepts\n+\n+The  Ozone Manager client already has a pluggable transport interface: `OmTransport` is a simple interface which can deliver `OMRequest` messages:\n+\n+```\n+public interface OmTransport {\n+\n+  /**\n+   * The main method to send out the request on the defined transport.\n+   */\n+  OMResponse submitRequest(OMRequest payload) throws IOException;\n+  ...\n+```\n+\n+ The proposal is to create a new **additional** transport, based on GRPC, which can do the per-request authentication. **Existing Hadoop clients will use the well-known Hadoop RPC client**, but S3g can start to use this specific transport to achieve better performance.\n+\n+As this is nothing more, just a transport: exactly the same messages (`OmRequest`) will be used, it's not a new RPC interface.\n+\n+Only one modification is required in the RPC interface: a new per-request`token` field should be introduced in `OMRequest` which is optional.\n+\n+A new GRPC service should be started in Ozone Manager, which receives `OMRequest` and for each request, the Hadoop `UserGroupInformation` is set based on the new token field (after authentication).\n+\n+`OzoneToken` identifier can be simplified (after deprecation period) with removing the S3 specific part, as it won't be required any more.\n+\n+With this approach the `OzoneClient` instances can be cached on S3g side (with persistent GRPC connections) as the authentication information is not part of the OzoneClient any more (added by the `OmTransport` implementation per request (in case of GRPC) or per connection (in case of HadoopRPC)).\n+\n+# Possible alternatives\n+\n+* It's possible to use pure Hadoop RPC client instead of Ozone Client which would make the client connection slightly cheaper (service discovery call is not required) but it's still require to create new connections for each requests (and downloading data without OzoneClient may have own challenges).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 201}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODUzMjU2NQ==", "bodyText": "Thanks the comments @xiaoyuyao\nWe had an offline discussion and I try to summarize what we discussed.\n\n.... S3g can validate OM token similar to the way DN validate OM block token\n\n\n\nWe couldn't move the authentication from OM to S3g as it's based on asymmetric encryption (requests are signed with a private key and OM re-produce the signature with the stored secret). Private access key shouldn't be moved out from OM. Therefore, S3g couldn't do authentication.\n\n\nPROXY_USER itself is per-connection (AFAIK), it doesn't fully solve the problem. We can do per-user Hadoop RPC connection caching, but despite the complexity it's not a full solution in an environment where we have thousands of users.\n\n\nAlso, the per-user Hadoop RPC connection caching on s3g side has some difficulties. The current caching logic is hard coded in static fields. To cache connection per user, s3g trust the user information, which is not possible. A request with a signature which is in valid format, but created with fake access key, couldn't re-use the cached and authenticated connection of the user.", "url": "https://github.com/apache/ozone/pull/1562#discussion_r528532565", "createdAt": "2020-11-23T08:26:52Z", "author": {"login": "elek"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM\n+date: 2020-11-09\n+jira: HDDS-4440\n+status: accepted\n+author: M\u00e1rton Elek\n+---\n+<!--\n+  Licensed under the Apache License, Version 2.0 (the \"License\");\n+  you may not use this file except in compliance with the License.\n+  You may obtain a copy of the License at\n+\n+   http://www.apache.org/licenses/LICENSE-2.0\n+\n+  Unless required by applicable law or agreed to in writing, software\n+  distributed under the License is distributed on an \"AS IS\" BASIS,\n+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+  See the License for the specific language governing permissions and\n+  limitations under the License. See accompanying LICENSE file.\n+-->\n+\n+# Overview\n+\n+* Hadoop RPC authenticate the calls at the beginning of the connections. All the subsequent messages on the same call will use existing, initialized authentication.\n+* S3 gateway sends the authentication as Hadoop RPC delegation token for **each requests**.\n+* To authenticate each of the S3 REST requests Ozone creates a new `OzoneClient` for eac HTTP requests, which introduces problems with performance and error handling.\n+* This proposal suggests to create a new transport (**in addition** to the existing Hadoop RPC) for the OMClientProtocol where the requests can be authenticated per-request.\n+\n+# Authentication with S3 gateway\n+\n+AWS S3 request authentication based on [signing the REST messages](https://docs.aws.amazon.com/AmazonS3/latest/dev/RESTAuthentication.html). Each of the HTTP requests must include and authentication header which contains the used the *access key id* and a signatures created with the help of the *secret key*.\n+\n+```\n+Authorization: AWS AWSAccessKeyId:Signature\n+```\n+\n+Ozone S3g is a REST gateway for Ozone which receives AWS compatible HTTP calls and forwards the requests to the Ozone Manager and Datanode services. Ozone S3g is **stateless**, it couldn't check any authentication information which are stored on the Ozone Manager side. It can check only the format of the signature.\n+\n+For the authentication S3g parses the HTTP header and sends all the relevant (and required) information to Ozone Manager which can check the signature with the help of stored *secret key*.\n+\n+This is implemented with the help of the delegation token mechanism of Hadoop RPC. Hadoop RPC supports Kerberos and token based authentication where tokens can be customized. The Ozone specific implementation `OzoneTokenIdentifier` contains a `type` field which can `DELEGATION_TOKEN` or `S3AUTHINFO`. The later one is used to authenticate the request based on S3 REST header (signature + required information).\n+\n+Both token and Kerberos based authentication are checked by Hadoop RPC during the connection initialization phase using the SASL standard. SASL defines the initial handshake of the creation where server can check the authentication information with a challenge-response mechanism.\n+\n+As a result Ozone S3g requires to create a new Hadoop RPC client for each of the HTTP requests as each requests may have different AWS authentication   information / signature. Ozone S3g creates a new `OzoneClient` for each of the requests which includes the creation of Hadoop RPC client.\n+\n+There are two problems with this approach:\n+\n+1.  **performance**: Creating a new `OzoneClient` requires to create new connection, to perform the SASL handshake and to send the initial discovery call to the OzoneManager to get the list of available services. It makes S3 performance very slow.\n+2. **error handling:** Creating new `OzoneClient` for each requests makes the propagation of error code harder with CDI.\n+\n+[CDI](http://cdi-spec.org/) is the specification of *Contexts and  Dependency Injection* for Java. Can be used for both JavaEE and JavaSE and it's integrated with most web frameworks. Ozone S3g uses this specification to inject different services to to REST handlers using `@Inject` annotation.\n+\n+`OzoneClient` is created by the `OzoneClientProduced`:\n+\n+```\n+@RequestScoped\n+public class OzoneClientProducer {\n+\n+  private OzoneClient client;\n+\n+  @Inject\n+  private SignatureProcessor signatureParser;\n+\n+  @Inject\n+  private OzoneConfiguration ozoneConfiguration;\n+\n+  @Inject\n+  private Text omService;\n+\n+  @Inject\n+  private String omServiceID;\n+\n+\n+  @Produces\n+  public OzoneClient createClient() throws OS3Exception, IOException {\n+    client = getClient(ozoneConfiguration);\n+    return client;\n+  }\n+...\n+}\n+```\n+\n+As we can see here, the producer is *request* scoped (see the annotation on the class), which means that the `OzoneClient` bean will be created for each request. If the client couldn't be created a specific exception will be thrown by the CDI framework (!) as one bean couldn't be injected with CDI. This error is different from the regular business exceptions therefore the normal exception handler (`OS3ExceptionMapper` implements `javax.ws.rs.ext.ExceptionMapper`) -- which can transform exceptions to HTTP error code -- doesn't apply. It can cause strange 500 error instead of some authentication error.\n+\n+## Caching\n+\n+Hadoop RPC has a very specific caching layer which is **not used** by Ozone S3g. This section describe the caching of the Hadoop RPC, but safe to skip (It explain how is the caching ignored).\n+\n+As creating new Hadoop RPC connection is an expensive operation Hadoop RPC has an internal caching mechanism to cache client and connections (!). This caching is hard-coded and based on static fields (couldn't be adjusted easily).\n+\n+Hadoop RPC client is usually created by `RPC.getProcolProxy`. For example:\n+\n+```\n+HelloWorldServicePB proxy = RPC.getProtocolProxy(\n+            HelloWorldServicePB.class,\n+            scmVersion,\n+            new InetSocketAddress(1234),\n+            UserGroupInformation.getCurrentUser(),\n+            configuration,\n+            new StandardSocketFactory(),\n+            Client.getRpcTimeout(configuration),\n+            retryPolicy).getProxy();\n+```\n+\n+This code fragment creates a new client which can be used from the code, and it uses multiple caches for client creation.\n+\n+1. Protocol engines are cached by `RPC.PROTOCOL_ENGINES` static field, but it's safe to assume that the `ProtobufRpcEngine` is used for most of the current applications.\n+\n+2. `ProtobufRpcEngine` has a static `ClientCache` field which caches the client instances with the `socketFactory` and `protocol` as the key.\n+\n+3. Finally the `Client.getConnection` method uses a cache to cache the connections:\n+\n+   ```\n+   connection = connections.computeIfAbsent(remoteId,\n+       id -> new Connection(id, serviceClass, removeMethod));\n+   ```\n+\n+   The key for the cache is the `remoteId` which includes all the configuration, connection parameters (like destination host) and `UserGroupInformation` (UGI).\n+\n+The caching of the connections can cause very interesting cases. As an example, let's assume that delegation token is invalidated with an RPC call. The workflow can be something like this:\n+\n+1. create protocol proxy (with token authentication)\n+2. invalidate token (rpc call)\n+3. close protocol proxy (connection may not be closed. depends from the cache)\n+4. create a new protocol proxy\n+5. If connection is cached (same UGI) services can be used even if the token is invalidated earlier (as the token is checked during the initialization of the tokens).\n+\n+Fortunately this behavior doesn't cause any problem in case of Ozone and S3g. UGI (which is part of the cache key of the connection cache) equals if (and only if) the underlying `Subject` is the same.\n+\n+```\n+public class UserGroupInformation {\n+  \n+  ...\n+  \n+  @Override\n+  public boolean equals(Object o) {\n+    if (o == this) {\n+      return true;\n+    } else if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    } else {\n+      return subject == ((UserGroupInformation) o).subject;\n+    }\n+  }\n+}\n+```\n+\n+ But the UGI initialization of Ozone always creates a new `Subject` instance for each request (even if the subject name is the same). In `OzoneClientProducer`:\n+\n+```\n+  UserGroupInformation remoteUser =\n+          UserGroupInformation.createRemoteUser(awsAccessId); // <-- new Subject is created\n+      \n+      if (OzoneSecurityUtil.isSecurityEnabled(config)) {\n+        try {\n+          OzoneTokenIdentifier identifier = new OzoneTokenIdentifier();\n+          //setup identifier\n+          \n+          Token<OzoneTokenIdentifier> token = new Token(identifier.getBytes(),\n+              identifier.getSignature().getBytes(UTF_8),\n+              identifier.getKind(),\n+              omService);\n+          remoteUser.addToken(token);\n+          ....\n+```\n+\n+**As a result Hadoop RPC caching doesn't apply to Ozone S3g**. It's a good news because it's secure, but bad news as the performance is bad.\n+\n+# Proposed change\n+\n+We need an RPC mechanism between the Ozone S3g service and Ozone Manager service which can support per-request authentication and accepts\n+\n+The  Ozone Manager client already has a pluggable transport interface: `OmTransport` is a simple interface which can deliver `OMRequest` messages:\n+\n+```\n+public interface OmTransport {\n+\n+  /**\n+   * The main method to send out the request on the defined transport.\n+   */\n+  OMResponse submitRequest(OMRequest payload) throws IOException;\n+  ...\n+```\n+\n+ The proposal is to create a new **additional** transport, based on GRPC, which can do the per-request authentication. **Existing Hadoop clients will use the well-known Hadoop RPC client**, but S3g can start to use this specific transport to achieve better performance.\n+\n+As this is nothing more, just a transport: exactly the same messages (`OmRequest`) will be used, it's not a new RPC interface.\n+\n+Only one modification is required in the RPC interface: a new per-request`token` field should be introduced in `OMRequest` which is optional.\n+\n+A new GRPC service should be started in Ozone Manager, which receives `OMRequest` and for each request, the Hadoop `UserGroupInformation` is set based on the new token field (after authentication).\n+\n+`OzoneToken` identifier can be simplified (after deprecation period) with removing the S3 specific part, as it won't be required any more.\n+\n+With this approach the `OzoneClient` instances can be cached on S3g side (with persistent GRPC connections) as the authentication information is not part of the OzoneClient any more (added by the `OmTransport` implementation per request (in case of GRPC) or per connection (in case of HadoopRPC)).\n+\n+# Possible alternatives\n+\n+* It's possible to use pure Hadoop RPC client instead of Ozone Client which would make the client connection slightly cheaper (service discovery call is not required) but it's still require to create new connections for each requests (and downloading data without OzoneClient may have own challenges).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU3NDA2Mw=="}, "originalCommit": {"oid": "ed2429b8ef341bda1f7287effda37d58b2910100"}, "originalPosition": 201}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzY5MDkwMzQxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/docs/content/design/s3-performance.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxODozMzo1OVrOIt2zvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0wMVQxOToxODoxMVrOIt4lCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDk1NDgxMg==", "bodyText": "Can we add the word proposal here in the title/summary?", "url": "https://github.com/apache/ozone/pull/1562#discussion_r584954812", "createdAt": "2021-03-01T18:33:59Z", "author": {"login": "arp7"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b0bf60dbf01add562556574358f0cfdaa5814065"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDk4MzgxOA==", "bodyText": "Sure. Added in 9c6ce91.", "url": "https://github.com/apache/ozone/pull/1562#discussion_r584983818", "createdAt": "2021-03-01T19:18:11Z", "author": {"login": "elek"}, "path": "hadoop-hdds/docs/content/design/s3-performance.md", "diffHunk": "@@ -0,0 +1,205 @@\n+---\n+title: Persistent OM connection for S3 gateway\n+summary: Use per-request authentication and persistent connections between S3g and OM", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU4NDk1NDgxMg=="}, "originalCommit": {"oid": "b0bf60dbf01add562556574358f0cfdaa5814065"}, "originalPosition": 3}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4707, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}