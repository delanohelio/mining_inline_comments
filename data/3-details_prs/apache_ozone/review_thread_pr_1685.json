{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM2MjI1MDI5", "number": 1685, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQwODoyMDoyOVrOFHs-TQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xOVQxMTo0OTo0OFrOFn_8zA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNjIxMTk3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockInputStream.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQwODoyMDoyOVrOIJP1XQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQwODoyMDoyOVrOIJP1XQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjU2NzUxNw==", "bodyText": "LOG.info -> LOG.warn?", "url": "https://github.com/apache/ozone/pull/1685#discussion_r546567517", "createdAt": "2020-12-21T08:20:29Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockInputStream.java", "diffHunk": "@@ -134,7 +135,12 @@ public synchronized void initialize() throws IOException {\n     try {\n       chunks = getChunkInfos();\n     } catch (ContainerNotFoundException ioEx) {\n-      refreshPipeline(ioEx);\n+      LOG.info(\"Unable to read information for block {} from pipeline {}: {}.\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7819426ccaf6dabe3d90ddb6f37733fb4989e30f"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzNjI3Nzk2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQwODo0Mzo0MFrOIJQbuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wOFQwMzoyMzozNlrOIQEYlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjU3NzMzNg==", "bodyText": "This will do an additional buffer copy here. Let's see if we can explore anything here to avoid buffer copy here:\nhttps://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/UnsafeByteOperations", "url": "https://github.com/apache/ozone/pull/1685#discussion_r546577336", "createdAt": "2020-12-21T08:43:40Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -301,36 +348,38 @@ private synchronized void readChunkFromContainer(int len) throws IOException {\n       startByteIndex = chunkPosition;\n     } else {\n       // Start reading the chunk from the last chunkPosition onwards.\n-      startByteIndex = bufferOffset + bufferLength;\n+      startByteIndex = bufferOffsetWrtChunkData + bufferLength;\n     }\n \n-    // bufferOffset and bufferLength are updated below, but if read fails\n+    // bufferOffsetWrtChunkData and bufferLength are updated after the data\n+    // is read from Container and put into the buffers, but if read fails\n     // and is retried, we need the previous position.  Position is reset after\n     // successful read in adjustBufferPosition()\n     storePosition();\n \n+    long adjustedBuffersOffset, adjustedBuffersLen;\n     if (verifyChecksum) {\n-      // Update the bufferOffset and bufferLength as per the checksum\n-      // boundary requirement.\n-      computeChecksumBoundaries(startByteIndex, len);\n+      // Adjust the chunk offset and length to include required checksum\n+      // boundaries\n+      Pair<Long, Long> adjustedOffsetAndLength =\n+          computeChecksumBoundaries(startByteIndex, len);\n+      adjustedBuffersOffset = adjustedOffsetAndLength.getLeft();\n+      adjustedBuffersLen = adjustedOffsetAndLength.getRight();\n     } else {\n       // Read from the startByteIndex\n-      bufferOffset = startByteIndex;\n-      bufferLength = len;\n+      adjustedBuffersOffset = startByteIndex;\n+      adjustedBuffersLen = len;\n     }\n \n     // Adjust the chunkInfo so that only the required bytes are read from\n     // the chunk.\n     final ChunkInfo adjustedChunkInfo = ChunkInfo.newBuilder(chunkInfo)\n-        .setOffset(bufferOffset + chunkInfo.getOffset())\n-        .setLen(bufferLength)\n+        .setOffset(adjustedBuffersOffset + chunkInfo.getOffset())\n+        .setLen(adjustedBuffersLen)\n         .build();\n \n-    ByteString byteString = readChunk(adjustedChunkInfo);\n-\n-    buffers = byteString.asReadOnlyByteBufferList();\n-    bufferIndex = 0;\n-    allocated = true;\n+    byte[] chunkData = readChunk(adjustedChunkInfo).toByteArray();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7819426ccaf6dabe3d90ddb6f37733fb4989e30f"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODI1NzE0Mg==", "bodyText": "We get ByteString from the response. But the returned ByteString does not have the underlying buffer boundary information. Hence ByteString#asReadOnlyByteBufferList() will return only one ByteBuffer with all the data irrespective of the backing arrays used to construct the ByteString.", "url": "https://github.com/apache/ozone/pull/1685#discussion_r548257142", "createdAt": "2020-12-23T21:32:41Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -301,36 +348,38 @@ private synchronized void readChunkFromContainer(int len) throws IOException {\n       startByteIndex = chunkPosition;\n     } else {\n       // Start reading the chunk from the last chunkPosition onwards.\n-      startByteIndex = bufferOffset + bufferLength;\n+      startByteIndex = bufferOffsetWrtChunkData + bufferLength;\n     }\n \n-    // bufferOffset and bufferLength are updated below, but if read fails\n+    // bufferOffsetWrtChunkData and bufferLength are updated after the data\n+    // is read from Container and put into the buffers, but if read fails\n     // and is retried, we need the previous position.  Position is reset after\n     // successful read in adjustBufferPosition()\n     storePosition();\n \n+    long adjustedBuffersOffset, adjustedBuffersLen;\n     if (verifyChecksum) {\n-      // Update the bufferOffset and bufferLength as per the checksum\n-      // boundary requirement.\n-      computeChecksumBoundaries(startByteIndex, len);\n+      // Adjust the chunk offset and length to include required checksum\n+      // boundaries\n+      Pair<Long, Long> adjustedOffsetAndLength =\n+          computeChecksumBoundaries(startByteIndex, len);\n+      adjustedBuffersOffset = adjustedOffsetAndLength.getLeft();\n+      adjustedBuffersLen = adjustedOffsetAndLength.getRight();\n     } else {\n       // Read from the startByteIndex\n-      bufferOffset = startByteIndex;\n-      bufferLength = len;\n+      adjustedBuffersOffset = startByteIndex;\n+      adjustedBuffersLen = len;\n     }\n \n     // Adjust the chunkInfo so that only the required bytes are read from\n     // the chunk.\n     final ChunkInfo adjustedChunkInfo = ChunkInfo.newBuilder(chunkInfo)\n-        .setOffset(bufferOffset + chunkInfo.getOffset())\n-        .setLen(bufferLength)\n+        .setOffset(adjustedBuffersOffset + chunkInfo.getOffset())\n+        .setLen(adjustedBuffersLen)\n         .build();\n \n-    ByteString byteString = readChunk(adjustedChunkInfo);\n-\n-    buffers = byteString.asReadOnlyByteBufferList();\n-    bufferIndex = 0;\n-    allocated = true;\n+    byte[] chunkData = readChunk(adjustedChunkInfo).toByteArray();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjU3NzMzNg=="}, "originalCommit": {"oid": "7819426ccaf6dabe3d90ddb6f37733fb4989e30f"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM3NzM0NQ==", "bodyText": "What if we read in small buffers on the server side itself and send it across as a list of bytestrings to the client?\nCopying a big buffer on the client read path will be slowing down the read.  Probably we should do some benchmarking to understand the effects of all these.", "url": "https://github.com/apache/ozone/pull/1685#discussion_r548377345", "createdAt": "2020-12-24T04:30:12Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -301,36 +348,38 @@ private synchronized void readChunkFromContainer(int len) throws IOException {\n       startByteIndex = chunkPosition;\n     } else {\n       // Start reading the chunk from the last chunkPosition onwards.\n-      startByteIndex = bufferOffset + bufferLength;\n+      startByteIndex = bufferOffsetWrtChunkData + bufferLength;\n     }\n \n-    // bufferOffset and bufferLength are updated below, but if read fails\n+    // bufferOffsetWrtChunkData and bufferLength are updated after the data\n+    // is read from Container and put into the buffers, but if read fails\n     // and is retried, we need the previous position.  Position is reset after\n     // successful read in adjustBufferPosition()\n     storePosition();\n \n+    long adjustedBuffersOffset, adjustedBuffersLen;\n     if (verifyChecksum) {\n-      // Update the bufferOffset and bufferLength as per the checksum\n-      // boundary requirement.\n-      computeChecksumBoundaries(startByteIndex, len);\n+      // Adjust the chunk offset and length to include required checksum\n+      // boundaries\n+      Pair<Long, Long> adjustedOffsetAndLength =\n+          computeChecksumBoundaries(startByteIndex, len);\n+      adjustedBuffersOffset = adjustedOffsetAndLength.getLeft();\n+      adjustedBuffersLen = adjustedOffsetAndLength.getRight();\n     } else {\n       // Read from the startByteIndex\n-      bufferOffset = startByteIndex;\n-      bufferLength = len;\n+      adjustedBuffersOffset = startByteIndex;\n+      adjustedBuffersLen = len;\n     }\n \n     // Adjust the chunkInfo so that only the required bytes are read from\n     // the chunk.\n     final ChunkInfo adjustedChunkInfo = ChunkInfo.newBuilder(chunkInfo)\n-        .setOffset(bufferOffset + chunkInfo.getOffset())\n-        .setLen(bufferLength)\n+        .setOffset(adjustedBuffersOffset + chunkInfo.getOffset())\n+        .setLen(adjustedBuffersLen)\n         .build();\n \n-    ByteString byteString = readChunk(adjustedChunkInfo);\n-\n-    buffers = byteString.asReadOnlyByteBufferList();\n-    bufferIndex = 0;\n-    allocated = true;\n+    byte[] chunkData = readChunk(adjustedChunkInfo).toByteArray();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjU3NzMzNg=="}, "originalCommit": {"oid": "7819426ccaf6dabe3d90ddb6f37733fb4989e30f"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0ODM3NzczMA==", "bodyText": "In case, this turns out to be unavoidable, we can also think about doing bytebuffer.compact()  which also does an intrinsic buffer copy to release the buffers but the logic would be more simpler.", "url": "https://github.com/apache/ozone/pull/1685#discussion_r548377730", "createdAt": "2020-12-24T04:32:51Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -301,36 +348,38 @@ private synchronized void readChunkFromContainer(int len) throws IOException {\n       startByteIndex = chunkPosition;\n     } else {\n       // Start reading the chunk from the last chunkPosition onwards.\n-      startByteIndex = bufferOffset + bufferLength;\n+      startByteIndex = bufferOffsetWrtChunkData + bufferLength;\n     }\n \n-    // bufferOffset and bufferLength are updated below, but if read fails\n+    // bufferOffsetWrtChunkData and bufferLength are updated after the data\n+    // is read from Container and put into the buffers, but if read fails\n     // and is retried, we need the previous position.  Position is reset after\n     // successful read in adjustBufferPosition()\n     storePosition();\n \n+    long adjustedBuffersOffset, adjustedBuffersLen;\n     if (verifyChecksum) {\n-      // Update the bufferOffset and bufferLength as per the checksum\n-      // boundary requirement.\n-      computeChecksumBoundaries(startByteIndex, len);\n+      // Adjust the chunk offset and length to include required checksum\n+      // boundaries\n+      Pair<Long, Long> adjustedOffsetAndLength =\n+          computeChecksumBoundaries(startByteIndex, len);\n+      adjustedBuffersOffset = adjustedOffsetAndLength.getLeft();\n+      adjustedBuffersLen = adjustedOffsetAndLength.getRight();\n     } else {\n       // Read from the startByteIndex\n-      bufferOffset = startByteIndex;\n-      bufferLength = len;\n+      adjustedBuffersOffset = startByteIndex;\n+      adjustedBuffersLen = len;\n     }\n \n     // Adjust the chunkInfo so that only the required bytes are read from\n     // the chunk.\n     final ChunkInfo adjustedChunkInfo = ChunkInfo.newBuilder(chunkInfo)\n-        .setOffset(bufferOffset + chunkInfo.getOffset())\n-        .setLen(bufferLength)\n+        .setOffset(adjustedBuffersOffset + chunkInfo.getOffset())\n+        .setLen(adjustedBuffersLen)\n         .build();\n \n-    ByteString byteString = readChunk(adjustedChunkInfo);\n-\n-    buffers = byteString.asReadOnlyByteBufferList();\n-    bufferIndex = 0;\n-    allocated = true;\n+    byte[] chunkData = readChunk(adjustedChunkInfo).toByteArray();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjU3NzMzNg=="}, "originalCommit": {"oid": "7819426ccaf6dabe3d90ddb6f37733fb4989e30f"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzAyNTU5Mw==", "bodyText": "What if we read in small buffers on the server side itself and send it across as a list of bytestrings to the client?\n\nThis might work but would require a change in the DN-Client protocol. Would have to analyze the compatibility issues and how to address them.\nIn case, this turns out to be unavoidable, we can also think about doing bytebuffer.compact() which also does an intrinsic buffer copy to release the buffers but the logic would be more simpler\n\nI am not sure if there is much gain in doing this. The code changes in this PR were required because the logic was inaccurate. It was working because there was always only one ByteBuffer.", "url": "https://github.com/apache/ozone/pull/1685#discussion_r553025593", "createdAt": "2021-01-07T00:01:00Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -301,36 +348,38 @@ private synchronized void readChunkFromContainer(int len) throws IOException {\n       startByteIndex = chunkPosition;\n     } else {\n       // Start reading the chunk from the last chunkPosition onwards.\n-      startByteIndex = bufferOffset + bufferLength;\n+      startByteIndex = bufferOffsetWrtChunkData + bufferLength;\n     }\n \n-    // bufferOffset and bufferLength are updated below, but if read fails\n+    // bufferOffsetWrtChunkData and bufferLength are updated after the data\n+    // is read from Container and put into the buffers, but if read fails\n     // and is retried, we need the previous position.  Position is reset after\n     // successful read in adjustBufferPosition()\n     storePosition();\n \n+    long adjustedBuffersOffset, adjustedBuffersLen;\n     if (verifyChecksum) {\n-      // Update the bufferOffset and bufferLength as per the checksum\n-      // boundary requirement.\n-      computeChecksumBoundaries(startByteIndex, len);\n+      // Adjust the chunk offset and length to include required checksum\n+      // boundaries\n+      Pair<Long, Long> adjustedOffsetAndLength =\n+          computeChecksumBoundaries(startByteIndex, len);\n+      adjustedBuffersOffset = adjustedOffsetAndLength.getLeft();\n+      adjustedBuffersLen = adjustedOffsetAndLength.getRight();\n     } else {\n       // Read from the startByteIndex\n-      bufferOffset = startByteIndex;\n-      bufferLength = len;\n+      adjustedBuffersOffset = startByteIndex;\n+      adjustedBuffersLen = len;\n     }\n \n     // Adjust the chunkInfo so that only the required bytes are read from\n     // the chunk.\n     final ChunkInfo adjustedChunkInfo = ChunkInfo.newBuilder(chunkInfo)\n-        .setOffset(bufferOffset + chunkInfo.getOffset())\n-        .setLen(bufferLength)\n+        .setOffset(adjustedBuffersOffset + chunkInfo.getOffset())\n+        .setLen(adjustedBuffersLen)\n         .build();\n \n-    ByteString byteString = readChunk(adjustedChunkInfo);\n-\n-    buffers = byteString.asReadOnlyByteBufferList();\n-    bufferIndex = 0;\n-    allocated = true;\n+    byte[] chunkData = readChunk(adjustedChunkInfo).toByteArray();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjU3NzMzNg=="}, "originalCommit": {"oid": "7819426ccaf6dabe3d90ddb6f37733fb4989e30f"}, "originalPosition": 216}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcxOTk1Ng==", "bodyText": "The basic problem we are trying to solve here is to minimize the memory overhead in the client. In order to solve this, adding an extra buffer copy overhead(with the patch) does not seem to be a reasonable idea to me. Let's discuss it in some more detail on how to address this.", "url": "https://github.com/apache/ozone/pull/1685#discussion_r553719956", "createdAt": "2021-01-08T03:23:36Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -301,36 +348,38 @@ private synchronized void readChunkFromContainer(int len) throws IOException {\n       startByteIndex = chunkPosition;\n     } else {\n       // Start reading the chunk from the last chunkPosition onwards.\n-      startByteIndex = bufferOffset + bufferLength;\n+      startByteIndex = bufferOffsetWrtChunkData + bufferLength;\n     }\n \n-    // bufferOffset and bufferLength are updated below, but if read fails\n+    // bufferOffsetWrtChunkData and bufferLength are updated after the data\n+    // is read from Container and put into the buffers, but if read fails\n     // and is retried, we need the previous position.  Position is reset after\n     // successful read in adjustBufferPosition()\n     storePosition();\n \n+    long adjustedBuffersOffset, adjustedBuffersLen;\n     if (verifyChecksum) {\n-      // Update the bufferOffset and bufferLength as per the checksum\n-      // boundary requirement.\n-      computeChecksumBoundaries(startByteIndex, len);\n+      // Adjust the chunk offset and length to include required checksum\n+      // boundaries\n+      Pair<Long, Long> adjustedOffsetAndLength =\n+          computeChecksumBoundaries(startByteIndex, len);\n+      adjustedBuffersOffset = adjustedOffsetAndLength.getLeft();\n+      adjustedBuffersLen = adjustedOffsetAndLength.getRight();\n     } else {\n       // Read from the startByteIndex\n-      bufferOffset = startByteIndex;\n-      bufferLength = len;\n+      adjustedBuffersOffset = startByteIndex;\n+      adjustedBuffersLen = len;\n     }\n \n     // Adjust the chunkInfo so that only the required bytes are read from\n     // the chunk.\n     final ChunkInfo adjustedChunkInfo = ChunkInfo.newBuilder(chunkInfo)\n-        .setOffset(bufferOffset + chunkInfo.getOffset())\n-        .setLen(bufferLength)\n+        .setOffset(adjustedBuffersOffset + chunkInfo.getOffset())\n+        .setLen(adjustedBuffersLen)\n         .build();\n \n-    ByteString byteString = readChunk(adjustedChunkInfo);\n-\n-    buffers = byteString.asReadOnlyByteBufferList();\n-    bufferIndex = 0;\n-    allocated = true;\n+    byte[] chunkData = readChunk(adjustedChunkInfo).toByteArray();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjU3NzMzNg=="}, "originalCommit": {"oid": "7819426ccaf6dabe3d90ddb6f37733fb4989e30f"}, "originalPosition": 216}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzc1MTgwNDEzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xNVQwNTo1MDoyN1rOI2i2zQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xNVQwNTo1MDoyN1rOI2i2zQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDA2NTEwMQ==", "bodyText": "Let's -> \"Let's say\"", "url": "https://github.com/apache/ozone/pull/1685#discussion_r594065101", "createdAt": "2021-03-15T05:50:27Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -66,13 +76,21 @@\n \n   // Index of the buffers corresponding to the current position of the buffers\n   private int bufferIndex;\n+  // bufferOffsets[i] stores the index of the first data byte in buffer i\n+  // (buffers.get(i)) w.r.t first byte in the buffers.\n+  // Let's each buffer has a capacity of 40 bytes. The bufferOffset for", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca9a5318b2da6edbf76ee9cbeabfb7d2e6c1cb33"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzc1MTgwNTA0OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xNVQwNTo1MDo1OVrOI2i3WQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xNVQxOTo1NTowNlrOI3F-1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDA2NTI0MQ==", "bodyText": "Unintended change?", "url": "https://github.com/apache/ozone/pull/1685#discussion_r594065241", "createdAt": "2021-03-15T05:50:59Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -87,7 +105,8 @@\n   ChunkInputStream(ChunkInfo chunkInfo, BlockID blockId,\n       XceiverClientFactory xceiverClientFactory,\n       Supplier<Pipeline> pipelineSupplier,\n-      boolean verifyChecksum, Token<? extends TokenIdentifier> token) {\n+      boolean verifyChecksum,\n+      Token<? extends TokenIdentifier> token) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca9a5318b2da6edbf76ee9cbeabfb7d2e6c1cb33"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDY0MDU5OA==", "bodyText": "Yup. Reverted.", "url": "https://github.com/apache/ozone/pull/1685#discussion_r594640598", "createdAt": "2021-03-15T19:55:06Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -87,7 +105,8 @@\n   ChunkInputStream(ChunkInfo chunkInfo, BlockID blockId,\n       XceiverClientFactory xceiverClientFactory,\n       Supplier<Pipeline> pipelineSupplier,\n-      boolean verifyChecksum, Token<? extends TokenIdentifier> token) {\n+      boolean verifyChecksum,\n+      Token<? extends TokenIdentifier> token) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDA2NTI0MQ=="}, "originalCommit": {"oid": "ca9a5318b2da6edbf76ee9cbeabfb7d2e6c1cb33"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzc1MTgyNjQwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/OzoneClientConfig.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xNVQwNjowMToxNVrOI2jDig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xNVQxOTo1Nzo0MVrOI3GFtw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDA2ODM2Mg==", "bodyText": "Let's not change the default for now. We can change once we do some tests and analyze performance .", "url": "https://github.com/apache/ozone/pull/1685#discussion_r594068362", "createdAt": "2021-03-15T06:01:15Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/OzoneClientConfig.java", "diffHunk": "@@ -109,13 +109,13 @@\n   private String checksumType = ChecksumType.CRC32.name();\n \n   @Config(key = \"bytes.per.checksum\",\n-      defaultValue = \"1MB\",\n+      defaultValue = \"256KB\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca9a5318b2da6edbf76ee9cbeabfb7d2e6c1cb33"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDY0MjM1OQ==", "bodyText": "Sure. Reverted back to 1MB.", "url": "https://github.com/apache/ozone/pull/1685#discussion_r594642359", "createdAt": "2021-03-15T19:57:41Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/OzoneClientConfig.java", "diffHunk": "@@ -109,13 +109,13 @@\n   private String checksumType = ChecksumType.CRC32.name();\n \n   @Config(key = \"bytes.per.checksum\",\n-      defaultValue = \"1MB\",\n+      defaultValue = \"256KB\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDA2ODM2Mg=="}, "originalCommit": {"oid": "ca9a5318b2da6edbf76ee9cbeabfb7d2e6c1cb33"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzc3NDgzNjc3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xOVQxMTo0MToyNFrOI57X_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xOVQxMTo0MToyNFrOI57X_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzYxMjU0Mw==", "bodyText": "If read from first location fails and we have to fall back to the temp chunk file, this would cause exception.", "url": "https://github.com/apache/ozone/pull/1685#discussion_r597612543", "createdAt": "2021-03-19T11:41:24Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "diffHunk": "@@ -255,16 +287,16 @@ public ChunkBuffer readChunk(Container container, BlockID blockID,\n         if (file.exists()) {\n           long offset = info.getOffset() - chunkFileOffset;\n           Preconditions.checkState(offset >= 0);\n-          ChunkUtils.readData(file, data, offset, len, volumeIOStats);\n-          return ChunkBuffer.wrap(data);\n+          ChunkUtils.readData(file, dataBuffers, offset, len, volumeIOStats);\n+          return ChunkBuffer.wrap(Lists.newArrayList(dataBuffers));\n         }\n       } catch (StorageContainerException ex) {\n         //UNABLE TO FIND chunk is not a problem as we will try with the\n         //next possible location\n         if (ex.getResult() != UNABLE_TO_FIND_CHUNK) {\n           throw ex;\n         }\n-        data.clear();\n+        dataBuffers = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6beb15d11a0f3733e5f3ed76403460f01f94e1ea"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzc3NDg0MjIxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xOVQxMTo0MzoxNFrOI57bfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xOVQxMTo0MzoxNFrOI57bfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzYxMzQzOQ==", "bodyText": "This block seems to be duplicated from FilePerBlock....  Can it be extracted?", "url": "https://github.com/apache/ozone/pull/1685#discussion_r597613439", "createdAt": "2021-03-19T11:43:14Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "diffHunk": "@@ -222,7 +228,33 @@ public ChunkBuffer readChunk(Container container, BlockID blockID,\n     }\n \n     long len = info.getLen();\n-    ByteBuffer data = ByteBuffer.allocate((int) len);\n+\n+    long bufferCapacity = 0;\n+    if (info.isReadDataIntoSingleBuffer()) {\n+      // Older client - read all chunk data into one single buffer.\n+      bufferCapacity = len;\n+    } else {\n+      // Set buffer capacity to checksum boundary size so that each buffer\n+      // corresponds to one checksum. If checksum is NONE, then set buffer\n+      // capacity to default (OZONE_CHUNK_READ_BUFFER_DEFAULT_SIZE_KEY = 64KB).\n+      ChecksumData checksumData = info.getChecksumData();\n+\n+      if (checksumData != null) {\n+        if (checksumData.getChecksumType() ==\n+            ContainerProtos.ChecksumType.NONE) {\n+          bufferCapacity = defaultReadBufferCapacity;\n+        } else {\n+          bufferCapacity = checksumData.getBytesPerChecksum();\n+        }\n+      }\n+    }\n+    // If the buffer capacity is 0, set all the data into one ByteBuffer\n+    if (bufferCapacity == 0) {\n+      bufferCapacity = len;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6beb15d11a0f3733e5f3ed76403460f01f94e1ea"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzc3NDg2NTQwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xOVQxMTo0OTo0OFrOI57pBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xOVQxMTo0OTo0OFrOI57pBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzYxNjkwMA==", "bodyText": "I think we should avoid streams on read/write path.  Earlier these were found to cause CPU usage hotspots.  See eg. HDDS-3702.\n(Also in few other instances below.)", "url": "https://github.com/apache/ozone/pull/1685#discussion_r597616900", "createdAt": "2021-03-19T11:49:48Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -364,7 +404,18 @@ protected ByteString readChunk(ChunkInfo readChunkInfo) throws IOException {\n       throw new IOException(\"Unexpected OzoneException: \" + e.toString(), e);\n     }\n \n-    return readChunkResponse.getData();\n+    if (readChunkResponse.hasData()) {\n+      return readChunkResponse.getData().asReadOnlyByteBufferList();\n+    } else if (readChunkResponse.hasDataBuffers()) {\n+      List<ByteString> buffersList = readChunkResponse.getDataBuffers()\n+          .getBuffersList();\n+      return buffersList.stream()\n+          .map(ByteString::asReadOnlyByteBuffer)\n+          .collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6beb15d11a0f3733e5f3ed76403460f01f94e1ea"}, "originalPosition": 206}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4601, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}