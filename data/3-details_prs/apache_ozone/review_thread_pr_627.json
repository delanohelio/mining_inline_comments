{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzgyNzY0MzEz", "number": 627, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QwNzowMjo1N1rODkiM1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwNTozNTo0NFrODk6rgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5NjM1NjY4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lock/LockManager.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QwNzowMjo1N1rOFw6PeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwNToyMDo0NVrOFxgTjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjgzMDIwMA==", "bodyText": "Nit: Is this necessary?  I don't see any checked exception being thrown in the try block, and unchecked exceptions don't need to be wrapped.", "url": "https://github.com/apache/ozone/pull/627#discussion_r386830200", "createdAt": "2020-03-03T07:02:57Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lock/LockManager.java", "diffHunk": "@@ -186,21 +186,42 @@ private ActiveLock getLockForLocking(final R resource) {\n      * be removed from the activeLocks map and returned to\n      * the object pool.\n      */\n-    return activeLocks.compute(resource, (k, v) -> {\n+    ActiveLock computedLock = activeLocks.compute(resource, (k, v) -> {\n       final ActiveLock lock;\n+      if (v == null) {\n+        return null;\n+      } else {\n+        lock = v;\n+      }\n+      lock.incrementActiveCount();\n+      return lock;\n+    });\n+\n+\n+    if (computedLock == null) {\n       try {\n-        if (v == null) {\n-          lock = lockPool.borrowObject();\n-        } else {\n-          lock = v;\n-        }\n-        lock.incrementActiveCount();\n+        final ActiveLock lock2 = lockPool.borrowObject();\n+        computedLock = activeLocks.compute(resource, (k, v) -> {\n+          final ActiveLock lock;\n+          try {\n+            if (v == null) {\n+              lock = lock2;\n+            } else {\n+              lockPool.returnObject(lock2);\n+              lock = v;\n+            }\n+            lock.incrementActiveCount();\n+          } catch (Exception ex) {\n+            throw new RuntimeException(ex);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "290b75f9ab103210a566ceb7c86245d3ac7f69cf"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQ1MzgzOQ==", "bodyText": "Based on Xiaoyu's comment reverted this change.", "url": "https://github.com/apache/ozone/pull/627#discussion_r387453839", "createdAt": "2020-03-04T05:20:45Z", "author": {"login": "bharatviswa504"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lock/LockManager.java", "diffHunk": "@@ -186,21 +186,42 @@ private ActiveLock getLockForLocking(final R resource) {\n      * be removed from the activeLocks map and returned to\n      * the object pool.\n      */\n-    return activeLocks.compute(resource, (k, v) -> {\n+    ActiveLock computedLock = activeLocks.compute(resource, (k, v) -> {\n       final ActiveLock lock;\n+      if (v == null) {\n+        return null;\n+      } else {\n+        lock = v;\n+      }\n+      lock.incrementActiveCount();\n+      return lock;\n+    });\n+\n+\n+    if (computedLock == null) {\n       try {\n-        if (v == null) {\n-          lock = lockPool.borrowObject();\n-        } else {\n-          lock = v;\n-        }\n-        lock.incrementActiveCount();\n+        final ActiveLock lock2 = lockPool.borrowObject();\n+        computedLock = activeLocks.compute(resource, (k, v) -> {\n+          final ActiveLock lock;\n+          try {\n+            if (v == null) {\n+              lock = lock2;\n+            } else {\n+              lockPool.returnObject(lock2);\n+              lock = v;\n+            }\n+            lock.incrementActiveCount();\n+          } catch (Exception ex) {\n+            throw new RuntimeException(ex);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NjgzMDIwMA=="}, "originalCommit": {"oid": "290b75f9ab103210a566ceb7c86245d3ac7f69cf"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTU0NzI1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/common/src/test/java/org/apache/hadoop/ozone/lock/TestLockManager.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjoyNDo1N1rOFxY2vw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwNTo1Njo1OVrOFxgz9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMzMTc3NQ==", "bodyText": "Thanks @bharatviswa504 for the patch. The PR LGTM. I understand we are trying to avoid wait forever while holding the lock of the pool map. However, it does not completely solve the lock resource exhausting problem. I have a cluster with this patch but create key still can live lock by the limited number of locks (100).\nMy question is why we are restricting # of locks here via hdds.lock.max.concurrency.  In my opinion, if we change the default from 100 to -1, this deadlock will not happen. This also match the default max pool size (-1) of GenericObjectPool, especially lock is a cheaper resource compared with threads or tcp connections. To allow handling 4K+ request per second, we should consider removing this key that can easily cause deadlock, especially when handler counter is several times larger than the lock pool size.", "url": "https://github.com/apache/ozone/pull/627#discussion_r387331775", "createdAt": "2020-03-03T22:24:57Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-hdds/common/src/test/java/org/apache/hadoop/ozone/lock/TestLockManager.java", "diffHunk": "@@ -170,4 +175,32 @@ public void testMultiReadWriteLockWithSameResource() throws Exception {\n     Assert.assertTrue(gotLock.get());\n   }\n \n+  @Test\n+  public void testConcurrentWriteLockWithDifferentResource() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    final int count = 100;\n+    conf.setInt(HDDS_LOCK_MAX_CONCURRENCY, count/4);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "290b75f9ab103210a566ceb7c86245d3ac7f69cf"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQ1MTA4Mw==", "bodyText": "Yes, if we set max pool size to -1, the max total objects created by GenericcObjectPool used in Integer.MAX. I agree we can change to -1, and we shall not see this issue. And we can remove this property, and set it to -1 so that we shall not see this kind of issue when someone mistakenly sets this to a low value and operates on the cluster with very large number of volumes/buckets in parallel. With this way, we shall not see lock contention during acquiring lock.\nBut few questions, right now we take a lock on resource name, so if there is a lock Object which is already in activeLocks map, we reuse that object if it does not we borrowObject from lockPool. So,  when we operate on more than 100 buckets simultaneously we should see this problem when creating keys or when more than 100 volumes operations are going in parallel we can see some contention in acquiring locks. So, it is not exactly related to handler count in OM. Just trying to understand is the issue is observed on the test cluster, where some test is working in parallel across more than 100 volumes/100 buckets and a lock contention is observed?", "url": "https://github.com/apache/ozone/pull/627#discussion_r387451083", "createdAt": "2020-03-04T05:07:38Z", "author": {"login": "bharatviswa504"}, "path": "hadoop-hdds/common/src/test/java/org/apache/hadoop/ozone/lock/TestLockManager.java", "diffHunk": "@@ -170,4 +175,32 @@ public void testMultiReadWriteLockWithSameResource() throws Exception {\n     Assert.assertTrue(gotLock.get());\n   }\n \n+  @Test\n+  public void testConcurrentWriteLockWithDifferentResource() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    final int count = 100;\n+    conf.setInt(HDDS_LOCK_MAX_CONCURRENCY, count/4);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMzMTc3NQ=="}, "originalCommit": {"oid": "290b75f9ab103210a566ceb7c86245d3ac7f69cf"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQ1Mzc2MQ==", "bodyText": "Thank You @xiaoyuyao for the review. Removed the config in the latest commit.", "url": "https://github.com/apache/ozone/pull/627#discussion_r387453761", "createdAt": "2020-03-04T05:20:29Z", "author": {"login": "bharatviswa504"}, "path": "hadoop-hdds/common/src/test/java/org/apache/hadoop/ozone/lock/TestLockManager.java", "diffHunk": "@@ -170,4 +175,32 @@ public void testMultiReadWriteLockWithSameResource() throws Exception {\n     Assert.assertTrue(gotLock.get());\n   }\n \n+  @Test\n+  public void testConcurrentWriteLockWithDifferentResource() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    final int count = 100;\n+    conf.setInt(HDDS_LOCK_MAX_CONCURRENCY, count/4);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMzMTc3NQ=="}, "originalCommit": {"oid": "290b75f9ab103210a566ceb7c86245d3ac7f69cf"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQ2MjEzNQ==", "bodyText": "bq. Just trying to understand is the issue is observed on the test cluster, where some test is working in parallel across more than 100 volumes/100 buckets and a lock contention is observed?\nI hit the problem last week and had been reviewing related code along with the OM stack dumps this week. Found the fix here yesterday, tried it and found that here may not completely solve the problem. Here is the root cause:\nOMKeyCreateRequest.validateAndUpdateCache requires two locks from the lock pool to finish a key creation. One write lock for the BUCKET at the beginning and one lock for longest prefix path close to the end during prepareKeyInfo call.\nIf you have 200 threads handlers but only 100 lock limit from the lock manager. When the first 100 key creation threads all grab the bucket lock, OM will stuck there forever as we don't have any locks from the pool for the second call. My initial thought is to add a timeout but later found removing the limit seems to be a better solution as they will quickly finish and return the lock without the limit.", "url": "https://github.com/apache/ozone/pull/627#discussion_r387462135", "createdAt": "2020-03-04T05:56:59Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-hdds/common/src/test/java/org/apache/hadoop/ozone/lock/TestLockManager.java", "diffHunk": "@@ -170,4 +175,32 @@ public void testMultiReadWriteLockWithSameResource() throws Exception {\n     Assert.assertTrue(gotLock.get());\n   }\n \n+  @Test\n+  public void testConcurrentWriteLockWithDifferentResource() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    final int count = 100;\n+    conf.setInt(HDDS_LOCK_MAX_CONCURRENCY, count/4);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzMzMTc3NQ=="}, "originalCommit": {"oid": "290b75f9ab103210a566ceb7c86245d3ac7f69cf"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQwMDM2NzM3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lock/LockManager.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwNTozNTo0NFrOFxggdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwNTo1ODo0M1rOFxg1mQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQ1NzE0Mg==", "bodyText": "NIT: we don't need to explicitly set -1 as the default maxTotal from GenericObjectPool is -1.", "url": "https://github.com/apache/ozone/pull/627#discussion_r387457142", "createdAt": "2020-03-04T05:35:44Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lock/LockManager.java", "diffHunk": "@@ -57,12 +56,9 @@ public LockManager(final Configuration conf) {\n    * @param fair - true to use fair lock ordering, else non-fair lock ordering.\n    */\n   public LockManager(final Configuration conf, boolean fair) {\n-    final int maxPoolSize = conf.getInt(\n-        HddsConfigKeys.HDDS_LOCK_MAX_CONCURRENCY,\n-        HddsConfigKeys.HDDS_LOCK_MAX_CONCURRENCY_DEFAULT);\n     lockPool =\n         new GenericObjectPool<>(new PooledLockFactory(fair));\n-    lockPool.setMaxTotal(maxPoolSize);\n+    lockPool.setMaxTotal(-1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "146c7410300b5bfa91d26c678c9e0ac795468ce8"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQ1NzM1Nw==", "bodyText": "yes, I did this expicitly, if in future releases if they changed the defaults it might break when we upgrade the commons-pool2.", "url": "https://github.com/apache/ozone/pull/627#discussion_r387457357", "createdAt": "2020-03-04T05:36:38Z", "author": {"login": "bharatviswa504"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lock/LockManager.java", "diffHunk": "@@ -57,12 +56,9 @@ public LockManager(final Configuration conf) {\n    * @param fair - true to use fair lock ordering, else non-fair lock ordering.\n    */\n   public LockManager(final Configuration conf, boolean fair) {\n-    final int maxPoolSize = conf.getInt(\n-        HddsConfigKeys.HDDS_LOCK_MAX_CONCURRENCY,\n-        HddsConfigKeys.HDDS_LOCK_MAX_CONCURRENCY_DEFAULT);\n     lockPool =\n         new GenericObjectPool<>(new PooledLockFactory(fair));\n-    lockPool.setMaxTotal(maxPoolSize);\n+    lockPool.setMaxTotal(-1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQ1NzE0Mg=="}, "originalCommit": {"oid": "146c7410300b5bfa91d26c678c9e0ac795468ce8"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQ2MjU1Mw==", "bodyText": "Sounds good. +1 pending CI.", "url": "https://github.com/apache/ozone/pull/627#discussion_r387462553", "createdAt": "2020-03-04T05:58:43Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/lock/LockManager.java", "diffHunk": "@@ -57,12 +56,9 @@ public LockManager(final Configuration conf) {\n    * @param fair - true to use fair lock ordering, else non-fair lock ordering.\n    */\n   public LockManager(final Configuration conf, boolean fair) {\n-    final int maxPoolSize = conf.getInt(\n-        HddsConfigKeys.HDDS_LOCK_MAX_CONCURRENCY,\n-        HddsConfigKeys.HDDS_LOCK_MAX_CONCURRENCY_DEFAULT);\n     lockPool =\n         new GenericObjectPool<>(new PooledLockFactory(fair));\n-    lockPool.setMaxTotal(maxPoolSize);\n+    lockPool.setMaxTotal(-1);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzQ1NzE0Mg=="}, "originalCommit": {"oid": "146c7410300b5bfa91d26c678c9e0ac795468ce8"}, "originalPosition": 18}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4945, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}