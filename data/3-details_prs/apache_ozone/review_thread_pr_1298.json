{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY0MTkyODM3", "number": 1298, "reviewThreads": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxOTozNjozMFrOEXWqfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMzo1NToyN1rOEd6smw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyOTI0MDMxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/SchemaOneDeletedBlocksTable.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxOTozNjozMFrOG_GcFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xM1QxOTo1NDozM1rOHAbWGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgxODk2Ng==", "bodyText": "Throw UnsupportedOperation for this implementation and schema 2 implementation.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r468818966", "createdAt": "2020-08-11T19:36:30Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/SchemaOneDeletedBlocksTable.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.BatchOperation;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheKey;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheValue;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * For RocksDB instances written using DB schema version 1, all data is\n+ * stored in the default column family. This differs from later schema\n+ * versions, which put deleted blocks in a different column family.\n+ * As a result, the block IDs used as keys for deleted blocks must be\n+ * prefixed in schema version 1 so that they can be differentiated from\n+ * regular blocks. However, these prefixes are not necessary in later schema\n+ * versions, because the deleted blocks and regular blocks are in different\n+ * column families.\n+ * <p>\n+ * Since clients must operate independently of the underlying schema version,\n+ * This class is returned to clients using {@link DatanodeStoreSchemaOneImpl}\n+ * instances, allowing them to access keys as if no prefix is\n+ * required, while it adds the prefix when necessary.\n+ * This means the client should omit the deleted prefix when putting and\n+ * getting keys, regardless of the schema version.\n+ * <p>\n+ * Note that this class will only apply prefixes to keys as parameters,\n+ * never as return types. This means that keys returned through iterators\n+ * like {@link SchemaOneDeletedBlocksTable#getSequentialRangeKVs},\n+ * {@link SchemaOneDeletedBlocksTable#getRangeKVs}, and\n+ * {@link SchemaOneDeletedBlocksTable#iterator} will return keys prefixed\n+ * with {@link SchemaOneDeletedBlocksTable#DELETED_KEY_PREFIX}.\n+ */\n+public class SchemaOneDeletedBlocksTable implements Table<String,\n+        ChunkInfoList> {\n+  public static final String DELETED_KEY_PREFIX = \"#deleted#\";\n+\n+  private final Table<String, ChunkInfoList> table;\n+\n+  public SchemaOneDeletedBlocksTable(Table<String, ChunkInfoList> table) {\n+    this.table = table;\n+  }\n+\n+  @Override\n+  public void put(String key, ChunkInfoList value) throws IOException {\n+    table.put(prefix(key), value);\n+  }\n+\n+  @Override\n+  public void putWithBatch(BatchOperation batch, String key,\n+                           ChunkInfoList value)\n+          throws IOException {\n+    table.putWithBatch(batch, prefix(key), value);\n+  }\n+\n+  @Override\n+  public boolean isEmpty() throws IOException {\n+    return table.isEmpty();\n+  }\n+\n+  @Override\n+  public void delete(String key) throws IOException {\n+    table.delete(prefix(key));\n+  }\n+\n+  @Override\n+  public void deleteWithBatch(BatchOperation batch, String key)\n+          throws IOException {\n+    table.deleteWithBatch(batch, prefix(key));\n+  }\n+\n+  /**\n+   * Because the actual underlying table in this schema version is the\n+   * default table where all keys are stored, this method will iterate\n+   * through all keys in the database.\n+   */\n+  @Override\n+  public TableIterator<String, ? extends KeyValue<String, ChunkInfoList>>\n+      iterator() {\n+    return table.iterator();\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3d0fa1f8da98c535999e46d56fe41c65a195d39"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDIxMDA3NA==", "bodyText": "In order to not support this method, the KeyValueBlockIterator will need a different way to gain access to the iterator it uses internally when filtering blocks by prefix. The updated version of the code will make this class internal to the AbstractDatanodeStore, and callers can get it using getters in the DatanodeStore interface that return the KeyValueBlockIterator's interface: BlockIterator. This way, the AbstractDatanodeStore can initialize it with an iterator retrieved from the block data table before wrapping it in a class to disable access to this method.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r470210074", "createdAt": "2020-08-13T19:54:33Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/SchemaOneDeletedBlocksTable.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.BatchOperation;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheKey;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheValue;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * For RocksDB instances written using DB schema version 1, all data is\n+ * stored in the default column family. This differs from later schema\n+ * versions, which put deleted blocks in a different column family.\n+ * As a result, the block IDs used as keys for deleted blocks must be\n+ * prefixed in schema version 1 so that they can be differentiated from\n+ * regular blocks. However, these prefixes are not necessary in later schema\n+ * versions, because the deleted blocks and regular blocks are in different\n+ * column families.\n+ * <p>\n+ * Since clients must operate independently of the underlying schema version,\n+ * This class is returned to clients using {@link DatanodeStoreSchemaOneImpl}\n+ * instances, allowing them to access keys as if no prefix is\n+ * required, while it adds the prefix when necessary.\n+ * This means the client should omit the deleted prefix when putting and\n+ * getting keys, regardless of the schema version.\n+ * <p>\n+ * Note that this class will only apply prefixes to keys as parameters,\n+ * never as return types. This means that keys returned through iterators\n+ * like {@link SchemaOneDeletedBlocksTable#getSequentialRangeKVs},\n+ * {@link SchemaOneDeletedBlocksTable#getRangeKVs}, and\n+ * {@link SchemaOneDeletedBlocksTable#iterator} will return keys prefixed\n+ * with {@link SchemaOneDeletedBlocksTable#DELETED_KEY_PREFIX}.\n+ */\n+public class SchemaOneDeletedBlocksTable implements Table<String,\n+        ChunkInfoList> {\n+  public static final String DELETED_KEY_PREFIX = \"#deleted#\";\n+\n+  private final Table<String, ChunkInfoList> table;\n+\n+  public SchemaOneDeletedBlocksTable(Table<String, ChunkInfoList> table) {\n+    this.table = table;\n+  }\n+\n+  @Override\n+  public void put(String key, ChunkInfoList value) throws IOException {\n+    table.put(prefix(key), value);\n+  }\n+\n+  @Override\n+  public void putWithBatch(BatchOperation batch, String key,\n+                           ChunkInfoList value)\n+          throws IOException {\n+    table.putWithBatch(batch, prefix(key), value);\n+  }\n+\n+  @Override\n+  public boolean isEmpty() throws IOException {\n+    return table.isEmpty();\n+  }\n+\n+  @Override\n+  public void delete(String key) throws IOException {\n+    table.delete(prefix(key));\n+  }\n+\n+  @Override\n+  public void deleteWithBatch(BatchOperation batch, String key)\n+          throws IOException {\n+    table.deleteWithBatch(batch, prefix(key));\n+  }\n+\n+  /**\n+   * Because the actual underlying table in this schema version is the\n+   * default table where all keys are stored, this method will iterate\n+   * through all keys in the database.\n+   */\n+  @Override\n+  public TableIterator<String, ? extends KeyValue<String, ChunkInfoList>>\n+      iterator() {\n+    return table.iterator();\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgxODk2Ng=="}, "originalCommit": {"oid": "a3d0fa1f8da98c535999e46d56fe41c65a195d39"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyOTMxMTU1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConsts.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxOTo1ODo0N1rOG_HIEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxOTo1ODo0N1rOG_HIEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgzMDIyNw==", "bodyText": "Rename, because this is not actually a prefix, but a piece of metadata. Also make sure it is placed in the metadata table when used.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r468830227", "createdAt": "2020-08-11T19:58:47Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConsts.java", "diffHunk": "@@ -140,7 +139,6 @@ public static Versioning getVersioning(boolean versioning) {\n   }\n \n   public static final String DELETING_KEY_PREFIX = \"#deleting#\";\n-  public static final String DELETED_KEY_PREFIX = \"#deleted#\";\n   public static final String DELETE_TRANSACTION_KEY_PREFIX = \"#delTX#\";\n   public static final String BLOCK_COMMIT_SEQUENCE_ID_PREFIX = \"#BCSID\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3d0fa1f8da98c535999e46d56fe41c65a195d39"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyOTM0MjQ1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMDowODozMlrOG_Hauw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzo0MzoxMlrOG_r27A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgzNTAwMw==", "bodyText": "Do we need to check schema version here, or will it always be the latest version?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r468835003", "createdAt": "2020-08-11T20:08:32Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "diffHunk": "@@ -91,8 +83,16 @@ public static void createContainerMetaData(File containerMetaDataPath, File\n           \" Path: \" + chunksPath);\n     }\n \n-    MetadataStore store = MetadataStoreBuilder.newBuilder().setConf(conf)\n-        .setCreateIfMissing(true).setDbFile(dbFile).build();\n+    DatanodeStore store;\n+    if (schemaVersion.equals(OzoneConsts.SCHEMA_V1)) {\n+      store = new DatanodeStoreSchemaOneImpl(conf, dbFile.getAbsolutePath());\n+    } else if (schemaVersion.equals(OzoneConsts.SCHEMA_V2)) {\n+      store = new DatanodeStoreSchemaTwoImpl(conf, dbFile.getAbsolutePath());\n+    } else {\n+      throw new IllegalArgumentException(\n+              \"Unrecognized schema version for container: \" + schemaVersion);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3d0fa1f8da98c535999e46d56fe41c65a195d39"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyNTU2Ng==", "bodyText": "The latest version was always being passed in by KeyValueContainer#create (the only place this method is called), but it uses the OzoneConsts.SCHEMA_LATEST variable to make sure that the latest version is always used for new containers. If this parameter is omitted, we will need to hardcode the current latest DatanodeStore implementation here and remember to update it on change. Probably best to leave as is so that if a new version is added and this method is not updated, an IllegalArgumentException is thrown.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r469425566", "createdAt": "2020-08-12T17:31:47Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "diffHunk": "@@ -91,8 +83,16 @@ public static void createContainerMetaData(File containerMetaDataPath, File\n           \" Path: \" + chunksPath);\n     }\n \n-    MetadataStore store = MetadataStoreBuilder.newBuilder().setConf(conf)\n-        .setCreateIfMissing(true).setDbFile(dbFile).build();\n+    DatanodeStore store;\n+    if (schemaVersion.equals(OzoneConsts.SCHEMA_V1)) {\n+      store = new DatanodeStoreSchemaOneImpl(conf, dbFile.getAbsolutePath());\n+    } else if (schemaVersion.equals(OzoneConsts.SCHEMA_V2)) {\n+      store = new DatanodeStoreSchemaTwoImpl(conf, dbFile.getAbsolutePath());\n+    } else {\n+      throw new IllegalArgumentException(\n+              \"Unrecognized schema version for container: \" + schemaVersion);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgzNTAwMw=="}, "originalCommit": {"oid": "a3d0fa1f8da98c535999e46d56fe41c65a195d39"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQzMjA0NA==", "bodyText": "Update documentation to clarify this.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r469432044", "createdAt": "2020-08-12T17:43:12Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "diffHunk": "@@ -91,8 +83,16 @@ public static void createContainerMetaData(File containerMetaDataPath, File\n           \" Path: \" + chunksPath);\n     }\n \n-    MetadataStore store = MetadataStoreBuilder.newBuilder().setConf(conf)\n-        .setCreateIfMissing(true).setDbFile(dbFile).build();\n+    DatanodeStore store;\n+    if (schemaVersion.equals(OzoneConsts.SCHEMA_V1)) {\n+      store = new DatanodeStoreSchemaOneImpl(conf, dbFile.getAbsolutePath());\n+    } else if (schemaVersion.equals(OzoneConsts.SCHEMA_V2)) {\n+      store = new DatanodeStoreSchemaTwoImpl(conf, dbFile.getAbsolutePath());\n+    } else {\n+      throw new IllegalArgumentException(\n+              \"Unrecognized schema version for container: \" + schemaVersion);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgzNTAwMw=="}, "originalCommit": {"oid": "a3d0fa1f8da98c535999e46d56fe41c65a195d39"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3NjEwOTYwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/DeleteBlocksCommandHandler.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMToyNDo1M1rOHF-91Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMToyNDo1M1rOHF-91Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAzNjU2NQ==", "bodyText": "Is the BlockData table loaded in memory when the store is initialized?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476036565", "createdAt": "2020-08-25T01:24:53Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/DeleteBlocksCommandHandler.java", "diffHunk": "@@ -209,30 +208,35 @@ private void deleteKeyValueContainerBlocks(\n     int newDeletionBlocks = 0;\n     try(ReferenceCountedDB containerDB =\n             BlockUtils.getDB(containerData, conf)) {\n-      for (Long blk : delTX.getLocalIDList()) {\n-        BatchOperation batch = new BatchOperation();\n-        byte[] blkBytes = Longs.toByteArray(blk);\n-        byte[] blkInfo = containerDB.getStore().get(blkBytes);\n+      Table<String, BlockData> blockDataTable =\n+              containerDB.getStore().getBlockDataTable();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3NjE0NTIxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/ContainerDataYaml.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMTozNDozNFrOHF_WWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMjoxMDozNVrOHOvsOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA0Mjg0MQ==", "bodyText": "When reading old containerDataYaml which does not container the Schema version field, what value would be returned? IIRC and it returns null, then we should set it to version V1.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476042841", "createdAt": "2020-08-25T01:34:34Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/ContainerDataYaml.java", "diffHunk": "@@ -280,6 +280,9 @@ public Object construct(Node node) {\n         String state = (String) nodes.get(OzoneConsts.STATE);\n         kvData\n             .setState(ContainerProtos.ContainerDataProto.State.valueOf(state));\n+        String schemaVersion = (String) nodes.get(OzoneConsts.SCHEMA_VERSION);\n+        kvData.setSchemaVersion(schemaVersion);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTIyMzQ4Mw==", "bodyText": "See #1298 (comment)", "url": "https://github.com/apache/ozone/pull/1298#discussion_r485223483", "createdAt": "2020-09-08T22:10:35Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/ContainerDataYaml.java", "diffHunk": "@@ -280,6 +280,9 @@ public Object construct(Node node) {\n         String state = (String) nodes.get(OzoneConsts.STATE);\n         kvData\n             .setState(ContainerProtos.ContainerDataProto.State.valueOf(state));\n+        String schemaVersion = (String) nodes.get(OzoneConsts.SCHEMA_VERSION);\n+        kvData.setSchemaVersion(schemaVersion);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA0Mjg0MQ=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3NjI0MDI5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/ContainerCache.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMTo1OTowNFrOHGAWeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMTo1OTowNFrOHGAWeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA1OTI1Nw==", "bodyText": "@arp7, is it ok to remove LevelDB support?\nThe option to configure DNs to use LevelDB was removed in 0.6.0. And since upgrade from previous versions is not supported, I think it is safe to remove LevelDB support.\nIf yes, we can open a new Jira to clean up the LevelDB code path. If no, we need to take care of that here.\nThe containerDBType parameter is redundant here if old LevelDB containers will not be supported.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476059257", "createdAt": "2020-08-25T01:59:04Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/ContainerCache.java", "diffHunk": "@@ -115,11 +116,14 @@ protected boolean removeLRU(LinkEntry entry) {\n    * @param containerID - ID of the container.\n    * @param containerDBType - DB type of the container.\n    * @param containerDBPath - DB path of the container.\n+   * @param schemaVersion - Schema version of the container.\n    * @param conf - Hadoop Configuration.\n    * @return ReferenceCountedDB.\n    */\n   public ReferenceCountedDB getDB(long containerID, String containerDBType,\n-                             String containerDBPath, ConfigurationSource conf)\n+                                  String containerDBPath,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MDIwMjU4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQxOTozMzo0OFrOHGmz5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMjoxMDoyMVrOHOvr6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY4OTM4MQ==", "bodyText": "importContainerData() is called when replicating containers. If an old container needs to be replicated, it would not have the schema version. This can be fixed by setting a default value for schema version if it does not exist in ContainerDataYaml Constructor.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476689381", "createdAt": "2020-08-25T19:33:48Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java", "diffHunk": "@@ -487,6 +486,7 @@ public void importContainerData(InputStream input,\n       containerData.setState(originalContainerData.getState());\n       containerData\n           .setContainerDBType(originalContainerData.getContainerDBType());\n+      containerData.setSchemaVersion(originalContainerData.getSchemaVersion());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjcyOTg4NA==", "bodyText": "I see that schema version is being set in KeyValueContainerUtil#parseKVContainerData.\nWe can explore the option of setting the default schema version (V1) while reading the Yaml itself so that it is never missed.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476729884", "createdAt": "2020-08-25T20:52:17Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java", "diffHunk": "@@ -487,6 +486,7 @@ public void importContainerData(InputStream input,\n       containerData.setState(originalContainerData.getState());\n       containerData\n           .setContainerDBType(originalContainerData.getContainerDBType());\n+      containerData.setSchemaVersion(originalContainerData.getSchemaVersion());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY4OTM4MQ=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTIyMzQwMQ==", "bodyText": "After discussion, decided to move the null check to KeyValueContainerData#setSchemaVersion. Placing it in the parser is more confusing and possibly error prone in the future if the code is modified. ContainerDataYaml calls this setter in KeyValueContainerData anyways.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r485223401", "createdAt": "2020-09-08T22:10:21Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java", "diffHunk": "@@ -487,6 +486,7 @@ public void importContainerData(InputStream input,\n       containerData.setState(originalContainerData.getState());\n       containerData\n           .setContainerDBType(originalContainerData.getContainerDBType());\n+      containerData.setSchemaVersion(originalContainerData.getSchemaVersion());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY4OTM4MQ=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MDcwMDc1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQyMTo0ODowMVrOHGrqtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNzo1Nzo1MlrOHL63gQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc2ODk0OA==", "bodyText": "Any reason for using intValue here instead of the long value as incrPendingDeletionBlocks takes in a long parameter?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476768948", "createdAt": "2020-08-25T21:48:01Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "diffHunk": "@@ -159,122 +178,126 @@ public static void parseKVContainerData(KeyValueContainerData kvContainerData,\n     }\n     kvContainerData.setDbFile(dbFile);\n \n+    if (kvContainerData.getSchemaVersion() == null) {\n+      // If this container has not specified a schema version, it is in the old\n+      // format with one default column family.\n+      kvContainerData.setSchemaVersion(OzoneConsts.SCHEMA_V1);\n+    }\n+\n \n     boolean isBlockMetadataSet = false;\n \n     try(ReferenceCountedDB containerDB = BlockUtils.getDB(kvContainerData,\n         config)) {\n \n+      Table<String, Long> metadataTable =\n+              containerDB.getStore().getMetadataTable();\n+\n       // Set pending deleted block count.\n-      byte[] pendingDeleteBlockCount =\n-          containerDB.getStore().get(DB_PENDING_DELETE_BLOCK_COUNT_KEY);\n+      Long pendingDeleteBlockCount =\n+          metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n       if (pendingDeleteBlockCount != null) {\n         kvContainerData.incrPendingDeletionBlocks(\n-            Longs.fromByteArray(pendingDeleteBlockCount));\n+                pendingDeleteBlockCount.intValue());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI2MDg2NQ==", "bodyText": "Nope. Actually even a .longValue() call is redundant with Java auto unboxing, so I'll just pass the pendingDeleteBlockCount as is.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482260865", "createdAt": "2020-09-02T17:57:52Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "diffHunk": "@@ -159,122 +178,126 @@ public static void parseKVContainerData(KeyValueContainerData kvContainerData,\n     }\n     kvContainerData.setDbFile(dbFile);\n \n+    if (kvContainerData.getSchemaVersion() == null) {\n+      // If this container has not specified a schema version, it is in the old\n+      // format with one default column family.\n+      kvContainerData.setSchemaVersion(OzoneConsts.SCHEMA_V1);\n+    }\n+\n \n     boolean isBlockMetadataSet = false;\n \n     try(ReferenceCountedDB containerDB = BlockUtils.getDB(kvContainerData,\n         config)) {\n \n+      Table<String, Long> metadataTable =\n+              containerDB.getStore().getMetadataTable();\n+\n       // Set pending deleted block count.\n-      byte[] pendingDeleteBlockCount =\n-          containerDB.getStore().get(DB_PENDING_DELETE_BLOCK_COUNT_KEY);\n+      Long pendingDeleteBlockCount =\n+          metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n       if (pendingDeleteBlockCount != null) {\n         kvContainerData.incrPendingDeletionBlocks(\n-            Longs.fromByteArray(pendingDeleteBlockCount));\n+                pendingDeleteBlockCount.intValue());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc2ODk0OA=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 123}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MDc3NjM2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQyMjowMzoxNVrOHGsbQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMToxMTo0MFrOHOuK7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc4MTM3OQ==", "bodyText": "The way usedBytes is calculated has been changed. I am not sure if there will be any implications if this calculation us wrong. Need to dig deeper.\nWe should probably separate this optimization into a separate Jira. What do you think?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476781379", "createdAt": "2020-08-25T22:03:15Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "diffHunk": "@@ -159,122 +178,126 @@ public static void parseKVContainerData(KeyValueContainerData kvContainerData,\n     }\n     kvContainerData.setDbFile(dbFile);\n \n+    if (kvContainerData.getSchemaVersion() == null) {\n+      // If this container has not specified a schema version, it is in the old\n+      // format with one default column family.\n+      kvContainerData.setSchemaVersion(OzoneConsts.SCHEMA_V1);\n+    }\n+\n \n     boolean isBlockMetadataSet = false;\n \n     try(ReferenceCountedDB containerDB = BlockUtils.getDB(kvContainerData,\n         config)) {\n \n+      Table<String, Long> metadataTable =\n+              containerDB.getStore().getMetadataTable();\n+\n       // Set pending deleted block count.\n-      byte[] pendingDeleteBlockCount =\n-          containerDB.getStore().get(DB_PENDING_DELETE_BLOCK_COUNT_KEY);\n+      Long pendingDeleteBlockCount =\n+          metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n       if (pendingDeleteBlockCount != null) {\n         kvContainerData.incrPendingDeletionBlocks(\n-            Longs.fromByteArray(pendingDeleteBlockCount));\n+                pendingDeleteBlockCount.intValue());\n       } else {\n         // Set pending deleted block count.\n         MetadataKeyFilters.KeyPrefixFilter filter =\n-            new MetadataKeyFilters.KeyPrefixFilter()\n-                .addFilter(OzoneConsts.DELETING_KEY_PREFIX);\n+                MetadataKeyFilters.getDeletingKeyFilter();\n         int numPendingDeletionBlocks =\n-            containerDB.getStore().getSequentialRangeKVs(null,\n-                Integer.MAX_VALUE, filter)\n-                .size();\n+            containerDB.getStore().getBlockDataTable()\n+            .getSequentialRangeKVs(null, Integer.MAX_VALUE, filter)\n+            .size();\n         kvContainerData.incrPendingDeletionBlocks(numPendingDeletionBlocks);\n       }\n \n       // Set delete transaction id.\n-      byte[] delTxnId =\n-          containerDB.getStore().get(DB_CONTAINER_DELETE_TRANSACTION_KEY);\n+      Long delTxnId =\n+          metadataTable.get(OzoneConsts.DELETE_TRANSACTION_KEY);\n       if (delTxnId != null) {\n         kvContainerData\n-            .updateDeleteTransactionId(Longs.fromByteArray(delTxnId));\n+            .updateDeleteTransactionId(delTxnId);\n       }\n \n       // Set BlockCommitSequenceId.\n-      byte[] bcsId = containerDB.getStore().get(\n-          DB_BLOCK_COMMIT_SEQUENCE_ID_KEY);\n+      Long bcsId = metadataTable.get(\n+          OzoneConsts.BLOCK_COMMIT_SEQUENCE_ID);\n       if (bcsId != null) {\n         kvContainerData\n-            .updateBlockCommitSequenceId(Longs.fromByteArray(bcsId));\n+            .updateBlockCommitSequenceId(bcsId);\n       }\n \n       // Set bytes used.\n       // commitSpace for Open Containers relies on usedBytes\n-      byte[] bytesUsed =\n-          containerDB.getStore().get(DB_CONTAINER_BYTES_USED_KEY);\n+      Long bytesUsed =\n+          metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED);\n       if (bytesUsed != null) {\n         isBlockMetadataSet = true;\n-        kvContainerData.setBytesUsed(Longs.fromByteArray(bytesUsed));\n+        kvContainerData.setBytesUsed(bytesUsed);\n       }\n \n       // Set block count.\n-      byte[] blockCount = containerDB.getStore().get(DB_BLOCK_COUNT_KEY);\n+      Long blockCount = metadataTable.get(OzoneConsts.BLOCK_COUNT);\n       if (blockCount != null) {\n         isBlockMetadataSet = true;\n-        kvContainerData.setKeyCount(Longs.fromByteArray(blockCount));\n+        kvContainerData.setKeyCount(blockCount);\n       }\n     }\n \n     if (!isBlockMetadataSet) {\n-      initializeUsedBytesAndBlockCount(kvContainerData);\n+      initializeUsedBytesAndBlockCount(kvContainerData, config);\n     }\n   }\n \n \n   /**\n    * Initialize bytes used and block count.\n-   * @param kvContainerData\n+   * @param kvData\n    * @throws IOException\n    */\n   private static void initializeUsedBytesAndBlockCount(\n-      KeyValueContainerData kvContainerData) throws IOException {\n-\n-    MetadataKeyFilters.KeyPrefixFilter filter =\n-            new MetadataKeyFilters.KeyPrefixFilter();\n+      KeyValueContainerData kvData, ConfigurationSource config)\n+          throws IOException {\n \n-    // Ignore all blocks except those with no prefix, or those with\n-    // #deleting# prefix.\n-    filter.addFilter(OzoneConsts.DELETED_KEY_PREFIX, true)\n-          .addFilter(OzoneConsts.DELETE_TRANSACTION_KEY_PREFIX, true)\n-          .addFilter(OzoneConsts.BLOCK_COMMIT_SEQUENCE_ID_PREFIX, true)\n-          .addFilter(OzoneConsts.BLOCK_COUNT, true)\n-          .addFilter(OzoneConsts.CONTAINER_BYTES_USED, true)\n-          .addFilter(OzoneConsts.PENDING_DELETE_BLOCK_COUNT, true);\n+    final String errorMessage = \"Failed to parse block data for\" +\n+            \" Container \" + kvData.getContainerID();\n \n     long blockCount = 0;\n-    try (KeyValueBlockIterator blockIter = new KeyValueBlockIterator(\n-        kvContainerData.getContainerID(),\n-        new File(kvContainerData.getContainerPath()), filter)) {\n-      long usedBytes = 0;\n-\n-\n-      boolean success = true;\n-      while (success) {\n-        try {\n-          if (blockIter.hasNext()) {\n-            BlockData block = blockIter.nextBlock();\n-            long blockLen = 0;\n-\n-            List< ContainerProtos.ChunkInfo > chunkInfoList = block.getChunks();\n-            for (ContainerProtos.ChunkInfo chunk : chunkInfoList) {\n-              ChunkInfo info = ChunkInfo.getFromProtoBuf(chunk);\n-              blockLen += info.getLen();\n-            }\n-\n-            usedBytes += blockLen;\n-            blockCount++;\n-          } else {\n-            success = false;\n+    long usedBytes = 0;\n+\n+    try(ReferenceCountedDB db = BlockUtils.getDB(kvData, config)) {\n+      // Count all regular blocks.\n+      try (BlockIterator<BlockData> blockIter =\n+                   db.getStore().getBlockIterator(\n+                           MetadataKeyFilters.getUnprefixedKeyFilter())) {\n+\n+        while (blockIter.hasNext()) {\n+          blockCount++;\n+          try {\n+            usedBytes += blockIter.nextBlock().getSize();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 251}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE5ODU3NQ==", "bodyText": "I will switch it back to the old calculation and we can do it in a separate jira later if we want.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r485198575", "createdAt": "2020-09-08T21:11:40Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "diffHunk": "@@ -159,122 +178,126 @@ public static void parseKVContainerData(KeyValueContainerData kvContainerData,\n     }\n     kvContainerData.setDbFile(dbFile);\n \n+    if (kvContainerData.getSchemaVersion() == null) {\n+      // If this container has not specified a schema version, it is in the old\n+      // format with one default column family.\n+      kvContainerData.setSchemaVersion(OzoneConsts.SCHEMA_V1);\n+    }\n+\n \n     boolean isBlockMetadataSet = false;\n \n     try(ReferenceCountedDB containerDB = BlockUtils.getDB(kvContainerData,\n         config)) {\n \n+      Table<String, Long> metadataTable =\n+              containerDB.getStore().getMetadataTable();\n+\n       // Set pending deleted block count.\n-      byte[] pendingDeleteBlockCount =\n-          containerDB.getStore().get(DB_PENDING_DELETE_BLOCK_COUNT_KEY);\n+      Long pendingDeleteBlockCount =\n+          metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n       if (pendingDeleteBlockCount != null) {\n         kvContainerData.incrPendingDeletionBlocks(\n-            Longs.fromByteArray(pendingDeleteBlockCount));\n+                pendingDeleteBlockCount.intValue());\n       } else {\n         // Set pending deleted block count.\n         MetadataKeyFilters.KeyPrefixFilter filter =\n-            new MetadataKeyFilters.KeyPrefixFilter()\n-                .addFilter(OzoneConsts.DELETING_KEY_PREFIX);\n+                MetadataKeyFilters.getDeletingKeyFilter();\n         int numPendingDeletionBlocks =\n-            containerDB.getStore().getSequentialRangeKVs(null,\n-                Integer.MAX_VALUE, filter)\n-                .size();\n+            containerDB.getStore().getBlockDataTable()\n+            .getSequentialRangeKVs(null, Integer.MAX_VALUE, filter)\n+            .size();\n         kvContainerData.incrPendingDeletionBlocks(numPendingDeletionBlocks);\n       }\n \n       // Set delete transaction id.\n-      byte[] delTxnId =\n-          containerDB.getStore().get(DB_CONTAINER_DELETE_TRANSACTION_KEY);\n+      Long delTxnId =\n+          metadataTable.get(OzoneConsts.DELETE_TRANSACTION_KEY);\n       if (delTxnId != null) {\n         kvContainerData\n-            .updateDeleteTransactionId(Longs.fromByteArray(delTxnId));\n+            .updateDeleteTransactionId(delTxnId);\n       }\n \n       // Set BlockCommitSequenceId.\n-      byte[] bcsId = containerDB.getStore().get(\n-          DB_BLOCK_COMMIT_SEQUENCE_ID_KEY);\n+      Long bcsId = metadataTable.get(\n+          OzoneConsts.BLOCK_COMMIT_SEQUENCE_ID);\n       if (bcsId != null) {\n         kvContainerData\n-            .updateBlockCommitSequenceId(Longs.fromByteArray(bcsId));\n+            .updateBlockCommitSequenceId(bcsId);\n       }\n \n       // Set bytes used.\n       // commitSpace for Open Containers relies on usedBytes\n-      byte[] bytesUsed =\n-          containerDB.getStore().get(DB_CONTAINER_BYTES_USED_KEY);\n+      Long bytesUsed =\n+          metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED);\n       if (bytesUsed != null) {\n         isBlockMetadataSet = true;\n-        kvContainerData.setBytesUsed(Longs.fromByteArray(bytesUsed));\n+        kvContainerData.setBytesUsed(bytesUsed);\n       }\n \n       // Set block count.\n-      byte[] blockCount = containerDB.getStore().get(DB_BLOCK_COUNT_KEY);\n+      Long blockCount = metadataTable.get(OzoneConsts.BLOCK_COUNT);\n       if (blockCount != null) {\n         isBlockMetadataSet = true;\n-        kvContainerData.setKeyCount(Longs.fromByteArray(blockCount));\n+        kvContainerData.setKeyCount(blockCount);\n       }\n     }\n \n     if (!isBlockMetadataSet) {\n-      initializeUsedBytesAndBlockCount(kvContainerData);\n+      initializeUsedBytesAndBlockCount(kvContainerData, config);\n     }\n   }\n \n \n   /**\n    * Initialize bytes used and block count.\n-   * @param kvContainerData\n+   * @param kvData\n    * @throws IOException\n    */\n   private static void initializeUsedBytesAndBlockCount(\n-      KeyValueContainerData kvContainerData) throws IOException {\n-\n-    MetadataKeyFilters.KeyPrefixFilter filter =\n-            new MetadataKeyFilters.KeyPrefixFilter();\n+      KeyValueContainerData kvData, ConfigurationSource config)\n+          throws IOException {\n \n-    // Ignore all blocks except those with no prefix, or those with\n-    // #deleting# prefix.\n-    filter.addFilter(OzoneConsts.DELETED_KEY_PREFIX, true)\n-          .addFilter(OzoneConsts.DELETE_TRANSACTION_KEY_PREFIX, true)\n-          .addFilter(OzoneConsts.BLOCK_COMMIT_SEQUENCE_ID_PREFIX, true)\n-          .addFilter(OzoneConsts.BLOCK_COUNT, true)\n-          .addFilter(OzoneConsts.CONTAINER_BYTES_USED, true)\n-          .addFilter(OzoneConsts.PENDING_DELETE_BLOCK_COUNT, true);\n+    final String errorMessage = \"Failed to parse block data for\" +\n+            \" Container \" + kvData.getContainerID();\n \n     long blockCount = 0;\n-    try (KeyValueBlockIterator blockIter = new KeyValueBlockIterator(\n-        kvContainerData.getContainerID(),\n-        new File(kvContainerData.getContainerPath()), filter)) {\n-      long usedBytes = 0;\n-\n-\n-      boolean success = true;\n-      while (success) {\n-        try {\n-          if (blockIter.hasNext()) {\n-            BlockData block = blockIter.nextBlock();\n-            long blockLen = 0;\n-\n-            List< ContainerProtos.ChunkInfo > chunkInfoList = block.getChunks();\n-            for (ContainerProtos.ChunkInfo chunk : chunkInfoList) {\n-              ChunkInfo info = ChunkInfo.getFromProtoBuf(chunk);\n-              blockLen += info.getLen();\n-            }\n-\n-            usedBytes += blockLen;\n-            blockCount++;\n-          } else {\n-            success = false;\n+    long usedBytes = 0;\n+\n+    try(ReferenceCountedDB db = BlockUtils.getDB(kvData, config)) {\n+      // Count all regular blocks.\n+      try (BlockIterator<BlockData> blockIter =\n+                   db.getStore().getBlockIterator(\n+                           MetadataKeyFilters.getUnprefixedKeyFilter())) {\n+\n+        while (blockIter.hasNext()) {\n+          blockCount++;\n+          try {\n+            usedBytes += blockIter.nextBlock().getSize();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc4MTM3OQ=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 251}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MTE2MjM4OnYy", "diffSide": "LEFT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/BlockManagerImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQyMzoxODo1OVrOHGwXqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxODowNjo0OFrOHL7MCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg0NTk5NA==", "bodyText": "blockKey variable is redundant now and can be removed.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476845994", "createdAt": "2020-08-25T23:18:59Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/BlockManagerImpl.java", "diffHunk": "@@ -262,14 +264,17 @@ public void deleteBlock(Container container, BlockID blockID) throws\n       getBlockByID(db, blockID);\n \n       // Update DB to delete block and set block count and bytes used.\n-      BatchOperation batch = new BatchOperation();\n-      batch.delete(blockKey);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI2NjEyMw==", "bodyText": "Will do", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482266123", "createdAt": "2020-09-02T18:06:48Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/BlockManagerImpl.java", "diffHunk": "@@ -262,14 +264,17 @@ public void deleteBlock(Container container, BlockID blockID) throws\n       getBlockByID(db, blockID);\n \n       // Update DB to delete block and set block count and bytes used.\n-      BatchOperation batch = new BatchOperation();\n-      batch.delete(blockKey);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg0NTk5NA=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MTIyNzg5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/BlockManagerImpl.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQyMzoyOTozOFrOHGxDtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMDo1Mjo0M1rOHOtnug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1NzI3MQ==", "bodyText": "All the usages of getBlockByID convert the returned byte array back to ContainerProtos.BlockData. We can avoid this serialization-deserialization.\nNoting it down here so that we can open a new Jira to optimize this.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476857271", "createdAt": "2020-08-25T23:29:38Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/BlockManagerImpl.java", "diffHunk": "@@ -324,14 +328,14 @@ public void shutdown() {\n \n   private byte[] getBlockByID(ReferenceCountedDB db, BlockID blockID)\n       throws IOException {\n-    byte[] blockKey = Longs.toByteArray(blockID.getLocalID());\n+    String blockKey = Long.toString(blockID.getLocalID());\n \n-    byte[] blockData = db.getStore().get(blockKey);\n+    BlockData blockData = db.getStore().getBlockDataTable().get(blockKey);\n     if (blockData == null) {\n       throw new StorageContainerException(NO_SUCH_BLOCK_ERR_MSG,\n           NO_SUCH_BLOCK);\n     }\n \n-    return blockData;\n+    return blockData.getProtoBufMessage().toByteArray();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI2NzU3OA==", "bodyText": "Good catch. We could fix it here since this pull request moves the coding/decoding into the tables and away from the caller, or do it in a new Jira.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482267578", "createdAt": "2020-09-02T18:09:42Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/BlockManagerImpl.java", "diffHunk": "@@ -324,14 +328,14 @@ public void shutdown() {\n \n   private byte[] getBlockByID(ReferenceCountedDB db, BlockID blockID)\n       throws IOException {\n-    byte[] blockKey = Longs.toByteArray(blockID.getLocalID());\n+    String blockKey = Long.toString(blockID.getLocalID());\n \n-    byte[] blockData = db.getStore().get(blockKey);\n+    BlockData blockData = db.getStore().getBlockDataTable().get(blockKey);\n     if (blockData == null) {\n       throw new StorageContainerException(NO_SUCH_BLOCK_ERR_MSG,\n           NO_SUCH_BLOCK);\n     }\n \n-    return blockData;\n+    return blockData.getProtoBufMessage().toByteArray();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1NzI3MQ=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE4OTU2Mg==", "bodyText": "When this patch is merged, I will change this in this jira: https://issues.apache.org/jira/browse/HDDS-4220", "url": "https://github.com/apache/ozone/pull/1298#discussion_r485189562", "createdAt": "2020-09-08T20:52:43Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/BlockManagerImpl.java", "diffHunk": "@@ -324,14 +328,14 @@ public void shutdown() {\n \n   private byte[] getBlockByID(ReferenceCountedDB db, BlockID blockID)\n       throws IOException {\n-    byte[] blockKey = Longs.toByteArray(blockID.getLocalID());\n+    String blockKey = Long.toString(blockID.getLocalID());\n \n-    byte[] blockData = db.getStore().get(blockKey);\n+    BlockData blockData = db.getStore().getBlockDataTable().get(blockKey);\n     if (blockData == null) {\n       throw new StorageContainerException(NO_SUCH_BLOCK_ERR_MSG,\n           NO_SUCH_BLOCK);\n     }\n \n-    return blockData;\n+    return blockData.getProtoBufMessage().toByteArray();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1NzI3MQ=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4NjI4Mzg2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/DatanodeSchemaTwoDBDefinition.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQyMzoyMzoyN1rOHHibDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxODoxNzozN1rOHL7isQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY2NjA2MA==", "bodyText": "Can we define the table names as static final fields either here or in OzoneConsts?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r477666060", "createdAt": "2020-08-26T23:23:27Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/DatanodeSchemaTwoDBDefinition.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.\u2002\u2002See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.\u2002\u2002The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.\u2002\u2002You may obtain a copy of the License at\n+ *\n+ * \u2002\u2002\u2002\u2002 http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.db.DBColumnFamilyDefinition;\n+import org.apache.hadoop.hdds.utils.db.LongCodec;\n+import org.apache.hadoop.hdds.utils.db.StringCodec;\n+import org.apache.hadoop.ozone.container.common.helpers.BlockData;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+\n+/**\n+ * This class defines the RocksDB structure for datanodes following schema\n+ * version 2, where the block data, metadata, and deleted block ids are put in\n+ * their own separate column families.\n+ */\n+public class DatanodeSchemaTwoDBDefinition extends\n+        AbstractDatanodeDBDefinition {\n+\n+  public static final DBColumnFamilyDefinition<String, BlockData>\n+          BLOCK_DATA =\n+          new DBColumnFamilyDefinition<>(\n+                  \"block_data\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI3MTkyMQ==", "bodyText": "Placing the table names here is consistent with the existing DBDefinitions, there are currently no table names defined in OzoneConsts. The names are basically static final already, since the DBColumnFamilyDefinieions are static final, and the name property of a DBColumnFamilyDefinition cannot be changed after object creation. The call to read a table name would be DatanodeSchemaTwoDBDefinition.BLOCK_DATA.getTableName(), and there is no setter for a caller to change this value.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482271921", "createdAt": "2020-09-02T18:17:37Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/DatanodeSchemaTwoDBDefinition.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.\u2002\u2002See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.\u2002\u2002The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.\u2002\u2002You may obtain a copy of the License at\n+ *\n+ * \u2002\u2002\u2002\u2002 http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.db.DBColumnFamilyDefinition;\n+import org.apache.hadoop.hdds.utils.db.LongCodec;\n+import org.apache.hadoop.hdds.utils.db.StringCodec;\n+import org.apache.hadoop.ozone.container.common.helpers.BlockData;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+\n+/**\n+ * This class defines the RocksDB structure for datanodes following schema\n+ * version 2, where the block data, metadata, and deleted block ids are put in\n+ * their own separate column families.\n+ */\n+public class DatanodeSchemaTwoDBDefinition extends\n+        AbstractDatanodeDBDefinition {\n+\n+  public static final DBColumnFamilyDefinition<String, BlockData>\n+          BLOCK_DATA =\n+          new DBColumnFamilyDefinition<>(\n+                  \"block_data\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY2NjA2MA=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4NjU2MTU0OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/DatanodeTable.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQyMzo1NjoyMlrOHHlVYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxODoyMjowNlrOHL7zeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzcxMzc2Mg==", "bodyText": "I think we don't need cache functionality for DatanodeTable? I see it being used only in OM.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r477713762", "createdAt": "2020-08-26T23:56:22Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/DatanodeTable.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.BatchOperation;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheKey;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheValue;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Wrapper class to represent a table in a datanode RocksDB instance.\n+ * This class can wrap any {@link Table} instance, but will throw\n+ * {@link UnsupportedOperationException} for {@link Table#iterator}.\n+ * This is because differing schema versions used in datanode DB layouts may\n+ * have differing underlying table structures, so iterating a table instance\n+ * directly, without taking into account key prefixes, may yield unexpected\n+ * results.\n+ */\n+public class DatanodeTable<KEY, VALUE> implements Table<KEY, VALUE> {\n+\n+  private final Table<KEY, VALUE> table;\n+\n+  public DatanodeTable(Table<KEY, VALUE> table) {\n+    this.table = table;\n+  }\n+\n+  @Override\n+  public void put(KEY key, VALUE value) throws IOException {\n+    table.put(key, value);\n+  }\n+\n+  @Override\n+  public void putWithBatch(BatchOperation batch, KEY key,\n+                           VALUE value) throws IOException {\n+    table.putWithBatch(batch, key, value);\n+  }\n+\n+  @Override\n+  public boolean isEmpty() throws IOException {\n+    return table.isEmpty();\n+  }\n+\n+  @Override\n+  public void delete(KEY key) throws IOException {\n+    table.delete(key);\n+  }\n+\n+  @Override\n+  public void deleteWithBatch(BatchOperation batch, KEY key)\n+          throws IOException {\n+    table.deleteWithBatch(batch, key);\n+  }\n+\n+  @Override\n+  public final TableIterator<KEY, ? extends KeyValue<KEY, VALUE>> iterator() {\n+    throw new UnsupportedOperationException(\"Iterating tables directly is not\" +\n+            \" supported for datanode containers due to differing schema \" +\n+            \"version.\");\n+  }\n+\n+  @Override\n+  public String getName() throws IOException {\n+    return table.getName();\n+  }\n+\n+  @Override\n+  public long getEstimatedKeyCount() throws IOException {\n+    return table.getEstimatedKeyCount();\n+  }\n+\n+  @Override\n+  public void addCacheEntry(CacheKey<KEY> cacheKey,\n+                            CacheValue<VALUE> cacheValue) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI3NjIxOA==", "bodyText": "I believe table cache is only used in OM. I will remove this to default to Not Implemented as defined in the Table interface.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482276218", "createdAt": "2020-09-02T18:22:06Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/DatanodeTable.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.BatchOperation;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheKey;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheValue;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Wrapper class to represent a table in a datanode RocksDB instance.\n+ * This class can wrap any {@link Table} instance, but will throw\n+ * {@link UnsupportedOperationException} for {@link Table#iterator}.\n+ * This is because differing schema versions used in datanode DB layouts may\n+ * have differing underlying table structures, so iterating a table instance\n+ * directly, without taking into account key prefixes, may yield unexpected\n+ * results.\n+ */\n+public class DatanodeTable<KEY, VALUE> implements Table<KEY, VALUE> {\n+\n+  private final Table<KEY, VALUE> table;\n+\n+  public DatanodeTable(Table<KEY, VALUE> table) {\n+    this.table = table;\n+  }\n+\n+  @Override\n+  public void put(KEY key, VALUE value) throws IOException {\n+    table.put(key, value);\n+  }\n+\n+  @Override\n+  public void putWithBatch(BatchOperation batch, KEY key,\n+                           VALUE value) throws IOException {\n+    table.putWithBatch(batch, key, value);\n+  }\n+\n+  @Override\n+  public boolean isEmpty() throws IOException {\n+    return table.isEmpty();\n+  }\n+\n+  @Override\n+  public void delete(KEY key) throws IOException {\n+    table.delete(key);\n+  }\n+\n+  @Override\n+  public void deleteWithBatch(BatchOperation batch, KEY key)\n+          throws IOException {\n+    table.deleteWithBatch(batch, key);\n+  }\n+\n+  @Override\n+  public final TableIterator<KEY, ? extends KeyValue<KEY, VALUE>> iterator() {\n+    throw new UnsupportedOperationException(\"Iterating tables directly is not\" +\n+            \" supported for datanode containers due to differing schema \" +\n+            \"version.\");\n+  }\n+\n+  @Override\n+  public String getName() throws IOException {\n+    return table.getName();\n+  }\n+\n+  @Override\n+  public long getEstimatedKeyCount() throws IOException {\n+    return table.getEstimatedKeyCount();\n+  }\n+\n+  @Override\n+  public void addCacheEntry(CacheKey<KEY> cacheKey,\n+                            CacheValue<VALUE> cacheValue) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzcxMzc2Mg=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4NjY0NzI4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/DatanodeTable.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwMDowNToyM1rOHHmOkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMToyNToxMFrOHOui0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzcyODQwMA==", "bodyText": "From what I could understand, the underlying table structure is TypedTabled. Is that correct?\nIf yes, should DatanodeTable extend TypedTable instead of Table<KEY, VALUE> so that it does not have to override all the methods?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r477728400", "createdAt": "2020-08-27T00:05:23Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/DatanodeTable.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.BatchOperation;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheKey;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheValue;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Wrapper class to represent a table in a datanode RocksDB instance.\n+ * This class can wrap any {@link Table} instance, but will throw\n+ * {@link UnsupportedOperationException} for {@link Table#iterator}.\n+ * This is because differing schema versions used in datanode DB layouts may\n+ * have differing underlying table structures, so iterating a table instance\n+ * directly, without taking into account key prefixes, may yield unexpected\n+ * results.\n+ */\n+public class DatanodeTable<KEY, VALUE> implements Table<KEY, VALUE> {\n+\n+  private final Table<KEY, VALUE> table;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTIwNDY4OQ==", "bodyText": "This would be nice, but unfortunately the AbstractDatanodeStore that is creating the DatanodeTables to return is getting pre-constructed instances of the Table interface from the DBColumnFamilyDefinition class. AbstractDatanodeStore must then convert these to DatanodeTables.\nLets say we make DatanodeTable extend TypedTable. Since AbstractDatanodeStore is given an existing instance that is not a DatanodeTable, creating a DatanodeTable from this requires assuming the original table is a TypedTable (which is currently true but may not hold in the future), casting it, and somehow copying all its internal state over to a new DatanodeTable.\nAlthough using composition here requires a lot of wrapper methods that just call into the composed object, it is ultimately a cleaner approach because it allows creating a DatanodeTable from any existing Table implementation.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r485204689", "createdAt": "2020-09-08T21:25:10Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/DatanodeTable.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.BatchOperation;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheKey;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheValue;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Wrapper class to represent a table in a datanode RocksDB instance.\n+ * This class can wrap any {@link Table} instance, but will throw\n+ * {@link UnsupportedOperationException} for {@link Table#iterator}.\n+ * This is because differing schema versions used in datanode DB layouts may\n+ * have differing underlying table structures, so iterating a table instance\n+ * directly, without taking into account key prefixes, may yield unexpected\n+ * results.\n+ */\n+public class DatanodeTable<KEY, VALUE> implements Table<KEY, VALUE> {\n+\n+  private final Table<KEY, VALUE> table;\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzcyODQwMA=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzU3NTc4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/SchemaOneChunkInfoListCodec.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxOTo1Mzo0MlrOHJSuvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxODoyNzo0OFrOHL8ItQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwNjEwOA==", "bodyText": "Shouldn't Schema 1 DBs store the deleted blocks in the old format?\nIIUC, if SchemaOneChunkInfoListCodec implements the Block ID codec, then the InvalidProtocolBufferException can be avoided.\nPlease let me know if there is any other reason for implementing it this way.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479506108", "createdAt": "2020-08-28T19:53:42Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/SchemaOneChunkInfoListCodec.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.\u2002\u2002See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.\u2002\u2002The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.\u2002\u2002You may obtain a copy of the License at\n+ *\n+ * \u2002\u2002\u2002\u2002 http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos;\n+import org.apache.hadoop.hdds.utils.db.Codec;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+import org.apache.ratis.thirdparty.com.google.protobuf.InvalidProtocolBufferException;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Codec for parsing {@link ContainerProtos.ChunkInfoList} objects from data\n+ * that may have been written using schema version one. Before upgrading\n+ * schema versions, deleted block IDs were stored with a duplicate copy of\n+ * their ID as the value in the database. After upgrading the code, any\n+ * deletes that happen on the DB will save the chunk information with the\n+ * deleted blocks instead, even if those deletes are performed on a database\n+ * created with schema version one.\n+ * <p>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI4MTY1Mw==", "bodyText": "Because tables are now strongly typed, reads and writes to a table must have the same type, regardless of schema version. Having a different type for schema v1 and v2 tables will force callers to know which schema version they are working with, which we do not want. Therefore, since the value type of deleted block data was changed, all new reads and writes must use this type. This means the data written with the old schema version can no longer be read, but since it was just a duplicate of the key value (effectively a placeholder), this is not an issue.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482281653", "createdAt": "2020-09-02T18:27:48Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/SchemaOneChunkInfoListCodec.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.\u2002\u2002See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.\u2002\u2002The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.\u2002\u2002You may obtain a copy of the License at\n+ *\n+ * \u2002\u2002\u2002\u2002 http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos;\n+import org.apache.hadoop.hdds.utils.db.Codec;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+import org.apache.ratis.thirdparty.com.google.protobuf.InvalidProtocolBufferException;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Codec for parsing {@link ContainerProtos.ChunkInfoList} objects from data\n+ * that may have been written using schema version one. Before upgrading\n+ * schema versions, deleted block IDs were stored with a duplicate copy of\n+ * their ID as the value in the database. After upgrading the code, any\n+ * deletes that happen on the DB will save the chunk information with the\n+ * deleted blocks instead, even if those deletes are performed on a database\n+ * created with schema version one.\n+ * <p>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwNjEwOA=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzU5MDc1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/SchemaOneDeletedBlocksTable.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxOTo1OToxM1rOHJS3tA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMDo1ODowM1rOHOtyKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwODQwNA==", "bodyText": "This would mean someone iterating through all the deleted blocks in a DN might get a mixture of keys with and without the deleted key prefix.\nIs there such a scenario in the code currently?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479508404", "createdAt": "2020-08-28T19:59:13Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/SchemaOneDeletedBlocksTable.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.BatchOperation;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheKey;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheValue;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * For RocksDB instances written using DB schema version 1, all data is\n+ * stored in the default column family. This differs from later schema\n+ * versions, which put deleted blocks in a different column family.\n+ * As a result, the block IDs used as keys for deleted blocks must be\n+ * prefixed in schema version 1 so that they can be differentiated from\n+ * regular blocks. However, these prefixes are not necessary in later schema\n+ * versions, because the deleted blocks and regular blocks are in different\n+ * column families.\n+ * <p>\n+ * Since clients must operate independently of the underlying schema version,\n+ * This class is returned to clients using {@link DatanodeStoreSchemaOneImpl}\n+ * instances, allowing them to access keys as if no prefix is\n+ * required, while it adds the prefix when necessary.\n+ * This means the client should omit the deleted prefix when putting and\n+ * getting keys, regardless of the schema version.\n+ * <p>\n+ * Note that this class will only apply prefixes to keys as parameters,\n+ * never as return types. This means that keys returned through iterators\n+ * like {@link SchemaOneDeletedBlocksTable#getSequentialRangeKVs}, and\n+ * {@link SchemaOneDeletedBlocksTable#getRangeKVs} will return keys prefixed\n+ * with {@link SchemaOneDeletedBlocksTable#DELETED_KEY_PREFIX}.\n+ */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE5MjIzNA==", "bodyText": "No such scenario that I am aware of, but I agree that this implementation is not ideal. I will change these methods to get the list from the parent class, and then make a pass through it to remove the prefixes before returning the list to the caller. This implementation would be similar to how TypedTable currently applies typing to iterator results returned from its raw table.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r485192234", "createdAt": "2020-09-08T20:58:03Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/SchemaOneDeletedBlocksTable.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.BatchOperation;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheKey;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheValue;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * For RocksDB instances written using DB schema version 1, all data is\n+ * stored in the default column family. This differs from later schema\n+ * versions, which put deleted blocks in a different column family.\n+ * As a result, the block IDs used as keys for deleted blocks must be\n+ * prefixed in schema version 1 so that they can be differentiated from\n+ * regular blocks. However, these prefixes are not necessary in later schema\n+ * versions, because the deleted blocks and regular blocks are in different\n+ * column families.\n+ * <p>\n+ * Since clients must operate independently of the underlying schema version,\n+ * This class is returned to clients using {@link DatanodeStoreSchemaOneImpl}\n+ * instances, allowing them to access keys as if no prefix is\n+ * required, while it adds the prefix when necessary.\n+ * This means the client should omit the deleted prefix when putting and\n+ * getting keys, regardless of the schema version.\n+ * <p>\n+ * Note that this class will only apply prefixes to keys as parameters,\n+ * never as return types. This means that keys returned through iterators\n+ * like {@link SchemaOneDeletedBlocksTable#getSequentialRangeKVs}, and\n+ * {@link SchemaOneDeletedBlocksTable#getRangeKVs} will return keys prefixed\n+ * with {@link SchemaOneDeletedBlocksTable#DELETED_KEY_PREFIX}.\n+ */", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwODQwNA=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzY4NzgyOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestBlockDeletingService.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMDozNjo1NlrOHJTyJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxODoyOToyOVrOHL8Oyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUyMzM2NA==", "bodyText": "This test is dependent on the order of deleted blocks. Does RocksDB iterator ensure that the deleting and deleted block keys are returned in the same sorted order?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479523364", "createdAt": "2020-08-28T20:36:56Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestBlockDeletingService.java", "diffHunk": "@@ -471,4 +470,59 @@ public void testBlockThrottle() throws Exception {\n       service.shutdown();\n     }\n   }\n+\n+  @Test\n+  public void testDeletedChunkInfo() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    conf.setInt(OZONE_BLOCK_DELETING_CONTAINER_LIMIT_PER_INTERVAL, 10);\n+    conf.setInt(OZONE_BLOCK_DELETING_LIMIT_PER_CONTAINER, 2);\n+    ContainerSet containerSet = new ContainerSet();\n+    createToDeleteBlocks(containerSet, conf, 1, 2, 3);\n+\n+    List<ContainerData> containerData = Lists.newArrayList();\n+    containerSet.listContainer(0L, 1, containerData);\n+\n+    try(ReferenceCountedDB meta = BlockUtils.getDB(\n+            (KeyValueContainerData) containerData.get(0), conf)) {\n+\n+      // Collect all ChunkInfo from blocks marked for deletion.\n+      List<? extends Table.KeyValue<String, BlockData>> deletingBlocks =\n+              meta.getStore().getBlockDataTable()\n+              .getRangeKVs(null, 100,\n+                      MetadataKeyFilters.getDeletingKeyFilter());\n+\n+      // Delete all blocks marked for deletion.\n+      BlockDeletingServiceTestImpl svc =\n+              getBlockDeletingService(containerSet, conf);\n+      svc.start();\n+      GenericTestUtils.waitFor(svc::isStarted, 100, 3000);\n+      deleteAndWait(svc, 1);\n+      svc.shutdown();\n+\n+      // Get deleted blocks from their table, and check their ChunkInfo lists\n+      // against those we saved for them before deletion.\n+      List<? extends Table.KeyValue<String, ChunkInfoList>> deletedBlocks =\n+              meta.getStore().getDeletedBlocksTable()\n+              .getRangeKVs(null, 100);\n+\n+      Assert.assertEquals(deletingBlocks.size(), deletedBlocks.size());\n+\n+      Iterator<? extends Table.KeyValue<String, BlockData>>\n+              deletingBlocksIter = deletingBlocks.iterator();\n+      Iterator<? extends Table.KeyValue<String, ChunkInfoList>>\n+              deletedBlocksIter = deletedBlocks.iterator();\n+\n+      while(deletingBlocksIter.hasNext() && deletedBlocksIter.hasNext())  {\n+        List<ContainerProtos.ChunkInfo> deletingChunks =\n+                deletingBlocksIter.next().getValue().getChunks();\n+        List<ContainerProtos.ChunkInfo> deletedChunks =\n+                deletedBlocksIter.next().getValue().asList();\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI4MzIxMQ==", "bodyText": "Yes, the iterator will return blocks in sorted order. The existing code already depends on this property in the existing test cases found in TestKeyValueBlockIterator.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482283211", "createdAt": "2020-09-02T18:29:29Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestBlockDeletingService.java", "diffHunk": "@@ -471,4 +470,59 @@ public void testBlockThrottle() throws Exception {\n       service.shutdown();\n     }\n   }\n+\n+  @Test\n+  public void testDeletedChunkInfo() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    conf.setInt(OZONE_BLOCK_DELETING_CONTAINER_LIMIT_PER_INTERVAL, 10);\n+    conf.setInt(OZONE_BLOCK_DELETING_LIMIT_PER_CONTAINER, 2);\n+    ContainerSet containerSet = new ContainerSet();\n+    createToDeleteBlocks(containerSet, conf, 1, 2, 3);\n+\n+    List<ContainerData> containerData = Lists.newArrayList();\n+    containerSet.listContainer(0L, 1, containerData);\n+\n+    try(ReferenceCountedDB meta = BlockUtils.getDB(\n+            (KeyValueContainerData) containerData.get(0), conf)) {\n+\n+      // Collect all ChunkInfo from blocks marked for deletion.\n+      List<? extends Table.KeyValue<String, BlockData>> deletingBlocks =\n+              meta.getStore().getBlockDataTable()\n+              .getRangeKVs(null, 100,\n+                      MetadataKeyFilters.getDeletingKeyFilter());\n+\n+      // Delete all blocks marked for deletion.\n+      BlockDeletingServiceTestImpl svc =\n+              getBlockDeletingService(containerSet, conf);\n+      svc.start();\n+      GenericTestUtils.waitFor(svc::isStarted, 100, 3000);\n+      deleteAndWait(svc, 1);\n+      svc.shutdown();\n+\n+      // Get deleted blocks from their table, and check their ChunkInfo lists\n+      // against those we saved for them before deletion.\n+      List<? extends Table.KeyValue<String, ChunkInfoList>> deletedBlocks =\n+              meta.getStore().getDeletedBlocksTable()\n+              .getRangeKVs(null, 100);\n+\n+      Assert.assertEquals(deletingBlocks.size(), deletedBlocks.size());\n+\n+      Iterator<? extends Table.KeyValue<String, BlockData>>\n+              deletingBlocksIter = deletingBlocks.iterator();\n+      Iterator<? extends Table.KeyValue<String, ChunkInfoList>>\n+              deletedBlocksIter = deletedBlocks.iterator();\n+\n+      while(deletingBlocksIter.hasNext() && deletedBlocksIter.hasNext())  {\n+        List<ContainerProtos.ChunkInfo> deletingChunks =\n+                deletingBlocksIter.next().getValue().getChunks();\n+        List<ContainerProtos.ChunkInfo> deletedChunks =\n+                deletedBlocksIter.next().getValue().asList();\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUyMzM2NA=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 177}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5Nzc4NTIxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestContainerCache.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMToxNzowMFrOHJUsxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMTowNTo0OVrOHOuAaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzODM3NQ==", "bodyText": "Nitpick: It would be good to have a method in ContainerCache which calls the current getDB with Schema_Latest. We can avoid specifying the schema version every time then.\npublic ReferenceCountedDB getDB(long containerID, String containerDBType, String containerDBPath,\n    ConfigurationSource conf) {\n    getDB(containerID, containerDBType, OzoneConsts.SCHEMA_LATEST, conf);\n}", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479538375", "createdAt": "2020-08-28T21:17:00Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestContainerCache.java", "diffHunk": "@@ -85,17 +86,17 @@ public void testContainerCacheEviction() throws Exception {\n \n     // Get 2 references out of the same db and verify the objects are same.\n     ReferenceCountedDB db1 = cache.getDB(1, \"RocksDB\",\n-        containerDir1.getPath(), conf);\n+            containerDir1.getPath(), OzoneConsts.SCHEMA_LATEST, conf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE5NTg4MQ==", "bodyText": "This could be done, but it may not be clear to the caller that the latest schema version would be applied. Since there is no mention of it in the method name, the caller may assume that the schema version would be automatically determined based on some property of the DB. Aside from this series of calls in one test, the method is only called once in the code, so I think it is OK to leave it the way it is for clarity.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r485195881", "createdAt": "2020-09-08T21:05:49Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestContainerCache.java", "diffHunk": "@@ -85,17 +86,17 @@ public void testContainerCacheEviction() throws Exception {\n \n     // Get 2 references out of the same db and verify the objects are same.\n     ReferenceCountedDB db1 = cache.getDB(1, \"RocksDB\",\n-        containerDir1.getPath(), conf);\n+            containerDir1.getPath(), OzoneConsts.SCHEMA_LATEST, conf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzODM3NQ=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5Nzg0NDE5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestSchemaOneBackwardsCompatibility.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMTo0MzoyMVrOHJVPiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMToxMDo0MlrOHOuJOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NzI3Mg==", "bodyText": "Shouldn't Bytes used be decreased by the deleted blocks size?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479547272", "createdAt": "2020-08-28T21:43:21Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestSchemaOneBackwardsCompatibility.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.common;\n+\n+import org.apache.commons.io.FileUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.container.ContainerTestHelper;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+import org.apache.hadoop.ozone.container.common.impl.ChunkLayOutVersion;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerDataYaml;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n+import org.apache.hadoop.ozone.container.common.interfaces.ContainerDispatcher;\n+import org.apache.hadoop.ozone.container.common.utils.ReferenceCountedDB;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainerData;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil;\n+import org.apache.hadoop.ozone.container.metadata.DatanodeStore;\n+import org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer;\n+import org.apache.hadoop.ozone.container.testutils.BlockDeletingServiceTestImpl;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.yaml.snakeyaml.Yaml;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.*;\n+\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_CONTAINER_LIMIT_PER_INTERVAL;\n+import static org.junit.Assert.*;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Tests processing of containers written with DB schema version 1,\n+ * which stores all its data in the default RocksDB column family.\n+ * Newer schema version will use a different column family layout, but they\n+ * should still be able to read, delete data, and update metadata for schema\n+ * version 1 containers.\n+ * <p>\n+ * The functionality executed by these tests assumes that all containers will\n+ * have to be closed before an upgrade, meaning that containers written with\n+ * schema version 1 will only ever be encountered in their closed state.\n+ * <p>\n+ * For an example of a RocksDB instance written with schema version 1, see\n+ * {@link TestDB}, which is used by these tests to load a pre created schema\n+ * version 1 RocksDB instance from test resources.\n+ */\n+public class TestSchemaOneBackwardsCompatibility {\n+  private OzoneConfiguration conf;\n+\n+  private File metadataDir;\n+  private File dbFile;\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+\n+  @Before\n+  public void setup() throws Exception {\n+    conf = new OzoneConfiguration();\n+    TestDB testDB = new TestDB();\n+\n+    // Copy data to the temporary folder so it can be safely modified.\n+    File tempMetadataDir =\n+            tempFolder.newFolder(Long.toString(TestDB.CONTAINER_ID),\n+                    OzoneConsts.CONTAINER_META_PATH);\n+\n+    FileUtils.copyDirectoryToDirectory(testDB.getDBDirectory(),\n+            tempMetadataDir);\n+    FileUtils.copyFileToDirectory(testDB.getContainerFile(), tempMetadataDir);\n+\n+    metadataDir = tempMetadataDir;\n+    File[] potentialDBFiles = metadataDir.listFiles((dir, name) ->\n+            name.equals(TestDB.DB_NAME));\n+\n+    if (potentialDBFiles == null || potentialDBFiles.length != 1) {\n+      throw new IOException(\"Failed load file named \" + TestDB.DB_NAME + \" \" +\n+              \"from the metadata directory \" + metadataDir.getAbsolutePath());\n+    }\n+\n+    dbFile = potentialDBFiles[0];\n+  }\n+\n+  /**\n+   * Because all tables in schema version one map back to the default table,\n+   * directly iterating any of the table instances should be forbidden.\n+   * Otherwise, the iterators for each table would read the entire default\n+   * table, return all database contents, and yield unexpected results.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDirectTableIterationDisabled() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      DatanodeStore store = refCountedDB.getStore();\n+\n+      assertTableIteratorUnsupported(store.getMetadataTable());\n+      assertTableIteratorUnsupported(store.getBlockDataTable());\n+      assertTableIteratorUnsupported(store.getDeletedBlocksTable());\n+    }\n+  }\n+\n+  private void assertTableIteratorUnsupported(Table<?, ?> table) {\n+    try {\n+      table.iterator();\n+      Assert.fail(\"Table iterator should have thrown \" +\n+              \"UnsupportedOperationException.\");\n+    } catch (UnsupportedOperationException ex) {\n+      // Exception thrown as expected.\n+    }\n+  }\n+\n+  /**\n+   * Counts the number of deleted, pending delete, and regular blocks in the\n+   * database, and checks that they match the expected values.\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testBlockIteration() throws IOException {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      assertEquals(TestDB.NUM_DELETED_BLOCKS, countDeletedBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countDeletingBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.KEY_COUNT - TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countUnprefixedBlocks(refCountedDB));\n+    }\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has metadata keys present.\n+   * The {@link KeyValueContainerUtil} will read these values to fill in a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithMetadata() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has no metadata keys present.\n+   * The {@link KeyValueContainerUtil} will scan the blocks in the database\n+   * to fill these metadata values into the database and into a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithoutMetadata() throws Exception {\n+    // Init the kvData enough values so we can get the database to modify for\n+    // testing and then read.\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    // Delete metadata keys from our copy of the DB.\n+    // This simulates them not being there to start with.\n+    try (ReferenceCountedDB db = BlockUtils.getDB(kvData, conf)) {\n+      Table<String, Long> metadataTable = db.getStore().getMetadataTable();\n+\n+      metadataTable.delete(OzoneConsts.BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+\n+      metadataTable.delete(OzoneConsts.CONTAINER_BYTES_USED);\n+      assertNull(metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));\n+\n+      metadataTable.delete(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT));\n+    }\n+\n+    // Create a new container data object, and fill in its metadata by\n+    // counting blocks from the database, since the metadata keys in the\n+    // database are now gone.\n+    kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading blocks marked for deletion from a container written in\n+   * schema version 1. Because the block deleting service both reads for\n+   * deleted blocks and deletes them, this test will modify its copy of the\n+   * database.\n+   */\n+  @Test\n+  public void testDelete() throws Exception {\n+    final long numBlocksToDelete = TestDB.NUM_PENDING_DELETION_BLOCKS;\n+\n+    runBlockDeletingService();\n+\n+    // Expected values after blocks with #deleting# prefix in original DB are\n+    // deleted.\n+    final long expectedDeletingBlocks =\n+            TestDB.NUM_PENDING_DELETION_BLOCKS - numBlocksToDelete;\n+    final long expectedDeletedBlocks =\n+            TestDB.NUM_DELETED_BLOCKS + numBlocksToDelete;\n+    final long expectedRegularBlocks =\n+            TestDB.KEY_COUNT - numBlocksToDelete;\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      // Test results via block iteration.\n+      assertEquals(expectedDeletingBlocks,\n+              countDeletingBlocks(refCountedDB));\n+      assertEquals(expectedDeletedBlocks,\n+              countDeletedBlocks(refCountedDB));\n+      assertEquals(expectedRegularBlocks,\n+              countUnprefixedBlocks(refCountedDB));\n+\n+      // Test table metadata.\n+      Table<String, Long> metadataTable =\n+              refCountedDB.getStore().getMetadataTable();\n+      assertEquals(expectedRegularBlocks + expectedDeletingBlocks,\n+              (long)metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+      assertEquals(TestDB.BYTES_USED,\n+              (long)metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 252}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI4NTYyNw==", "bodyText": "Yes, I think it should. I will investigate why this passes because it does look incorrect.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482285627", "createdAt": "2020-09-02T18:32:04Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestSchemaOneBackwardsCompatibility.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.common;\n+\n+import org.apache.commons.io.FileUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.container.ContainerTestHelper;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+import org.apache.hadoop.ozone.container.common.impl.ChunkLayOutVersion;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerDataYaml;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n+import org.apache.hadoop.ozone.container.common.interfaces.ContainerDispatcher;\n+import org.apache.hadoop.ozone.container.common.utils.ReferenceCountedDB;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainerData;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil;\n+import org.apache.hadoop.ozone.container.metadata.DatanodeStore;\n+import org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer;\n+import org.apache.hadoop.ozone.container.testutils.BlockDeletingServiceTestImpl;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.yaml.snakeyaml.Yaml;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.*;\n+\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_CONTAINER_LIMIT_PER_INTERVAL;\n+import static org.junit.Assert.*;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Tests processing of containers written with DB schema version 1,\n+ * which stores all its data in the default RocksDB column family.\n+ * Newer schema version will use a different column family layout, but they\n+ * should still be able to read, delete data, and update metadata for schema\n+ * version 1 containers.\n+ * <p>\n+ * The functionality executed by these tests assumes that all containers will\n+ * have to be closed before an upgrade, meaning that containers written with\n+ * schema version 1 will only ever be encountered in their closed state.\n+ * <p>\n+ * For an example of a RocksDB instance written with schema version 1, see\n+ * {@link TestDB}, which is used by these tests to load a pre created schema\n+ * version 1 RocksDB instance from test resources.\n+ */\n+public class TestSchemaOneBackwardsCompatibility {\n+  private OzoneConfiguration conf;\n+\n+  private File metadataDir;\n+  private File dbFile;\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+\n+  @Before\n+  public void setup() throws Exception {\n+    conf = new OzoneConfiguration();\n+    TestDB testDB = new TestDB();\n+\n+    // Copy data to the temporary folder so it can be safely modified.\n+    File tempMetadataDir =\n+            tempFolder.newFolder(Long.toString(TestDB.CONTAINER_ID),\n+                    OzoneConsts.CONTAINER_META_PATH);\n+\n+    FileUtils.copyDirectoryToDirectory(testDB.getDBDirectory(),\n+            tempMetadataDir);\n+    FileUtils.copyFileToDirectory(testDB.getContainerFile(), tempMetadataDir);\n+\n+    metadataDir = tempMetadataDir;\n+    File[] potentialDBFiles = metadataDir.listFiles((dir, name) ->\n+            name.equals(TestDB.DB_NAME));\n+\n+    if (potentialDBFiles == null || potentialDBFiles.length != 1) {\n+      throw new IOException(\"Failed load file named \" + TestDB.DB_NAME + \" \" +\n+              \"from the metadata directory \" + metadataDir.getAbsolutePath());\n+    }\n+\n+    dbFile = potentialDBFiles[0];\n+  }\n+\n+  /**\n+   * Because all tables in schema version one map back to the default table,\n+   * directly iterating any of the table instances should be forbidden.\n+   * Otherwise, the iterators for each table would read the entire default\n+   * table, return all database contents, and yield unexpected results.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDirectTableIterationDisabled() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      DatanodeStore store = refCountedDB.getStore();\n+\n+      assertTableIteratorUnsupported(store.getMetadataTable());\n+      assertTableIteratorUnsupported(store.getBlockDataTable());\n+      assertTableIteratorUnsupported(store.getDeletedBlocksTable());\n+    }\n+  }\n+\n+  private void assertTableIteratorUnsupported(Table<?, ?> table) {\n+    try {\n+      table.iterator();\n+      Assert.fail(\"Table iterator should have thrown \" +\n+              \"UnsupportedOperationException.\");\n+    } catch (UnsupportedOperationException ex) {\n+      // Exception thrown as expected.\n+    }\n+  }\n+\n+  /**\n+   * Counts the number of deleted, pending delete, and regular blocks in the\n+   * database, and checks that they match the expected values.\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testBlockIteration() throws IOException {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      assertEquals(TestDB.NUM_DELETED_BLOCKS, countDeletedBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countDeletingBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.KEY_COUNT - TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countUnprefixedBlocks(refCountedDB));\n+    }\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has metadata keys present.\n+   * The {@link KeyValueContainerUtil} will read these values to fill in a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithMetadata() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has no metadata keys present.\n+   * The {@link KeyValueContainerUtil} will scan the blocks in the database\n+   * to fill these metadata values into the database and into a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithoutMetadata() throws Exception {\n+    // Init the kvData enough values so we can get the database to modify for\n+    // testing and then read.\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    // Delete metadata keys from our copy of the DB.\n+    // This simulates them not being there to start with.\n+    try (ReferenceCountedDB db = BlockUtils.getDB(kvData, conf)) {\n+      Table<String, Long> metadataTable = db.getStore().getMetadataTable();\n+\n+      metadataTable.delete(OzoneConsts.BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+\n+      metadataTable.delete(OzoneConsts.CONTAINER_BYTES_USED);\n+      assertNull(metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));\n+\n+      metadataTable.delete(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT));\n+    }\n+\n+    // Create a new container data object, and fill in its metadata by\n+    // counting blocks from the database, since the metadata keys in the\n+    // database are now gone.\n+    kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading blocks marked for deletion from a container written in\n+   * schema version 1. Because the block deleting service both reads for\n+   * deleted blocks and deletes them, this test will modify its copy of the\n+   * database.\n+   */\n+  @Test\n+  public void testDelete() throws Exception {\n+    final long numBlocksToDelete = TestDB.NUM_PENDING_DELETION_BLOCKS;\n+\n+    runBlockDeletingService();\n+\n+    // Expected values after blocks with #deleting# prefix in original DB are\n+    // deleted.\n+    final long expectedDeletingBlocks =\n+            TestDB.NUM_PENDING_DELETION_BLOCKS - numBlocksToDelete;\n+    final long expectedDeletedBlocks =\n+            TestDB.NUM_DELETED_BLOCKS + numBlocksToDelete;\n+    final long expectedRegularBlocks =\n+            TestDB.KEY_COUNT - numBlocksToDelete;\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      // Test results via block iteration.\n+      assertEquals(expectedDeletingBlocks,\n+              countDeletingBlocks(refCountedDB));\n+      assertEquals(expectedDeletedBlocks,\n+              countDeletedBlocks(refCountedDB));\n+      assertEquals(expectedRegularBlocks,\n+              countUnprefixedBlocks(refCountedDB));\n+\n+      // Test table metadata.\n+      Table<String, Long> metadataTable =\n+              refCountedDB.getStore().getMetadataTable();\n+      assertEquals(expectedRegularBlocks + expectedDeletingBlocks,\n+              (long)metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+      assertEquals(TestDB.BYTES_USED,\n+              (long)metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NzI3Mg=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 252}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTE5ODEzOA==", "bodyText": "It turns out the bytes used is not updated due to the mock objects used to create the block deleting service for tests. This is actually consistent with the way the existing TestBlockDeletingService unit tests work, and there is a comment in those tests noting that the bytes used will not be updated. I will follow the lead of TestBlockDeletingService and replace this assert with a comment explaining this.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r485198138", "createdAt": "2020-09-08T21:10:42Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestSchemaOneBackwardsCompatibility.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.common;\n+\n+import org.apache.commons.io.FileUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.container.ContainerTestHelper;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+import org.apache.hadoop.ozone.container.common.impl.ChunkLayOutVersion;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerDataYaml;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n+import org.apache.hadoop.ozone.container.common.interfaces.ContainerDispatcher;\n+import org.apache.hadoop.ozone.container.common.utils.ReferenceCountedDB;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainerData;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil;\n+import org.apache.hadoop.ozone.container.metadata.DatanodeStore;\n+import org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer;\n+import org.apache.hadoop.ozone.container.testutils.BlockDeletingServiceTestImpl;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.yaml.snakeyaml.Yaml;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.*;\n+\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_CONTAINER_LIMIT_PER_INTERVAL;\n+import static org.junit.Assert.*;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Tests processing of containers written with DB schema version 1,\n+ * which stores all its data in the default RocksDB column family.\n+ * Newer schema version will use a different column family layout, but they\n+ * should still be able to read, delete data, and update metadata for schema\n+ * version 1 containers.\n+ * <p>\n+ * The functionality executed by these tests assumes that all containers will\n+ * have to be closed before an upgrade, meaning that containers written with\n+ * schema version 1 will only ever be encountered in their closed state.\n+ * <p>\n+ * For an example of a RocksDB instance written with schema version 1, see\n+ * {@link TestDB}, which is used by these tests to load a pre created schema\n+ * version 1 RocksDB instance from test resources.\n+ */\n+public class TestSchemaOneBackwardsCompatibility {\n+  private OzoneConfiguration conf;\n+\n+  private File metadataDir;\n+  private File dbFile;\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+\n+  @Before\n+  public void setup() throws Exception {\n+    conf = new OzoneConfiguration();\n+    TestDB testDB = new TestDB();\n+\n+    // Copy data to the temporary folder so it can be safely modified.\n+    File tempMetadataDir =\n+            tempFolder.newFolder(Long.toString(TestDB.CONTAINER_ID),\n+                    OzoneConsts.CONTAINER_META_PATH);\n+\n+    FileUtils.copyDirectoryToDirectory(testDB.getDBDirectory(),\n+            tempMetadataDir);\n+    FileUtils.copyFileToDirectory(testDB.getContainerFile(), tempMetadataDir);\n+\n+    metadataDir = tempMetadataDir;\n+    File[] potentialDBFiles = metadataDir.listFiles((dir, name) ->\n+            name.equals(TestDB.DB_NAME));\n+\n+    if (potentialDBFiles == null || potentialDBFiles.length != 1) {\n+      throw new IOException(\"Failed load file named \" + TestDB.DB_NAME + \" \" +\n+              \"from the metadata directory \" + metadataDir.getAbsolutePath());\n+    }\n+\n+    dbFile = potentialDBFiles[0];\n+  }\n+\n+  /**\n+   * Because all tables in schema version one map back to the default table,\n+   * directly iterating any of the table instances should be forbidden.\n+   * Otherwise, the iterators for each table would read the entire default\n+   * table, return all database contents, and yield unexpected results.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDirectTableIterationDisabled() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      DatanodeStore store = refCountedDB.getStore();\n+\n+      assertTableIteratorUnsupported(store.getMetadataTable());\n+      assertTableIteratorUnsupported(store.getBlockDataTable());\n+      assertTableIteratorUnsupported(store.getDeletedBlocksTable());\n+    }\n+  }\n+\n+  private void assertTableIteratorUnsupported(Table<?, ?> table) {\n+    try {\n+      table.iterator();\n+      Assert.fail(\"Table iterator should have thrown \" +\n+              \"UnsupportedOperationException.\");\n+    } catch (UnsupportedOperationException ex) {\n+      // Exception thrown as expected.\n+    }\n+  }\n+\n+  /**\n+   * Counts the number of deleted, pending delete, and regular blocks in the\n+   * database, and checks that they match the expected values.\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testBlockIteration() throws IOException {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      assertEquals(TestDB.NUM_DELETED_BLOCKS, countDeletedBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countDeletingBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.KEY_COUNT - TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countUnprefixedBlocks(refCountedDB));\n+    }\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has metadata keys present.\n+   * The {@link KeyValueContainerUtil} will read these values to fill in a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithMetadata() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has no metadata keys present.\n+   * The {@link KeyValueContainerUtil} will scan the blocks in the database\n+   * to fill these metadata values into the database and into a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithoutMetadata() throws Exception {\n+    // Init the kvData enough values so we can get the database to modify for\n+    // testing and then read.\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    // Delete metadata keys from our copy of the DB.\n+    // This simulates them not being there to start with.\n+    try (ReferenceCountedDB db = BlockUtils.getDB(kvData, conf)) {\n+      Table<String, Long> metadataTable = db.getStore().getMetadataTable();\n+\n+      metadataTable.delete(OzoneConsts.BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+\n+      metadataTable.delete(OzoneConsts.CONTAINER_BYTES_USED);\n+      assertNull(metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));\n+\n+      metadataTable.delete(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT));\n+    }\n+\n+    // Create a new container data object, and fill in its metadata by\n+    // counting blocks from the database, since the metadata keys in the\n+    // database are now gone.\n+    kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading blocks marked for deletion from a container written in\n+   * schema version 1. Because the block deleting service both reads for\n+   * deleted blocks and deletes them, this test will modify its copy of the\n+   * database.\n+   */\n+  @Test\n+  public void testDelete() throws Exception {\n+    final long numBlocksToDelete = TestDB.NUM_PENDING_DELETION_BLOCKS;\n+\n+    runBlockDeletingService();\n+\n+    // Expected values after blocks with #deleting# prefix in original DB are\n+    // deleted.\n+    final long expectedDeletingBlocks =\n+            TestDB.NUM_PENDING_DELETION_BLOCKS - numBlocksToDelete;\n+    final long expectedDeletedBlocks =\n+            TestDB.NUM_DELETED_BLOCKS + numBlocksToDelete;\n+    final long expectedRegularBlocks =\n+            TestDB.KEY_COUNT - numBlocksToDelete;\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      // Test results via block iteration.\n+      assertEquals(expectedDeletingBlocks,\n+              countDeletingBlocks(refCountedDB));\n+      assertEquals(expectedDeletedBlocks,\n+              countDeletedBlocks(refCountedDB));\n+      assertEquals(expectedRegularBlocks,\n+              countUnprefixedBlocks(refCountedDB));\n+\n+      // Test table metadata.\n+      Table<String, Long> metadataTable =\n+              refCountedDB.getStore().getMetadataTable();\n+      assertEquals(expectedRegularBlocks + expectedDeletingBlocks,\n+              (long)metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+      assertEquals(TestDB.BYTES_USED,\n+              (long)metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NzI3Mg=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 252}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5Nzg0NTU0OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestSchemaOneBackwardsCompatibility.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMTo0NDowM1rOHJVQUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxODozMjo0NVrOHL8adw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NzQ3NQ==", "bodyText": "Unused code block.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479547475", "createdAt": "2020-08-28T21:44:03Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestSchemaOneBackwardsCompatibility.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.common;\n+\n+import org.apache.commons.io.FileUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.container.ContainerTestHelper;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+import org.apache.hadoop.ozone.container.common.impl.ChunkLayOutVersion;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerDataYaml;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n+import org.apache.hadoop.ozone.container.common.interfaces.ContainerDispatcher;\n+import org.apache.hadoop.ozone.container.common.utils.ReferenceCountedDB;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainerData;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil;\n+import org.apache.hadoop.ozone.container.metadata.DatanodeStore;\n+import org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer;\n+import org.apache.hadoop.ozone.container.testutils.BlockDeletingServiceTestImpl;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.yaml.snakeyaml.Yaml;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.*;\n+\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_CONTAINER_LIMIT_PER_INTERVAL;\n+import static org.junit.Assert.*;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Tests processing of containers written with DB schema version 1,\n+ * which stores all its data in the default RocksDB column family.\n+ * Newer schema version will use a different column family layout, but they\n+ * should still be able to read, delete data, and update metadata for schema\n+ * version 1 containers.\n+ * <p>\n+ * The functionality executed by these tests assumes that all containers will\n+ * have to be closed before an upgrade, meaning that containers written with\n+ * schema version 1 will only ever be encountered in their closed state.\n+ * <p>\n+ * For an example of a RocksDB instance written with schema version 1, see\n+ * {@link TestDB}, which is used by these tests to load a pre created schema\n+ * version 1 RocksDB instance from test resources.\n+ */\n+public class TestSchemaOneBackwardsCompatibility {\n+  private OzoneConfiguration conf;\n+\n+  private File metadataDir;\n+  private File dbFile;\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+\n+  @Before\n+  public void setup() throws Exception {\n+    conf = new OzoneConfiguration();\n+    TestDB testDB = new TestDB();\n+\n+    // Copy data to the temporary folder so it can be safely modified.\n+    File tempMetadataDir =\n+            tempFolder.newFolder(Long.toString(TestDB.CONTAINER_ID),\n+                    OzoneConsts.CONTAINER_META_PATH);\n+\n+    FileUtils.copyDirectoryToDirectory(testDB.getDBDirectory(),\n+            tempMetadataDir);\n+    FileUtils.copyFileToDirectory(testDB.getContainerFile(), tempMetadataDir);\n+\n+    metadataDir = tempMetadataDir;\n+    File[] potentialDBFiles = metadataDir.listFiles((dir, name) ->\n+            name.equals(TestDB.DB_NAME));\n+\n+    if (potentialDBFiles == null || potentialDBFiles.length != 1) {\n+      throw new IOException(\"Failed load file named \" + TestDB.DB_NAME + \" \" +\n+              \"from the metadata directory \" + metadataDir.getAbsolutePath());\n+    }\n+\n+    dbFile = potentialDBFiles[0];\n+  }\n+\n+  /**\n+   * Because all tables in schema version one map back to the default table,\n+   * directly iterating any of the table instances should be forbidden.\n+   * Otherwise, the iterators for each table would read the entire default\n+   * table, return all database contents, and yield unexpected results.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDirectTableIterationDisabled() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      DatanodeStore store = refCountedDB.getStore();\n+\n+      assertTableIteratorUnsupported(store.getMetadataTable());\n+      assertTableIteratorUnsupported(store.getBlockDataTable());\n+      assertTableIteratorUnsupported(store.getDeletedBlocksTable());\n+    }\n+  }\n+\n+  private void assertTableIteratorUnsupported(Table<?, ?> table) {\n+    try {\n+      table.iterator();\n+      Assert.fail(\"Table iterator should have thrown \" +\n+              \"UnsupportedOperationException.\");\n+    } catch (UnsupportedOperationException ex) {\n+      // Exception thrown as expected.\n+    }\n+  }\n+\n+  /**\n+   * Counts the number of deleted, pending delete, and regular blocks in the\n+   * database, and checks that they match the expected values.\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testBlockIteration() throws IOException {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      assertEquals(TestDB.NUM_DELETED_BLOCKS, countDeletedBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countDeletingBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.KEY_COUNT - TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countUnprefixedBlocks(refCountedDB));\n+    }\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has metadata keys present.\n+   * The {@link KeyValueContainerUtil} will read these values to fill in a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithMetadata() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has no metadata keys present.\n+   * The {@link KeyValueContainerUtil} will scan the blocks in the database\n+   * to fill these metadata values into the database and into a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithoutMetadata() throws Exception {\n+    // Init the kvData enough values so we can get the database to modify for\n+    // testing and then read.\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    // Delete metadata keys from our copy of the DB.\n+    // This simulates them not being there to start with.\n+    try (ReferenceCountedDB db = BlockUtils.getDB(kvData, conf)) {\n+      Table<String, Long> metadataTable = db.getStore().getMetadataTable();\n+\n+      metadataTable.delete(OzoneConsts.BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+\n+      metadataTable.delete(OzoneConsts.CONTAINER_BYTES_USED);\n+      assertNull(metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));\n+\n+      metadataTable.delete(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT));\n+    }\n+\n+    // Create a new container data object, and fill in its metadata by\n+    // counting blocks from the database, since the metadata keys in the\n+    // database are now gone.\n+    kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading blocks marked for deletion from a container written in\n+   * schema version 1. Because the block deleting service both reads for\n+   * deleted blocks and deletes them, this test will modify its copy of the\n+   * database.\n+   */\n+  @Test\n+  public void testDelete() throws Exception {\n+    final long numBlocksToDelete = TestDB.NUM_PENDING_DELETION_BLOCKS;\n+\n+    runBlockDeletingService();\n+\n+    // Expected values after blocks with #deleting# prefix in original DB are\n+    // deleted.\n+    final long expectedDeletingBlocks =\n+            TestDB.NUM_PENDING_DELETION_BLOCKS - numBlocksToDelete;\n+    final long expectedDeletedBlocks =\n+            TestDB.NUM_DELETED_BLOCKS + numBlocksToDelete;\n+    final long expectedRegularBlocks =\n+            TestDB.KEY_COUNT - numBlocksToDelete;\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      // Test results via block iteration.\n+      assertEquals(expectedDeletingBlocks,\n+              countDeletingBlocks(refCountedDB));\n+      assertEquals(expectedDeletedBlocks,\n+              countDeletedBlocks(refCountedDB));\n+      assertEquals(expectedRegularBlocks,\n+              countUnprefixedBlocks(refCountedDB));\n+\n+      // Test table metadata.\n+      Table<String, Long> metadataTable =\n+              refCountedDB.getStore().getMetadataTable();\n+      assertEquals(expectedRegularBlocks + expectedDeletingBlocks,\n+              (long)metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+      assertEquals(TestDB.BYTES_USED,\n+              (long)metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));\n+    }\n+  }\n+\n+  /**\n+   * Tests reading the chunk info saved from a block that was deleted from a\n+   * database in schema version one. Blocks deleted from schema version one\n+   * before the upgrade will have the block ID saved as their value. Trying\n+   * to retrieve this value as a {@link ChunkInfoList} should fail. Blocks\n+   * deleted from schema version one after the upgrade should have their\n+   * {@link ChunkInfoList} saved as the corresponding value in the deleted\n+   * blocks table. Reading these values should succeed.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadDeletedBlockChunkInfo() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      // Read blocks that were already deleted before the upgrade.\n+      List<? extends Table.KeyValue<String, ChunkInfoList>> deletedBlocks =\n+              refCountedDB.getStore()\n+                      .getDeletedBlocksTable().getRangeKVs(null, 100);\n+\n+      Set<String> preUpgradeBlocks = new HashSet<>();\n+\n+      for(Table.KeyValue<String, ChunkInfoList> chunkListKV: deletedBlocks) {\n+        preUpgradeBlocks.add(chunkListKV.getKey());\n+        try {\n+          chunkListKV.getValue();\n+          Assert.fail(\"No exception thrown when trying to retrieve old \" +\n+                  \"deleted blocks values as chunk lists.\");\n+        } catch(IOException ex) {\n+          // Exception thrown as expected.\n+        }\n+      }\n+\n+      Assert.assertEquals(TestDB.NUM_DELETED_BLOCKS, preUpgradeBlocks.size());\n+\n+      runBlockDeletingService();\n+\n+      // After the block deleting service runs, get the updated list of\n+      // deleted blocks.\n+      deletedBlocks = refCountedDB.getStore()\n+                      .getDeletedBlocksTable().getRangeKVs(null, 100);\n+\n+      int numPostUpgradeDeletesFound = 0;\n+      for(Table.KeyValue<String, ChunkInfoList> chunkListKV: deletedBlocks) {\n+        if (!preUpgradeBlocks.contains(chunkListKV.getKey())) {\n+          numPostUpgradeDeletesFound++;\n+          Assert.assertNotNull(chunkListKV.getValue());\n+        }\n+      }\n+\n+      // The blocks that were originally marked for deletion should now be\n+      // deleted.\n+      Assert.assertEquals(TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              numPostUpgradeDeletesFound);\n+    }\n+\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      List<? extends Table.KeyValue<String, ChunkInfoList>> deletedBlocks =\n+              refCountedDB.getStore().getDeletedBlocksTable()\n+                      .getRangeKVs(null, 100);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 317}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI4NjE5OQ==", "bodyText": "Yes, looks like a copy/paste leftover, will remove.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482286199", "createdAt": "2020-09-02T18:32:45Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestSchemaOneBackwardsCompatibility.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.common;\n+\n+import org.apache.commons.io.FileUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.container.ContainerTestHelper;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+import org.apache.hadoop.ozone.container.common.impl.ChunkLayOutVersion;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerDataYaml;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n+import org.apache.hadoop.ozone.container.common.interfaces.ContainerDispatcher;\n+import org.apache.hadoop.ozone.container.common.utils.ReferenceCountedDB;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainerData;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil;\n+import org.apache.hadoop.ozone.container.metadata.DatanodeStore;\n+import org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer;\n+import org.apache.hadoop.ozone.container.testutils.BlockDeletingServiceTestImpl;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.yaml.snakeyaml.Yaml;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.*;\n+\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_CONTAINER_LIMIT_PER_INTERVAL;\n+import static org.junit.Assert.*;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Tests processing of containers written with DB schema version 1,\n+ * which stores all its data in the default RocksDB column family.\n+ * Newer schema version will use a different column family layout, but they\n+ * should still be able to read, delete data, and update metadata for schema\n+ * version 1 containers.\n+ * <p>\n+ * The functionality executed by these tests assumes that all containers will\n+ * have to be closed before an upgrade, meaning that containers written with\n+ * schema version 1 will only ever be encountered in their closed state.\n+ * <p>\n+ * For an example of a RocksDB instance written with schema version 1, see\n+ * {@link TestDB}, which is used by these tests to load a pre created schema\n+ * version 1 RocksDB instance from test resources.\n+ */\n+public class TestSchemaOneBackwardsCompatibility {\n+  private OzoneConfiguration conf;\n+\n+  private File metadataDir;\n+  private File dbFile;\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+\n+  @Before\n+  public void setup() throws Exception {\n+    conf = new OzoneConfiguration();\n+    TestDB testDB = new TestDB();\n+\n+    // Copy data to the temporary folder so it can be safely modified.\n+    File tempMetadataDir =\n+            tempFolder.newFolder(Long.toString(TestDB.CONTAINER_ID),\n+                    OzoneConsts.CONTAINER_META_PATH);\n+\n+    FileUtils.copyDirectoryToDirectory(testDB.getDBDirectory(),\n+            tempMetadataDir);\n+    FileUtils.copyFileToDirectory(testDB.getContainerFile(), tempMetadataDir);\n+\n+    metadataDir = tempMetadataDir;\n+    File[] potentialDBFiles = metadataDir.listFiles((dir, name) ->\n+            name.equals(TestDB.DB_NAME));\n+\n+    if (potentialDBFiles == null || potentialDBFiles.length != 1) {\n+      throw new IOException(\"Failed load file named \" + TestDB.DB_NAME + \" \" +\n+              \"from the metadata directory \" + metadataDir.getAbsolutePath());\n+    }\n+\n+    dbFile = potentialDBFiles[0];\n+  }\n+\n+  /**\n+   * Because all tables in schema version one map back to the default table,\n+   * directly iterating any of the table instances should be forbidden.\n+   * Otherwise, the iterators for each table would read the entire default\n+   * table, return all database contents, and yield unexpected results.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDirectTableIterationDisabled() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      DatanodeStore store = refCountedDB.getStore();\n+\n+      assertTableIteratorUnsupported(store.getMetadataTable());\n+      assertTableIteratorUnsupported(store.getBlockDataTable());\n+      assertTableIteratorUnsupported(store.getDeletedBlocksTable());\n+    }\n+  }\n+\n+  private void assertTableIteratorUnsupported(Table<?, ?> table) {\n+    try {\n+      table.iterator();\n+      Assert.fail(\"Table iterator should have thrown \" +\n+              \"UnsupportedOperationException.\");\n+    } catch (UnsupportedOperationException ex) {\n+      // Exception thrown as expected.\n+    }\n+  }\n+\n+  /**\n+   * Counts the number of deleted, pending delete, and regular blocks in the\n+   * database, and checks that they match the expected values.\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testBlockIteration() throws IOException {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      assertEquals(TestDB.NUM_DELETED_BLOCKS, countDeletedBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countDeletingBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.KEY_COUNT - TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countUnprefixedBlocks(refCountedDB));\n+    }\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has metadata keys present.\n+   * The {@link KeyValueContainerUtil} will read these values to fill in a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithMetadata() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has no metadata keys present.\n+   * The {@link KeyValueContainerUtil} will scan the blocks in the database\n+   * to fill these metadata values into the database and into a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithoutMetadata() throws Exception {\n+    // Init the kvData enough values so we can get the database to modify for\n+    // testing and then read.\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    // Delete metadata keys from our copy of the DB.\n+    // This simulates them not being there to start with.\n+    try (ReferenceCountedDB db = BlockUtils.getDB(kvData, conf)) {\n+      Table<String, Long> metadataTable = db.getStore().getMetadataTable();\n+\n+      metadataTable.delete(OzoneConsts.BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+\n+      metadataTable.delete(OzoneConsts.CONTAINER_BYTES_USED);\n+      assertNull(metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));\n+\n+      metadataTable.delete(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT));\n+    }\n+\n+    // Create a new container data object, and fill in its metadata by\n+    // counting blocks from the database, since the metadata keys in the\n+    // database are now gone.\n+    kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading blocks marked for deletion from a container written in\n+   * schema version 1. Because the block deleting service both reads for\n+   * deleted blocks and deletes them, this test will modify its copy of the\n+   * database.\n+   */\n+  @Test\n+  public void testDelete() throws Exception {\n+    final long numBlocksToDelete = TestDB.NUM_PENDING_DELETION_BLOCKS;\n+\n+    runBlockDeletingService();\n+\n+    // Expected values after blocks with #deleting# prefix in original DB are\n+    // deleted.\n+    final long expectedDeletingBlocks =\n+            TestDB.NUM_PENDING_DELETION_BLOCKS - numBlocksToDelete;\n+    final long expectedDeletedBlocks =\n+            TestDB.NUM_DELETED_BLOCKS + numBlocksToDelete;\n+    final long expectedRegularBlocks =\n+            TestDB.KEY_COUNT - numBlocksToDelete;\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      // Test results via block iteration.\n+      assertEquals(expectedDeletingBlocks,\n+              countDeletingBlocks(refCountedDB));\n+      assertEquals(expectedDeletedBlocks,\n+              countDeletedBlocks(refCountedDB));\n+      assertEquals(expectedRegularBlocks,\n+              countUnprefixedBlocks(refCountedDB));\n+\n+      // Test table metadata.\n+      Table<String, Long> metadataTable =\n+              refCountedDB.getStore().getMetadataTable();\n+      assertEquals(expectedRegularBlocks + expectedDeletingBlocks,\n+              (long)metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+      assertEquals(TestDB.BYTES_USED,\n+              (long)metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));\n+    }\n+  }\n+\n+  /**\n+   * Tests reading the chunk info saved from a block that was deleted from a\n+   * database in schema version one. Blocks deleted from schema version one\n+   * before the upgrade will have the block ID saved as their value. Trying\n+   * to retrieve this value as a {@link ChunkInfoList} should fail. Blocks\n+   * deleted from schema version one after the upgrade should have their\n+   * {@link ChunkInfoList} saved as the corresponding value in the deleted\n+   * blocks table. Reading these values should succeed.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadDeletedBlockChunkInfo() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      // Read blocks that were already deleted before the upgrade.\n+      List<? extends Table.KeyValue<String, ChunkInfoList>> deletedBlocks =\n+              refCountedDB.getStore()\n+                      .getDeletedBlocksTable().getRangeKVs(null, 100);\n+\n+      Set<String> preUpgradeBlocks = new HashSet<>();\n+\n+      for(Table.KeyValue<String, ChunkInfoList> chunkListKV: deletedBlocks) {\n+        preUpgradeBlocks.add(chunkListKV.getKey());\n+        try {\n+          chunkListKV.getValue();\n+          Assert.fail(\"No exception thrown when trying to retrieve old \" +\n+                  \"deleted blocks values as chunk lists.\");\n+        } catch(IOException ex) {\n+          // Exception thrown as expected.\n+        }\n+      }\n+\n+      Assert.assertEquals(TestDB.NUM_DELETED_BLOCKS, preUpgradeBlocks.size());\n+\n+      runBlockDeletingService();\n+\n+      // After the block deleting service runs, get the updated list of\n+      // deleted blocks.\n+      deletedBlocks = refCountedDB.getStore()\n+                      .getDeletedBlocksTable().getRangeKVs(null, 100);\n+\n+      int numPostUpgradeDeletesFound = 0;\n+      for(Table.KeyValue<String, ChunkInfoList> chunkListKV: deletedBlocks) {\n+        if (!preUpgradeBlocks.contains(chunkListKV.getKey())) {\n+          numPostUpgradeDeletesFound++;\n+          Assert.assertNotNull(chunkListKV.getValue());\n+        }\n+      }\n+\n+      // The blocks that were originally marked for deletion should now be\n+      // deleted.\n+      Assert.assertEquals(TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              numPostUpgradeDeletesFound);\n+    }\n+\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      List<? extends Table.KeyValue<String, ChunkInfoList>> deletedBlocks =\n+              refCountedDB.getStore().getDeletedBlocksTable()\n+                      .getRangeKVs(null, 100);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NzQ3NQ=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 317}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzkwMjY2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMjoxMzoyNlrOHJVxeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wNFQxMToyNDozMFrOHNJ6fA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1NTk2MA==", "bodyText": "The general continuation indent used throughout the code is 4.\n@elek, any idea if continuation indent should be 4 or does it not matter? Checkstyle CI seems to not care about it.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479555960", "createdAt": "2020-08-28T22:13:26Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java", "diffHunk": "@@ -88,196 +86,314 @@ public void setUp() throws Exception {\n     conf = new OzoneConfiguration();\n     conf.set(HDDS_DATANODE_DIR_KEY, testRoot.getAbsolutePath());\n     volumeSet = new MutableVolumeSet(UUID.randomUUID().toString(), conf);\n+\n+    containerData = new KeyValueContainerData(105L,\n+            layout,\n+            (long) StorageUnit.GB.toBytes(1), UUID.randomUUID().toString(),\n+            UUID.randomUUID().toString());\n+    // Init the container.\n+    container = new KeyValueContainer(containerData, conf);\n+    container.create(volumeSet, new RoundRobinVolumeChoosingPolicy(), UUID\n+            .randomUUID().toString());\n+    db = BlockUtils.getDB(containerData, conf);\n   }\n \n \n   @After\n-  public void tearDown() {\n+  public void tearDown() throws Exception {\n+    db.close();\n+    db.cleanup();\n     volumeSet.shutdown();\n     FileUtil.fullyDelete(testRoot);\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithMixedBlocks() throws Exception {\n-\n-    long containerID = 100L;\n-    int deletedBlocks = 5;\n+    int deletingBlocks = 5;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerID, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks,\n+            deletingBlocks);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI4NzA5Mw==", "bodyText": "Not sure why lines 115-116 are split up, I will combine them. Looks like I had my IntelliJ line continuation set for 8 instead of 4. I will see if I can change that.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482287093", "createdAt": "2020-09-02T18:33:47Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java", "diffHunk": "@@ -88,196 +86,314 @@ public void setUp() throws Exception {\n     conf = new OzoneConfiguration();\n     conf.set(HDDS_DATANODE_DIR_KEY, testRoot.getAbsolutePath());\n     volumeSet = new MutableVolumeSet(UUID.randomUUID().toString(), conf);\n+\n+    containerData = new KeyValueContainerData(105L,\n+            layout,\n+            (long) StorageUnit.GB.toBytes(1), UUID.randomUUID().toString(),\n+            UUID.randomUUID().toString());\n+    // Init the container.\n+    container = new KeyValueContainer(containerData, conf);\n+    container.create(volumeSet, new RoundRobinVolumeChoosingPolicy(), UUID\n+            .randomUUID().toString());\n+    db = BlockUtils.getDB(containerData, conf);\n   }\n \n \n   @After\n-  public void tearDown() {\n+  public void tearDown() throws Exception {\n+    db.close();\n+    db.cleanup();\n     volumeSet.shutdown();\n     FileUtil.fullyDelete(testRoot);\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithMixedBlocks() throws Exception {\n-\n-    long containerID = 100L;\n-    int deletedBlocks = 5;\n+    int deletingBlocks = 5;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerID, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks,\n+            deletingBlocks);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1NTk2MA=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzU1NTk2NA==", "bodyText": "@elek, any idea if continuation indent should be 4 or does it not matter? Checkstyle CI seems to not care about it.\n\nI don't think there is any rule. It can be reasonable to introduce new one (to reduce noise during merges) but in that case we should provide good auto-formatting rules to avoid any overhead.\n(Similar to the import format, we can improve checkstyle, but we also need autoformatter rules).\nI think @maobaolong had a few ideas, but the effort is blocked by #921", "url": "https://github.com/apache/ozone/pull/1298#discussion_r483555964", "createdAt": "2020-09-04T11:24:30Z", "author": {"login": "elek"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java", "diffHunk": "@@ -88,196 +86,314 @@ public void setUp() throws Exception {\n     conf = new OzoneConfiguration();\n     conf.set(HDDS_DATANODE_DIR_KEY, testRoot.getAbsolutePath());\n     volumeSet = new MutableVolumeSet(UUID.randomUUID().toString(), conf);\n+\n+    containerData = new KeyValueContainerData(105L,\n+            layout,\n+            (long) StorageUnit.GB.toBytes(1), UUID.randomUUID().toString(),\n+            UUID.randomUUID().toString());\n+    // Init the container.\n+    container = new KeyValueContainer(containerData, conf);\n+    container.create(volumeSet, new RoundRobinVolumeChoosingPolicy(), UUID\n+            .randomUUID().toString());\n+    db = BlockUtils.getDB(containerData, conf);\n   }\n \n \n   @After\n-  public void tearDown() {\n+  public void tearDown() throws Exception {\n+    db.close();\n+    db.cleanup();\n     volumeSet.shutdown();\n     FileUtil.fullyDelete(testRoot);\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithMixedBlocks() throws Exception {\n-\n-    long containerID = 100L;\n-    int deletedBlocks = 5;\n+    int deletingBlocks = 5;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerID, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks,\n+            deletingBlocks);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1NTk2MA=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 95}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5ODAzMzAxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMzozNjo1NFrOHJW7MA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxODozNDo0MVrOHL8heA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU3NDgzMg==", "bodyText": "Nitpick: Expected value comes before Actual value in assert. It might be confusing in case there is an exception.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479574832", "createdAt": "2020-08-28T23:36:54Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java", "diffHunk": "@@ -88,196 +86,314 @@ public void setUp() throws Exception {\n     conf = new OzoneConfiguration();\n     conf.set(HDDS_DATANODE_DIR_KEY, testRoot.getAbsolutePath());\n     volumeSet = new MutableVolumeSet(UUID.randomUUID().toString(), conf);\n+\n+    containerData = new KeyValueContainerData(105L,\n+            layout,\n+            (long) StorageUnit.GB.toBytes(1), UUID.randomUUID().toString(),\n+            UUID.randomUUID().toString());\n+    // Init the container.\n+    container = new KeyValueContainer(containerData, conf);\n+    container.create(volumeSet, new RoundRobinVolumeChoosingPolicy(), UUID\n+            .randomUUID().toString());\n+    db = BlockUtils.getDB(containerData, conf);\n   }\n \n \n   @After\n-  public void tearDown() {\n+  public void tearDown() throws Exception {\n+    db.close();\n+    db.cleanup();\n     volumeSet.shutdown();\n     FileUtil.fullyDelete(testRoot);\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithMixedBlocks() throws Exception {\n-\n-    long containerID = 100L;\n-    int deletedBlocks = 5;\n+    int deletingBlocks = 5;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerID, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks,\n+            deletingBlocks);\n \n-      int counter = 0;\n+    // Default filter used is all unprefixed blocks.\n+    List<Long> unprefixedBlockIDs = blockIDs.get(\"\");\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n+\n+      Iterator<Long> blockIDIter = unprefixedBlockIDs.iterator();\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(), (long)blockIDIter.next());\n       }\n-\n       assertFalse(keyValueBlockIterator.hasNext());\n+      assertFalse(blockIDIter.hasNext());\n \n       keyValueBlockIterator.seekToFirst();\n-      counter = 0;\n+      blockIDIter = unprefixedBlockIDs.iterator();\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(), (long)blockIDIter.next());\n       }\n       assertFalse(keyValueBlockIterator.hasNext());\n+      assertFalse(blockIDIter.hasNext());\n \n       try {\n         keyValueBlockIterator.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithNextBlock() throws Exception {\n-    long containerID = 101L;\n-    createContainerWithBlocks(containerID, 2, 0);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n-      long blockID = 0L;\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n+    List<Long> blockIDs = createContainerWithBlocks(CONTAINER_ID, 2);\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n+      assertEquals((long)blockIDs.get(0),\n+              keyValueBlockIterator.nextBlock().getLocalID());\n+      assertEquals((long)blockIDs.get(1),\n+              keyValueBlockIterator.nextBlock().getLocalID());\n \n       try {\n         keyValueBlockIterator.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithHasNext() throws Exception {\n-    long containerID = 102L;\n-    createContainerWithBlocks(containerID, 2, 0);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n-      long blockID = 0L;\n+    List<Long> blockIDs = createContainerWithBlocks(CONTAINER_ID, 2);\n+    try(BlockIterator<BlockData> blockIter =\n+                db.getStore().getBlockIterator()) {\n \n       // Even calling multiple times hasNext() should not move entry forward.\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      keyValueBlockIterator.seekToLast();\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      keyValueBlockIterator.seekToFirst();\n-      blockID = 0L;\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertEquals((long)blockIDs.get(0),\n+              blockIter.nextBlock().getLocalID());\n+\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertEquals((long)blockIDs.get(1), blockIter.nextBlock().getLocalID());\n+\n+      blockIter.seekToFirst();\n+      assertEquals((long)blockIDs.get(0), blockIter.nextBlock().getLocalID());\n+      assertEquals((long)blockIDs.get(1), blockIter.nextBlock().getLocalID());\n \n       try {\n-        keyValueBlockIterator.nextBlock();\n+        blockIter.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithFilter() throws Exception {\n-    long containerId = 103L;\n-    int deletedBlocks = 10;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerId, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerId, new File(containerPath), MetadataKeyFilters\n-        .getDeletingKeyFilter())) {\n-\n-      int counter = 5;\n+    int deletingBlocks = 5;\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks, deletingBlocks);\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator(\n+                        MetadataKeyFilters.getDeletingKeyFilter())) {\n+      List<Long> deletingBlockIDs =\n+              blockIDs.get(OzoneConsts.DELETING_KEY_PREFIX);\n+      int counter = 0;\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(),\n+                (long)deletingBlockIDs.get(counter));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 255}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI4Nzk5Mg==", "bodyText": "Yes, I will swap the values.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482287992", "createdAt": "2020-09-02T18:34:41Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java", "diffHunk": "@@ -88,196 +86,314 @@ public void setUp() throws Exception {\n     conf = new OzoneConfiguration();\n     conf.set(HDDS_DATANODE_DIR_KEY, testRoot.getAbsolutePath());\n     volumeSet = new MutableVolumeSet(UUID.randomUUID().toString(), conf);\n+\n+    containerData = new KeyValueContainerData(105L,\n+            layout,\n+            (long) StorageUnit.GB.toBytes(1), UUID.randomUUID().toString(),\n+            UUID.randomUUID().toString());\n+    // Init the container.\n+    container = new KeyValueContainer(containerData, conf);\n+    container.create(volumeSet, new RoundRobinVolumeChoosingPolicy(), UUID\n+            .randomUUID().toString());\n+    db = BlockUtils.getDB(containerData, conf);\n   }\n \n \n   @After\n-  public void tearDown() {\n+  public void tearDown() throws Exception {\n+    db.close();\n+    db.cleanup();\n     volumeSet.shutdown();\n     FileUtil.fullyDelete(testRoot);\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithMixedBlocks() throws Exception {\n-\n-    long containerID = 100L;\n-    int deletedBlocks = 5;\n+    int deletingBlocks = 5;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerID, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks,\n+            deletingBlocks);\n \n-      int counter = 0;\n+    // Default filter used is all unprefixed blocks.\n+    List<Long> unprefixedBlockIDs = blockIDs.get(\"\");\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n+\n+      Iterator<Long> blockIDIter = unprefixedBlockIDs.iterator();\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(), (long)blockIDIter.next());\n       }\n-\n       assertFalse(keyValueBlockIterator.hasNext());\n+      assertFalse(blockIDIter.hasNext());\n \n       keyValueBlockIterator.seekToFirst();\n-      counter = 0;\n+      blockIDIter = unprefixedBlockIDs.iterator();\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(), (long)blockIDIter.next());\n       }\n       assertFalse(keyValueBlockIterator.hasNext());\n+      assertFalse(blockIDIter.hasNext());\n \n       try {\n         keyValueBlockIterator.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithNextBlock() throws Exception {\n-    long containerID = 101L;\n-    createContainerWithBlocks(containerID, 2, 0);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n-      long blockID = 0L;\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n+    List<Long> blockIDs = createContainerWithBlocks(CONTAINER_ID, 2);\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n+      assertEquals((long)blockIDs.get(0),\n+              keyValueBlockIterator.nextBlock().getLocalID());\n+      assertEquals((long)blockIDs.get(1),\n+              keyValueBlockIterator.nextBlock().getLocalID());\n \n       try {\n         keyValueBlockIterator.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithHasNext() throws Exception {\n-    long containerID = 102L;\n-    createContainerWithBlocks(containerID, 2, 0);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n-      long blockID = 0L;\n+    List<Long> blockIDs = createContainerWithBlocks(CONTAINER_ID, 2);\n+    try(BlockIterator<BlockData> blockIter =\n+                db.getStore().getBlockIterator()) {\n \n       // Even calling multiple times hasNext() should not move entry forward.\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      keyValueBlockIterator.seekToLast();\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      keyValueBlockIterator.seekToFirst();\n-      blockID = 0L;\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertEquals((long)blockIDs.get(0),\n+              blockIter.nextBlock().getLocalID());\n+\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertEquals((long)blockIDs.get(1), blockIter.nextBlock().getLocalID());\n+\n+      blockIter.seekToFirst();\n+      assertEquals((long)blockIDs.get(0), blockIter.nextBlock().getLocalID());\n+      assertEquals((long)blockIDs.get(1), blockIter.nextBlock().getLocalID());\n \n       try {\n-        keyValueBlockIterator.nextBlock();\n+        blockIter.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithFilter() throws Exception {\n-    long containerId = 103L;\n-    int deletedBlocks = 10;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerId, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerId, new File(containerPath), MetadataKeyFilters\n-        .getDeletingKeyFilter())) {\n-\n-      int counter = 5;\n+    int deletingBlocks = 5;\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks, deletingBlocks);\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator(\n+                        MetadataKeyFilters.getDeletingKeyFilter())) {\n+      List<Long> deletingBlockIDs =\n+              blockIDs.get(OzoneConsts.DELETING_KEY_PREFIX);\n+      int counter = 0;\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(),\n+                (long)deletingBlockIDs.get(counter));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU3NDgzMg=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 255}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5ODA0OTk5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMzo0OToyNlrOHJXEYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxODozNToyNlrOHL8kkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU3NzE4NA==", "bodyText": "Javadoc is not clear.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479577184", "createdAt": "2020-08-28T23:49:26Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java", "diffHunk": "@@ -88,196 +86,314 @@ public void setUp() throws Exception {\n     conf = new OzoneConfiguration();\n     conf.set(HDDS_DATANODE_DIR_KEY, testRoot.getAbsolutePath());\n     volumeSet = new MutableVolumeSet(UUID.randomUUID().toString(), conf);\n+\n+    containerData = new KeyValueContainerData(105L,\n+            layout,\n+            (long) StorageUnit.GB.toBytes(1), UUID.randomUUID().toString(),\n+            UUID.randomUUID().toString());\n+    // Init the container.\n+    container = new KeyValueContainer(containerData, conf);\n+    container.create(volumeSet, new RoundRobinVolumeChoosingPolicy(), UUID\n+            .randomUUID().toString());\n+    db = BlockUtils.getDB(containerData, conf);\n   }\n \n \n   @After\n-  public void tearDown() {\n+  public void tearDown() throws Exception {\n+    db.close();\n+    db.cleanup();\n     volumeSet.shutdown();\n     FileUtil.fullyDelete(testRoot);\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithMixedBlocks() throws Exception {\n-\n-    long containerID = 100L;\n-    int deletedBlocks = 5;\n+    int deletingBlocks = 5;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerID, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks,\n+            deletingBlocks);\n \n-      int counter = 0;\n+    // Default filter used is all unprefixed blocks.\n+    List<Long> unprefixedBlockIDs = blockIDs.get(\"\");\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n+\n+      Iterator<Long> blockIDIter = unprefixedBlockIDs.iterator();\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(), (long)blockIDIter.next());\n       }\n-\n       assertFalse(keyValueBlockIterator.hasNext());\n+      assertFalse(blockIDIter.hasNext());\n \n       keyValueBlockIterator.seekToFirst();\n-      counter = 0;\n+      blockIDIter = unprefixedBlockIDs.iterator();\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(), (long)blockIDIter.next());\n       }\n       assertFalse(keyValueBlockIterator.hasNext());\n+      assertFalse(blockIDIter.hasNext());\n \n       try {\n         keyValueBlockIterator.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithNextBlock() throws Exception {\n-    long containerID = 101L;\n-    createContainerWithBlocks(containerID, 2, 0);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n-      long blockID = 0L;\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n+    List<Long> blockIDs = createContainerWithBlocks(CONTAINER_ID, 2);\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n+      assertEquals((long)blockIDs.get(0),\n+              keyValueBlockIterator.nextBlock().getLocalID());\n+      assertEquals((long)blockIDs.get(1),\n+              keyValueBlockIterator.nextBlock().getLocalID());\n \n       try {\n         keyValueBlockIterator.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithHasNext() throws Exception {\n-    long containerID = 102L;\n-    createContainerWithBlocks(containerID, 2, 0);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n-      long blockID = 0L;\n+    List<Long> blockIDs = createContainerWithBlocks(CONTAINER_ID, 2);\n+    try(BlockIterator<BlockData> blockIter =\n+                db.getStore().getBlockIterator()) {\n \n       // Even calling multiple times hasNext() should not move entry forward.\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      keyValueBlockIterator.seekToLast();\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      keyValueBlockIterator.seekToFirst();\n-      blockID = 0L;\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertEquals((long)blockIDs.get(0),\n+              blockIter.nextBlock().getLocalID());\n+\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertEquals((long)blockIDs.get(1), blockIter.nextBlock().getLocalID());\n+\n+      blockIter.seekToFirst();\n+      assertEquals((long)blockIDs.get(0), blockIter.nextBlock().getLocalID());\n+      assertEquals((long)blockIDs.get(1), blockIter.nextBlock().getLocalID());\n \n       try {\n-        keyValueBlockIterator.nextBlock();\n+        blockIter.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithFilter() throws Exception {\n-    long containerId = 103L;\n-    int deletedBlocks = 10;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerId, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerId, new File(containerPath), MetadataKeyFilters\n-        .getDeletingKeyFilter())) {\n-\n-      int counter = 5;\n+    int deletingBlocks = 5;\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks, deletingBlocks);\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator(\n+                        MetadataKeyFilters.getDeletingKeyFilter())) {\n+      List<Long> deletingBlockIDs =\n+              blockIDs.get(OzoneConsts.DELETING_KEY_PREFIX);\n+      int counter = 0;\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(),\n+                (long)deletingBlockIDs.get(counter));\n+        counter++;\n       }\n-      assertEquals(10, counter);\n+\n+      assertEquals(deletingBlocks, counter);\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithOnlyDeletedBlocks() throws\n       Exception {\n-    long containerId = 104L;\n-    createContainerWithBlocks(containerId, 0, 5);\n+    createContainerWithBlocks(CONTAINER_ID, 0, 5);\n     String containerPath = new File(containerData.getMetadataPath())\n         .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerId, new File(containerPath))) {\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n       //As all blocks are deleted blocks, blocks does not match with normal key\n       // filter.\n       assertFalse(keyValueBlockIterator.hasNext());\n     }\n   }\n \n+  /**\n+   * Due to RocksDB internals, prefixed keys may be grouped all at the\n+   * beginning or end of the key iteration, depending on the serialization\n+   * used. Keys of the same prefix are grouped\n+   * together. This method runs the same set of tests on the iterator first\n+   * positively filtering one prefix, and then positively filtering\n+   * a second prefix. If the sets of keys with prefix one, prefix\n+   * two, and no prefixes are not empty, it follows that the filter will\n+   * encounter both of the following cases:\n+   *\n+   * 1. A failing key followed by a passing key.\n+   * 2. A passing key followed by a failing key.\n+   *\n+   * Note that with the current block data table implementation, there is\n+   * only ever one type of prefix. This test adds a dummy second prefix type\n+   * to ensure that the iterator will continue to work if more prefixes are\n+   * added in the future.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testKeyValueBlockIteratorWithAdvancedFilter() throws\n+          Exception {\n+    // Block data table currently only uses one prefix type.\n+    // Introduce a second prefix type to make sure the iterator functions\n+    // correctly if more prefixes were to be added in the future.\n+    final String secondPrefix = \"#FOOBAR#\";\n+    Map<String, Integer> prefixCounts = new HashMap<>();\n+    prefixCounts.put(OzoneConsts.DELETING_KEY_PREFIX, 3);\n+    prefixCounts.put(\"\", 3);\n+    prefixCounts.put(secondPrefix, 3);\n+\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            prefixCounts);\n+    // Test deleting filter.\n+    testWithFilter(MetadataKeyFilters.getDeletingKeyFilter(),\n+            blockIDs.get(OzoneConsts.DELETING_KEY_PREFIX));\n+\n+    // Test arbitrary filter.\n+    MetadataKeyFilters.KeyPrefixFilter secondFilter =\n+            new MetadataKeyFilters.KeyPrefixFilter()\n+            .addFilter(secondPrefix);\n+    testWithFilter(secondFilter, blockIDs.get(secondPrefix));\n+  }\n+\n+  /**\n+   * Helper method to run some iterator tests with a provided filter.\n+   */\n+  private void testWithFilter(MetadataKeyFilters.KeyPrefixFilter filter,\n+                              List<Long> expectedIDs) throws Exception {\n+    try(BlockIterator<BlockData> iterator =\n+                db.getStore().getBlockIterator(filter)) {\n+      // Test seek.\n+      iterator.seekToFirst();\n+      long firstID = iterator.nextBlock().getLocalID();\n+      assertEquals(expectedIDs.get(0).longValue(), firstID);\n+      assertTrue(iterator.hasNext());\n+\n+      // Test atypical iteration use.\n+      iterator.seekToFirst();\n+      int numIDsSeen = 0;\n+      for (long id: expectedIDs) {\n+        assertEquals(iterator.nextBlock().getLocalID(), id);\n+        numIDsSeen++;\n+\n+        // Test that iterator can handle sporadic hasNext() calls.\n+        if (id % 2 == 0 && numIDsSeen < expectedIDs.size()) {\n+          assertTrue(iterator.hasNext());\n+        }\n+      }\n+\n+      assertFalse(iterator.hasNext());\n+    }\n+  }\n+\n+  /**\n+   * Creates a container with specified number of unprefixed (normal) blocks.\n+   * @param containerId\n+   * @param normalBlocks\n+   * @return The list of block IDs of normal blocks that were created.\n+   * @throws Exception\n+   */\n+  private List<Long> createContainerWithBlocks(long containerId,\n+            int normalBlocks) throws Exception {\n+    return createContainerWithBlocks(containerId, normalBlocks, 0).get(\"\");\n+  }\n+\n   /**\n    * Creates a container with specified number of normal blocks and deleted\n-   * blocks. First it will insert normal blocks, and then it will insert\n-   * deleted blocks.\n+   * blocks.\n+   * deleting blocks, then it will insert deleted blocks.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 374}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI4ODc4NQ==", "bodyText": "Looks like I made some bizarre copy/paste typo, will fix.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482288785", "createdAt": "2020-09-02T18:35:26Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java", "diffHunk": "@@ -88,196 +86,314 @@ public void setUp() throws Exception {\n     conf = new OzoneConfiguration();\n     conf.set(HDDS_DATANODE_DIR_KEY, testRoot.getAbsolutePath());\n     volumeSet = new MutableVolumeSet(UUID.randomUUID().toString(), conf);\n+\n+    containerData = new KeyValueContainerData(105L,\n+            layout,\n+            (long) StorageUnit.GB.toBytes(1), UUID.randomUUID().toString(),\n+            UUID.randomUUID().toString());\n+    // Init the container.\n+    container = new KeyValueContainer(containerData, conf);\n+    container.create(volumeSet, new RoundRobinVolumeChoosingPolicy(), UUID\n+            .randomUUID().toString());\n+    db = BlockUtils.getDB(containerData, conf);\n   }\n \n \n   @After\n-  public void tearDown() {\n+  public void tearDown() throws Exception {\n+    db.close();\n+    db.cleanup();\n     volumeSet.shutdown();\n     FileUtil.fullyDelete(testRoot);\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithMixedBlocks() throws Exception {\n-\n-    long containerID = 100L;\n-    int deletedBlocks = 5;\n+    int deletingBlocks = 5;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerID, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks,\n+            deletingBlocks);\n \n-      int counter = 0;\n+    // Default filter used is all unprefixed blocks.\n+    List<Long> unprefixedBlockIDs = blockIDs.get(\"\");\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n+\n+      Iterator<Long> blockIDIter = unprefixedBlockIDs.iterator();\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(), (long)blockIDIter.next());\n       }\n-\n       assertFalse(keyValueBlockIterator.hasNext());\n+      assertFalse(blockIDIter.hasNext());\n \n       keyValueBlockIterator.seekToFirst();\n-      counter = 0;\n+      blockIDIter = unprefixedBlockIDs.iterator();\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(), (long)blockIDIter.next());\n       }\n       assertFalse(keyValueBlockIterator.hasNext());\n+      assertFalse(blockIDIter.hasNext());\n \n       try {\n         keyValueBlockIterator.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithNextBlock() throws Exception {\n-    long containerID = 101L;\n-    createContainerWithBlocks(containerID, 2, 0);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n-      long blockID = 0L;\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n+    List<Long> blockIDs = createContainerWithBlocks(CONTAINER_ID, 2);\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n+      assertEquals((long)blockIDs.get(0),\n+              keyValueBlockIterator.nextBlock().getLocalID());\n+      assertEquals((long)blockIDs.get(1),\n+              keyValueBlockIterator.nextBlock().getLocalID());\n \n       try {\n         keyValueBlockIterator.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithHasNext() throws Exception {\n-    long containerID = 102L;\n-    createContainerWithBlocks(containerID, 2, 0);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n-      long blockID = 0L;\n+    List<Long> blockIDs = createContainerWithBlocks(CONTAINER_ID, 2);\n+    try(BlockIterator<BlockData> blockIter =\n+                db.getStore().getBlockIterator()) {\n \n       // Even calling multiple times hasNext() should not move entry forward.\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      keyValueBlockIterator.seekToLast();\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      keyValueBlockIterator.seekToFirst();\n-      blockID = 0L;\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertEquals((long)blockIDs.get(0),\n+              blockIter.nextBlock().getLocalID());\n+\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertEquals((long)blockIDs.get(1), blockIter.nextBlock().getLocalID());\n+\n+      blockIter.seekToFirst();\n+      assertEquals((long)blockIDs.get(0), blockIter.nextBlock().getLocalID());\n+      assertEquals((long)blockIDs.get(1), blockIter.nextBlock().getLocalID());\n \n       try {\n-        keyValueBlockIterator.nextBlock();\n+        blockIter.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithFilter() throws Exception {\n-    long containerId = 103L;\n-    int deletedBlocks = 10;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerId, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerId, new File(containerPath), MetadataKeyFilters\n-        .getDeletingKeyFilter())) {\n-\n-      int counter = 5;\n+    int deletingBlocks = 5;\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks, deletingBlocks);\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator(\n+                        MetadataKeyFilters.getDeletingKeyFilter())) {\n+      List<Long> deletingBlockIDs =\n+              blockIDs.get(OzoneConsts.DELETING_KEY_PREFIX);\n+      int counter = 0;\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(),\n+                (long)deletingBlockIDs.get(counter));\n+        counter++;\n       }\n-      assertEquals(10, counter);\n+\n+      assertEquals(deletingBlocks, counter);\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithOnlyDeletedBlocks() throws\n       Exception {\n-    long containerId = 104L;\n-    createContainerWithBlocks(containerId, 0, 5);\n+    createContainerWithBlocks(CONTAINER_ID, 0, 5);\n     String containerPath = new File(containerData.getMetadataPath())\n         .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerId, new File(containerPath))) {\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n       //As all blocks are deleted blocks, blocks does not match with normal key\n       // filter.\n       assertFalse(keyValueBlockIterator.hasNext());\n     }\n   }\n \n+  /**\n+   * Due to RocksDB internals, prefixed keys may be grouped all at the\n+   * beginning or end of the key iteration, depending on the serialization\n+   * used. Keys of the same prefix are grouped\n+   * together. This method runs the same set of tests on the iterator first\n+   * positively filtering one prefix, and then positively filtering\n+   * a second prefix. If the sets of keys with prefix one, prefix\n+   * two, and no prefixes are not empty, it follows that the filter will\n+   * encounter both of the following cases:\n+   *\n+   * 1. A failing key followed by a passing key.\n+   * 2. A passing key followed by a failing key.\n+   *\n+   * Note that with the current block data table implementation, there is\n+   * only ever one type of prefix. This test adds a dummy second prefix type\n+   * to ensure that the iterator will continue to work if more prefixes are\n+   * added in the future.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testKeyValueBlockIteratorWithAdvancedFilter() throws\n+          Exception {\n+    // Block data table currently only uses one prefix type.\n+    // Introduce a second prefix type to make sure the iterator functions\n+    // correctly if more prefixes were to be added in the future.\n+    final String secondPrefix = \"#FOOBAR#\";\n+    Map<String, Integer> prefixCounts = new HashMap<>();\n+    prefixCounts.put(OzoneConsts.DELETING_KEY_PREFIX, 3);\n+    prefixCounts.put(\"\", 3);\n+    prefixCounts.put(secondPrefix, 3);\n+\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            prefixCounts);\n+    // Test deleting filter.\n+    testWithFilter(MetadataKeyFilters.getDeletingKeyFilter(),\n+            blockIDs.get(OzoneConsts.DELETING_KEY_PREFIX));\n+\n+    // Test arbitrary filter.\n+    MetadataKeyFilters.KeyPrefixFilter secondFilter =\n+            new MetadataKeyFilters.KeyPrefixFilter()\n+            .addFilter(secondPrefix);\n+    testWithFilter(secondFilter, blockIDs.get(secondPrefix));\n+  }\n+\n+  /**\n+   * Helper method to run some iterator tests with a provided filter.\n+   */\n+  private void testWithFilter(MetadataKeyFilters.KeyPrefixFilter filter,\n+                              List<Long> expectedIDs) throws Exception {\n+    try(BlockIterator<BlockData> iterator =\n+                db.getStore().getBlockIterator(filter)) {\n+      // Test seek.\n+      iterator.seekToFirst();\n+      long firstID = iterator.nextBlock().getLocalID();\n+      assertEquals(expectedIDs.get(0).longValue(), firstID);\n+      assertTrue(iterator.hasNext());\n+\n+      // Test atypical iteration use.\n+      iterator.seekToFirst();\n+      int numIDsSeen = 0;\n+      for (long id: expectedIDs) {\n+        assertEquals(iterator.nextBlock().getLocalID(), id);\n+        numIDsSeen++;\n+\n+        // Test that iterator can handle sporadic hasNext() calls.\n+        if (id % 2 == 0 && numIDsSeen < expectedIDs.size()) {\n+          assertTrue(iterator.hasNext());\n+        }\n+      }\n+\n+      assertFalse(iterator.hasNext());\n+    }\n+  }\n+\n+  /**\n+   * Creates a container with specified number of unprefixed (normal) blocks.\n+   * @param containerId\n+   * @param normalBlocks\n+   * @return The list of block IDs of normal blocks that were created.\n+   * @throws Exception\n+   */\n+  private List<Long> createContainerWithBlocks(long containerId,\n+            int normalBlocks) throws Exception {\n+    return createContainerWithBlocks(containerId, normalBlocks, 0).get(\"\");\n+  }\n+\n   /**\n    * Creates a container with specified number of normal blocks and deleted\n-   * blocks. First it will insert normal blocks, and then it will insert\n-   * deleted blocks.\n+   * blocks.\n+   * deleting blocks, then it will insert deleted blocks.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU3NzE4NA=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 374}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5ODA1ODUxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/container-service/src/test/resources/123-dn-container.db/LOG", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMzo1NToyN1rOHJXI5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxODozNjowMlrOHL8miw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU3ODM0MA==", "bodyText": "LOCK and LOG files are not required to load the  DB.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479578340", "createdAt": "2020-08-28T23:55:27Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/test/resources/123-dn-container.db/LOG", "diffHunk": "@@ -0,0 +1,284 @@\n+2020/08/03-15:13:40.359520 7f80eb7a9700 RocksDB version: 6.8.1\n+2020/08/03-15:13:40.359563 7f80eb7a9700 Git sha rocksdb_build_git_sha:\n+2020/08/03-15:13:40.359566 7f80eb7a9700 Compile date Apr 26 2020", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI4OTI5MQ==", "bodyText": "Good to know, I will remove them from the test resources.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r482289291", "createdAt": "2020-09-02T18:36:02Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/test/resources/123-dn-container.db/LOG", "diffHunk": "@@ -0,0 +1,284 @@\n+2020/08/03-15:13:40.359520 7f80eb7a9700 RocksDB version: 6.8.1\n+2020/08/03-15:13:40.359563 7f80eb7a9700 Git sha rocksdb_build_git_sha:\n+2020/08/03-15:13:40.359566 7f80eb7a9700 Compile date Apr 26 2020", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU3ODM0MA=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 3}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3851, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}