{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTEwMTc1OTUz", "number": 1523, "title": "HDDS-4320. Let Ozone input streams implement CanUnbuffer", "bodyText": "What changes were proposed in this pull request?\nImplement the CanUnbuffer interface in Ozone input streams (ChunkInputStream, BlockInputStream): release buffers and disconnect the client when unbuffer() is called.  Upper level input streams (KeyInputStream, OzoneInputStream, OzoneFSInputStream) just delegate to the the underlying implementations.  This will allow Impala to cache file handles for Ozone.\nhttps://issues.apache.org/jira/browse/HDDS-4320\nHow was this patch tested?\nAdded contract test (base test copied from Hadoop 3.3) for both OFS and O3FS:\nhttps://github.com/adoroszlai/hadoop-ozone/runs/1309754540#step:4:3093\nhttps://github.com/adoroszlai/hadoop-ozone/runs/1309754540#step:4:3107\nalso added unit test:\nhttps://github.com/adoroszlai/hadoop-ozone/runs/1309754295#step:3:3540", "createdAt": "2020-10-26T17:03:38Z", "url": "https://github.com/apache/ozone/pull/1523", "merged": true, "mergeCommit": {"oid": "f1e46cb25f815ff857ecb4e4fc9ab44395a65943"}, "closed": true, "closedAt": "2020-12-02T11:03:00Z", "author": {"login": "adoroszlai"}, "timelineItems": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdWUJgLgH2gAyNTEwMTc1OTUzOjcyNmRlZjgxYzlhMWQ2ZmY0MjE5M2M1ZWYyMWJlZWE5NDU5YzlhYmI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdh_27UgFqTU0MjI2NDI1Nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "726def81c9a1d6ff42193c5ef21beea9459c9abb", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/726def81c9a1d6ff42193c5ef21beea9459c9abb", "committedDate": "2020-10-26T13:12:03Z", "message": "HDDS-4320. Let Ozone input streams implement CanUnbuffer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5d3b1514a3783d5c48f02a8cbfffddabbdaac212", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/5d3b1514a3783d5c48f02a8cbfffddabbdaac212", "committedDate": "2020-10-26T15:44:26Z", "message": "Add unit test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f1db1b13a2c85b142bdf82d9c0c401c8b28eba3a", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/f1db1b13a2c85b142bdf82d9c0c401c8b28eba3a", "committedDate": "2020-10-26T20:31:15Z", "message": "trigger new CI check"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "00f0c4eecd3053c108c0621e8f29a83d07288015", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/00f0c4eecd3053c108c0621e8f29a83d07288015", "committedDate": "2020-10-27T05:43:24Z", "message": "trigger new CI check"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "13e274f47d077022adcb12a09df1a333cfe64d86", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/13e274f47d077022adcb12a09df1a333cfe64d86", "committedDate": "2020-11-17T16:45:24Z", "message": "Merge remote-tracking branch 'origin/master' into HDDS-4320"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "263ce7888903530656baf596b9f827636ec2ed28", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/263ce7888903530656baf596b9f827636ec2ed28", "committedDate": "2020-11-19T14:47:16Z", "message": "Merge remote-tracking branch 'origin/master' into HDDS-4320"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aa910488eb4f64eed7f2c3c3b6c15122e1f83cd7", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/aa910488eb4f64eed7f2c3c3b6c15122e1f83cd7", "committedDate": "2020-11-20T13:32:39Z", "message": "Refresh pipeline if read fails with CONTAINER_NOT_FOUND"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "658a16b9c0483da99bda30250de67ed19e9f39f8", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/658a16b9c0483da99bda30250de67ed19e9f39f8", "committedDate": "2020-11-20T15:00:03Z", "message": "Fix findbugs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/96394e72bb6f6d7544c8812f08f490f6002b1a30", "committedDate": "2020-11-23T14:09:31Z", "message": "Add integration test; Refresh pipeline on any read error"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM2NzA0NDUw", "url": "https://github.com/apache/ozone/pull/1523#pullrequestreview-536704450", "createdAt": "2020-11-23T17:39:43Z", "commit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxMTcwMTI3", "url": "https://github.com/apache/ozone/pull/1523#pullrequestreview-541170127", "createdAt": "2020-11-30T18:14:44Z", "commit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODoxNDo0NVrOH8HmjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwMDowMTo1N1rOH8SuUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgwMTE2NQ==", "bodyText": "This error message might be confusing if the refresh pipeline function exists. In the else case under refreshPipelineFunction != null, can we change the log level to info maybe?", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532801165", "createdAt": "2020-11-30T18:14:45Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockInputStream.java", "diffHunk": "@@ -171,6 +162,23 @@ public synchronized void initialize() throws IOException {\n     }\n   }\n \n+  private void refreshPipeline(IOException cause) throws IOException {\n+    LOG.error(\"Unable to read information for block {} from pipeline {}: {}\",\n+        blockID, pipeline.getId(), cause.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg0MjAxMQ==", "bodyText": "Can we also please update the javadoc for chunkPosition?", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532842011", "createdAt": "2020-11-30T19:23:33Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -292,6 +305,11 @@ private synchronized void readChunkFromContainer(int len) throws IOException {\n       startByteIndex = bufferOffset + bufferLength;\n     }\n \n+    // bufferOffset and bufferLength are updated below, but if read fails\n+    // and is retried, we need the previous position.  Position is reset after\n+    // successful read in adjustBufferPosition()\n+    storePosition();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk3OTUyMg==", "bodyText": "waitForReplicaCount=3 will encompass waitForReplicaCount=2.", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532979522", "createdAt": "2020-11-30T23:51:08Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "diffHunk": "@@ -397,4 +410,118 @@ public void testSkip() throws Exception {\n       Assert.assertEquals(inputData[chunkSize + 50 + i], readData[i]);\n     }\n   }\n+\n+  @Test\n+  public void readAfterReplication() throws Exception {\n+    testReadAfterReplication(false);\n+  }\n+\n+  @Test\n+  public void unbuffer() throws Exception {\n+    testReadAfterReplication(true);\n+  }\n+\n+  private void testReadAfterReplication(boolean doUnbuffer) throws Exception {\n+    Assume.assumeTrue(cluster.getHddsDatanodes().size() > 3);\n+\n+    int dataLength = 2 * chunkSize;\n+    String keyName = getKeyName();\n+    OzoneOutputStream key = TestHelper.createKey(keyName,\n+        ReplicationType.RATIS, dataLength, objectStore, volumeName, bucketName);\n+\n+    byte[] data = writeRandomBytes(key, dataLength);\n+\n+    OmKeyArgs keyArgs = new OmKeyArgs.Builder().setVolumeName(volumeName)\n+        .setBucketName(bucketName)\n+        .setKeyName(keyName)\n+        .setType(HddsProtos.ReplicationType.RATIS)\n+        .setFactor(HddsProtos.ReplicationFactor.THREE)\n+        .build();\n+    OmKeyInfo keyInfo = cluster.getOzoneManager().lookupKey(keyArgs);\n+\n+    OmKeyLocationInfoGroup locations = keyInfo.getLatestVersionLocations();\n+    Assert.assertNotNull(locations);\n+    List<OmKeyLocationInfo> locationInfoList = locations.getLocationList();\n+    Assert.assertEquals(1, locationInfoList.size());\n+    OmKeyLocationInfo loc = locationInfoList.get(0);\n+    long containerID = loc.getContainerID();\n+    Assert.assertEquals(3, countReplicas(containerID, cluster));\n+\n+    TestHelper.waitForContainerClose(cluster, containerID);\n+\n+    List<DatanodeDetails> pipelineNodes = loc.getPipeline().getNodes();\n+\n+    // read chunk data\n+    try (KeyInputStream keyInputStream = (KeyInputStream) objectStore\n+        .getVolume(volumeName).getBucket(bucketName)\n+        .readKey(keyName).getInputStream()) {\n+\n+      int b = keyInputStream.read();\n+      Assert.assertNotEquals(-1, b);\n+\n+      if (doUnbuffer) {\n+        keyInputStream.unbuffer();\n+      }\n+\n+      // stop one node, wait for container to be replicated to another one\n+      cluster.shutdownHddsDatanode(pipelineNodes.get(0));\n+      waitForNodeToBecomeDead(pipelineNodes.get(0));\n+      waitForReplicaCount(containerID, 2, cluster);\n+      waitForReplicaCount(containerID, 3, cluster);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 228}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MDQzMQ==", "bodyText": "How about naming this method something like readAfterReplicationWithUnbuffering or something to represent what the test verifies?", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532980431", "createdAt": "2020-11-30T23:53:45Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "diffHunk": "@@ -397,4 +410,118 @@ public void testSkip() throws Exception {\n       Assert.assertEquals(inputData[chunkSize + 50 + i], readData[i]);\n     }\n   }\n+\n+  @Test\n+  public void readAfterReplication() throws Exception {\n+    testReadAfterReplication(false);\n+  }\n+\n+  @Test\n+  public void unbuffer() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MTk2Ng==", "bodyText": "What are the transient cases in which this might fail?\nShould there be a retry in those cases? Otherwise these tests might fail intermittently.", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532981966", "createdAt": "2020-11-30T23:57:48Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/contract/AbstractContractUnbufferTest.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract;\n+\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.createFile;\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.dataset;\n+\n+/**\n+ * Contract tests for {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer}.\n+ * Note: this is from Hadoop 3.3, can be removed after dependency upgrade.\n+ */\n+public abstract class AbstractContractUnbufferTest\n+    extends AbstractFSContractTestBase {\n+\n+  private Path file;\n+  private byte[] fileBytes;\n+\n+  private static final String SUPPORTS_UNBUFFER = \"supports-unbuffer\";\n+\n+  @Override\n+  public void setup() throws Exception {\n+    super.setup();\n+    skipIfUnsupported(SUPPORTS_UNBUFFER);\n+    file = path(\"unbufferFile\");\n+    fileBytes = dataset(TEST_FILE_LEN, 0, 255);\n+    createFile(getFileSystem(), file, true, fileBytes);\n+  }\n+\n+  @Test\n+  public void testUnbufferAfterRead() throws IOException {\n+    describe(\"unbuffer a file after a single read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferBeforeRead() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferEmptyFile() throws IOException {\n+    Path emptyFile = path(\"emptyUnbufferFile\");\n+    getFileSystem().create(emptyFile, true).close();\n+    describe(\"unbuffer an empty file\");\n+    try (FSDataInputStream stream = getFileSystem().open(emptyFile)) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferOnClosedFile() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    FSDataInputStream stream = null;\n+    try {\n+      stream = getFileSystem().open(file);\n+      validateFullFileContents(stream);\n+    } finally {\n+      if (stream != null) {\n+        stream.close();\n+      }\n+    }\n+    if (stream != null) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testMultipleUnbuffers() throws IOException {\n+    describe(\"unbuffer a file multiple times\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferMultipleReads() throws IOException {\n+    describe(\"unbuffer a file multiple times\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 8, 0);\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 8, TEST_FILE_LEN / 8);\n+      validateFileContents(stream, TEST_FILE_LEN / 4, TEST_FILE_LEN / 4);\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 2, TEST_FILE_LEN / 2);\n+      unbuffer(stream);\n+      assertEquals(\"stream should be at end of file\", TEST_FILE_LEN,\n+              stream.getPos());\n+    }\n+  }\n+\n+  private void unbuffer(FSDataInputStream stream) throws IOException {\n+    long pos = stream.getPos();\n+    stream.unbuffer();\n+    assertEquals(\"unbuffer unexpectedly changed the stream position\", pos,\n+            stream.getPos());\n+  }\n+\n+  protected void validateFullFileContents(FSDataInputStream stream)\n+          throws IOException {\n+    validateFileContents(stream, TEST_FILE_LEN, 0);\n+  }\n+\n+  protected void validateFileContents(FSDataInputStream stream, int length,\n+                                      int startIndex)\n+          throws IOException {\n+    byte[] streamData = new byte[length];\n+    assertEquals(\"failed to read expected number of bytes from \"\n+            + \"stream. This may be transient\",\n+        length, stream.read(streamData));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4Mjk4NA==", "bodyText": "typo in description", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532982984", "createdAt": "2020-12-01T00:00:50Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/contract/AbstractContractUnbufferTest.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract;\n+\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.createFile;\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.dataset;\n+\n+/**\n+ * Contract tests for {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer}.\n+ * Note: this is from Hadoop 3.3, can be removed after dependency upgrade.\n+ */\n+public abstract class AbstractContractUnbufferTest\n+    extends AbstractFSContractTestBase {\n+\n+  private Path file;\n+  private byte[] fileBytes;\n+\n+  private static final String SUPPORTS_UNBUFFER = \"supports-unbuffer\";\n+\n+  @Override\n+  public void setup() throws Exception {\n+    super.setup();\n+    skipIfUnsupported(SUPPORTS_UNBUFFER);\n+    file = path(\"unbufferFile\");\n+    fileBytes = dataset(TEST_FILE_LEN, 0, 255);\n+    createFile(getFileSystem(), file, true, fileBytes);\n+  }\n+\n+  @Test\n+  public void testUnbufferAfterRead() throws IOException {\n+    describe(\"unbuffer a file after a single read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferBeforeRead() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferEmptyFile() throws IOException {\n+    Path emptyFile = path(\"emptyUnbufferFile\");\n+    getFileSystem().create(emptyFile, true).close();\n+    describe(\"unbuffer an empty file\");\n+    try (FSDataInputStream stream = getFileSystem().open(emptyFile)) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferOnClosedFile() throws IOException {\n+    describe(\"unbuffer a file before a read\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MzM3Nw==", "bodyText": "unbuffer() here just checks that the position is maintained. Would it be possible to also verify that the buffers are actually released?", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532983377", "createdAt": "2020-12-01T00:01:57Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/contract/AbstractContractUnbufferTest.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract;\n+\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.createFile;\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.dataset;\n+\n+/**\n+ * Contract tests for {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer}.\n+ * Note: this is from Hadoop 3.3, can be removed after dependency upgrade.\n+ */\n+public abstract class AbstractContractUnbufferTest\n+    extends AbstractFSContractTestBase {\n+\n+  private Path file;\n+  private byte[] fileBytes;\n+\n+  private static final String SUPPORTS_UNBUFFER = \"supports-unbuffer\";\n+\n+  @Override\n+  public void setup() throws Exception {\n+    super.setup();\n+    skipIfUnsupported(SUPPORTS_UNBUFFER);\n+    file = path(\"unbufferFile\");\n+    fileBytes = dataset(TEST_FILE_LEN, 0, 255);\n+    createFile(getFileSystem(), file, true, fileBytes);\n+  }\n+\n+  @Test\n+  public void testUnbufferAfterRead() throws IOException {\n+    describe(\"unbuffer a file after a single read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferBeforeRead() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferEmptyFile() throws IOException {\n+    Path emptyFile = path(\"emptyUnbufferFile\");\n+    getFileSystem().create(emptyFile, true).close();\n+    describe(\"unbuffer an empty file\");\n+    try (FSDataInputStream stream = getFileSystem().open(emptyFile)) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferOnClosedFile() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    FSDataInputStream stream = null;\n+    try {\n+      stream = getFileSystem().open(file);\n+      validateFullFileContents(stream);\n+    } finally {\n+      if (stream != null) {\n+        stream.close();\n+      }\n+    }\n+    if (stream != null) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testMultipleUnbuffers() throws IOException {\n+    describe(\"unbuffer a file multiple times\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferMultipleReads() throws IOException {\n+    describe(\"unbuffer a file multiple times\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 8, 0);\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 8, TEST_FILE_LEN / 8);\n+      validateFileContents(stream, TEST_FILE_LEN / 4, TEST_FILE_LEN / 4);\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 2, TEST_FILE_LEN / 2);\n+      unbuffer(stream);\n+      assertEquals(\"stream should be at end of file\", TEST_FILE_LEN,\n+              stream.getPos());\n+    }\n+  }\n+\n+  private void unbuffer(FSDataInputStream stream) throws IOException {\n+    long pos = stream.getPos();\n+    stream.unbuffer();\n+    assertEquals(\"unbuffer unexpectedly changed the stream position\", pos,\n+            stream.getPos());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 131}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6550545d1669ffe54d9ce0032cf54bb04dd6db6c", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/6550545d1669ffe54d9ce0032cf54bb04dd6db6c", "committedDate": "2020-12-01T08:22:29Z", "message": "Merge remote-tracking branch 'origin/master' into HDDS-4320"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ec191ebf80dcc7ffadc4ef01793192c57586252d", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/ec191ebf80dcc7ffadc4ef01793192c57586252d", "committedDate": "2020-12-01T09:13:24Z", "message": "Address review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyMjY0MjU3", "url": "https://github.com/apache/ozone/pull/1523#pullrequestreview-542264257", "createdAt": "2020-12-01T20:20:45Z", "commit": {"oid": "ec191ebf80dcc7ffadc4ef01793192c57586252d"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2180, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}