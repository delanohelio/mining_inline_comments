{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTEwMTc1OTUz", "number": 1523, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODoxNDo0NVrOE-x2fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwMDowMTo1N1rOE-5FUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MjYzOTM1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockInputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxODoxNDo0NVrOH8HmjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwODo1NDozOFrOH8eOxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgwMTE2NQ==", "bodyText": "This error message might be confusing if the refresh pipeline function exists. In the else case under refreshPipelineFunction != null, can we change the log level to info maybe?", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532801165", "createdAt": "2020-11-30T18:14:45Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockInputStream.java", "diffHunk": "@@ -171,6 +162,23 @@ public synchronized void initialize() throws IOException {\n     }\n   }\n \n+  private void refreshPipeline(IOException cause) throws IOException {\n+    LOG.error(\"Unable to read information for block {} from pipeline {}: {}\",\n+        blockID, pipeline.getId(), cause.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE3MTkwOA==", "bodyText": "It was an error previously, I only changed the message a bit from:\nLOG.error(\"Unable to read block information from pipeline.\");\n\nbut I agree, it should be logged at lower level.", "url": "https://github.com/apache/ozone/pull/1523#discussion_r533171908", "createdAt": "2020-12-01T08:54:38Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockInputStream.java", "diffHunk": "@@ -171,6 +162,23 @@ public synchronized void initialize() throws IOException {\n     }\n   }\n \n+  private void refreshPipeline(IOException cause) throws IOException {\n+    LOG.error(\"Unable to read information for block {} from pipeline {}: {}\",\n+        blockID, pipeline.getId(), cause.getMessage());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjgwMTE2NQ=="}, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MjkwNjYxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxOToyMzozM1rOH8KGGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxOToyMzozM1rOH8KGGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg0MjAxMQ==", "bodyText": "Can we also please update the javadoc for chunkPosition?", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532842011", "createdAt": "2020-11-30T19:23:33Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -292,6 +305,11 @@ private synchronized void readChunkFromContainer(int len) throws IOException {\n       startByteIndex = bufferOffset + bufferLength;\n     }\n \n+    // bufferOffset and bufferLength are updated below, but if read fails\n+    // and is retried, we need the previous position.  Position is reset after\n+    // successful read in adjustBufferPosition()\n+    storePosition();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0Mzc5ODI1OnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMzo1MTowOFrOH8SfQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwOTowMjoxM1rOH8ezEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk3OTUyMg==", "bodyText": "waitForReplicaCount=3 will encompass waitForReplicaCount=2.", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532979522", "createdAt": "2020-11-30T23:51:08Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "diffHunk": "@@ -397,4 +410,118 @@ public void testSkip() throws Exception {\n       Assert.assertEquals(inputData[chunkSize + 50 + i], readData[i]);\n     }\n   }\n+\n+  @Test\n+  public void readAfterReplication() throws Exception {\n+    testReadAfterReplication(false);\n+  }\n+\n+  @Test\n+  public void unbuffer() throws Exception {\n+    testReadAfterReplication(true);\n+  }\n+\n+  private void testReadAfterReplication(boolean doUnbuffer) throws Exception {\n+    Assume.assumeTrue(cluster.getHddsDatanodes().size() > 3);\n+\n+    int dataLength = 2 * chunkSize;\n+    String keyName = getKeyName();\n+    OzoneOutputStream key = TestHelper.createKey(keyName,\n+        ReplicationType.RATIS, dataLength, objectStore, volumeName, bucketName);\n+\n+    byte[] data = writeRandomBytes(key, dataLength);\n+\n+    OmKeyArgs keyArgs = new OmKeyArgs.Builder().setVolumeName(volumeName)\n+        .setBucketName(bucketName)\n+        .setKeyName(keyName)\n+        .setType(HddsProtos.ReplicationType.RATIS)\n+        .setFactor(HddsProtos.ReplicationFactor.THREE)\n+        .build();\n+    OmKeyInfo keyInfo = cluster.getOzoneManager().lookupKey(keyArgs);\n+\n+    OmKeyLocationInfoGroup locations = keyInfo.getLatestVersionLocations();\n+    Assert.assertNotNull(locations);\n+    List<OmKeyLocationInfo> locationInfoList = locations.getLocationList();\n+    Assert.assertEquals(1, locationInfoList.size());\n+    OmKeyLocationInfo loc = locationInfoList.get(0);\n+    long containerID = loc.getContainerID();\n+    Assert.assertEquals(3, countReplicas(containerID, cluster));\n+\n+    TestHelper.waitForContainerClose(cluster, containerID);\n+\n+    List<DatanodeDetails> pipelineNodes = loc.getPipeline().getNodes();\n+\n+    // read chunk data\n+    try (KeyInputStream keyInputStream = (KeyInputStream) objectStore\n+        .getVolume(volumeName).getBucket(bucketName)\n+        .readKey(keyName).getInputStream()) {\n+\n+      int b = keyInputStream.read();\n+      Assert.assertNotEquals(-1, b);\n+\n+      if (doUnbuffer) {\n+        keyInputStream.unbuffer();\n+      }\n+\n+      // stop one node, wait for container to be replicated to another one\n+      cluster.shutdownHddsDatanode(pipelineNodes.get(0));\n+      waitForNodeToBecomeDead(pipelineNodes.get(0));\n+      waitForReplicaCount(containerID, 2, cluster);\n+      waitForReplicaCount(containerID, 3, cluster);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 228}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE4MTIwMw==", "bodyText": "We need to waitForReplicaCount=2 first to ensure one of the original replicas is lost.", "url": "https://github.com/apache/ozone/pull/1523#discussion_r533181203", "createdAt": "2020-12-01T09:02:13Z", "author": {"login": "adoroszlai"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "diffHunk": "@@ -397,4 +410,118 @@ public void testSkip() throws Exception {\n       Assert.assertEquals(inputData[chunkSize + 50 + i], readData[i]);\n     }\n   }\n+\n+  @Test\n+  public void readAfterReplication() throws Exception {\n+    testReadAfterReplication(false);\n+  }\n+\n+  @Test\n+  public void unbuffer() throws Exception {\n+    testReadAfterReplication(true);\n+  }\n+\n+  private void testReadAfterReplication(boolean doUnbuffer) throws Exception {\n+    Assume.assumeTrue(cluster.getHddsDatanodes().size() > 3);\n+\n+    int dataLength = 2 * chunkSize;\n+    String keyName = getKeyName();\n+    OzoneOutputStream key = TestHelper.createKey(keyName,\n+        ReplicationType.RATIS, dataLength, objectStore, volumeName, bucketName);\n+\n+    byte[] data = writeRandomBytes(key, dataLength);\n+\n+    OmKeyArgs keyArgs = new OmKeyArgs.Builder().setVolumeName(volumeName)\n+        .setBucketName(bucketName)\n+        .setKeyName(keyName)\n+        .setType(HddsProtos.ReplicationType.RATIS)\n+        .setFactor(HddsProtos.ReplicationFactor.THREE)\n+        .build();\n+    OmKeyInfo keyInfo = cluster.getOzoneManager().lookupKey(keyArgs);\n+\n+    OmKeyLocationInfoGroup locations = keyInfo.getLatestVersionLocations();\n+    Assert.assertNotNull(locations);\n+    List<OmKeyLocationInfo> locationInfoList = locations.getLocationList();\n+    Assert.assertEquals(1, locationInfoList.size());\n+    OmKeyLocationInfo loc = locationInfoList.get(0);\n+    long containerID = loc.getContainerID();\n+    Assert.assertEquals(3, countReplicas(containerID, cluster));\n+\n+    TestHelper.waitForContainerClose(cluster, containerID);\n+\n+    List<DatanodeDetails> pipelineNodes = loc.getPipeline().getNodes();\n+\n+    // read chunk data\n+    try (KeyInputStream keyInputStream = (KeyInputStream) objectStore\n+        .getVolume(volumeName).getBucket(bucketName)\n+        .readKey(keyName).getInputStream()) {\n+\n+      int b = keyInputStream.read();\n+      Assert.assertNotEquals(-1, b);\n+\n+      if (doUnbuffer) {\n+        keyInputStream.unbuffer();\n+      }\n+\n+      // stop one node, wait for container to be replicated to another one\n+      cluster.shutdownHddsDatanode(pipelineNodes.get(0));\n+      waitForNodeToBecomeDead(pipelineNodes.get(0));\n+      waitForReplicaCount(containerID, 2, cluster);\n+      waitForReplicaCount(containerID, 3, cluster);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk3OTUyMg=="}, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 228}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzgwMzkxOnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMzo1Mzo0NVrOH8Sizw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMzo1Mzo0NVrOH8Sizw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MDQzMQ==", "bodyText": "How about naming this method something like readAfterReplicationWithUnbuffering or something to represent what the test verifies?", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532980431", "createdAt": "2020-11-30T23:53:45Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/client/rpc/TestKeyInputStream.java", "diffHunk": "@@ -397,4 +410,118 @@ public void testSkip() throws Exception {\n       Assert.assertEquals(inputData[chunkSize + 50 + i], readData[i]);\n     }\n   }\n+\n+  @Test\n+  public void readAfterReplication() throws Exception {\n+    testReadAfterReplication(false);\n+  }\n+\n+  @Test\n+  public void unbuffer() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 178}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzgxNDcwOnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/contract/AbstractContractUnbufferTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMzo1Nzo0OFrOH8Sozg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwOToxMDo1NlrOH8fcWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MTk2Ng==", "bodyText": "What are the transient cases in which this might fail?\nShould there be a retry in those cases? Otherwise these tests might fail intermittently.", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532981966", "createdAt": "2020-11-30T23:57:48Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/contract/AbstractContractUnbufferTest.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract;\n+\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.createFile;\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.dataset;\n+\n+/**\n+ * Contract tests for {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer}.\n+ * Note: this is from Hadoop 3.3, can be removed after dependency upgrade.\n+ */\n+public abstract class AbstractContractUnbufferTest\n+    extends AbstractFSContractTestBase {\n+\n+  private Path file;\n+  private byte[] fileBytes;\n+\n+  private static final String SUPPORTS_UNBUFFER = \"supports-unbuffer\";\n+\n+  @Override\n+  public void setup() throws Exception {\n+    super.setup();\n+    skipIfUnsupported(SUPPORTS_UNBUFFER);\n+    file = path(\"unbufferFile\");\n+    fileBytes = dataset(TEST_FILE_LEN, 0, 255);\n+    createFile(getFileSystem(), file, true, fileBytes);\n+  }\n+\n+  @Test\n+  public void testUnbufferAfterRead() throws IOException {\n+    describe(\"unbuffer a file after a single read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferBeforeRead() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferEmptyFile() throws IOException {\n+    Path emptyFile = path(\"emptyUnbufferFile\");\n+    getFileSystem().create(emptyFile, true).close();\n+    describe(\"unbuffer an empty file\");\n+    try (FSDataInputStream stream = getFileSystem().open(emptyFile)) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferOnClosedFile() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    FSDataInputStream stream = null;\n+    try {\n+      stream = getFileSystem().open(file);\n+      validateFullFileContents(stream);\n+    } finally {\n+      if (stream != null) {\n+        stream.close();\n+      }\n+    }\n+    if (stream != null) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testMultipleUnbuffers() throws IOException {\n+    describe(\"unbuffer a file multiple times\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferMultipleReads() throws IOException {\n+    describe(\"unbuffer a file multiple times\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 8, 0);\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 8, TEST_FILE_LEN / 8);\n+      validateFileContents(stream, TEST_FILE_LEN / 4, TEST_FILE_LEN / 4);\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 2, TEST_FILE_LEN / 2);\n+      unbuffer(stream);\n+      assertEquals(\"stream should be at end of file\", TEST_FILE_LEN,\n+              stream.getPos());\n+    }\n+  }\n+\n+  private void unbuffer(FSDataInputStream stream) throws IOException {\n+    long pos = stream.getPos();\n+    stream.unbuffer();\n+    assertEquals(\"unbuffer unexpectedly changed the stream position\", pos,\n+            stream.getPos());\n+  }\n+\n+  protected void validateFullFileContents(FSDataInputStream stream)\n+          throws IOException {\n+    validateFileContents(stream, TEST_FILE_LEN, 0);\n+  }\n+\n+  protected void validateFileContents(FSDataInputStream stream, int length,\n+                                      int startIndex)\n+          throws IOException {\n+    byte[] streamData = new byte[length];\n+    assertEquals(\"failed to read expected number of bytes from \"\n+            + \"stream. This may be transient\",\n+        length, stream.read(streamData));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE5MTc3MQ==", "bodyText": "I don't know.  AbstractContractUnbufferTest is copied as is from Hadoop 3.3 source, since we cannot yet upgrade dependency to that version.  The only changes I had to make were for checkstyle (too long line) and the new constant SUPPORTS_UNBUFFER introduced in ContractOptions (which the test implements).  I'd rather keep it close to the original version (and post any improvements/fixes to apache/hadoop first).", "url": "https://github.com/apache/ozone/pull/1523#discussion_r533191771", "createdAt": "2020-12-01T09:10:56Z", "author": {"login": "adoroszlai"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/contract/AbstractContractUnbufferTest.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract;\n+\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.createFile;\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.dataset;\n+\n+/**\n+ * Contract tests for {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer}.\n+ * Note: this is from Hadoop 3.3, can be removed after dependency upgrade.\n+ */\n+public abstract class AbstractContractUnbufferTest\n+    extends AbstractFSContractTestBase {\n+\n+  private Path file;\n+  private byte[] fileBytes;\n+\n+  private static final String SUPPORTS_UNBUFFER = \"supports-unbuffer\";\n+\n+  @Override\n+  public void setup() throws Exception {\n+    super.setup();\n+    skipIfUnsupported(SUPPORTS_UNBUFFER);\n+    file = path(\"unbufferFile\");\n+    fileBytes = dataset(TEST_FILE_LEN, 0, 255);\n+    createFile(getFileSystem(), file, true, fileBytes);\n+  }\n+\n+  @Test\n+  public void testUnbufferAfterRead() throws IOException {\n+    describe(\"unbuffer a file after a single read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferBeforeRead() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferEmptyFile() throws IOException {\n+    Path emptyFile = path(\"emptyUnbufferFile\");\n+    getFileSystem().create(emptyFile, true).close();\n+    describe(\"unbuffer an empty file\");\n+    try (FSDataInputStream stream = getFileSystem().open(emptyFile)) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferOnClosedFile() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    FSDataInputStream stream = null;\n+    try {\n+      stream = getFileSystem().open(file);\n+      validateFullFileContents(stream);\n+    } finally {\n+      if (stream != null) {\n+        stream.close();\n+      }\n+    }\n+    if (stream != null) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testMultipleUnbuffers() throws IOException {\n+    describe(\"unbuffer a file multiple times\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferMultipleReads() throws IOException {\n+    describe(\"unbuffer a file multiple times\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 8, 0);\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 8, TEST_FILE_LEN / 8);\n+      validateFileContents(stream, TEST_FILE_LEN / 4, TEST_FILE_LEN / 4);\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 2, TEST_FILE_LEN / 2);\n+      unbuffer(stream);\n+      assertEquals(\"stream should be at end of file\", TEST_FILE_LEN,\n+              stream.getPos());\n+    }\n+  }\n+\n+  private void unbuffer(FSDataInputStream stream) throws IOException {\n+    long pos = stream.getPos();\n+    stream.unbuffer();\n+    assertEquals(\"unbuffer unexpectedly changed the stream position\", pos,\n+            stream.getPos());\n+  }\n+\n+  protected void validateFullFileContents(FSDataInputStream stream)\n+          throws IOException {\n+    validateFileContents(stream, TEST_FILE_LEN, 0);\n+  }\n+\n+  protected void validateFileContents(FSDataInputStream stream, int length,\n+                                      int startIndex)\n+          throws IOException {\n+    byte[] streamData = new byte[length];\n+    assertEquals(\"failed to read expected number of bytes from \"\n+            + \"stream. This may be transient\",\n+        length, stream.read(streamData));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MTk2Ng=="}, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzgyMTU2OnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/contract/AbstractContractUnbufferTest.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwMDowMDo1MFrOH8SsyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwMDowMDo1MFrOH8SsyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4Mjk4NA==", "bodyText": "typo in description", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532982984", "createdAt": "2020-12-01T00:00:50Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/contract/AbstractContractUnbufferTest.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract;\n+\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.createFile;\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.dataset;\n+\n+/**\n+ * Contract tests for {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer}.\n+ * Note: this is from Hadoop 3.3, can be removed after dependency upgrade.\n+ */\n+public abstract class AbstractContractUnbufferTest\n+    extends AbstractFSContractTestBase {\n+\n+  private Path file;\n+  private byte[] fileBytes;\n+\n+  private static final String SUPPORTS_UNBUFFER = \"supports-unbuffer\";\n+\n+  @Override\n+  public void setup() throws Exception {\n+    super.setup();\n+    skipIfUnsupported(SUPPORTS_UNBUFFER);\n+    file = path(\"unbufferFile\");\n+    fileBytes = dataset(TEST_FILE_LEN, 0, 255);\n+    createFile(getFileSystem(), file, true, fileBytes);\n+  }\n+\n+  @Test\n+  public void testUnbufferAfterRead() throws IOException {\n+    describe(\"unbuffer a file after a single read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferBeforeRead() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferEmptyFile() throws IOException {\n+    Path emptyFile = path(\"emptyUnbufferFile\");\n+    getFileSystem().create(emptyFile, true).close();\n+    describe(\"unbuffer an empty file\");\n+    try (FSDataInputStream stream = getFileSystem().open(emptyFile)) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferOnClosedFile() throws IOException {\n+    describe(\"unbuffer a file before a read\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzgyNDE4OnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/contract/AbstractContractUnbufferTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwMDowMTo1N1rOH8SuUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwOToxMDo1M1rOH8fcIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MzM3Nw==", "bodyText": "unbuffer() here just checks that the position is maintained. Would it be possible to also verify that the buffers are actually released?", "url": "https://github.com/apache/ozone/pull/1523#discussion_r532983377", "createdAt": "2020-12-01T00:01:57Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/contract/AbstractContractUnbufferTest.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract;\n+\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.createFile;\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.dataset;\n+\n+/**\n+ * Contract tests for {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer}.\n+ * Note: this is from Hadoop 3.3, can be removed after dependency upgrade.\n+ */\n+public abstract class AbstractContractUnbufferTest\n+    extends AbstractFSContractTestBase {\n+\n+  private Path file;\n+  private byte[] fileBytes;\n+\n+  private static final String SUPPORTS_UNBUFFER = \"supports-unbuffer\";\n+\n+  @Override\n+  public void setup() throws Exception {\n+    super.setup();\n+    skipIfUnsupported(SUPPORTS_UNBUFFER);\n+    file = path(\"unbufferFile\");\n+    fileBytes = dataset(TEST_FILE_LEN, 0, 255);\n+    createFile(getFileSystem(), file, true, fileBytes);\n+  }\n+\n+  @Test\n+  public void testUnbufferAfterRead() throws IOException {\n+    describe(\"unbuffer a file after a single read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferBeforeRead() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferEmptyFile() throws IOException {\n+    Path emptyFile = path(\"emptyUnbufferFile\");\n+    getFileSystem().create(emptyFile, true).close();\n+    describe(\"unbuffer an empty file\");\n+    try (FSDataInputStream stream = getFileSystem().open(emptyFile)) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferOnClosedFile() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    FSDataInputStream stream = null;\n+    try {\n+      stream = getFileSystem().open(file);\n+      validateFullFileContents(stream);\n+    } finally {\n+      if (stream != null) {\n+        stream.close();\n+      }\n+    }\n+    if (stream != null) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testMultipleUnbuffers() throws IOException {\n+    describe(\"unbuffer a file multiple times\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferMultipleReads() throws IOException {\n+    describe(\"unbuffer a file multiple times\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 8, 0);\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 8, TEST_FILE_LEN / 8);\n+      validateFileContents(stream, TEST_FILE_LEN / 4, TEST_FILE_LEN / 4);\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 2, TEST_FILE_LEN / 2);\n+      unbuffer(stream);\n+      assertEquals(\"stream should be at end of file\", TEST_FILE_LEN,\n+              stream.getPos());\n+    }\n+  }\n+\n+  private void unbuffer(FSDataInputStream stream) throws IOException {\n+    long pos = stream.getPos();\n+    stream.unbuffer();\n+    assertEquals(\"unbuffer unexpectedly changed the stream position\", pos,\n+            stream.getPos());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE5MTcxMg==", "bodyText": "I don't think it can, as it does not know about implementation internals.  This is verified in TestChunkInputStream instead.  Hadoop also has additional implementation-specific test for this reason.", "url": "https://github.com/apache/ozone/pull/1523#discussion_r533191712", "createdAt": "2020-12-01T09:10:53Z", "author": {"login": "adoroszlai"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/fs/contract/AbstractContractUnbufferTest.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *       http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.contract;\n+\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.createFile;\n+import static org.apache.hadoop.fs.contract.ContractTestUtils.dataset;\n+\n+/**\n+ * Contract tests for {@link org.apache.hadoop.fs.CanUnbuffer#unbuffer}.\n+ * Note: this is from Hadoop 3.3, can be removed after dependency upgrade.\n+ */\n+public abstract class AbstractContractUnbufferTest\n+    extends AbstractFSContractTestBase {\n+\n+  private Path file;\n+  private byte[] fileBytes;\n+\n+  private static final String SUPPORTS_UNBUFFER = \"supports-unbuffer\";\n+\n+  @Override\n+  public void setup() throws Exception {\n+    super.setup();\n+    skipIfUnsupported(SUPPORTS_UNBUFFER);\n+    file = path(\"unbufferFile\");\n+    fileBytes = dataset(TEST_FILE_LEN, 0, 255);\n+    createFile(getFileSystem(), file, true, fileBytes);\n+  }\n+\n+  @Test\n+  public void testUnbufferAfterRead() throws IOException {\n+    describe(\"unbuffer a file after a single read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferBeforeRead() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferEmptyFile() throws IOException {\n+    Path emptyFile = path(\"emptyUnbufferFile\");\n+    getFileSystem().create(emptyFile, true).close();\n+    describe(\"unbuffer an empty file\");\n+    try (FSDataInputStream stream = getFileSystem().open(emptyFile)) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferOnClosedFile() throws IOException {\n+    describe(\"unbuffer a file before a read\");\n+    FSDataInputStream stream = null;\n+    try {\n+      stream = getFileSystem().open(file);\n+      validateFullFileContents(stream);\n+    } finally {\n+      if (stream != null) {\n+        stream.close();\n+      }\n+    }\n+    if (stream != null) {\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testMultipleUnbuffers() throws IOException {\n+    describe(\"unbuffer a file multiple times\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      unbuffer(stream);\n+      validateFullFileContents(stream);\n+      unbuffer(stream);\n+      unbuffer(stream);\n+    }\n+  }\n+\n+  @Test\n+  public void testUnbufferMultipleReads() throws IOException {\n+    describe(\"unbuffer a file multiple times\");\n+    try (FSDataInputStream stream = getFileSystem().open(file)) {\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 8, 0);\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 8, TEST_FILE_LEN / 8);\n+      validateFileContents(stream, TEST_FILE_LEN / 4, TEST_FILE_LEN / 4);\n+      unbuffer(stream);\n+      validateFileContents(stream, TEST_FILE_LEN / 2, TEST_FILE_LEN / 2);\n+      unbuffer(stream);\n+      assertEquals(\"stream should be at end of file\", TEST_FILE_LEN,\n+              stream.getPos());\n+    }\n+  }\n+\n+  private void unbuffer(FSDataInputStream stream) throws IOException {\n+    long pos = stream.getPos();\n+    stream.unbuffer();\n+    assertEquals(\"unbuffer unexpectedly changed the stream position\", pos,\n+            stream.getPos());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk4MzM3Nw=="}, "originalCommit": {"oid": "96394e72bb6f6d7544c8812f08f490f6002b1a30"}, "originalPosition": 131}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4655, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}