{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY5MDQxNjkx", "number": 1336, "title": "HDDS-4119. Improve performance of the BufferPool management of Ozone client", "bodyText": "What changes were proposed in this pull request?\nTeragen reported to be slow with low number of mappers compared to HDFS.\nIn my test (one pipeline, 3 yarn nodes) 10 g teragen with HDFS was ~3 mins but with Ozone it was 6 mins. It could be fixed with using more mappers, but when I investigated the execution I found a few problems reagrding to the BufferPool management.\n\nIncrementalChunkBuffer is slow and it might not be required as BufferPool itself is incremental\nFor each write operation the bufferPool.allocateBufferIfNeeded is called which can be a slow operation (positions should be  calculated).\nThere is no explicit support for write(byte) operations\n\nIn the flamegraphs it's clearly visible that with low number of mappers the client is busy with buffer operations. After the patch the Rpc call and the checksum calculation give the majority of the time.\nOverall write performance is improved with at least 30% when minimal number of threads/mappers are used.\nThanks\nSpecial thanks to @lokeshj1703, who helped me find the small mistakes in the original verison of the patch.\nWhat is the link to the Apache JIRA\nhttps://issues.apache.org/jira/browse/HDDS-4119\nHow was this patch tested?\nTeragen 10/100g with 2/30 mappers.\n(https://github.com/elek/ozone-perf-env/tree/master/teragen-hdfs)", "createdAt": "2020-08-17T20:05:05Z", "url": "https://github.com/apache/ozone/pull/1336", "merged": true, "mergeCommit": {"oid": "72e3215846bfaaa562aa9fb7a15f87f27e97867c"}, "closed": true, "closedAt": "2020-09-11T14:25:04Z", "author": {"login": "elek"}, "timelineItems": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc-b23UgH2gAyNDY5MDQxNjkxOjE2MzQ3Yjc4YzQxZGU5ODJkODFhMDg3NmZiMTJhZTczMGYxOWJkZGE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdG0UopgFqTQ4Mzk2NDk3Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "16347b78c41de982d81a0876fb12ae730f19bdda", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/16347b78c41de982d81a0876fb12ae730f19bdda", "committedDate": "2020-08-13T08:36:45Z", "message": "teragenfix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "61a8e6a48f4d04d0c23bf1b28a93ece33e9e9929", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/61a8e6a48f4d04d0c23bf1b28a93ece33e9e9929", "committedDate": "2020-08-13T08:38:54Z", "message": "revert genesis changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2e32191450c90f533e2d96b98f326f4d70bbafcd", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/2e32191450c90f533e2d96b98f326f4d70bbafcd", "committedDate": "2020-08-13T09:50:02Z", "message": "cleanup patch"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "962cfd52c79f418d5fdf0ab92c2d8305fce5fe19", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/962cfd52c79f418d5fdf0ab92c2d8305fce5fe19", "committedDate": "2020-08-13T10:26:25Z", "message": "Cleanup tests and block output stream"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d4456c00dc085284a80341d298a41496179af3e7", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/d4456c00dc085284a80341d298a41496179af3e7", "committedDate": "2020-08-13T12:17:35Z", "message": "fix buffer pool allocation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1c4f272e9e68a77aab4a60e44231af5e4ca184a3", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/1c4f272e9e68a77aab4a60e44231af5e4ca184a3", "committedDate": "2020-08-13T13:58:21Z", "message": "unit test fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "65a542ffef1a78e64bf19a50a8dcc99f9a4eaf16", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/65a542ffef1a78e64bf19a50a8dcc99f9a4eaf16", "committedDate": "2020-08-13T15:29:32Z", "message": "additional debug log"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "256db2a7426f6d5c3e08ef7b1c4358cdca30efa5", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/256db2a7426f6d5c3e08ef7b1c4358cdca30efa5", "committedDate": "2020-08-14T07:40:51Z", "message": "fix write(byte) with the help of Lokesh"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "118d8ce41e65ed9947108abce7ba3d2a77bd4a5a", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/118d8ce41e65ed9947108abce7ba3d2a77bd4a5a", "committedDate": "2020-08-15T05:56:25Z", "message": "Additional fixes from Lokesh"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "514f7110c09defd3cb482b6cba99665acc0a1ccd", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/514f7110c09defd3cb482b6cba99665acc0a1ccd", "committedDate": "2020-08-17T10:08:50Z", "message": "rat and checkstyle fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/dd99deb1ae7b7a8bb5eed17681c473bdfb410f30", "committedDate": "2020-08-17T10:18:55Z", "message": "checkstyle fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDczMjI5OTMz", "url": "https://github.com/apache/ozone/pull/1336#pullrequestreview-473229933", "createdAt": "2020-08-24T08:39:45Z", "commit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwODozOTo0NVrOHFaBeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwOTozMToyN1rOHFcLHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQzMTI4OQ==", "bodyText": "Can you please extract these 2 lines to a separate method (and replace other 2 duplicate fragments, too)?", "url": "https://github.com/apache/ozone/pull/1336#discussion_r475431289", "createdAt": "2020-08-24T08:39:45Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "diffHunk": "@@ -154,6 +159,16 @@ public BlockOutputStream(BlockID blockID,\n     this.bufferPool = bufferPool;\n     this.bytesPerChecksum = bytesPerChecksum;\n \n+    //number of buffers used before doing a flush\n+    currentBuffer = bufferPool.getCurrentBuffer();\n+    currentBufferRemaining =\n+        currentBuffer != null ? currentBuffer.remaining() : 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQzOTQzMg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                return put(buf, 0, 1);  }\n          \n          \n            \n                return put(buf, 0, 1);\n          \n          \n            \n              }", "url": "https://github.com/apache/ozone/pull/1336#discussion_r475439432", "createdAt": "2020-08-24T08:53:37Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChunkBuffer.java", "diffHunk": "@@ -88,6 +86,12 @@ default ChunkBuffer put(byte[] b) {\n     return put(ByteBuffer.wrap(b));\n   }\n \n+  /** Similar to {@link ByteBuffer#put(byte[])}. */\n+  default ChunkBuffer put(byte b) {\n+    byte[] buf = new byte[1];\n+    buf[0] = (byte) b;\n+    return put(buf, 0, 1);  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ0MDUzNw==", "bodyText": "\ud83c\udded\ud83c\uddfa", "url": "https://github.com/apache/ozone/pull/1336#discussion_r475440537", "createdAt": "2020-08-24T08:55:31Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/test/java/org/apache/hadoop/hdds/scm/storage/TestBlockOutputStreamCorrectness.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.storage;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.hdds.client.BlockID;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.MockDatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ChecksumType;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerCommandRequestProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerCommandResponseProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.GetCommittedBlockLengthResponseProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.PutBlockResponseProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.Result;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.Type;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationFactor;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationType;\n+import org.apache.hadoop.hdds.scm.XceiverClientManager;\n+import org.apache.hadoop.hdds.scm.XceiverClientReply;\n+import org.apache.hadoop.hdds.scm.XceiverClientSpi;\n+import org.apache.hadoop.hdds.scm.pipeline.Pipeline;\n+import org.apache.hadoop.hdds.scm.pipeline.Pipeline.Builder;\n+import org.apache.hadoop.hdds.scm.pipeline.Pipeline.PipelineState;\n+import org.apache.hadoop.hdds.scm.pipeline.PipelineID;\n+\n+import org.apache.ratis.thirdparty.com.google.protobuf.ByteString;\n+import org.jetbrains.annotations.NotNull;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+/**\n+ * UNIT test for BlockOutputStream.\n+ * <p>\n+ * Compares bytes written to the stream and received in the ChunkWriteRequests.\n+ */\n+public class TestBlockOutputStreamCorrectness {\n+\n+  private static final long SEED = 18480315L;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ0MzEyMg==", "bodyText": "Can we move this logic to MockPipeline for reuse?", "url": "https://github.com/apache/ozone/pull/1336#discussion_r475443122", "createdAt": "2020-08-24T08:59:57Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/test/java/org/apache/hadoop/hdds/scm/storage/TestBlockOutputStreamCorrectness.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.storage;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.hdds.client.BlockID;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.MockDatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ChecksumType;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerCommandRequestProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerCommandResponseProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.GetCommittedBlockLengthResponseProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.PutBlockResponseProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.Result;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.Type;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationFactor;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationType;\n+import org.apache.hadoop.hdds.scm.XceiverClientManager;\n+import org.apache.hadoop.hdds.scm.XceiverClientReply;\n+import org.apache.hadoop.hdds.scm.XceiverClientSpi;\n+import org.apache.hadoop.hdds.scm.pipeline.Pipeline;\n+import org.apache.hadoop.hdds.scm.pipeline.Pipeline.Builder;\n+import org.apache.hadoop.hdds.scm.pipeline.Pipeline.PipelineState;\n+import org.apache.hadoop.hdds.scm.pipeline.PipelineID;\n+\n+import org.apache.ratis.thirdparty.com.google.protobuf.ByteString;\n+import org.jetbrains.annotations.NotNull;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+/**\n+ * UNIT test for BlockOutputStream.\n+ * <p>\n+ * Compares bytes written to the stream and received in the ChunkWriteRequests.\n+ */\n+public class TestBlockOutputStreamCorrectness {\n+\n+  private static final long SEED = 18480315L;\n+\n+  private int writeUnitSize = 1;\n+\n+  @Test\n+  public void test() throws IOException {\n+\n+    final BufferPool bufferPool = new BufferPool(4 * 1024 * 1024, 32 / 4);\n+\n+    for (int block = 0; block < 10; block++) {\n+      BlockOutputStream outputStream =\n+          createBlockOutputStream(bufferPool);\n+\n+      Random random = new Random(SEED);\n+\n+      int max = 256 * 1024 * 1024 / writeUnitSize;\n+\n+      byte[] writeBuffer = new byte[writeUnitSize];\n+      for (int t = 0; t < max; t++) {\n+        if (writeUnitSize > 1) {\n+          for (int i = 0; i < writeBuffer.length; i++) {\n+            writeBuffer[i] = (byte) random.nextInt();\n+          }\n+          outputStream.write(writeBuffer, 0, writeBuffer.length);\n+        } else {\n+          outputStream.write((byte) random.nextInt());\n+        }\n+      }\n+      outputStream.close();\n+    }\n+  }\n+\n+  @NotNull\n+  private BlockOutputStream createBlockOutputStream(BufferPool bufferPool)\n+      throws IOException {\n+    List<DatanodeDetails> nodes = new ArrayList<>();\n+    nodes.add(MockDatanodeDetails.randomDatanodeDetails());\n+    nodes.add(MockDatanodeDetails.randomDatanodeDetails());\n+    nodes.add(MockDatanodeDetails.randomDatanodeDetails());\n+\n+    final Pipeline pipeline = new Builder()\n+        .setFactor(ReplicationFactor.THREE)\n+        .setType(ReplicationType.RATIS)\n+        .setState(PipelineState.OPEN)\n+        .setId(PipelineID.randomId())\n+        .setNodes(nodes)\n+        .build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ0NzExNQ==", "bodyText": "Can you please also change TestChunkBuffer#runTestIncrementalChunkBuffer to explicitly create IncrementalChunkBuffer?  Currently it uses this factory method, and so with this patch it really tests ChunkBufferImplWithByteBuffer.", "url": "https://github.com/apache/ozone/pull/1336#discussion_r475447115", "createdAt": "2020-08-24T09:05:25Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChunkBuffer.java", "diffHunk": "@@ -44,9 +45,6 @@ static ChunkBuffer allocate(int capacity) {\n    *   When increment <= 0, entire buffer is allocated in the beginning.\n    */\n   static ChunkBuffer allocate(int capacity, int increment) {\n-    if (increment > 0 && increment < capacity) {\n-      return new IncrementalChunkBuffer(capacity, increment, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ1NTIyMQ==", "bodyText": "For consistency, I think we should move the condition to allocateNewBuffer and rename it to allocateNewBufferIfNeeded.", "url": "https://github.com/apache/ozone/pull/1336#discussion_r475455221", "createdAt": "2020-08-24T09:19:52Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "diffHunk": "@@ -209,9 +224,16 @@ public IOException getIoException() {\n   @Override\n   public void write(int b) throws IOException {\n     checkOpen();\n-    byte[] buf = new byte[1];\n-    buf[0] = (byte) b;\n-    write(buf, 0, 1);\n+    if (currentBufferRemaining == 0) {\n+      allocateNewBuffer();\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ1NTUzMg==", "bodyText": "For consistency, I think we should move the condition to a new method writeChunkIfNeeded.", "url": "https://github.com/apache/ozone/pull/1336#discussion_r475455532", "createdAt": "2020-08-24T09:20:20Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "diffHunk": "@@ -209,9 +224,16 @@ public IOException getIoException() {\n   @Override\n   public void write(int b) throws IOException {\n     checkOpen();\n-    byte[] buf = new byte[1];\n-    buf[0] = (byte) b;\n-    write(buf, 0, 1);\n+    if (currentBufferRemaining == 0) {\n+      allocateNewBuffer();\n+    }\n+    currentBuffer.put((byte) b);\n+    currentBufferRemaining--;\n+    if (currentBufferRemaining == 0) {\n+      writeChunk(currentBuffer);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ2NjUyNg==", "bodyText": "Should we also update currentBufferRemaining?", "url": "https://github.com/apache/ozone/pull/1336#discussion_r475466526", "createdAt": "2020-08-24T09:31:27Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "diffHunk": "@@ -481,7 +516,7 @@ private void handleFlush(boolean close)\n     checkOpen();\n     // flush the last chunk data residing on the currentBuffer\n     if (totalDataFlushedLength < writtenDataLength) {\n-      final ChunkBuffer currentBuffer = bufferPool.getCurrentBuffer();\n+      currentBuffer = bufferPool.getCurrentBuffer();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 199}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a2d8fc5dbfebd9b906a756d074ad6795fb5c6d40", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/a2d8fc5dbfebd9b906a756d074ad6795fb5c6d40", "committedDate": "2020-08-26T11:21:21Z", "message": "Address review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ad9c07c94ed9ea7139255770125379a65195c933", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/ad9c07c94ed9ea7139255770125379a65195c933", "committedDate": "2020-08-26T11:31:58Z", "message": "checkstyle fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9ab01a7753b75009997cc33a338bbef282230687", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/9ab01a7753b75009997cc33a338bbef282230687", "committedDate": "2020-08-27T11:52:03Z", "message": "move conditions to the helper methods"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc2NjYyMzcz", "url": "https://github.com/apache/ozone/pull/1336#pullrequestreview-476662373", "createdAt": "2020-08-27T12:18:53Z", "commit": {"oid": "9ab01a7753b75009997cc33a338bbef282230687"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMjoxODo1M1rOHINsIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMjoxODo1M1rOHINsIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM3NDk0NA==", "bodyText": "Note the condition is different here, might not be safe to replace with writeChunkIfNeeded().", "url": "https://github.com/apache/ozone/pull/1336#discussion_r478374944", "createdAt": "2020-08-27T12:18:53Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "diffHunk": "@@ -481,11 +514,9 @@ private void handleFlush(boolean close)\n     checkOpen();\n     // flush the last chunk data residing on the currentBuffer\n     if (totalDataFlushedLength < writtenDataLength) {\n-      final ChunkBuffer currentBuffer = bufferPool.getCurrentBuffer();\n+      refreshCurrentBuffer(bufferPool);\n       Preconditions.checkArgument(currentBuffer.position() > 0);\n-      if (currentBuffer.hasRemaining()) {\n-        writeChunk(currentBuffer);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ab01a7753b75009997cc33a338bbef282230687"}, "originalPosition": 233}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "40721a1613a70ac41feb8349e6cf46534ff2a7ac", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/40721a1613a70ac41feb8349e6cf46534ff2a7ac", "committedDate": "2020-08-27T14:47:19Z", "message": "Revert single writeChunk() call with different condition"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8969b42ab8a2864c3029a01f9847568434c81714", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/8969b42ab8a2864c3029a01f9847568434c81714", "committedDate": "2020-08-27T15:03:08Z", "message": "restore orginal writeChunk logic in handleFlush"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7bf5b29acf7e65239749dcf6cf965f6f0e7e0028", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/7bf5b29acf7e65239749dcf6cf965f6f0e7e0028", "committedDate": "2020-08-31T13:04:32Z", "message": "Use incremental chunk buffer for time being"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bc5b38b04c52c3369262651d1df3bfc512cdc80e", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/bc5b38b04c52c3369262651d1df3bfc512cdc80e", "committedDate": "2020-08-31T13:08:12Z", "message": "Merge remote-tracking branch 'elek/HDDS-4119' into HDDS-4119"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0bce14d3a033b2e4a91980d9a04ab6a1f50615be", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/0bce14d3a033b2e4a91980d9a04ab6a1f50615be", "committedDate": "2020-09-01T08:39:25Z", "message": "Merge remote-tracking branch 'origin/master' into HDDS-4119"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c4144cc9bbfe952cd9a86dd980fdeade05e40229", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/c4144cc9bbfe952cd9a86dd980fdeade05e40229", "committedDate": "2020-09-01T12:11:41Z", "message": "fix merge problem"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "23ba2d1d071942a6adaa92f4ca30b0edfbb1d5b0", "author": {"user": {"login": "elek", "name": "Elek, M\u00e1rton"}}, "url": "https://github.com/apache/ozone/commit/23ba2d1d071942a6adaa92f4ca30b0edfbb1d5b0", "committedDate": "2020-09-02T13:34:09Z", "message": "revert change in retry"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgzOTY0OTcz", "url": "https://github.com/apache/ozone/pull/1336#pullrequestreview-483964973", "createdAt": "2020-09-08T09:38:23Z", "commit": {"oid": "23ba2d1d071942a6adaa92f4ca30b0edfbb1d5b0"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2738, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}