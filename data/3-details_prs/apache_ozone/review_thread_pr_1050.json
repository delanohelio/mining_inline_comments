{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMyNDY2OTUx", "number": 1050, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwNzoxOTo0NFrOEEkdhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQwODoxNzo0NFrOEJC6-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMjI3MTQzOnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/dev-support/checks/acceptance.sh", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwNzoxOTo0NFrOGiRa5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMDoyOTowMFrOGpb9Pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU5MDE4MQ==", "bodyText": "Now acceptance.sh requires jacoco-agent.jar, which is only built when invoked with -Pjacoco.  I would like to suggest moving this definition to the GitHub Actions definition (in the env section, where KEEP_IMAGE is defined, too).", "url": "https://github.com/apache/ozone/pull/1050#discussion_r438590181", "createdAt": "2020-06-11T07:19:44Z", "author": {"login": "adoroszlai"}, "path": "hadoop-ozone/dev-support/checks/acceptance.sh", "diffHunk": "@@ -17,19 +17,32 @@ DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n cd \"$DIR/../../..\" || exit 1\n \n REPORT_DIR=${OUTPUT_DIR:-\"$DIR/../../../target/acceptance\"}\n-mkdir -p \"$REPORT_DIR\"\n \n OZONE_VERSION=$(grep \"<ozone.version>\" \"pom.xml\" | sed 's/<[^>]*>//g'|  sed 's/^[ \\t]*//')\n DIST_DIR=\"$DIR/../../dist/target/ozone-$OZONE_VERSION\"\n \n if [ ! -d \"$DIST_DIR\" ]; then\n     echo \"Distribution dir is missing. Doing a full build\"\n-    \"$DIR/build.sh\"\n+    \"$DIR/build.sh\" -Pjacoco\n fi\n \n+mkdir -p \"$REPORT_DIR\"\n+\n+export HADOOP_OPTS='-javaagent:/opt/hadoop/share/jacoco/jacoco-agent.jar=destfile=/tmp/jacoco.exec,includes=org.apache.hadoop.ozone.*:org.apache.hadoop.hdds'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20ae3ed05b2b29ff8bb831b9a7cc28c974fa92fc"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwMjg0Ng==", "bodyText": "I think this is fixed. I prefer to keep the logic in the shell scripts to keep our build as github independent as possible, but with the improved approach this is done (ENV in the github file, logic in the script)", "url": "https://github.com/apache/ozone/pull/1050#discussion_r446102846", "createdAt": "2020-06-26T10:29:00Z", "author": {"login": "elek"}, "path": "hadoop-ozone/dev-support/checks/acceptance.sh", "diffHunk": "@@ -17,19 +17,32 @@ DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n cd \"$DIR/../../..\" || exit 1\n \n REPORT_DIR=${OUTPUT_DIR:-\"$DIR/../../../target/acceptance\"}\n-mkdir -p \"$REPORT_DIR\"\n \n OZONE_VERSION=$(grep \"<ozone.version>\" \"pom.xml\" | sed 's/<[^>]*>//g'|  sed 's/^[ \\t]*//')\n DIST_DIR=\"$DIR/../../dist/target/ozone-$OZONE_VERSION\"\n \n if [ ! -d \"$DIST_DIR\" ]; then\n     echo \"Distribution dir is missing. Doing a full build\"\n-    \"$DIR/build.sh\"\n+    \"$DIR/build.sh\" -Pjacoco\n fi\n \n+mkdir -p \"$REPORT_DIR\"\n+\n+export HADOOP_OPTS='-javaagent:/opt/hadoop/share/jacoco/jacoco-agent.jar=destfile=/tmp/jacoco.exec,includes=org.apache.hadoop.ozone.*:org.apache.hadoop.hdds'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU5MDE4MQ=="}, "originalCommit": {"oid": "20ae3ed05b2b29ff8bb831b9a7cc28c974fa92fc"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMjI4MzgwOnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/dist/src/main/compose/ozone-csi/docker-compose.yaml", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwNzoyNDowNVrOGiRi1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwOTo1Nzo1NFrOGiWmGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU5MjIxNA==", "bodyText": "I think we can avoid these by passing the variable via -e option of docker-compose, similar to how it is done for some other variables (SECURITY_ENABLED, etc.):\nhttps://github.com/apache/hadoop-ozone/blob/53395a0ed75d96575a7dc26a7adec4eefabb2b74/hadoop-ozone/dist/src/main/compose/testlib.sh#L107", "url": "https://github.com/apache/ozone/pull/1050#discussion_r438592214", "createdAt": "2020-06-11T07:24:05Z", "author": {"login": "adoroszlai"}, "path": "hadoop-ozone/dist/src/main/compose/ozone-csi/docker-compose.yaml", "diffHunk": "@@ -23,6 +23,8 @@ services:\n       - ../..:/opt/hadoop\n     env_file:\n       - docker-config\n+    environment:\n+      HADOOP_OPTS: ${HADOOP_OPTS}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20ae3ed05b2b29ff8bb831b9a7cc28c974fa92fc"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODY0MDM5OA==", "bodyText": "We need it for docker-compose up. Do you see any possibility to set generic environment variables for all the containers? This approach works only for docker-compose exec IMHO.", "url": "https://github.com/apache/ozone/pull/1050#discussion_r438640398", "createdAt": "2020-06-11T08:55:05Z", "author": {"login": "elek"}, "path": "hadoop-ozone/dist/src/main/compose/ozone-csi/docker-compose.yaml", "diffHunk": "@@ -23,6 +23,8 @@ services:\n       - ../..:/opt/hadoop\n     env_file:\n       - docker-config\n+    environment:\n+      HADOOP_OPTS: ${HADOOP_OPTS}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU5MjIxNA=="}, "originalCommit": {"oid": "20ae3ed05b2b29ff8bb831b9a7cc28c974fa92fc"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODY3NDk3MQ==", "bodyText": "Sorry, I confused the two commands.", "url": "https://github.com/apache/ozone/pull/1050#discussion_r438674971", "createdAt": "2020-06-11T09:57:54Z", "author": {"login": "adoroszlai"}, "path": "hadoop-ozone/dist/src/main/compose/ozone-csi/docker-compose.yaml", "diffHunk": "@@ -23,6 +23,8 @@ services:\n       - ../..:/opt/hadoop\n     env_file:\n       - docker-config\n+    environment:\n+      HADOOP_OPTS: ${HADOOP_OPTS}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU5MjIxNA=="}, "originalCommit": {"oid": "20ae3ed05b2b29ff8bb831b9a7cc28c974fa92fc"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NTE3NjMwOnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/dist/src/main/compose/test-all.sh", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxMzozNzozMFrOGlvrVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwNzo0Mzo0NFrOGovFog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIzMTYzOA==", "bodyText": "I don't see any *.exec files in acceptance.zip for the latest run.  Am I missing something?", "url": "https://github.com/apache/ozone/pull/1050#discussion_r442231638", "createdAt": "2020-06-18T13:37:30Z", "author": {"login": "adoroszlai"}, "path": "hadoop-ozone/dist/src/main/compose/test-all.sh", "diffHunk": "@@ -41,8 +46,13 @@ for test in $(find \"$SCRIPT_DIR\" -name test.sh | sort); do\n       echo \"ERROR: Test execution of $(dirname \"$test\") is FAILED!!!!\"\n   fi\n   RESULT_DIR=\"$(dirname \"$test\")/result\"\n-  cp \"$RESULT_DIR\"/robot-*.xml \"$RESULT_DIR\"/docker-*.log \"$RESULT_DIR\"/*.out* \"$ALL_RESULT_DIR\"/\n+  cp \"$RESULT_DIR\"/robot-*.xml \"$RESULT_DIR\"/docker-*.log \"$RESULT_DIR\"/*.out* \"$RESULT_DIR\"/*.exec \"$ALL_RESULT_DIR\"/\n done\n \n rebot -N \"smoketests\" -d \"$SCRIPT_DIR/result\" \"$SCRIPT_DIR/result/robot-*.xml\"\n+if [ \"$OZONE_WITH_COVERAGE\" ]; then\n+  cp /tmp/jacoco-combined.exec \"$SCRIPT_DIR/result/\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ab7fd30f15c42e4094e9176f8d30d186d51acb2"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTM2NzcxNA==", "bodyText": "Should work now.", "url": "https://github.com/apache/ozone/pull/1050#discussion_r445367714", "createdAt": "2020-06-25T07:43:44Z", "author": {"login": "elek"}, "path": "hadoop-ozone/dist/src/main/compose/test-all.sh", "diffHunk": "@@ -41,8 +46,13 @@ for test in $(find \"$SCRIPT_DIR\" -name test.sh | sort); do\n       echo \"ERROR: Test execution of $(dirname \"$test\") is FAILED!!!!\"\n   fi\n   RESULT_DIR=\"$(dirname \"$test\")/result\"\n-  cp \"$RESULT_DIR\"/robot-*.xml \"$RESULT_DIR\"/docker-*.log \"$RESULT_DIR\"/*.out* \"$ALL_RESULT_DIR\"/\n+  cp \"$RESULT_DIR\"/robot-*.xml \"$RESULT_DIR\"/docker-*.log \"$RESULT_DIR\"/*.out* \"$RESULT_DIR\"/*.exec \"$ALL_RESULT_DIR\"/\n done\n \n rebot -N \"smoketests\" -d \"$SCRIPT_DIR/result\" \"$SCRIPT_DIR/result/robot-*.xml\"\n+if [ \"$OZONE_WITH_COVERAGE\" ]; then\n+  cp /tmp/jacoco-combined.exec \"$SCRIPT_DIR/result/\"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIzMTYzOA=="}, "originalCommit": {"oid": "5ab7fd30f15c42e4094e9176f8d30d186d51acb2"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NTE5ODMzOnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/dist/src/main/compose/test-all.sh", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQxMzo0Mjo0M1rOGlv5lA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNTozODowNVrOGpAcdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIzNTI4NA==", "bodyText": "Some of the compose environments have a dedicated network.  I think those would have problems starting SCM:\nhadoop-ozone/dist/src/main/compose/ozone-topology/docker-compose.yaml\nhadoop-ozone/dist/src/main/compose/ozonesecure-om-ha/docker-compose.yaml\nhadoop-ozone/dist/src/main/compose/ozonesecure-mr/docker-compose.yaml\n\n(I'm not sure because I have trouble starting any of the environments with coverage enabled.)", "url": "https://github.com/apache/ozone/pull/1050#discussion_r442235284", "createdAt": "2020-06-18T13:42:43Z", "author": {"login": "adoroszlai"}, "path": "hadoop-ozone/dist/src/main/compose/test-all.sh", "diffHunk": "@@ -19,17 +19,22 @@\n #\n # Test executor to test all the compose/*/test.sh test scripts.\n #\n-\n SCRIPT_DIR=$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null && pwd )\n ALL_RESULT_DIR=\"$SCRIPT_DIR/result\"\n-\n+PROJECT_DIR=\"$SCRIPT_DIR/..\"\n mkdir -p \"$ALL_RESULT_DIR\"\n-rm \"$ALL_RESULT_DIR/*\"\n+rm \"$ALL_RESULT_DIR/*\" || true\n+\n+if [ \"$OZONE_WITH_COVERAGE\" ]; then\n+   java -cp \"$PROJECT_DIR\"/share/coverage/$(ls \"$PROJECT_DIR\"/share/coverage | grep test-util):\"$PROJECT_DIR\"/share/coverage/jacoco-core.jar org.apache.hadoop.test.JacocoServer &\n+   DOCKER_BRIDGE_IP=$(docker network inspect bridge --format='{{(index .IPAM.Config 0).Gateway}}')", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ab7fd30f15c42e4094e9176f8d30d186d51acb2"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjgzMTU0MA==", "bodyText": "(I'm not sure because I have trouble starting any of the environments with coverage enabled.)\n\nHow did you try? Docker bridge ip supposed to be available on all the containers. But can be wrong on OSX. But worked well on github environments.", "url": "https://github.com/apache/ozone/pull/1050#discussion_r442831540", "createdAt": "2020-06-19T13:10:58Z", "author": {"login": "elek"}, "path": "hadoop-ozone/dist/src/main/compose/test-all.sh", "diffHunk": "@@ -19,17 +19,22 @@\n #\n # Test executor to test all the compose/*/test.sh test scripts.\n #\n-\n SCRIPT_DIR=$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null && pwd )\n ALL_RESULT_DIR=\"$SCRIPT_DIR/result\"\n-\n+PROJECT_DIR=\"$SCRIPT_DIR/..\"\n mkdir -p \"$ALL_RESULT_DIR\"\n-rm \"$ALL_RESULT_DIR/*\"\n+rm \"$ALL_RESULT_DIR/*\" || true\n+\n+if [ \"$OZONE_WITH_COVERAGE\" ]; then\n+   java -cp \"$PROJECT_DIR\"/share/coverage/$(ls \"$PROJECT_DIR\"/share/coverage | grep test-util):\"$PROJECT_DIR\"/share/coverage/jacoco-core.jar org.apache.hadoop.test.JacocoServer &\n+   DOCKER_BRIDGE_IP=$(docker network inspect bridge --format='{{(index .IPAM.Config 0).Gateway}}')", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIzNTI4NA=="}, "originalCommit": {"oid": "5ab7fd30f15c42e4094e9176f8d30d186d51acb2"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY1MjA4Ng==", "bodyText": "IP is available, can be pinged, but connection to port is refused.  Confirmed that Jacoco server is running on localhost:6300.\nI'm experimenting with running Jacoco in its own container.", "url": "https://github.com/apache/ozone/pull/1050#discussion_r445652086", "createdAt": "2020-06-25T15:38:05Z", "author": {"login": "adoroszlai"}, "path": "hadoop-ozone/dist/src/main/compose/test-all.sh", "diffHunk": "@@ -19,17 +19,22 @@\n #\n # Test executor to test all the compose/*/test.sh test scripts.\n #\n-\n SCRIPT_DIR=$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null && pwd )\n ALL_RESULT_DIR=\"$SCRIPT_DIR/result\"\n-\n+PROJECT_DIR=\"$SCRIPT_DIR/..\"\n mkdir -p \"$ALL_RESULT_DIR\"\n-rm \"$ALL_RESULT_DIR/*\"\n+rm \"$ALL_RESULT_DIR/*\" || true\n+\n+if [ \"$OZONE_WITH_COVERAGE\" ]; then\n+   java -cp \"$PROJECT_DIR\"/share/coverage/$(ls \"$PROJECT_DIR\"/share/coverage | grep test-util):\"$PROJECT_DIR\"/share/coverage/jacoco-core.jar org.apache.hadoop.test.JacocoServer &\n+   DOCKER_BRIDGE_IP=$(docker network inspect bridge --format='{{(index .IPAM.Config 0).Gateway}}')", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIzNTI4NA=="}, "originalCommit": {"oid": "5ab7fd30f15c42e4094e9176f8d30d186d51acb2"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3OTIwNTA1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/test-utils/src/main/java/org/apache/hadoop/test/JacocoServer.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQwODoxNzo0NFrOGpX_YA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNzowMjo1MFrOGpoPkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjAzNzg1Ng==", "bodyText": "I've found that these calls to ExecutionDataWriter's methods should be synchronized to avoid producing garbage files, which result in the following exception when trying to read them:\nException in thread \"main\" java.io.UTFDataFormatException: malformed input around byte 2\n  at java.io.DataInputStream.readUTF(DataInputStream.java:634)\n  at java.io.DataInputStream.readUTF(DataInputStream.java:564)\n  at org.jacoco.cli.internal.core.data.ExecutionDataReader.readExecutionData(ExecutionDataReader.java:149)\n\nor similar Unknown block type error.", "url": "https://github.com/apache/ozone/pull/1050#discussion_r446037856", "createdAt": "2020-06-26T08:17:44Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/test-utils/src/main/java/org/apache/hadoop/test/JacocoServer.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.test;\n+\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.ServerSocket;\n+import java.net.Socket;\n+\n+import org.jacoco.core.data.ExecutionDataWriter;\n+import org.jacoco.core.runtime.RemoteControlReader;\n+import org.jacoco.core.runtime.RemoteControlWriter;\n+\n+/**\n+ * Simple TPC server to collect all the Jacoco coverage data.\n+ */\n+public final class JacocoServer {\n+\n+  private static int port = 6300;\n+\n+  private static String destinationFile = \"/tmp/jacoco-combined.exec\";\n+\n+  private JacocoServer() {\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:EmptyStatement\")\n+  public static void main(String[] args) throws IOException {\n+    ExecutionDataWriter destination =\n+        new ExecutionDataWriter(new FileOutputStream(destinationFile));\n+    ServerSocket serverSocket = new ServerSocket(port);\n+    Runtime.getRuntime().addShutdownHook(new Thread(() -> {\n+      try {\n+        destination.flush();\n+        serverSocket.close();\n+      } catch (Exception ex) {\n+        ex.printStackTrace();\n+      }\n+    }));\n+\n+    while (true) {\n+      final Socket socket = serverSocket.accept();\n+      new Thread(() -> {\n+        try {\n+          RemoteControlWriter writer =\n+              new RemoteControlWriter(socket.getOutputStream());\n+          RemoteControlReader reader =\n+              new RemoteControlReader(socket.getInputStream());\n+          reader.setSessionInfoVisitor(destination::visitSessionInfo);\n+          reader.setExecutionDataVisitor(destination::visitClassExecution);\n+          while (reader.read()) {\n+            ;//read until the end of the stream.\n+          }\n+          destination.flush();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "539d2e03c84a4a9522f65a42feb682e9621c1e34"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwNDA1Nw==", "bodyText": "Thanks to investigate it. I uploaded a synchronized version.\nThe main problem is that the same functionality is written in Jacoco project under LGPL which couldn't be imported. I wrote my own version from scratch, and this mistake clearly proves that it's independent, as I added my own mistakes ;-)", "url": "https://github.com/apache/ozone/pull/1050#discussion_r446104057", "createdAt": "2020-06-26T10:31:50Z", "author": {"login": "elek"}, "path": "hadoop-hdds/test-utils/src/main/java/org/apache/hadoop/test/JacocoServer.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.test;\n+\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.ServerSocket;\n+import java.net.Socket;\n+\n+import org.jacoco.core.data.ExecutionDataWriter;\n+import org.jacoco.core.runtime.RemoteControlReader;\n+import org.jacoco.core.runtime.RemoteControlWriter;\n+\n+/**\n+ * Simple TPC server to collect all the Jacoco coverage data.\n+ */\n+public final class JacocoServer {\n+\n+  private static int port = 6300;\n+\n+  private static String destinationFile = \"/tmp/jacoco-combined.exec\";\n+\n+  private JacocoServer() {\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:EmptyStatement\")\n+  public static void main(String[] args) throws IOException {\n+    ExecutionDataWriter destination =\n+        new ExecutionDataWriter(new FileOutputStream(destinationFile));\n+    ServerSocket serverSocket = new ServerSocket(port);\n+    Runtime.getRuntime().addShutdownHook(new Thread(() -> {\n+      try {\n+        destination.flush();\n+        serverSocket.close();\n+      } catch (Exception ex) {\n+        ex.printStackTrace();\n+      }\n+    }));\n+\n+    while (true) {\n+      final Socket socket = serverSocket.accept();\n+      new Thread(() -> {\n+        try {\n+          RemoteControlWriter writer =\n+              new RemoteControlWriter(socket.getOutputStream());\n+          RemoteControlReader reader =\n+              new RemoteControlReader(socket.getInputStream());\n+          reader.setSessionInfoVisitor(destination::visitSessionInfo);\n+          reader.setExecutionDataVisitor(destination::visitClassExecution);\n+          while (reader.read()) {\n+            ;//read until the end of the stream.\n+          }\n+          destination.flush();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjAzNzg1Ng=="}, "originalCommit": {"oid": "539d2e03c84a4a9522f65a42feb682e9621c1e34"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NzQxOA==", "bodyText": "destination.flush(); should be synced, too.", "url": "https://github.com/apache/ozone/pull/1050#discussion_r446167418", "createdAt": "2020-06-26T12:57:48Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/test-utils/src/main/java/org/apache/hadoop/test/JacocoServer.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.test;\n+\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.ServerSocket;\n+import java.net.Socket;\n+\n+import org.jacoco.core.data.ExecutionDataWriter;\n+import org.jacoco.core.runtime.RemoteControlReader;\n+import org.jacoco.core.runtime.RemoteControlWriter;\n+\n+/**\n+ * Simple TPC server to collect all the Jacoco coverage data.\n+ */\n+public final class JacocoServer {\n+\n+  private static int port = 6300;\n+\n+  private static String destinationFile = \"/tmp/jacoco-combined.exec\";\n+\n+  private JacocoServer() {\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:EmptyStatement\")\n+  public static void main(String[] args) throws IOException {\n+    ExecutionDataWriter destination =\n+        new ExecutionDataWriter(new FileOutputStream(destinationFile));\n+    ServerSocket serverSocket = new ServerSocket(port);\n+    Runtime.getRuntime().addShutdownHook(new Thread(() -> {\n+      try {\n+        destination.flush();\n+        serverSocket.close();\n+      } catch (Exception ex) {\n+        ex.printStackTrace();\n+      }\n+    }));\n+\n+    while (true) {\n+      final Socket socket = serverSocket.accept();\n+      new Thread(() -> {\n+        try {\n+          RemoteControlWriter writer =\n+              new RemoteControlWriter(socket.getOutputStream());\n+          RemoteControlReader reader =\n+              new RemoteControlReader(socket.getInputStream());\n+          reader.setSessionInfoVisitor(destination::visitSessionInfo);\n+          reader.setExecutionDataVisitor(destination::visitClassExecution);\n+          while (reader.read()) {\n+            ;//read until the end of the stream.\n+          }\n+          destination.flush();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjAzNzg1Ng=="}, "originalCommit": {"oid": "539d2e03c84a4a9522f65a42feb682e9621c1e34"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjMwNDE0Ng==", "bodyText": "added.", "url": "https://github.com/apache/ozone/pull/1050#discussion_r446304146", "createdAt": "2020-06-26T17:02:50Z", "author": {"login": "elek"}, "path": "hadoop-hdds/test-utils/src/main/java/org/apache/hadoop/test/JacocoServer.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.test;\n+\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.ServerSocket;\n+import java.net.Socket;\n+\n+import org.jacoco.core.data.ExecutionDataWriter;\n+import org.jacoco.core.runtime.RemoteControlReader;\n+import org.jacoco.core.runtime.RemoteControlWriter;\n+\n+/**\n+ * Simple TPC server to collect all the Jacoco coverage data.\n+ */\n+public final class JacocoServer {\n+\n+  private static int port = 6300;\n+\n+  private static String destinationFile = \"/tmp/jacoco-combined.exec\";\n+\n+  private JacocoServer() {\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:EmptyStatement\")\n+  public static void main(String[] args) throws IOException {\n+    ExecutionDataWriter destination =\n+        new ExecutionDataWriter(new FileOutputStream(destinationFile));\n+    ServerSocket serverSocket = new ServerSocket(port);\n+    Runtime.getRuntime().addShutdownHook(new Thread(() -> {\n+      try {\n+        destination.flush();\n+        serverSocket.close();\n+      } catch (Exception ex) {\n+        ex.printStackTrace();\n+      }\n+    }));\n+\n+    while (true) {\n+      final Socket socket = serverSocket.accept();\n+      new Thread(() -> {\n+        try {\n+          RemoteControlWriter writer =\n+              new RemoteControlWriter(socket.getOutputStream());\n+          RemoteControlReader reader =\n+              new RemoteControlReader(socket.getInputStream());\n+          reader.setSessionInfoVisitor(destination::visitSessionInfo);\n+          reader.setExecutionDataVisitor(destination::visitClassExecution);\n+          while (reader.read()) {\n+            ;//read until the end of the stream.\n+          }\n+          destination.flush();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjAzNzg1Ng=="}, "originalCommit": {"oid": "539d2e03c84a4a9522f65a42feb682e9621c1e34"}, "originalPosition": 68}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4240, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}