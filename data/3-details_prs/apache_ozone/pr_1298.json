{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY0MTkyODM3", "number": 1298, "title": "HDDS-3869. Use different column families for datanode block and metadata", "bodyText": "What changes were proposed in this pull request?\nThis pull request adds column family support for Datanode RocksDB instances. Originally, datanodes placed all of their data under the default column family in RocksDB. This differs from OM and SCM which organize their data into different column families based on its type. This feature will move the datanode code off of the database utilities in the hadoop.hdds.utils package (which has no column family support), and move them to the newer utilities used by OM and SCM in the hadoop.hdds.utils.db package (which has column family support). It then divides data for new datanode containers into three column families. This is implemented in a backwards compatible way, so that containers written in the old format can still be used. Since this is a rather large pull request, a breakdown of the different components is provided below.\nMigration From Old to New Database Interfaces\nIn order to divide container data into different column families, datanode code had to first be moved off of the old database utilities in the hadoop.hdds.utils package (which has no column family support), and moved to the newer utilities used by OM and SCM in the hadoop.hdds.utils.db package (which has column family support). In addition to the ability to use column families, this also provides a strong typing layer to the tables. The majority of file changes are just swapping calls made using the old interface to their equivalent versions in the new interface. This allowed removing the byte stream conversions that previously had to be done when interacting with the database, since the new interface provides codecs to handle this.\nThe code using the new interface should exhibit identical behavior to that using the old interface. In cases where the new interface lacked functionality present in the old interface, that functionality was added to the new interface. Apart from these minor extensions, this pull request does not modify the new interface or existing implementations, but adds new implementations to the interface needed by the datanode.\nNew Container Layout\nAny new containers created with this code will divide data in to three column families:\n\n\nblock_data\n\nKey type: String\n\nBlock ID with optional prefix.\n\n\nValue type: BlockData\n\n\n\nmetadata\n\nKey type: String\n\nName of metadata field.\n\n\nValue type: Long\n\nThe value of the field.\n\n\n\n\n\ndeleted_blocks\n\nKey type: String\n\nBlock ID with optional prefix.\n\n\nValue type: ChunkInfoList\n\nA new type created that allows encoding/decoding and saving the chunk information associated with blocks that have been deleted.\nThis value is not currently used by the code (except in tests), but is included for potential future use.\nThe underlying chunks are still deleted, only the information about them is retained.\n\n\n\n\n\nBackwards Compatibility\nTo distinguish between containers created in the original layout with only a default column family, and containers created in the new layout with three column families, a new field was added to the .container files, called schemaVersion. schemaVersion 1, or a missing schemaVersion value, indicates that the container was created in the old layout. schemaVersion 2 indicates that the container was created in the new layout. The code will use the proper DatanodeStore implementation for the corresponding schema version, and callers do not need to change their operations based on the schema version.\nUnfortunately, the existing design of the interface requires callers to specify which column families (called Tables by the interface) they interact with, which makes it difficult to abstract out this piece of the implementation. In order to allow the same set of calls to work with both schema versions, callers always specify tables as if they are using schema version 2. If they are in fact using schema version 2, database interactions proceed as expected. If they are using schema version 1, however, calls into the block_data, metadata, or deleted_blocks tables will be redirected to the one default table. This comes with a few issues that had to be resolved:\n\n\nSince deleted blocks are in their own table in schema version 2, they do not need a prefix to separate them from regular blocks.\n\n\nIn schema version 1, however, the prefix is still necessary, otherwise block ID keys for deleted and regular blocks would have identical format and indistinguishable at the default table level.\n\n\nThis is solved with the SchemaOneDeletedBlocksTable class, which is returned as the Table implementation when callers ask for the deleted blocks table but are using schema version 1.\n\n\nThis class automatically adds the #deleted# prefix to caller data, so callers can read and write as if their data is actually in a separate table.\n\n\nThe definition of the #deleted# prefix was moved out of the OzoneConsts and MetadataKeyFilters classes and into SchemaOneDeletedBlocksTable, since this field is now specific only to this case.\n\n\n\n\n\n\nDatabases that had blocks deleted from them before this pull request will have the deleted block ID key mapped to the block ID value, instead of having the chunk information for that block saved as the value.\n\nThis is solved by the SchemaOneChunkInfoListCodec class, which will chain the default exception thrown by a trying to read a block ID as a ChunkInfoList to another IOException with a more detailed error message explaining why the chunk information may be invalid.\nCallers were already forced to handle checked IOExceptions from this method according to the Codec interface, so there should be no surprise behavior if this occurs.\n\n\n\nCalls to Table#iterator will return an iterator over the whole underlying table structure.\n\n\nOn schema version 1, this means calls like blockDataTable.iterator() will iterate all data in the default table, not just that pertaining to block information.\n\nWithout an explicit list of key formats defined for each table (which is fragile if prefixes or metadata are added/changed), this method cannot function properly.\n\n\n\nSince this method is not used directly by datanode code, all DatanodeStore implementations return table instances that throwUnsupportedOperationException for this method.\n\nEven though this method functions correctly in the new database layout, the exception is thrown for both implementations for consistency.\nThe DatanodeTable class is a wrapper around existing Table instances that is used to disable this method.\n\n\n\nBecause the KeyValueBlockIterator used this method from the block data table to get keys to apply filters to while iterating, the KeyValueBlockIterator was made a private internal class of AbstractDatanodeStore, which provides it with the correct table iterator to use.\n\nCallers now access this class using getters declared in DatanodeStore, whose return type is the BlockIterator interface that KeyValueBlockIterator implements.\n\n\n\n\n\nDatabase interactions in the original code used mixed serialization methods for block data keys.\n\n\nThe original database utils used on the datanode allowed the caller to serialize data however they wanted.\n\nThe database methods accepted streams of bytes instead of Java objects.\n\n\n\nThe new database utils used on OM and SCM and now used on the datanode in this patch enforce a strong typing layer.\n\nThe database methods accept Java objects, and serialize them internally.\n\n\n\nIn the original code that wrote existing schema version 1 containers, keys for block data were encoded as longs if they had no prefix, and strings if they had a prefix.\n\n\nAs a result, the codec used in the new interface must determine whether key data should be decoded as Strings or longs.\n\nThis codec must be able to correctly decode all types of keys, since its use in BlockIterator will scan the entire default table and pick out just block data, which may or may not have the #deleting# prefix applied.\n\n\n\nThe SchemaOneKeyCodec class is introduced to handle this problem. It is only needed for schema 1 implementations.\n\n\nWhen the codec is given a byte stream to decode, it runs the following checks to determine whether it should be decoded as a long or String:\n\nThe data is decoded as a String.\nIf the string matches the format of a metadata key or block ID key with a prefix, it is decoded as a String.\nIf the string does not match these formats and is 8 bytes long, it is decoded as a long.\n\nFor a consistent return type, the long will be converted to its String equivalent before returning.\n\n\nIf the string does not match these formats and is not 8 bytes long, it is decoded as a String.\n\n\n\n\n\nNote that it is technically possible, although highly unlikely, that the data was originally encoded as a long, but also happens to match the regex of a known string format when decoded.\n\nIn this case the codec will incorrectly decode the data as a string.\nIt is impossible to determine with 100% accuracy whether a stream of bytes was encoded from a String or a long.\n\nFor every 8 character String, there is a long value that encodes to the same byte array.\n\n\n\n\n\n\n\nUpgrade Logic\nIn order to provide fast upgrades, the code will not reformat existing schema version 1 containers to schema version 2. It will instead rely on backwards compatibility to interact with schema version 1 containers (which should be closed before upgrade). Any new container created after the upgrade will use schema version 2. For this reason, the schema version is tracked on a per container basis in each .container file, and not at the datanode level, since datanodes may end up with a mix of schema version 1 and 2 containers. Once this code is incorporated into an upgrade, no new schema version 1 containers will be created. It would be possible at a later time to reformat all schema version 1 containers to schema version 2, but this would drastically slow down the upgrade if large amounts of schema 1 data is present. For this reason, backwards compatibility was favored over reformatting to deal with schema version 1 containers in this patch.\nGuide to File Changes\n\n\nThe following new files were created to supply the datanode with necessary functionality:\n\n\nFor defining the database layout:\n\nAbstractDatanodeDBDefinition.java\nDatanodeSchemaOneDBDefinition.java\nDatanodeSchemaTwoDBDefinition.java\n\n\n\nFor interacting the with the database:\n\nDatanodeStore.java\nAbstractDatanodeStore.java\nDatanodeStoreSchemaOneImpl.java\nDatanodeStoreSchemaTwoImpl.java\n\n\n\nFor encoding/decoding byte streams to/from the database:\n\nChunkInfoList.java\nChunkInfoListCodec.java\nBlockDataCodec.java\n\n\n\nWrappers for backwards compatibility:\n\nSchemaOneDeletedBlocksTable.java\nDatanodeTable.java\nSchemaOneChunkInfoListCodec.java\nSchemaOneKeyCodec\n\n\n\nUnit testing:\n\nTestSchemaOneBackwardsCompatibility.java\n\n\n\n\n\nAdditionally, a RocksDB database and associated .container file were created using the original code to test backwards compatibility.\n\nThese files are internal to RocksDB and can be ignored, even though they show up in the diff.\nTheir contents are documented in TestSchemaOneBackwardsCompatibility#TestDB.\n\n\n\nMost other changes are switching existing code over to the new interface, or adding equivalent functionality that was present in the old interface to the new interface.\n\n\nWhat is the link to the Apache JIRA\nHDDS-3869\nHow was this patch tested?\n\n\nTo ensure equivalent functionality between the old and new interfaces and their implementations, the code was run through the existing unit test framework.\n\nSince this creates all containers from scratch, these tests were all run on using databases in schema version 2.\n\n\n\nTo ensure backwards compatibility, a new set of tests was created in TestSchemaOneBackwardsCompatibility.java.\n\nThese tests use a RocksDB database and container file that were generated using the original code before this pull request.\nThis tests reading all data in the database, and writing everything except new block data.\n\nSince all containers will be closed before upgrade, no new blocks will be added to containers encountered with schema version 1.\n\n\n\n\n\nTestBlockDeletingService#testDeletedChunkInfo was added to verify that when using a database in schema version 2, chunk information from deleted blocks is saved correctly.\n\n\nIn TestKeyValueBlockIterator, use of the #deleted# blocks prefix was removed, since deleted blocks were moved to their own column family outside of the block data table.\n\n\nThe specific prefixes used in these tests is arbitrary, as long as they are two different prefixes present in the block data table.\n\nSince there is currently only a #deleting# prefix in the block data table, support was added to create new dummy prefixes for testing.\n\n\n\nThe new unit test testKeyValueBlockIteratorWithAdvancedFilter was also added to this class.\n\nThis test makes sure that the iterator will continue to function if new prefixes are added to the block data table in the future.\nThis condition was not guaranteed to occur in any of the existing unit tests.", "createdAt": "2020-08-06T18:26:09Z", "url": "https://github.com/apache/ozone/pull/1298", "merged": true, "mergeCommit": {"oid": "275653e0a997852be44ed55ccd8dabcc255cb399"}, "closed": true, "closedAt": "2020-10-01T21:02:08Z", "author": {"login": "errose28"}, "timelineItems": {"totalCount": 256, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc8ARwcAH2gAyNDY0MTkyODM3OjhhYWI4ZjUyOWY5NjU2MmI1ZjFhYjIwMWU2OTRmZGI3ZGNjNzQ3ODg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdOXtS8AFqTUwMDcwNzUwNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "8aab8f529f96562b5f1ab201e694fdb7dcc74788", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/8aab8f529f96562b5f1ab201e694fdb7dcc74788", "committedDate": "2020-08-05T19:20:56Z", "message": "Switch TestContainerCache to use new DatanodeStore interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eff1ad5c3b01ee7b5c8af1d09b5dd4bda9f28c1d", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/eff1ad5c3b01ee7b5c8af1d09b5dd4bda9f28c1d", "committedDate": "2020-08-06T14:37:05Z", "message": "Fix issues highlighted by GitHub pull request diffs\n\nMostly removing wildcard imports automatically added by Intellij.\nDelete the now unused NoData and NoDataCodec classes.\nMinor documentation and readability fixes."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f08767eebf1044cee52a2c83468ad1613ded563c", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/f08767eebf1044cee52a2c83468ad1613ded563c", "committedDate": "2020-08-06T15:46:42Z", "message": "Remove unused ChunkInfoCodec class"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "38de96efa8912290dea3643e17eea456271daa51", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/38de96efa8912290dea3643e17eea456271daa51", "committedDate": "2020-08-06T16:03:12Z", "message": "Remove old log backup files from test database"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dd09d5b7f9229e20fc4b6aaa5b797ee4c444e5a2", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/dd09d5b7f9229e20fc4b6aaa5b797ee4c444e5a2", "committedDate": "2020-08-06T16:37:07Z", "message": "Remove old TODO message"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a3d0fa1f8da98c535999e46d56fe41c65a195d39", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/a3d0fa1f8da98c535999e46d56fe41c65a195d39", "committedDate": "2020-08-07T14:46:09Z", "message": "Fix checkstyle violation"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1MzcwMzYy", "url": "https://github.com/apache/ozone/pull/1298#pullrequestreview-465370362", "createdAt": "2020-08-11T19:36:30Z", "commit": {"oid": "a3d0fa1f8da98c535999e46d56fe41c65a195d39"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxOTozNjozMFrOG_GcFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxOTozNjozMFrOG_GcFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgxODk2Ng==", "bodyText": "Throw UnsupportedOperation for this implementation and schema 2 implementation.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r468818966", "createdAt": "2020-08-11T19:36:30Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/SchemaOneDeletedBlocksTable.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.BatchOperation;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheKey;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheValue;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * For RocksDB instances written using DB schema version 1, all data is\n+ * stored in the default column family. This differs from later schema\n+ * versions, which put deleted blocks in a different column family.\n+ * As a result, the block IDs used as keys for deleted blocks must be\n+ * prefixed in schema version 1 so that they can be differentiated from\n+ * regular blocks. However, these prefixes are not necessary in later schema\n+ * versions, because the deleted blocks and regular blocks are in different\n+ * column families.\n+ * <p>\n+ * Since clients must operate independently of the underlying schema version,\n+ * This class is returned to clients using {@link DatanodeStoreSchemaOneImpl}\n+ * instances, allowing them to access keys as if no prefix is\n+ * required, while it adds the prefix when necessary.\n+ * This means the client should omit the deleted prefix when putting and\n+ * getting keys, regardless of the schema version.\n+ * <p>\n+ * Note that this class will only apply prefixes to keys as parameters,\n+ * never as return types. This means that keys returned through iterators\n+ * like {@link SchemaOneDeletedBlocksTable#getSequentialRangeKVs},\n+ * {@link SchemaOneDeletedBlocksTable#getRangeKVs}, and\n+ * {@link SchemaOneDeletedBlocksTable#iterator} will return keys prefixed\n+ * with {@link SchemaOneDeletedBlocksTable#DELETED_KEY_PREFIX}.\n+ */\n+public class SchemaOneDeletedBlocksTable implements Table<String,\n+        ChunkInfoList> {\n+  public static final String DELETED_KEY_PREFIX = \"#deleted#\";\n+\n+  private final Table<String, ChunkInfoList> table;\n+\n+  public SchemaOneDeletedBlocksTable(Table<String, ChunkInfoList> table) {\n+    this.table = table;\n+  }\n+\n+  @Override\n+  public void put(String key, ChunkInfoList value) throws IOException {\n+    table.put(prefix(key), value);\n+  }\n+\n+  @Override\n+  public void putWithBatch(BatchOperation batch, String key,\n+                           ChunkInfoList value)\n+          throws IOException {\n+    table.putWithBatch(batch, prefix(key), value);\n+  }\n+\n+  @Override\n+  public boolean isEmpty() throws IOException {\n+    return table.isEmpty();\n+  }\n+\n+  @Override\n+  public void delete(String key) throws IOException {\n+    table.delete(prefix(key));\n+  }\n+\n+  @Override\n+  public void deleteWithBatch(BatchOperation batch, String key)\n+          throws IOException {\n+    table.deleteWithBatch(batch, prefix(key));\n+  }\n+\n+  /**\n+   * Because the actual underlying table in this schema version is the\n+   * default table where all keys are stored, this method will iterate\n+   * through all keys in the database.\n+   */\n+  @Override\n+  public TableIterator<String, ? extends KeyValue<String, ChunkInfoList>>\n+      iterator() {\n+    return table.iterator();\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3d0fa1f8da98c535999e46d56fe41c65a195d39"}, "originalPosition": 103}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1Mzg0NjYx", "url": "https://github.com/apache/ozone/pull/1298#pullrequestreview-465384661", "createdAt": "2020-08-11T19:58:47Z", "commit": {"oid": "a3d0fa1f8da98c535999e46d56fe41c65a195d39"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxOTo1ODo0N1rOG_HIEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxOTo1ODo0N1rOG_HIEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgzMDIyNw==", "bodyText": "Rename, because this is not actually a prefix, but a piece of metadata. Also make sure it is placed in the metadata table when used.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r468830227", "createdAt": "2020-08-11T19:58:47Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConsts.java", "diffHunk": "@@ -140,7 +139,6 @@ public static Versioning getVersioning(boolean versioning) {\n   }\n \n   public static final String DELETING_KEY_PREFIX = \"#deleting#\";\n-  public static final String DELETED_KEY_PREFIX = \"#deleted#\";\n   public static final String DELETE_TRANSACTION_KEY_PREFIX = \"#delTX#\";\n   public static final String BLOCK_COMMIT_SEQUENCE_ID_PREFIX = \"#BCSID\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3d0fa1f8da98c535999e46d56fe41c65a195d39"}, "originalPosition": 14}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY1MzkwODQ3", "url": "https://github.com/apache/ozone/pull/1298#pullrequestreview-465390847", "createdAt": "2020-08-11T20:08:32Z", "commit": {"oid": "a3d0fa1f8da98c535999e46d56fe41c65a195d39"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMDowODozMlrOG_Hauw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMDowODozMlrOG_Hauw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgzNTAwMw==", "bodyText": "Do we need to check schema version here, or will it always be the latest version?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r468835003", "createdAt": "2020-08-11T20:08:32Z", "author": {"login": "errose28"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "diffHunk": "@@ -91,8 +83,16 @@ public static void createContainerMetaData(File containerMetaDataPath, File\n           \" Path: \" + chunksPath);\n     }\n \n-    MetadataStore store = MetadataStoreBuilder.newBuilder().setConf(conf)\n-        .setCreateIfMissing(true).setDbFile(dbFile).build();\n+    DatanodeStore store;\n+    if (schemaVersion.equals(OzoneConsts.SCHEMA_V1)) {\n+      store = new DatanodeStoreSchemaOneImpl(conf, dbFile.getAbsolutePath());\n+    } else if (schemaVersion.equals(OzoneConsts.SCHEMA_V2)) {\n+      store = new DatanodeStoreSchemaTwoImpl(conf, dbFile.getAbsolutePath());\n+    } else {\n+      throw new IllegalArgumentException(\n+              \"Unrecognized schema version for container: \" + schemaVersion);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a3d0fa1f8da98c535999e46d56fe41c65a195d39"}, "originalPosition": 62}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2de4e706995652fa63c93200e35ee936ad3fa2eb", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/2de4e706995652fa63c93200e35ee936ad3fa2eb", "committedDate": "2020-08-12T13:50:47Z", "message": "Merge branch 'master' into HDDS-3869\n\n* master: (28 commits)\n  HDDS-4037. Incorrect container numberOfKeys and usedBytes in SCM after key deletion (#1295)\n  HDDS-3232. Include the byteman scripts in the distribution tar file (#1309)\n  HDDS-4095. Byteman script to debug HCFS performance (#1311)\n  HDDS-4057. Failed acceptance test missing from bundle (#1283)\n  HDDS-4040. [OFS] BasicRootedOzoneFileSystem to support batchDelete (#1286)\n  HDDS-4061. Pending delete blocks are not always included in #BLOCKCOUNT metadata (#1288)\n  HDDS-4067. Implement toString for OMTransactionInfo (#1300)\n  HDDS-3878. Make OMHA serviceID optional if one (but only one) is defined in the config (#1149)\n  HDDS-3833. Use Pipeline choose policy to choose pipeline from exist pipeline list (#1096)\n  HDDS-3979. Make bufferSize configurable for stream copy (#1212)\n  HDDS-4048. Show more information while SCM version info mismatch (#1278)\n  HDDS-4078. Use HDDS InterfaceAudience/Stability annotations (#1302)\n  HDDS-4034. Add Unit Test for HadoopNestedDirGenerator. (#1266)\n  HDDS-4076. Translate CSI.md into Chinese (#1299)\n  HDDS-4046. Extensible subcommands for CLI applications (#1276)\n  HDDS-4051. Remove whitelist/blacklist terminology from Ozone (#1306)\n  HDDS-4055. Cleanup GitHub workflow (#1282)\n  HDDS-4042. Update documentation for the GA release (#1269)\n  HDDS-4066. Add core-site.xml to intellij configuration (#1292)\n  HDDS-4073. Remove leftover robot.robot (#1297)\n  ..."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e4c6f86f77a1dd7f5726d91bf151ccdc2ca40299", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/e4c6f86f77a1dd7f5726d91bf151ccdc2ca40299", "committedDate": "2020-08-12T14:16:29Z", "message": "Remove \"PREFIX\" from the names of metadata keys that are not actually prefixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "20b4a1289f44d44622ba8b0caac91e6a489d927a", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/20b4a1289f44d44622ba8b0caac91e6a489d927a", "committedDate": "2020-08-12T17:15:20Z", "message": "Remove BCSID from metadata key filters, and update TestKeyValueBlockIterator to not use it as a block data prefix\n\nTestKeyValueBlockIterator does not care about the values of the prefixes it uses, it\nonly needs two different types of block prefixes, along with unprefixed blocks,\nto run its tests. The existing tests were refactored to allow specifying an\narbitrary prefix to be created for testing, and make it easier to validate that\nthe iterator returned the correct blocks for each prefix."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "81d0333838bd66696623ab800409b4a7e19022a2", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/81d0333838bd66696623ab800409b4a7e19022a2", "committedDate": "2020-08-12T17:43:21Z", "message": "Update docs on KeyValueContainerUtil.createContainerMetadata to reflect shcema version usage"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e614fb352f1a9f19af7dfc555f49a655fbe0c671", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/e614fb352f1a9f19af7dfc555f49a655fbe0c671", "committedDate": "2020-08-12T19:06:35Z", "message": "Add DatanodeTable that disallows iterating tables directly without key prefix filters"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a70e3471df379c2157c3e31d8a31042bebeb7e9", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/7a70e3471df379c2157c3e31d8a31042bebeb7e9", "committedDate": "2020-08-12T20:42:16Z", "message": "Add test to ensure direct table iterating is disabled for schema version one"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d362c3d3aa8b88f183b09f047da46c9ed752fca1", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/d362c3d3aa8b88f183b09f047da46c9ed752fca1", "committedDate": "2020-08-13T14:27:26Z", "message": "Add internal implementation of BlockIterator to AbstractDatanodeStore"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2a0285b266eefa0b1fcd1faf01f552b2df61023c", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/2a0285b266eefa0b1fcd1faf01f552b2df61023c", "committedDate": "2020-08-13T14:38:14Z", "message": "Remove Container#blockIterator method that was only called in a unit test\n\nAll block iterator implementations will now be retrieved from DatanodeStore."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f46fc83c0497f5556fcaacf1f7fc2743201f6578", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/f46fc83c0497f5556fcaacf1f7fc2743201f6578", "committedDate": "2020-08-13T16:38:53Z", "message": "Move KeyValueBlockIterator implementation inside AbstractDatanodeStore\n\nStill need to get the container ID passed in to it for log messages."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2211daeec0967f62ee0088386837fae0159bc687", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/2211daeec0967f62ee0088386837fae0159bc687", "committedDate": "2020-08-13T17:05:22Z", "message": "Add container ID to log messages for new KeyValueBlockIterator"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2785fe82463a78e80b3a1e1786c68bea37124578", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/2785fe82463a78e80b3a1e1786c68bea37124578", "committedDate": "2020-08-13T17:20:37Z", "message": "Fix mizzed places where containerID was required"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e6a9ffee7d23697818da81dbf9916e2ea1f069ca", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/e6a9ffee7d23697818da81dbf9916e2ea1f069ca", "committedDate": "2020-08-13T17:37:49Z", "message": "Fix null pointer exceptions in unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3a3b73d8339fed724a5b734bf304714fb8af98d9", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/3a3b73d8339fed724a5b734bf304714fb8af98d9", "committedDate": "2020-08-13T19:17:46Z", "message": "Merge branch 'table-iterator-impl' into HDDS-3869\n\n* table-iterator-impl:\n  Fix null pointer exceptions in unit tests\n  Fix mizzed places where containerID was required\n  Add container ID to log messages for new KeyValueBlockIterator\n  Move KeyValueBlockIterator implementation inside AbstractDatanodeStore\n  Remove Container#blockIterator method that was only called in a unit test\n  Add internal implementation of BlockIterator to AbstractDatanodeStore"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a295c541ae6a3437470c65dd0e4537d44fbfbb30", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/a295c541ae6a3437470c65dd0e4537d44fbfbb30", "committedDate": "2020-08-13T20:26:14Z", "message": "Fix checkstyle violations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e62e815c20e684f80c4a42c78c239ee2dd9dbc7c", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/e62e815c20e684f80c4a42c78c239ee2dd9dbc7c", "committedDate": "2020-08-13T20:28:04Z", "message": "Merge branch 'master' into HDDS-3869\n\n* master:\n  HDDS-4099. No Log4j 2 configuration file found error appears in CLI (#1318)\n  HDDS-4108. ozone debug ldb scan without arguments results in core dump (#1317)\n  HDDS-4009. Recon Overview page: The volume, bucket and key counts are not accurate (#1305)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/5415d60c02bd76a5d0544e3d8c3d59e2e4817092", "committedDate": "2020-08-14T13:33:42Z", "message": "Fix integratino test issues where ReferenceCountedDB was not being properly cleaned up"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "352faf65b92d001259da6c67d5e463815b43fc89", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/352faf65b92d001259da6c67d5e463815b43fc89", "committedDate": "2020-09-01T14:16:31Z", "message": "Add documentation explaining why schema version is tracked per container\n\nSince containers in older schema versions are currently not reformatted to\nnewer schema versions, a datanode may have containers with a mix of schema\nversions, requiring this property to be tracked on a per container basis."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc0MDMwODUy", "url": "https://github.com/apache/ozone/pull/1298#pullrequestreview-474030852", "createdAt": "2020-08-25T01:24:53Z", "commit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "state": "COMMENTED", "comments": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQwMToyNDo1M1rOHF-91Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMzo1NToyN1rOHJXI5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjAzNjU2NQ==", "bodyText": "Is the BlockData table loaded in memory when the store is initialized?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476036565", "createdAt": "2020-08-25T01:24:53Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/statemachine/commandhandler/DeleteBlocksCommandHandler.java", "diffHunk": "@@ -209,30 +208,35 @@ private void deleteKeyValueContainerBlocks(\n     int newDeletionBlocks = 0;\n     try(ReferenceCountedDB containerDB =\n             BlockUtils.getDB(containerData, conf)) {\n-      for (Long blk : delTX.getLocalIDList()) {\n-        BatchOperation batch = new BatchOperation();\n-        byte[] blkBytes = Longs.toByteArray(blk);\n-        byte[] blkInfo = containerDB.getStore().get(blkBytes);\n+      Table<String, BlockData> blockDataTable =\n+              containerDB.getStore().getBlockDataTable();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA0Mjg0MQ==", "bodyText": "When reading old containerDataYaml which does not container the Schema version field, what value would be returned? IIRC and it returns null, then we should set it to version V1.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476042841", "createdAt": "2020-08-25T01:34:34Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/impl/ContainerDataYaml.java", "diffHunk": "@@ -280,6 +280,9 @@ public Object construct(Node node) {\n         String state = (String) nodes.get(OzoneConsts.STATE);\n         kvData\n             .setState(ContainerProtos.ContainerDataProto.State.valueOf(state));\n+        String schemaVersion = (String) nodes.get(OzoneConsts.SCHEMA_VERSION);\n+        kvData.setSchemaVersion(schemaVersion);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjA1OTI1Nw==", "bodyText": "@arp7, is it ok to remove LevelDB support?\nThe option to configure DNs to use LevelDB was removed in 0.6.0. And since upgrade from previous versions is not supported, I think it is safe to remove LevelDB support.\nIf yes, we can open a new Jira to clean up the LevelDB code path. If no, we need to take care of that here.\nThe containerDBType parameter is redundant here if old LevelDB containers will not be supported.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476059257", "createdAt": "2020-08-25T01:59:04Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/ContainerCache.java", "diffHunk": "@@ -115,11 +116,14 @@ protected boolean removeLRU(LinkEntry entry) {\n    * @param containerID - ID of the container.\n    * @param containerDBType - DB type of the container.\n    * @param containerDBPath - DB path of the container.\n+   * @param schemaVersion - Schema version of the container.\n    * @param conf - Hadoop Configuration.\n    * @return ReferenceCountedDB.\n    */\n   public ReferenceCountedDB getDB(long containerID, String containerDBType,\n-                             String containerDBPath, ConfigurationSource conf)\n+                                  String containerDBPath,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY4OTM4MQ==", "bodyText": "importContainerData() is called when replicating containers. If an old container needs to be replicated, it would not have the schema version. This can be fixed by setting a default value for schema version if it does not exist in ContainerDataYaml Constructor.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476689381", "createdAt": "2020-08-25T19:33:48Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java", "diffHunk": "@@ -487,6 +486,7 @@ public void importContainerData(InputStream input,\n       containerData.setState(originalContainerData.getState());\n       containerData\n           .setContainerDBType(originalContainerData.getContainerDBType());\n+      containerData.setSchemaVersion(originalContainerData.getSchemaVersion());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjcyOTg4NA==", "bodyText": "I see that schema version is being set in KeyValueContainerUtil#parseKVContainerData.\nWe can explore the option of setting the default schema version (V1) while reading the Yaml itself so that it is never missed.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476729884", "createdAt": "2020-08-25T20:52:17Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java", "diffHunk": "@@ -487,6 +486,7 @@ public void importContainerData(InputStream input,\n       containerData.setState(originalContainerData.getState());\n       containerData\n           .setContainerDBType(originalContainerData.getContainerDBType());\n+      containerData.setSchemaVersion(originalContainerData.getSchemaVersion());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjY4OTM4MQ=="}, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc2ODk0OA==", "bodyText": "Any reason for using intValue here instead of the long value as incrPendingDeletionBlocks takes in a long parameter?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476768948", "createdAt": "2020-08-25T21:48:01Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "diffHunk": "@@ -159,122 +178,126 @@ public static void parseKVContainerData(KeyValueContainerData kvContainerData,\n     }\n     kvContainerData.setDbFile(dbFile);\n \n+    if (kvContainerData.getSchemaVersion() == null) {\n+      // If this container has not specified a schema version, it is in the old\n+      // format with one default column family.\n+      kvContainerData.setSchemaVersion(OzoneConsts.SCHEMA_V1);\n+    }\n+\n \n     boolean isBlockMetadataSet = false;\n \n     try(ReferenceCountedDB containerDB = BlockUtils.getDB(kvContainerData,\n         config)) {\n \n+      Table<String, Long> metadataTable =\n+              containerDB.getStore().getMetadataTable();\n+\n       // Set pending deleted block count.\n-      byte[] pendingDeleteBlockCount =\n-          containerDB.getStore().get(DB_PENDING_DELETE_BLOCK_COUNT_KEY);\n+      Long pendingDeleteBlockCount =\n+          metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n       if (pendingDeleteBlockCount != null) {\n         kvContainerData.incrPendingDeletionBlocks(\n-            Longs.fromByteArray(pendingDeleteBlockCount));\n+                pendingDeleteBlockCount.intValue());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc4MTM3OQ==", "bodyText": "The way usedBytes is calculated has been changed. I am not sure if there will be any implications if this calculation us wrong. Need to dig deeper.\nWe should probably separate this optimization into a separate Jira. What do you think?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476781379", "createdAt": "2020-08-25T22:03:15Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/helpers/KeyValueContainerUtil.java", "diffHunk": "@@ -159,122 +178,126 @@ public static void parseKVContainerData(KeyValueContainerData kvContainerData,\n     }\n     kvContainerData.setDbFile(dbFile);\n \n+    if (kvContainerData.getSchemaVersion() == null) {\n+      // If this container has not specified a schema version, it is in the old\n+      // format with one default column family.\n+      kvContainerData.setSchemaVersion(OzoneConsts.SCHEMA_V1);\n+    }\n+\n \n     boolean isBlockMetadataSet = false;\n \n     try(ReferenceCountedDB containerDB = BlockUtils.getDB(kvContainerData,\n         config)) {\n \n+      Table<String, Long> metadataTable =\n+              containerDB.getStore().getMetadataTable();\n+\n       // Set pending deleted block count.\n-      byte[] pendingDeleteBlockCount =\n-          containerDB.getStore().get(DB_PENDING_DELETE_BLOCK_COUNT_KEY);\n+      Long pendingDeleteBlockCount =\n+          metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n       if (pendingDeleteBlockCount != null) {\n         kvContainerData.incrPendingDeletionBlocks(\n-            Longs.fromByteArray(pendingDeleteBlockCount));\n+                pendingDeleteBlockCount.intValue());\n       } else {\n         // Set pending deleted block count.\n         MetadataKeyFilters.KeyPrefixFilter filter =\n-            new MetadataKeyFilters.KeyPrefixFilter()\n-                .addFilter(OzoneConsts.DELETING_KEY_PREFIX);\n+                MetadataKeyFilters.getDeletingKeyFilter();\n         int numPendingDeletionBlocks =\n-            containerDB.getStore().getSequentialRangeKVs(null,\n-                Integer.MAX_VALUE, filter)\n-                .size();\n+            containerDB.getStore().getBlockDataTable()\n+            .getSequentialRangeKVs(null, Integer.MAX_VALUE, filter)\n+            .size();\n         kvContainerData.incrPendingDeletionBlocks(numPendingDeletionBlocks);\n       }\n \n       // Set delete transaction id.\n-      byte[] delTxnId =\n-          containerDB.getStore().get(DB_CONTAINER_DELETE_TRANSACTION_KEY);\n+      Long delTxnId =\n+          metadataTable.get(OzoneConsts.DELETE_TRANSACTION_KEY);\n       if (delTxnId != null) {\n         kvContainerData\n-            .updateDeleteTransactionId(Longs.fromByteArray(delTxnId));\n+            .updateDeleteTransactionId(delTxnId);\n       }\n \n       // Set BlockCommitSequenceId.\n-      byte[] bcsId = containerDB.getStore().get(\n-          DB_BLOCK_COMMIT_SEQUENCE_ID_KEY);\n+      Long bcsId = metadataTable.get(\n+          OzoneConsts.BLOCK_COMMIT_SEQUENCE_ID);\n       if (bcsId != null) {\n         kvContainerData\n-            .updateBlockCommitSequenceId(Longs.fromByteArray(bcsId));\n+            .updateBlockCommitSequenceId(bcsId);\n       }\n \n       // Set bytes used.\n       // commitSpace for Open Containers relies on usedBytes\n-      byte[] bytesUsed =\n-          containerDB.getStore().get(DB_CONTAINER_BYTES_USED_KEY);\n+      Long bytesUsed =\n+          metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED);\n       if (bytesUsed != null) {\n         isBlockMetadataSet = true;\n-        kvContainerData.setBytesUsed(Longs.fromByteArray(bytesUsed));\n+        kvContainerData.setBytesUsed(bytesUsed);\n       }\n \n       // Set block count.\n-      byte[] blockCount = containerDB.getStore().get(DB_BLOCK_COUNT_KEY);\n+      Long blockCount = metadataTable.get(OzoneConsts.BLOCK_COUNT);\n       if (blockCount != null) {\n         isBlockMetadataSet = true;\n-        kvContainerData.setKeyCount(Longs.fromByteArray(blockCount));\n+        kvContainerData.setKeyCount(blockCount);\n       }\n     }\n \n     if (!isBlockMetadataSet) {\n-      initializeUsedBytesAndBlockCount(kvContainerData);\n+      initializeUsedBytesAndBlockCount(kvContainerData, config);\n     }\n   }\n \n \n   /**\n    * Initialize bytes used and block count.\n-   * @param kvContainerData\n+   * @param kvData\n    * @throws IOException\n    */\n   private static void initializeUsedBytesAndBlockCount(\n-      KeyValueContainerData kvContainerData) throws IOException {\n-\n-    MetadataKeyFilters.KeyPrefixFilter filter =\n-            new MetadataKeyFilters.KeyPrefixFilter();\n+      KeyValueContainerData kvData, ConfigurationSource config)\n+          throws IOException {\n \n-    // Ignore all blocks except those with no prefix, or those with\n-    // #deleting# prefix.\n-    filter.addFilter(OzoneConsts.DELETED_KEY_PREFIX, true)\n-          .addFilter(OzoneConsts.DELETE_TRANSACTION_KEY_PREFIX, true)\n-          .addFilter(OzoneConsts.BLOCK_COMMIT_SEQUENCE_ID_PREFIX, true)\n-          .addFilter(OzoneConsts.BLOCK_COUNT, true)\n-          .addFilter(OzoneConsts.CONTAINER_BYTES_USED, true)\n-          .addFilter(OzoneConsts.PENDING_DELETE_BLOCK_COUNT, true);\n+    final String errorMessage = \"Failed to parse block data for\" +\n+            \" Container \" + kvData.getContainerID();\n \n     long blockCount = 0;\n-    try (KeyValueBlockIterator blockIter = new KeyValueBlockIterator(\n-        kvContainerData.getContainerID(),\n-        new File(kvContainerData.getContainerPath()), filter)) {\n-      long usedBytes = 0;\n-\n-\n-      boolean success = true;\n-      while (success) {\n-        try {\n-          if (blockIter.hasNext()) {\n-            BlockData block = blockIter.nextBlock();\n-            long blockLen = 0;\n-\n-            List< ContainerProtos.ChunkInfo > chunkInfoList = block.getChunks();\n-            for (ContainerProtos.ChunkInfo chunk : chunkInfoList) {\n-              ChunkInfo info = ChunkInfo.getFromProtoBuf(chunk);\n-              blockLen += info.getLen();\n-            }\n-\n-            usedBytes += blockLen;\n-            blockCount++;\n-          } else {\n-            success = false;\n+    long usedBytes = 0;\n+\n+    try(ReferenceCountedDB db = BlockUtils.getDB(kvData, config)) {\n+      // Count all regular blocks.\n+      try (BlockIterator<BlockData> blockIter =\n+                   db.getStore().getBlockIterator(\n+                           MetadataKeyFilters.getUnprefixedKeyFilter())) {\n+\n+        while (blockIter.hasNext()) {\n+          blockCount++;\n+          try {\n+            usedBytes += blockIter.nextBlock().getSize();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 251}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg0NTk5NA==", "bodyText": "blockKey variable is redundant now and can be removed.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476845994", "createdAt": "2020-08-25T23:18:59Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/BlockManagerImpl.java", "diffHunk": "@@ -262,14 +264,17 @@ public void deleteBlock(Container container, BlockID blockID) throws\n       getBlockByID(db, blockID);\n \n       // Update DB to delete block and set block count and bytes used.\n-      BatchOperation batch = new BatchOperation();\n-      batch.delete(blockKey);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njg1NzI3MQ==", "bodyText": "All the usages of getBlockByID convert the returned byte array back to ContainerProtos.BlockData. We can avoid this serialization-deserialization.\nNoting it down here so that we can open a new Jira to optimize this.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r476857271", "createdAt": "2020-08-25T23:29:38Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/BlockManagerImpl.java", "diffHunk": "@@ -324,14 +328,14 @@ public void shutdown() {\n \n   private byte[] getBlockByID(ReferenceCountedDB db, BlockID blockID)\n       throws IOException {\n-    byte[] blockKey = Longs.toByteArray(blockID.getLocalID());\n+    String blockKey = Long.toString(blockID.getLocalID());\n \n-    byte[] blockData = db.getStore().get(blockKey);\n+    BlockData blockData = db.getStore().getBlockDataTable().get(blockKey);\n     if (blockData == null) {\n       throw new StorageContainerException(NO_SUCH_BLOCK_ERR_MSG,\n           NO_SUCH_BLOCK);\n     }\n \n-    return blockData;\n+    return blockData.getProtoBufMessage().toByteArray();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzY2NjA2MA==", "bodyText": "Can we define the table names as static final fields either here or in OzoneConsts?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r477666060", "createdAt": "2020-08-26T23:23:27Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/DatanodeSchemaTwoDBDefinition.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.\u2002\u2002See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.\u2002\u2002The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.\u2002\u2002You may obtain a copy of the License at\n+ *\n+ * \u2002\u2002\u2002\u2002 http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.db.DBColumnFamilyDefinition;\n+import org.apache.hadoop.hdds.utils.db.LongCodec;\n+import org.apache.hadoop.hdds.utils.db.StringCodec;\n+import org.apache.hadoop.ozone.container.common.helpers.BlockData;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+\n+/**\n+ * This class defines the RocksDB structure for datanodes following schema\n+ * version 2, where the block data, metadata, and deleted block ids are put in\n+ * their own separate column families.\n+ */\n+public class DatanodeSchemaTwoDBDefinition extends\n+        AbstractDatanodeDBDefinition {\n+\n+  public static final DBColumnFamilyDefinition<String, BlockData>\n+          BLOCK_DATA =\n+          new DBColumnFamilyDefinition<>(\n+                  \"block_data\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzcxMzc2Mg==", "bodyText": "I think we don't need cache functionality for DatanodeTable? I see it being used only in OM.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r477713762", "createdAt": "2020-08-26T23:56:22Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/DatanodeTable.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.BatchOperation;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheKey;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheValue;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Wrapper class to represent a table in a datanode RocksDB instance.\n+ * This class can wrap any {@link Table} instance, but will throw\n+ * {@link UnsupportedOperationException} for {@link Table#iterator}.\n+ * This is because differing schema versions used in datanode DB layouts may\n+ * have differing underlying table structures, so iterating a table instance\n+ * directly, without taking into account key prefixes, may yield unexpected\n+ * results.\n+ */\n+public class DatanodeTable<KEY, VALUE> implements Table<KEY, VALUE> {\n+\n+  private final Table<KEY, VALUE> table;\n+\n+  public DatanodeTable(Table<KEY, VALUE> table) {\n+    this.table = table;\n+  }\n+\n+  @Override\n+  public void put(KEY key, VALUE value) throws IOException {\n+    table.put(key, value);\n+  }\n+\n+  @Override\n+  public void putWithBatch(BatchOperation batch, KEY key,\n+                           VALUE value) throws IOException {\n+    table.putWithBatch(batch, key, value);\n+  }\n+\n+  @Override\n+  public boolean isEmpty() throws IOException {\n+    return table.isEmpty();\n+  }\n+\n+  @Override\n+  public void delete(KEY key) throws IOException {\n+    table.delete(key);\n+  }\n+\n+  @Override\n+  public void deleteWithBatch(BatchOperation batch, KEY key)\n+          throws IOException {\n+    table.deleteWithBatch(batch, key);\n+  }\n+\n+  @Override\n+  public final TableIterator<KEY, ? extends KeyValue<KEY, VALUE>> iterator() {\n+    throw new UnsupportedOperationException(\"Iterating tables directly is not\" +\n+            \" supported for datanode containers due to differing schema \" +\n+            \"version.\");\n+  }\n+\n+  @Override\n+  public String getName() throws IOException {\n+    return table.getName();\n+  }\n+\n+  @Override\n+  public long getEstimatedKeyCount() throws IOException {\n+    return table.getEstimatedKeyCount();\n+  }\n+\n+  @Override\n+  public void addCacheEntry(CacheKey<KEY> cacheKey,\n+                            CacheValue<VALUE> cacheValue) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzcyODQwMA==", "bodyText": "From what I could understand, the underlying table structure is TypedTabled. Is that correct?\nIf yes, should DatanodeTable extend TypedTable instead of Table<KEY, VALUE> so that it does not have to override all the methods?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r477728400", "createdAt": "2020-08-27T00:05:23Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/DatanodeTable.java", "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.BatchOperation;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheKey;\n+import org.apache.hadoop.hdds.utils.db.TableIterator;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheValue;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * Wrapper class to represent a table in a datanode RocksDB instance.\n+ * This class can wrap any {@link Table} instance, but will throw\n+ * {@link UnsupportedOperationException} for {@link Table#iterator}.\n+ * This is because differing schema versions used in datanode DB layouts may\n+ * have differing underlying table structures, so iterating a table instance\n+ * directly, without taking into account key prefixes, may yield unexpected\n+ * results.\n+ */\n+public class DatanodeTable<KEY, VALUE> implements Table<KEY, VALUE> {\n+\n+  private final Table<KEY, VALUE> table;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwNjEwOA==", "bodyText": "Shouldn't Schema 1 DBs store the deleted blocks in the old format?\nIIUC, if SchemaOneChunkInfoListCodec implements the Block ID codec, then the InvalidProtocolBufferException can be avoided.\nPlease let me know if there is any other reason for implementing it this way.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479506108", "createdAt": "2020-08-28T19:53:42Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/SchemaOneChunkInfoListCodec.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.\u2002\u2002See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.\u2002\u2002The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.\u2002\u2002You may obtain a copy of the License at\n+ *\n+ * \u2002\u2002\u2002\u2002 http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos;\n+import org.apache.hadoop.hdds.utils.db.Codec;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+import org.apache.ratis.thirdparty.com.google.protobuf.InvalidProtocolBufferException;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Codec for parsing {@link ContainerProtos.ChunkInfoList} objects from data\n+ * that may have been written using schema version one. Before upgrading\n+ * schema versions, deleted block IDs were stored with a duplicate copy of\n+ * their ID as the value in the database. After upgrading the code, any\n+ * deletes that happen on the DB will save the chunk information with the\n+ * deleted blocks instead, even if those deletes are performed on a database\n+ * created with schema version one.\n+ * <p>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwODQwNA==", "bodyText": "This would mean someone iterating through all the deleted blocks in a DN might get a mixture of keys with and without the deleted key prefix.\nIs there such a scenario in the code currently?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479508404", "createdAt": "2020-08-28T19:59:13Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/metadata/SchemaOneDeletedBlocksTable.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.metadata;\n+\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.hdds.utils.db.BatchOperation;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheKey;\n+import org.apache.hadoop.hdds.utils.db.cache.CacheValue;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+/**\n+ * For RocksDB instances written using DB schema version 1, all data is\n+ * stored in the default column family. This differs from later schema\n+ * versions, which put deleted blocks in a different column family.\n+ * As a result, the block IDs used as keys for deleted blocks must be\n+ * prefixed in schema version 1 so that they can be differentiated from\n+ * regular blocks. However, these prefixes are not necessary in later schema\n+ * versions, because the deleted blocks and regular blocks are in different\n+ * column families.\n+ * <p>\n+ * Since clients must operate independently of the underlying schema version,\n+ * This class is returned to clients using {@link DatanodeStoreSchemaOneImpl}\n+ * instances, allowing them to access keys as if no prefix is\n+ * required, while it adds the prefix when necessary.\n+ * This means the client should omit the deleted prefix when putting and\n+ * getting keys, regardless of the schema version.\n+ * <p>\n+ * Note that this class will only apply prefixes to keys as parameters,\n+ * never as return types. This means that keys returned through iterators\n+ * like {@link SchemaOneDeletedBlocksTable#getSequentialRangeKVs}, and\n+ * {@link SchemaOneDeletedBlocksTable#getRangeKVs} will return keys prefixed\n+ * with {@link SchemaOneDeletedBlocksTable#DELETED_KEY_PREFIX}.\n+ */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUyMzM2NA==", "bodyText": "This test is dependent on the order of deleted blocks. Does RocksDB iterator ensure that the deleting and deleted block keys are returned in the same sorted order?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479523364", "createdAt": "2020-08-28T20:36:56Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestBlockDeletingService.java", "diffHunk": "@@ -471,4 +470,59 @@ public void testBlockThrottle() throws Exception {\n       service.shutdown();\n     }\n   }\n+\n+  @Test\n+  public void testDeletedChunkInfo() throws Exception {\n+    OzoneConfiguration conf = new OzoneConfiguration();\n+    conf.setInt(OZONE_BLOCK_DELETING_CONTAINER_LIMIT_PER_INTERVAL, 10);\n+    conf.setInt(OZONE_BLOCK_DELETING_LIMIT_PER_CONTAINER, 2);\n+    ContainerSet containerSet = new ContainerSet();\n+    createToDeleteBlocks(containerSet, conf, 1, 2, 3);\n+\n+    List<ContainerData> containerData = Lists.newArrayList();\n+    containerSet.listContainer(0L, 1, containerData);\n+\n+    try(ReferenceCountedDB meta = BlockUtils.getDB(\n+            (KeyValueContainerData) containerData.get(0), conf)) {\n+\n+      // Collect all ChunkInfo from blocks marked for deletion.\n+      List<? extends Table.KeyValue<String, BlockData>> deletingBlocks =\n+              meta.getStore().getBlockDataTable()\n+              .getRangeKVs(null, 100,\n+                      MetadataKeyFilters.getDeletingKeyFilter());\n+\n+      // Delete all blocks marked for deletion.\n+      BlockDeletingServiceTestImpl svc =\n+              getBlockDeletingService(containerSet, conf);\n+      svc.start();\n+      GenericTestUtils.waitFor(svc::isStarted, 100, 3000);\n+      deleteAndWait(svc, 1);\n+      svc.shutdown();\n+\n+      // Get deleted blocks from their table, and check their ChunkInfo lists\n+      // against those we saved for them before deletion.\n+      List<? extends Table.KeyValue<String, ChunkInfoList>> deletedBlocks =\n+              meta.getStore().getDeletedBlocksTable()\n+              .getRangeKVs(null, 100);\n+\n+      Assert.assertEquals(deletingBlocks.size(), deletedBlocks.size());\n+\n+      Iterator<? extends Table.KeyValue<String, BlockData>>\n+              deletingBlocksIter = deletingBlocks.iterator();\n+      Iterator<? extends Table.KeyValue<String, ChunkInfoList>>\n+              deletedBlocksIter = deletedBlocks.iterator();\n+\n+      while(deletingBlocksIter.hasNext() && deletedBlocksIter.hasNext())  {\n+        List<ContainerProtos.ChunkInfo> deletingChunks =\n+                deletingBlocksIter.next().getValue().getChunks();\n+        List<ContainerProtos.ChunkInfo> deletedChunks =\n+                deletedBlocksIter.next().getValue().asList();\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzODM3NQ==", "bodyText": "Nitpick: It would be good to have a method in ContainerCache which calls the current getDB with Schema_Latest. We can avoid specifying the schema version every time then.\npublic ReferenceCountedDB getDB(long containerID, String containerDBType, String containerDBPath,\n    ConfigurationSource conf) {\n    getDB(containerID, containerDBType, OzoneConsts.SCHEMA_LATEST, conf);\n}", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479538375", "createdAt": "2020-08-28T21:17:00Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestContainerCache.java", "diffHunk": "@@ -85,17 +86,17 @@ public void testContainerCacheEviction() throws Exception {\n \n     // Get 2 references out of the same db and verify the objects are same.\n     ReferenceCountedDB db1 = cache.getDB(1, \"RocksDB\",\n-        containerDir1.getPath(), conf);\n+            containerDir1.getPath(), OzoneConsts.SCHEMA_LATEST, conf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NzI3Mg==", "bodyText": "Shouldn't Bytes used be decreased by the deleted blocks size?", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479547272", "createdAt": "2020-08-28T21:43:21Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestSchemaOneBackwardsCompatibility.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.common;\n+\n+import org.apache.commons.io.FileUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.container.ContainerTestHelper;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+import org.apache.hadoop.ozone.container.common.impl.ChunkLayOutVersion;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerDataYaml;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n+import org.apache.hadoop.ozone.container.common.interfaces.ContainerDispatcher;\n+import org.apache.hadoop.ozone.container.common.utils.ReferenceCountedDB;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainerData;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil;\n+import org.apache.hadoop.ozone.container.metadata.DatanodeStore;\n+import org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer;\n+import org.apache.hadoop.ozone.container.testutils.BlockDeletingServiceTestImpl;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.yaml.snakeyaml.Yaml;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.*;\n+\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_CONTAINER_LIMIT_PER_INTERVAL;\n+import static org.junit.Assert.*;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Tests processing of containers written with DB schema version 1,\n+ * which stores all its data in the default RocksDB column family.\n+ * Newer schema version will use a different column family layout, but they\n+ * should still be able to read, delete data, and update metadata for schema\n+ * version 1 containers.\n+ * <p>\n+ * The functionality executed by these tests assumes that all containers will\n+ * have to be closed before an upgrade, meaning that containers written with\n+ * schema version 1 will only ever be encountered in their closed state.\n+ * <p>\n+ * For an example of a RocksDB instance written with schema version 1, see\n+ * {@link TestDB}, which is used by these tests to load a pre created schema\n+ * version 1 RocksDB instance from test resources.\n+ */\n+public class TestSchemaOneBackwardsCompatibility {\n+  private OzoneConfiguration conf;\n+\n+  private File metadataDir;\n+  private File dbFile;\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+\n+  @Before\n+  public void setup() throws Exception {\n+    conf = new OzoneConfiguration();\n+    TestDB testDB = new TestDB();\n+\n+    // Copy data to the temporary folder so it can be safely modified.\n+    File tempMetadataDir =\n+            tempFolder.newFolder(Long.toString(TestDB.CONTAINER_ID),\n+                    OzoneConsts.CONTAINER_META_PATH);\n+\n+    FileUtils.copyDirectoryToDirectory(testDB.getDBDirectory(),\n+            tempMetadataDir);\n+    FileUtils.copyFileToDirectory(testDB.getContainerFile(), tempMetadataDir);\n+\n+    metadataDir = tempMetadataDir;\n+    File[] potentialDBFiles = metadataDir.listFiles((dir, name) ->\n+            name.equals(TestDB.DB_NAME));\n+\n+    if (potentialDBFiles == null || potentialDBFiles.length != 1) {\n+      throw new IOException(\"Failed load file named \" + TestDB.DB_NAME + \" \" +\n+              \"from the metadata directory \" + metadataDir.getAbsolutePath());\n+    }\n+\n+    dbFile = potentialDBFiles[0];\n+  }\n+\n+  /**\n+   * Because all tables in schema version one map back to the default table,\n+   * directly iterating any of the table instances should be forbidden.\n+   * Otherwise, the iterators for each table would read the entire default\n+   * table, return all database contents, and yield unexpected results.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDirectTableIterationDisabled() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      DatanodeStore store = refCountedDB.getStore();\n+\n+      assertTableIteratorUnsupported(store.getMetadataTable());\n+      assertTableIteratorUnsupported(store.getBlockDataTable());\n+      assertTableIteratorUnsupported(store.getDeletedBlocksTable());\n+    }\n+  }\n+\n+  private void assertTableIteratorUnsupported(Table<?, ?> table) {\n+    try {\n+      table.iterator();\n+      Assert.fail(\"Table iterator should have thrown \" +\n+              \"UnsupportedOperationException.\");\n+    } catch (UnsupportedOperationException ex) {\n+      // Exception thrown as expected.\n+    }\n+  }\n+\n+  /**\n+   * Counts the number of deleted, pending delete, and regular blocks in the\n+   * database, and checks that they match the expected values.\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testBlockIteration() throws IOException {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      assertEquals(TestDB.NUM_DELETED_BLOCKS, countDeletedBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countDeletingBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.KEY_COUNT - TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countUnprefixedBlocks(refCountedDB));\n+    }\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has metadata keys present.\n+   * The {@link KeyValueContainerUtil} will read these values to fill in a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithMetadata() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has no metadata keys present.\n+   * The {@link KeyValueContainerUtil} will scan the blocks in the database\n+   * to fill these metadata values into the database and into a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithoutMetadata() throws Exception {\n+    // Init the kvData enough values so we can get the database to modify for\n+    // testing and then read.\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    // Delete metadata keys from our copy of the DB.\n+    // This simulates them not being there to start with.\n+    try (ReferenceCountedDB db = BlockUtils.getDB(kvData, conf)) {\n+      Table<String, Long> metadataTable = db.getStore().getMetadataTable();\n+\n+      metadataTable.delete(OzoneConsts.BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+\n+      metadataTable.delete(OzoneConsts.CONTAINER_BYTES_USED);\n+      assertNull(metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));\n+\n+      metadataTable.delete(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT));\n+    }\n+\n+    // Create a new container data object, and fill in its metadata by\n+    // counting blocks from the database, since the metadata keys in the\n+    // database are now gone.\n+    kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading blocks marked for deletion from a container written in\n+   * schema version 1. Because the block deleting service both reads for\n+   * deleted blocks and deletes them, this test will modify its copy of the\n+   * database.\n+   */\n+  @Test\n+  public void testDelete() throws Exception {\n+    final long numBlocksToDelete = TestDB.NUM_PENDING_DELETION_BLOCKS;\n+\n+    runBlockDeletingService();\n+\n+    // Expected values after blocks with #deleting# prefix in original DB are\n+    // deleted.\n+    final long expectedDeletingBlocks =\n+            TestDB.NUM_PENDING_DELETION_BLOCKS - numBlocksToDelete;\n+    final long expectedDeletedBlocks =\n+            TestDB.NUM_DELETED_BLOCKS + numBlocksToDelete;\n+    final long expectedRegularBlocks =\n+            TestDB.KEY_COUNT - numBlocksToDelete;\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      // Test results via block iteration.\n+      assertEquals(expectedDeletingBlocks,\n+              countDeletingBlocks(refCountedDB));\n+      assertEquals(expectedDeletedBlocks,\n+              countDeletedBlocks(refCountedDB));\n+      assertEquals(expectedRegularBlocks,\n+              countUnprefixedBlocks(refCountedDB));\n+\n+      // Test table metadata.\n+      Table<String, Long> metadataTable =\n+              refCountedDB.getStore().getMetadataTable();\n+      assertEquals(expectedRegularBlocks + expectedDeletingBlocks,\n+              (long)metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+      assertEquals(TestDB.BYTES_USED,\n+              (long)metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 252}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NzQ3NQ==", "bodyText": "Unused code block.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479547475", "createdAt": "2020-08-28T21:44:03Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/common/TestSchemaOneBackwardsCompatibility.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.ozone.container.common;\n+\n+import org.apache.commons.io.FileUtils;\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.utils.MetadataKeyFilters;\n+import org.apache.hadoop.hdds.utils.db.Table;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+import org.apache.hadoop.ozone.container.ContainerTestHelper;\n+import org.apache.hadoop.ozone.container.common.helpers.ChunkInfoList;\n+import org.apache.hadoop.ozone.container.common.impl.ChunkLayOutVersion;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerDataYaml;\n+import org.apache.hadoop.ozone.container.common.impl.ContainerSet;\n+import org.apache.hadoop.ozone.container.common.interfaces.ContainerDispatcher;\n+import org.apache.hadoop.ozone.container.common.utils.ReferenceCountedDB;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainer;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueContainerData;\n+import org.apache.hadoop.ozone.container.keyvalue.KeyValueHandler;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.BlockUtils;\n+import org.apache.hadoop.ozone.container.keyvalue.helpers.KeyValueContainerUtil;\n+import org.apache.hadoop.ozone.container.metadata.DatanodeStore;\n+import org.apache.hadoop.ozone.container.ozoneimpl.OzoneContainer;\n+import org.apache.hadoop.ozone.container.testutils.BlockDeletingServiceTestImpl;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.yaml.snakeyaml.Yaml;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URL;\n+import java.util.*;\n+\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_BLOCK_DELETING_CONTAINER_LIMIT_PER_INTERVAL;\n+import static org.junit.Assert.*;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Tests processing of containers written with DB schema version 1,\n+ * which stores all its data in the default RocksDB column family.\n+ * Newer schema version will use a different column family layout, but they\n+ * should still be able to read, delete data, and update metadata for schema\n+ * version 1 containers.\n+ * <p>\n+ * The functionality executed by these tests assumes that all containers will\n+ * have to be closed before an upgrade, meaning that containers written with\n+ * schema version 1 will only ever be encountered in their closed state.\n+ * <p>\n+ * For an example of a RocksDB instance written with schema version 1, see\n+ * {@link TestDB}, which is used by these tests to load a pre created schema\n+ * version 1 RocksDB instance from test resources.\n+ */\n+public class TestSchemaOneBackwardsCompatibility {\n+  private OzoneConfiguration conf;\n+\n+  private File metadataDir;\n+  private File dbFile;\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+\n+  @Before\n+  public void setup() throws Exception {\n+    conf = new OzoneConfiguration();\n+    TestDB testDB = new TestDB();\n+\n+    // Copy data to the temporary folder so it can be safely modified.\n+    File tempMetadataDir =\n+            tempFolder.newFolder(Long.toString(TestDB.CONTAINER_ID),\n+                    OzoneConsts.CONTAINER_META_PATH);\n+\n+    FileUtils.copyDirectoryToDirectory(testDB.getDBDirectory(),\n+            tempMetadataDir);\n+    FileUtils.copyFileToDirectory(testDB.getContainerFile(), tempMetadataDir);\n+\n+    metadataDir = tempMetadataDir;\n+    File[] potentialDBFiles = metadataDir.listFiles((dir, name) ->\n+            name.equals(TestDB.DB_NAME));\n+\n+    if (potentialDBFiles == null || potentialDBFiles.length != 1) {\n+      throw new IOException(\"Failed load file named \" + TestDB.DB_NAME + \" \" +\n+              \"from the metadata directory \" + metadataDir.getAbsolutePath());\n+    }\n+\n+    dbFile = potentialDBFiles[0];\n+  }\n+\n+  /**\n+   * Because all tables in schema version one map back to the default table,\n+   * directly iterating any of the table instances should be forbidden.\n+   * Otherwise, the iterators for each table would read the entire default\n+   * table, return all database contents, and yield unexpected results.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDirectTableIterationDisabled() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      DatanodeStore store = refCountedDB.getStore();\n+\n+      assertTableIteratorUnsupported(store.getMetadataTable());\n+      assertTableIteratorUnsupported(store.getBlockDataTable());\n+      assertTableIteratorUnsupported(store.getDeletedBlocksTable());\n+    }\n+  }\n+\n+  private void assertTableIteratorUnsupported(Table<?, ?> table) {\n+    try {\n+      table.iterator();\n+      Assert.fail(\"Table iterator should have thrown \" +\n+              \"UnsupportedOperationException.\");\n+    } catch (UnsupportedOperationException ex) {\n+      // Exception thrown as expected.\n+    }\n+  }\n+\n+  /**\n+   * Counts the number of deleted, pending delete, and regular blocks in the\n+   * database, and checks that they match the expected values.\n+   * @throws IOException\n+   */\n+  @Test\n+  public void testBlockIteration() throws IOException {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(kvData, conf)) {\n+      assertEquals(TestDB.NUM_DELETED_BLOCKS, countDeletedBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countDeletingBlocks(refCountedDB));\n+\n+      assertEquals(TestDB.KEY_COUNT - TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              countUnprefixedBlocks(refCountedDB));\n+    }\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has metadata keys present.\n+   * The {@link KeyValueContainerUtil} will read these values to fill in a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithMetadata() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading of a container that was written in schema version 1, when\n+   * the container has no metadata keys present.\n+   * The {@link KeyValueContainerUtil} will scan the blocks in the database\n+   * to fill these metadata values into the database and into a\n+   * {@link KeyValueContainerData} object.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadWithoutMetadata() throws Exception {\n+    // Init the kvData enough values so we can get the database to modify for\n+    // testing and then read.\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    // Delete metadata keys from our copy of the DB.\n+    // This simulates them not being there to start with.\n+    try (ReferenceCountedDB db = BlockUtils.getDB(kvData, conf)) {\n+      Table<String, Long> metadataTable = db.getStore().getMetadataTable();\n+\n+      metadataTable.delete(OzoneConsts.BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+\n+      metadataTable.delete(OzoneConsts.CONTAINER_BYTES_USED);\n+      assertNull(metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));\n+\n+      metadataTable.delete(OzoneConsts.PENDING_DELETE_BLOCK_COUNT);\n+      assertNull(metadataTable.get(OzoneConsts.PENDING_DELETE_BLOCK_COUNT));\n+    }\n+\n+    // Create a new container data object, and fill in its metadata by\n+    // counting blocks from the database, since the metadata keys in the\n+    // database are now gone.\n+    kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+    checkContainerData(kvData);\n+  }\n+\n+  /**\n+   * Tests reading blocks marked for deletion from a container written in\n+   * schema version 1. Because the block deleting service both reads for\n+   * deleted blocks and deletes them, this test will modify its copy of the\n+   * database.\n+   */\n+  @Test\n+  public void testDelete() throws Exception {\n+    final long numBlocksToDelete = TestDB.NUM_PENDING_DELETION_BLOCKS;\n+\n+    runBlockDeletingService();\n+\n+    // Expected values after blocks with #deleting# prefix in original DB are\n+    // deleted.\n+    final long expectedDeletingBlocks =\n+            TestDB.NUM_PENDING_DELETION_BLOCKS - numBlocksToDelete;\n+    final long expectedDeletedBlocks =\n+            TestDB.NUM_DELETED_BLOCKS + numBlocksToDelete;\n+    final long expectedRegularBlocks =\n+            TestDB.KEY_COUNT - numBlocksToDelete;\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      // Test results via block iteration.\n+      assertEquals(expectedDeletingBlocks,\n+              countDeletingBlocks(refCountedDB));\n+      assertEquals(expectedDeletedBlocks,\n+              countDeletedBlocks(refCountedDB));\n+      assertEquals(expectedRegularBlocks,\n+              countUnprefixedBlocks(refCountedDB));\n+\n+      // Test table metadata.\n+      Table<String, Long> metadataTable =\n+              refCountedDB.getStore().getMetadataTable();\n+      assertEquals(expectedRegularBlocks + expectedDeletingBlocks,\n+              (long)metadataTable.get(OzoneConsts.BLOCK_COUNT));\n+      assertEquals(TestDB.BYTES_USED,\n+              (long)metadataTable.get(OzoneConsts.CONTAINER_BYTES_USED));\n+    }\n+  }\n+\n+  /**\n+   * Tests reading the chunk info saved from a block that was deleted from a\n+   * database in schema version one. Blocks deleted from schema version one\n+   * before the upgrade will have the block ID saved as their value. Trying\n+   * to retrieve this value as a {@link ChunkInfoList} should fail. Blocks\n+   * deleted from schema version one after the upgrade should have their\n+   * {@link ChunkInfoList} saved as the corresponding value in the deleted\n+   * blocks table. Reading these values should succeed.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testReadDeletedBlockChunkInfo() throws Exception {\n+    KeyValueContainerData kvData = newKvData();\n+    KeyValueContainerUtil.parseKVContainerData(kvData, conf);\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      // Read blocks that were already deleted before the upgrade.\n+      List<? extends Table.KeyValue<String, ChunkInfoList>> deletedBlocks =\n+              refCountedDB.getStore()\n+                      .getDeletedBlocksTable().getRangeKVs(null, 100);\n+\n+      Set<String> preUpgradeBlocks = new HashSet<>();\n+\n+      for(Table.KeyValue<String, ChunkInfoList> chunkListKV: deletedBlocks) {\n+        preUpgradeBlocks.add(chunkListKV.getKey());\n+        try {\n+          chunkListKV.getValue();\n+          Assert.fail(\"No exception thrown when trying to retrieve old \" +\n+                  \"deleted blocks values as chunk lists.\");\n+        } catch(IOException ex) {\n+          // Exception thrown as expected.\n+        }\n+      }\n+\n+      Assert.assertEquals(TestDB.NUM_DELETED_BLOCKS, preUpgradeBlocks.size());\n+\n+      runBlockDeletingService();\n+\n+      // After the block deleting service runs, get the updated list of\n+      // deleted blocks.\n+      deletedBlocks = refCountedDB.getStore()\n+                      .getDeletedBlocksTable().getRangeKVs(null, 100);\n+\n+      int numPostUpgradeDeletesFound = 0;\n+      for(Table.KeyValue<String, ChunkInfoList> chunkListKV: deletedBlocks) {\n+        if (!preUpgradeBlocks.contains(chunkListKV.getKey())) {\n+          numPostUpgradeDeletesFound++;\n+          Assert.assertNotNull(chunkListKV.getValue());\n+        }\n+      }\n+\n+      // The blocks that were originally marked for deletion should now be\n+      // deleted.\n+      Assert.assertEquals(TestDB.NUM_PENDING_DELETION_BLOCKS,\n+              numPostUpgradeDeletesFound);\n+    }\n+\n+\n+    try(ReferenceCountedDB refCountedDB = BlockUtils.getDB(newKvData(), conf)) {\n+      List<? extends Table.KeyValue<String, ChunkInfoList>> deletedBlocks =\n+              refCountedDB.getStore().getDeletedBlocksTable()\n+                      .getRangeKVs(null, 100);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 317}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU1NTk2MA==", "bodyText": "The general continuation indent used throughout the code is 4.\n@elek, any idea if continuation indent should be 4 or does it not matter? Checkstyle CI seems to not care about it.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479555960", "createdAt": "2020-08-28T22:13:26Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java", "diffHunk": "@@ -88,196 +86,314 @@ public void setUp() throws Exception {\n     conf = new OzoneConfiguration();\n     conf.set(HDDS_DATANODE_DIR_KEY, testRoot.getAbsolutePath());\n     volumeSet = new MutableVolumeSet(UUID.randomUUID().toString(), conf);\n+\n+    containerData = new KeyValueContainerData(105L,\n+            layout,\n+            (long) StorageUnit.GB.toBytes(1), UUID.randomUUID().toString(),\n+            UUID.randomUUID().toString());\n+    // Init the container.\n+    container = new KeyValueContainer(containerData, conf);\n+    container.create(volumeSet, new RoundRobinVolumeChoosingPolicy(), UUID\n+            .randomUUID().toString());\n+    db = BlockUtils.getDB(containerData, conf);\n   }\n \n \n   @After\n-  public void tearDown() {\n+  public void tearDown() throws Exception {\n+    db.close();\n+    db.cleanup();\n     volumeSet.shutdown();\n     FileUtil.fullyDelete(testRoot);\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithMixedBlocks() throws Exception {\n-\n-    long containerID = 100L;\n-    int deletedBlocks = 5;\n+    int deletingBlocks = 5;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerID, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks,\n+            deletingBlocks);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 95}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU3NDgzMg==", "bodyText": "Nitpick: Expected value comes before Actual value in assert. It might be confusing in case there is an exception.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479574832", "createdAt": "2020-08-28T23:36:54Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java", "diffHunk": "@@ -88,196 +86,314 @@ public void setUp() throws Exception {\n     conf = new OzoneConfiguration();\n     conf.set(HDDS_DATANODE_DIR_KEY, testRoot.getAbsolutePath());\n     volumeSet = new MutableVolumeSet(UUID.randomUUID().toString(), conf);\n+\n+    containerData = new KeyValueContainerData(105L,\n+            layout,\n+            (long) StorageUnit.GB.toBytes(1), UUID.randomUUID().toString(),\n+            UUID.randomUUID().toString());\n+    // Init the container.\n+    container = new KeyValueContainer(containerData, conf);\n+    container.create(volumeSet, new RoundRobinVolumeChoosingPolicy(), UUID\n+            .randomUUID().toString());\n+    db = BlockUtils.getDB(containerData, conf);\n   }\n \n \n   @After\n-  public void tearDown() {\n+  public void tearDown() throws Exception {\n+    db.close();\n+    db.cleanup();\n     volumeSet.shutdown();\n     FileUtil.fullyDelete(testRoot);\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithMixedBlocks() throws Exception {\n-\n-    long containerID = 100L;\n-    int deletedBlocks = 5;\n+    int deletingBlocks = 5;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerID, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks,\n+            deletingBlocks);\n \n-      int counter = 0;\n+    // Default filter used is all unprefixed blocks.\n+    List<Long> unprefixedBlockIDs = blockIDs.get(\"\");\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n+\n+      Iterator<Long> blockIDIter = unprefixedBlockIDs.iterator();\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(), (long)blockIDIter.next());\n       }\n-\n       assertFalse(keyValueBlockIterator.hasNext());\n+      assertFalse(blockIDIter.hasNext());\n \n       keyValueBlockIterator.seekToFirst();\n-      counter = 0;\n+      blockIDIter = unprefixedBlockIDs.iterator();\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(), (long)blockIDIter.next());\n       }\n       assertFalse(keyValueBlockIterator.hasNext());\n+      assertFalse(blockIDIter.hasNext());\n \n       try {\n         keyValueBlockIterator.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithNextBlock() throws Exception {\n-    long containerID = 101L;\n-    createContainerWithBlocks(containerID, 2, 0);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n-      long blockID = 0L;\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n+    List<Long> blockIDs = createContainerWithBlocks(CONTAINER_ID, 2);\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n+      assertEquals((long)blockIDs.get(0),\n+              keyValueBlockIterator.nextBlock().getLocalID());\n+      assertEquals((long)blockIDs.get(1),\n+              keyValueBlockIterator.nextBlock().getLocalID());\n \n       try {\n         keyValueBlockIterator.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithHasNext() throws Exception {\n-    long containerID = 102L;\n-    createContainerWithBlocks(containerID, 2, 0);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n-      long blockID = 0L;\n+    List<Long> blockIDs = createContainerWithBlocks(CONTAINER_ID, 2);\n+    try(BlockIterator<BlockData> blockIter =\n+                db.getStore().getBlockIterator()) {\n \n       // Even calling multiple times hasNext() should not move entry forward.\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      keyValueBlockIterator.seekToLast();\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      keyValueBlockIterator.seekToFirst();\n-      blockID = 0L;\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertEquals((long)blockIDs.get(0),\n+              blockIter.nextBlock().getLocalID());\n+\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertEquals((long)blockIDs.get(1), blockIter.nextBlock().getLocalID());\n+\n+      blockIter.seekToFirst();\n+      assertEquals((long)blockIDs.get(0), blockIter.nextBlock().getLocalID());\n+      assertEquals((long)blockIDs.get(1), blockIter.nextBlock().getLocalID());\n \n       try {\n-        keyValueBlockIterator.nextBlock();\n+        blockIter.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithFilter() throws Exception {\n-    long containerId = 103L;\n-    int deletedBlocks = 10;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerId, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerId, new File(containerPath), MetadataKeyFilters\n-        .getDeletingKeyFilter())) {\n-\n-      int counter = 5;\n+    int deletingBlocks = 5;\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks, deletingBlocks);\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator(\n+                        MetadataKeyFilters.getDeletingKeyFilter())) {\n+      List<Long> deletingBlockIDs =\n+              blockIDs.get(OzoneConsts.DELETING_KEY_PREFIX);\n+      int counter = 0;\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(),\n+                (long)deletingBlockIDs.get(counter));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 255}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU3NzE4NA==", "bodyText": "Javadoc is not clear.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479577184", "createdAt": "2020-08-28T23:49:26Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/test/java/org/apache/hadoop/ozone/container/keyvalue/TestKeyValueBlockIterator.java", "diffHunk": "@@ -88,196 +86,314 @@ public void setUp() throws Exception {\n     conf = new OzoneConfiguration();\n     conf.set(HDDS_DATANODE_DIR_KEY, testRoot.getAbsolutePath());\n     volumeSet = new MutableVolumeSet(UUID.randomUUID().toString(), conf);\n+\n+    containerData = new KeyValueContainerData(105L,\n+            layout,\n+            (long) StorageUnit.GB.toBytes(1), UUID.randomUUID().toString(),\n+            UUID.randomUUID().toString());\n+    // Init the container.\n+    container = new KeyValueContainer(containerData, conf);\n+    container.create(volumeSet, new RoundRobinVolumeChoosingPolicy(), UUID\n+            .randomUUID().toString());\n+    db = BlockUtils.getDB(containerData, conf);\n   }\n \n \n   @After\n-  public void tearDown() {\n+  public void tearDown() throws Exception {\n+    db.close();\n+    db.cleanup();\n     volumeSet.shutdown();\n     FileUtil.fullyDelete(testRoot);\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithMixedBlocks() throws Exception {\n-\n-    long containerID = 100L;\n-    int deletedBlocks = 5;\n+    int deletingBlocks = 5;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerID, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks,\n+            deletingBlocks);\n \n-      int counter = 0;\n+    // Default filter used is all unprefixed blocks.\n+    List<Long> unprefixedBlockIDs = blockIDs.get(\"\");\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n+\n+      Iterator<Long> blockIDIter = unprefixedBlockIDs.iterator();\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(), (long)blockIDIter.next());\n       }\n-\n       assertFalse(keyValueBlockIterator.hasNext());\n+      assertFalse(blockIDIter.hasNext());\n \n       keyValueBlockIterator.seekToFirst();\n-      counter = 0;\n+      blockIDIter = unprefixedBlockIDs.iterator();\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(), (long)blockIDIter.next());\n       }\n       assertFalse(keyValueBlockIterator.hasNext());\n+      assertFalse(blockIDIter.hasNext());\n \n       try {\n         keyValueBlockIterator.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithNextBlock() throws Exception {\n-    long containerID = 101L;\n-    createContainerWithBlocks(containerID, 2, 0);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n-      long blockID = 0L;\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n+    List<Long> blockIDs = createContainerWithBlocks(CONTAINER_ID, 2);\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n+      assertEquals((long)blockIDs.get(0),\n+              keyValueBlockIterator.nextBlock().getLocalID());\n+      assertEquals((long)blockIDs.get(1),\n+              keyValueBlockIterator.nextBlock().getLocalID());\n \n       try {\n         keyValueBlockIterator.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithHasNext() throws Exception {\n-    long containerID = 102L;\n-    createContainerWithBlocks(containerID, 2, 0);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerID, new File(containerPath))) {\n-      long blockID = 0L;\n+    List<Long> blockIDs = createContainerWithBlocks(CONTAINER_ID, 2);\n+    try(BlockIterator<BlockData> blockIter =\n+                db.getStore().getBlockIterator()) {\n \n       // Even calling multiple times hasNext() should not move entry forward.\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      keyValueBlockIterator.seekToLast();\n-      assertTrue(keyValueBlockIterator.hasNext());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n-\n-      keyValueBlockIterator.seekToFirst();\n-      blockID = 0L;\n-      assertEquals(blockID++, keyValueBlockIterator.nextBlock().getLocalID());\n-      assertEquals(blockID, keyValueBlockIterator.nextBlock().getLocalID());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertEquals((long)blockIDs.get(0),\n+              blockIter.nextBlock().getLocalID());\n+\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertTrue(blockIter.hasNext());\n+      assertEquals((long)blockIDs.get(1), blockIter.nextBlock().getLocalID());\n+\n+      blockIter.seekToFirst();\n+      assertEquals((long)blockIDs.get(0), blockIter.nextBlock().getLocalID());\n+      assertEquals((long)blockIDs.get(1), blockIter.nextBlock().getLocalID());\n \n       try {\n-        keyValueBlockIterator.nextBlock();\n+        blockIter.nextBlock();\n       } catch (NoSuchElementException ex) {\n         GenericTestUtils.assertExceptionContains(\"Block Iterator reached end \" +\n-            \"for ContainerID \" + containerID, ex);\n+            \"for ContainerID \" + CONTAINER_ID, ex);\n       }\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithFilter() throws Exception {\n-    long containerId = 103L;\n-    int deletedBlocks = 10;\n     int normalBlocks = 5;\n-    createContainerWithBlocks(containerId, normalBlocks, deletedBlocks);\n-    String containerPath = new File(containerData.getMetadataPath())\n-        .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerId, new File(containerPath), MetadataKeyFilters\n-        .getDeletingKeyFilter())) {\n-\n-      int counter = 5;\n+    int deletingBlocks = 5;\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            normalBlocks, deletingBlocks);\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator(\n+                        MetadataKeyFilters.getDeletingKeyFilter())) {\n+      List<Long> deletingBlockIDs =\n+              blockIDs.get(OzoneConsts.DELETING_KEY_PREFIX);\n+      int counter = 0;\n       while (keyValueBlockIterator.hasNext()) {\n         BlockData blockData = keyValueBlockIterator.nextBlock();\n-        assertEquals(blockData.getLocalID(), counter++);\n+        assertEquals(blockData.getLocalID(),\n+                (long)deletingBlockIDs.get(counter));\n+        counter++;\n       }\n-      assertEquals(10, counter);\n+\n+      assertEquals(deletingBlocks, counter);\n     }\n   }\n \n   @Test\n   public void testKeyValueBlockIteratorWithOnlyDeletedBlocks() throws\n       Exception {\n-    long containerId = 104L;\n-    createContainerWithBlocks(containerId, 0, 5);\n+    createContainerWithBlocks(CONTAINER_ID, 0, 5);\n     String containerPath = new File(containerData.getMetadataPath())\n         .getParent();\n-    try(KeyValueBlockIterator keyValueBlockIterator = new KeyValueBlockIterator(\n-        containerId, new File(containerPath))) {\n+    try(BlockIterator<BlockData> keyValueBlockIterator =\n+                db.getStore().getBlockIterator()) {\n       //As all blocks are deleted blocks, blocks does not match with normal key\n       // filter.\n       assertFalse(keyValueBlockIterator.hasNext());\n     }\n   }\n \n+  /**\n+   * Due to RocksDB internals, prefixed keys may be grouped all at the\n+   * beginning or end of the key iteration, depending on the serialization\n+   * used. Keys of the same prefix are grouped\n+   * together. This method runs the same set of tests on the iterator first\n+   * positively filtering one prefix, and then positively filtering\n+   * a second prefix. If the sets of keys with prefix one, prefix\n+   * two, and no prefixes are not empty, it follows that the filter will\n+   * encounter both of the following cases:\n+   *\n+   * 1. A failing key followed by a passing key.\n+   * 2. A passing key followed by a failing key.\n+   *\n+   * Note that with the current block data table implementation, there is\n+   * only ever one type of prefix. This test adds a dummy second prefix type\n+   * to ensure that the iterator will continue to work if more prefixes are\n+   * added in the future.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testKeyValueBlockIteratorWithAdvancedFilter() throws\n+          Exception {\n+    // Block data table currently only uses one prefix type.\n+    // Introduce a second prefix type to make sure the iterator functions\n+    // correctly if more prefixes were to be added in the future.\n+    final String secondPrefix = \"#FOOBAR#\";\n+    Map<String, Integer> prefixCounts = new HashMap<>();\n+    prefixCounts.put(OzoneConsts.DELETING_KEY_PREFIX, 3);\n+    prefixCounts.put(\"\", 3);\n+    prefixCounts.put(secondPrefix, 3);\n+\n+    Map<String, List<Long>> blockIDs = createContainerWithBlocks(CONTAINER_ID,\n+            prefixCounts);\n+    // Test deleting filter.\n+    testWithFilter(MetadataKeyFilters.getDeletingKeyFilter(),\n+            blockIDs.get(OzoneConsts.DELETING_KEY_PREFIX));\n+\n+    // Test arbitrary filter.\n+    MetadataKeyFilters.KeyPrefixFilter secondFilter =\n+            new MetadataKeyFilters.KeyPrefixFilter()\n+            .addFilter(secondPrefix);\n+    testWithFilter(secondFilter, blockIDs.get(secondPrefix));\n+  }\n+\n+  /**\n+   * Helper method to run some iterator tests with a provided filter.\n+   */\n+  private void testWithFilter(MetadataKeyFilters.KeyPrefixFilter filter,\n+                              List<Long> expectedIDs) throws Exception {\n+    try(BlockIterator<BlockData> iterator =\n+                db.getStore().getBlockIterator(filter)) {\n+      // Test seek.\n+      iterator.seekToFirst();\n+      long firstID = iterator.nextBlock().getLocalID();\n+      assertEquals(expectedIDs.get(0).longValue(), firstID);\n+      assertTrue(iterator.hasNext());\n+\n+      // Test atypical iteration use.\n+      iterator.seekToFirst();\n+      int numIDsSeen = 0;\n+      for (long id: expectedIDs) {\n+        assertEquals(iterator.nextBlock().getLocalID(), id);\n+        numIDsSeen++;\n+\n+        // Test that iterator can handle sporadic hasNext() calls.\n+        if (id % 2 == 0 && numIDsSeen < expectedIDs.size()) {\n+          assertTrue(iterator.hasNext());\n+        }\n+      }\n+\n+      assertFalse(iterator.hasNext());\n+    }\n+  }\n+\n+  /**\n+   * Creates a container with specified number of unprefixed (normal) blocks.\n+   * @param containerId\n+   * @param normalBlocks\n+   * @return The list of block IDs of normal blocks that were created.\n+   * @throws Exception\n+   */\n+  private List<Long> createContainerWithBlocks(long containerId,\n+            int normalBlocks) throws Exception {\n+    return createContainerWithBlocks(containerId, normalBlocks, 0).get(\"\");\n+  }\n+\n   /**\n    * Creates a container with specified number of normal blocks and deleted\n-   * blocks. First it will insert normal blocks, and then it will insert\n-   * deleted blocks.\n+   * blocks.\n+   * deleting blocks, then it will insert deleted blocks.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 374}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU3ODM0MA==", "bodyText": "LOCK and LOG files are not required to load the  DB.", "url": "https://github.com/apache/ozone/pull/1298#discussion_r479578340", "createdAt": "2020-08-28T23:55:27Z", "author": {"login": "hanishakoneru"}, "path": "hadoop-hdds/container-service/src/test/resources/123-dn-container.db/LOG", "diffHunk": "@@ -0,0 +1,284 @@\n+2020/08/03-15:13:40.359520 7f80eb7a9700 RocksDB version: 6.8.1\n+2020/08/03-15:13:40.359563 7f80eb7a9700 Git sha rocksdb_build_git_sha:\n+2020/08/03-15:13:40.359566 7f80eb7a9700 Compile date Apr 26 2020", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5415d60c02bd76a5d0544e3d8c3d59e2e4817092"}, "originalPosition": 3}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c00f46a37fba58ac9c9036c2d3c262f75fd264b2", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/c00f46a37fba58ac9c9036c2d3c262f75fd264b2", "committedDate": "2020-09-02T21:16:36Z", "message": "Fix comments, dead code, and formatting issues raised in code review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1501acbad98861fce113f9e68a3900d127276e77", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/1501acbad98861fce113f9e68a3900d127276e77", "committedDate": "2020-09-03T16:12:12Z", "message": "Fix checkstyle error"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "de0cae3f070598a2f6150da940e1a4a3b78c9805", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/de0cae3f070598a2f6150da940e1a4a3b78c9805", "committedDate": "2020-09-03T17:08:19Z", "message": "Remove unused implementation of table cache method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d68a63b1ec623564ffa58fa3df421fd88cfb946c", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/d68a63b1ec623564ffa58fa3df421fd88cfb946c", "committedDate": "2020-09-03T19:22:14Z", "message": "Remove prefixes from keys in returned lists\n\nRequires doing an extra pass through the generated results."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f29ea9d791f1fe392ef77ca48b731f35e0b725ce", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/f29ea9d791f1fe392ef77ca48b731f35e0b725ce", "committedDate": "2020-09-03T19:38:17Z", "message": "Add unit test to check that deleted block table iterators have no prefixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2b11bc631f958184f0a41291a75e94bf66690247", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/2b11bc631f958184f0a41291a75e94bf66690247", "committedDate": "2020-09-08T20:39:25Z", "message": "Merge branch 'preprocess-schema1-iterators' into HDDS-3869\n\n* preprocess-schema1-iterators:\n  Add unit test to check that deleted block table iterators have no prefixes\n  Remove prefixes from keys in returned lists"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "43cac801e344f64e8000d82f9cd0b9cd17c0df3d", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/43cac801e344f64e8000d82f9cd0b9cd17c0df3d", "committedDate": "2020-09-08T21:40:47Z", "message": "Switch block length calculation back to original method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b2910162610a16e35513b5835c988fea642bd58f", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/b2910162610a16e35513b5835c988fea642bd58f", "committedDate": "2020-09-08T21:48:48Z", "message": "Remove check of bytes used after test block delete\n\nA comment was added to explain why the value is not updated due to mock classes used."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "899c4acff7cfb7f3f97c28257ae72b20fe86feee", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/899c4acff7cfb7f3f97c28257ae72b20fe86feee", "committedDate": "2020-09-09T14:11:59Z", "message": "Move schema version null check to container data's schema version setter\n\nA null value for schema version indicates it has schema version 1."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2d4146686034cb63a78b7f1a47d634c9881636f9", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/2d4146686034cb63a78b7f1a47d634c9881636f9", "committedDate": "2020-09-09T15:15:05Z", "message": "All unit tests pass with new method of reading in container\n\nNew container reading method gets the container file the same way the actual volume reading code does."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "43a71791b716fc36b4f13a55e81781d843eaba87", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/43a71791b716fc36b4f13a55e81781d843eaba87", "committedDate": "2020-09-09T15:38:57Z", "message": "Remove fileds from .container file that must be calculated at run time"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ce1466364ade66afaf3a2288f219def12af85d38", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/ce1466364ade66afaf3a2288f219def12af85d38", "committedDate": "2020-09-09T15:42:54Z", "message": "Merge branch 'HDDS-3869-refactor-schema-tests' into HDDS-3869\n\n* HDDS-3869-refactor-schema-tests:\n  Remove fileds from .container file that must be calculated at run time\n  All unit tests pass with new method of reading in container"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "30b1fcd346f3eb4dd3a611ec7ce738f3b27139b5", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/30b1fcd346f3eb4dd3a611ec7ce738f3b27139b5", "committedDate": "2020-09-09T15:56:16Z", "message": "Add more comments on usage of DatanodeTable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "467d71a8db813d74f3bdae937d8aea3ff184ea9c", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/467d71a8db813d74f3bdae937d8aea3ff184ea9c", "committedDate": "2020-09-09T16:13:56Z", "message": "Fix checkstyle errors"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg4MjEyNDkz", "url": "https://github.com/apache/ozone/pull/1298#pullrequestreview-488212493", "createdAt": "2020-09-14T23:01:20Z", "commit": {"oid": "467d71a8db813d74f3bdae937d8aea3ff184ea9c"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9662789ce404df655bc90826ad72654673afe6d0", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/9662789ce404df655bc90826ad72654673afe6d0", "committedDate": "2020-09-21T20:16:54Z", "message": "Add options cache optimization from HDDS-2283 to new API\n\nThis optimization saves RocksDB options objects based on the configuration\nused, so that the expensive JNI call to create a new object is not performed\nevery time a new container is created. In HDDS-2283 and this commit,\nthis feature was not done for WriteOptions."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "22182bb7fd608a47efc48e5a448b7b5853ffc912", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/22182bb7fd608a47efc48e5a448b7b5853ffc912", "committedDate": "2020-09-21T20:20:08Z", "message": "Merge branch 'HDDS-3869-cache-db-options' into HDDS-3869\n\n* HDDS-3869-cache-db-options:\n  Add options cache optimization from HDDS-2283 to new API"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dd5bbd928c17453373e3d1de1ce87d07fc956cea", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/dd5bbd928c17453373e3d1de1ce87d07fc956cea", "committedDate": "2020-09-24T20:49:39Z", "message": "Move schema version null check back to the container data parser\n\nThe default schema version on null value cannot be set when reading in the .container file, because it will fail checksum validation."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3effacda78c2748c0408516fc41662fdc5d0260f", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/3effacda78c2748c0408516fc41662fdc5d0260f", "committedDate": "2020-09-24T21:51:27Z", "message": "Remove options cache implementation from new database utils\n\nThis implementation causes seg faults at the C++ level in RocksDB."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "711017e5effa66bd22f7ee0994de2ec42d96e8bf", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/711017e5effa66bd22f7ee0994de2ec42d96e8bf", "committedDate": "2020-09-30T15:45:47Z", "message": "Add unit tests to expose issues with encoding/decoding block data keys"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c34291be1122667edc16c4b4a75e920f80e792a6", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/c34291be1122667edc16c4b4a75e920f80e792a6", "committedDate": "2020-09-30T18:33:14Z", "message": "Add new codec for keys that passes unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "471092d8648338956f209d25546c5c95906674f6", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/471092d8648338956f209d25546c5c95906674f6", "committedDate": "2020-09-30T21:00:55Z", "message": "Add comments and clearer class name for new codec"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "620c9a74a5e6839e3849bc87f70243e0b3fdc940", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/620c9a74a5e6839e3849bc87f70243e0b3fdc940", "committedDate": "2020-09-30T21:05:49Z", "message": "Fix checkstyle errors"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c6ff9e1129baba110179cab7cb88d12fa154616c", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/c6ff9e1129baba110179cab7cb88d12fa154616c", "committedDate": "2020-09-30T21:06:10Z", "message": "Merge branch 'HDDS-3869-fix-block-keycodecs' into HDDS-3869\n\n* HDDS-3869-fix-block-keycodecs:\n  Fix checkstyle errors\n  Add comments and clearer class name for new codec\n  Add new codec for keys that passes unit tests\n  Add unit tests to expose issues with encoding/decoding block data keys"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f1646012420978445b519108df09e8419d904c31", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/f1646012420978445b519108df09e8419d904c31", "committedDate": "2020-10-01T13:17:30Z", "message": "Change incorrect string keys to longs in DB\n\nThe change was accidentally made in the target resource originally, not the source files."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "23283dfc3b1f0c8c15881249f1deda1c6224ba23", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/23283dfc3b1f0c8c15881249f1deda1c6224ba23", "committedDate": "2020-10-01T16:34:36Z", "message": "Fix codec bugs to pass acceptance tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "183ffc86d83d678e39d0118abf0c8ff69839d23c", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/183ffc86d83d678e39d0118abf0c8ff69839d23c", "committedDate": "2020-10-01T17:02:33Z", "message": "Add test of block iterator"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "82bcb8e452887216103db0b30631ba4912e0ca3e", "author": {"user": {"login": "errose28", "name": "Ethan Rose"}}, "url": "https://github.com/apache/ozone/commit/82bcb8e452887216103db0b30631ba4912e0ca3e", "committedDate": "2020-10-01T18:29:15Z", "message": "Fix checkstyle violations"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAwNzA3NTA2", "url": "https://github.com/apache/ozone/pull/1298#pullrequestreview-500707506", "createdAt": "2020-10-01T20:49:28Z", "commit": {"oid": "82bcb8e452887216103db0b30631ba4912e0ca3e"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fce5e9fdaf108c4c89024c34593a9f5db1d7088e", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/fce5e9fdaf108c4c89024c34593a9f5db1d7088e", "committedDate": "2020-07-21T13:37:18Z", "message": "Switch key type of block data table from long to string\n\nAlthough the block IDs are longs, the prefixes sometimes used requires the type\nto be a string."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e1cf80afe17efd50826d5a61b837ab53ae1e08c1", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/e1cf80afe17efd50826d5a61b837ab53ae1e08c1", "committedDate": "2020-07-21T13:37:22Z", "message": "Switch calls to sequential and non sequential getRangeKV() calls to the new interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "af84fa54db12378b951d929de7654e2bd623b47b", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/af84fa54db12378b951d929de7654e2bd623b47b", "committedDate": "2020-07-21T13:37:22Z", "message": "Switch KeyValueBlockIterator to new iterator interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb87771e44f99c31a4e0ec2ad1a3b8f45f5b4e7a", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/bb87771e44f99c31a4e0ec2ad1a3b8f45f5b4e7a", "committedDate": "2020-07-21T13:37:22Z", "message": "Make ContainerCache.getDB() not require the schema version\n\nIf it pulls a cached container, we don't need to set it up.\nIf it needs to create a new container, we always use the two table version."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ecd876e2e32c5ce39396c746d7b5734266c9afe1", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/ecd876e2e32c5ce39396c746d7b5734266c9afe1", "committedDate": "2020-07-21T13:37:22Z", "message": "Remove unused imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c81e2fc6de792be8ef65dc71de24b03cfa7a279c", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/c81e2fc6de792be8ef65dc71de24b03cfa7a279c", "committedDate": "2020-07-21T13:37:22Z", "message": "Add new constructor to DBStore builder that allows specifying the DBDefintiion"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a5bd13cd2d8ddedd265956270d4b94d928535863", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/a5bd13cd2d8ddedd265956270d4b94d928535863", "committedDate": "2020-07-21T13:37:22Z", "message": "Add getDBLocation method for Datanode DBDefinition to use instead of location config key"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c5974bd908b4fe87258305d623567a09c89c0d42", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/c5974bd908b4fe87258305d623567a09c89c0d42", "committedDate": "2020-07-21T13:37:22Z", "message": "Update container cache test to provide a schema version when using the cache"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "765d5aea2595fbc7e09ad068175976117ca760a9", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/765d5aea2595fbc7e09ad068175976117ca760a9", "committedDate": "2020-07-21T13:37:22Z", "message": "Update method header and fix string comparison for schema version"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0d760943e7bf237851913b8314d1f36895a00e4c", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/0d760943e7bf237851913b8314d1f36895a00e4c", "committedDate": "2020-07-21T13:37:22Z", "message": "Remove extra import in BlockManagerImpl that was accidentally added"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ca442c36655e64e13daacd12c6c2a96910b5abbb", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/ca442c36655e64e13daacd12c6c2a96910b5abbb", "committedDate": "2020-07-21T13:37:22Z", "message": "Re add schema version parameter to in BlockUtils after it was removed by the merge"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "94d373b45c401b65987d46c67a12f2d2879de280", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/94d373b45c401b65987d46c67a12f2d2879de280", "committedDate": "2020-07-21T13:37:22Z", "message": "Remove extra import that was already added"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "157b427d8b4d89ba950a7f06ec1be6450f8dab65", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/157b427d8b4d89ba950a7f06ec1be6450f8dab65", "committedDate": "2020-07-21T13:37:22Z", "message": "Fix bug where DB name was used instead of column family name"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a58773fd9fd6f500be52db34baeace802306f344", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/a58773fd9fd6f500be52db34baeace802306f344", "committedDate": "2020-07-21T13:37:22Z", "message": "Fix bugs with missing column families and mismatched ContainerCache params"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "39389c9a58303f63dc62a3d447c80665c372c872", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/39389c9a58303f63dc62a3d447c80665c372c872", "committedDate": "2020-07-21T13:37:22Z", "message": "Fix bug to allow null key to be used as start key in key range query"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "242452a49747141453aef9b1853feee1aa67c6b1", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/242452a49747141453aef9b1853feee1aa67c6b1", "committedDate": "2020-07-21T13:37:22Z", "message": "Make DeleteBlocksCommandHandler.deleteKeyValueContainerBlocks() use block data table instead of metadata table"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "300f445e566288f96d9ab600d548681b4f4f649d", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/300f445e566288f96d9ab600d548681b4f4f649d", "committedDate": "2020-07-21T13:37:22Z", "message": "Fix null start key error that was missed in the original fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8967675dee11a361126d7f01f573eb4596803376", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/8967675dee11a361126d7f01f573eb4596803376", "committedDate": "2020-07-21T13:37:22Z", "message": "Remove early batch operation commit in BlockDeletingService"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0ae45e213d0712067f8732499b623575a57d7714", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/0ae45e213d0712067f8732499b623575a57d7714", "committedDate": "2020-07-21T13:37:22Z", "message": "Fix bug where .db file was created in wrong directory\n\nThe added method DBDefinition.getDBLocation() is now defined to return the parent directory of the .db file."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0941deec80a88cdc36790dcac6a5e39f468ea7b2", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/0941deec80a88cdc36790dcac6a5e39f468ea7b2", "committedDate": "2020-07-21T13:37:22Z", "message": "Fix bug where default column family was being registered twice"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab2fd6cf98bb52fb6dc32b74fcbda803d7b9e6cf", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/ab2fd6cf98bb52fb6dc32b74fcbda803d7b9e6cf", "committedDate": "2020-07-21T13:37:22Z", "message": "Make testContainerImportExport write to block and metadata tables where needed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2e89fc0e08c010216df686d7083aa81f3a197e79", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/2e89fc0e08c010216df686d7083aa81f3a197e79", "committedDate": "2020-07-21T13:37:23Z", "message": "Minor readability change"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ccc58847c7f62afd939a6a2b283075067188823a", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/ccc58847c7f62afd939a6a2b283075067188823a", "committedDate": "2020-07-21T13:39:00Z", "message": "Remove missed git artifacts after conflict"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "79b7fc4973e396b7c461ed3538120e2fad56ed8d", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/79b7fc4973e396b7c461ed3538120e2fad56ed8d", "committedDate": "2020-07-21T13:39:00Z", "message": "Switch merged in unit test to new interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6f53850bacb8a1aee64082605ac8576805ef4cfa", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/6f53850bacb8a1aee64082605ac8576805ef4cfa", "committedDate": "2020-07-21T13:39:00Z", "message": "Update merged unit test to use new interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a3bc1866528efb8c9ea48210b470f5bd5623e9e7", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/a3bc1866528efb8c9ea48210b470f5bd5623e9e7", "committedDate": "2020-07-21T13:39:00Z", "message": "Add schema version param to ContainerCache.getDB() after it was accidentally removed in merge"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8807d0c61350572be827e4229fd85d3531f2df4a", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/8807d0c61350572be827e4229fd85d3531f2df4a", "committedDate": "2020-07-21T13:39:01Z", "message": "Remove duplicate method call introduced accidentally when resolving merge conflict"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "081e3e064133aa181a2a9e396948638d8b286e47", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/081e3e064133aa181a2a9e396948638d8b286e47", "committedDate": "2020-07-21T13:40:31Z", "message": "Move new key value block iterator implementation and tests to new interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5b59a73793f3a5414d748f54933b263f29ff2425", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/5b59a73793f3a5414d748f54933b263f29ff2425", "committedDate": "2020-07-21T13:40:35Z", "message": "Import schema version when importing container data from export"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "98dc6b772bbdae17f50afaa2f332e1b5aaae755e", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/98dc6b772bbdae17f50afaa2f332e1b5aaae755e", "committedDate": "2020-07-21T13:40:35Z", "message": "Move block delete to correct table and remove debugging print statement"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c08d6b99e6021e3551df7aa3e03f221b3fdc1bf5", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/c08d6b99e6021e3551df7aa3e03f221b3fdc1bf5", "committedDate": "2020-07-21T13:40:35Z", "message": "Have block deleting service test look for #deleted# keys in metadata table"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "690f8178dff8aea10ed7e89d2600253229fc427f", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/690f8178dff8aea10ed7e89d2600253229fc427f", "committedDate": "2020-07-21T13:40:35Z", "message": "HDDS-3987. Encrypted bucket creation failed with INVALID_REQUEST Encryption cannot be set for bucket links (#1221)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "da17469aff5ece05810568fceac4b0d650b14914", "author": {"user": {"login": "smengcl", "name": "Siyao Meng"}}, "url": "https://github.com/apache/ozone/commit/da17469aff5ece05810568fceac4b0d650b14914", "committedDate": "2020-07-21T13:40:35Z", "message": "HDDS-3982. Disable moveToTrash in o3fs and ofs temporarily (#1215)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "75d1273da6bb400aec0bc7611592cb8346756077", "author": {"user": {"login": "lokeshj1703", "name": "Lokesh Jain"}}, "url": "https://github.com/apache/ozone/commit/75d1273da6bb400aec0bc7611592cb8346756077", "committedDate": "2020-07-21T13:40:35Z", "message": "Update ratis to 1.0.0 (#1222)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "801d9aebd51eeb770b4675b7b76f3f1d59601500", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/801d9aebd51eeb770b4675b7b76f3f1d59601500", "committedDate": "2020-07-21T13:40:35Z", "message": "HDDS-3813. Upgrade Ratis third-party, too (#1229)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5e7f2e4610dca4dd5de55cc39d9e5e700464cac3", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/5e7f2e4610dca4dd5de55cc39d9e5e700464cac3", "committedDate": "2020-07-21T14:59:52Z", "message": "Make block iterator tests use deleted blocks table, and remove the now unused #deleted#\n\nUnit tests for BlockDeletingService and KeyValueBlockIterator pass."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2fa068b69faa2e9f2690094f1374745157777ca3", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/2fa068b69faa2e9f2690094f1374745157777ca3", "committedDate": "2020-07-21T15:26:18Z", "message": "Merge branch 'add-deleted-block-table' into HDDS-3869\n\n* add-deleted-block-table: (63 commits)\n  Make block iterator tests use deleted blocks table, and remove the now unused #deleted#\n  Replace uses of #deleted# key prefix with access to new deleted blocks table\n  Add deleted blocks table to base level DB wrappers\n  Have block deleting service test look for #deleted# keys in metadata table\n  Move block delete to correct table and remove debugging print statement\n  Import schema version when importing container data from export\n  HDDS-3984. Support filter and search the columns in recon UI (#1218)\n  HDDS-3806. Support recognize aws v2 Authorization header. (#1098)\n  HDDS-3955. Unable to list intermediate paths on keys created using S3G. (#1196)\n  HDDS-3741. Reload old OM state if Install Snapshot from Leader fails (#1129)\n  Move new key value block iterator implementation and tests to new interface\n  Fix checkstyle violations\n  HDDS-3965. SCM failed to start up for duplicated pipeline detected. (#1210)\n  Update comments\n  Add comments on added helper method\n  Remove seekToLast() from iterator interface, implementation, and tests\n  Add more robust unit test with alternating key matches\n  All unit tests pass after allowing keys with deleted and deleting prefixes to be made\n  HDDS-3855. Add upgrade smoketest (#1142)\n  HDDS-3964. Ratis config key mismatch (#1204)\n  ..."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5c911b0e6cb9004eb63438f30a68bdc2d3d13e3b", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/5c911b0e6cb9004eb63438f30a68bdc2d3d13e3b", "committedDate": "2020-07-21T20:51:44Z", "message": "Fix checkstyle and findbugs errors"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf6fb9bd0358fb47f6af9cebc9ac6dd7fb52c93a", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/cf6fb9bd0358fb47f6af9cebc9ac6dd7fb52c93a", "committedDate": "2020-07-22T15:00:22Z", "message": "Remove now unused key prefix encodings from OzoneConsts"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "24036f595871df92b9c06da5d9f43fc9aa905856", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/24036f595871df92b9c06da5d9f43fc9aa905856", "committedDate": "2020-07-22T15:06:22Z", "message": "Make new OzoneConst SCHEMA_LATEST variable to hold newest schema version"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d71a8e509a390615317f266e2c7d395b7e8d91a2", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/d71a8e509a390615317f266e2c7d395b7e8d91a2", "committedDate": "2020-07-22T15:25:35Z", "message": "Create placeholder and codec for empty byte array"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5d94ed15244b731f1c6702efb0ab61e7eebb9627", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/5d94ed15244b731f1c6702efb0ab61e7eebb9627", "committedDate": "2020-07-22T15:34:33Z", "message": "Update classes to use the new NoData value for deleted blocks table"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b5af1b8b4e1bf30840fba7bf9b51cf76e39d9f5e", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/b5af1b8b4e1bf30840fba7bf9b51cf76e39d9f5e", "committedDate": "2020-07-22T19:28:27Z", "message": "Add test for write and read of schema version to container data file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a74145552ef7c04133a4833e4f2d6e3922749772", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/a74145552ef7c04133a4833e4f2d6e3922749772", "committedDate": "2020-07-23T13:36:30Z", "message": "Add test of schema version set and get for KeyValueContainerData"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5f03f9d37ef0f32adf3eff94bd842058418fcb08", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/5f03f9d37ef0f32adf3eff94bd842058418fcb08", "committedDate": "2020-07-23T15:29:56Z", "message": "Create initial outline of files for testing schema version backwards compatability"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "83270063d1e8076cf7c48f700736f5ab4597ec96", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/83270063d1e8076cf7c48f700736f5ab4597ec96", "committedDate": "2020-07-23T20:47:46Z", "message": "Fix naming of db resources so they are picked up by unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1842c0f3b75ed6c4bb5a5cc7665c00d1b414945c", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/1842c0f3b75ed6c4bb5a5cc7665c00d1b414945c", "committedDate": "2020-07-24T14:55:00Z", "message": "Correct values in test resources and copy and r/w them from temp folder"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8669e6dc5b2c829a673f96ef69637af8b9c1cc12", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/8669e6dc5b2c829a673f96ef69637af8b9c1cc12", "committedDate": "2020-07-24T16:31:24Z", "message": "Add test for reading container when metadata is absent"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f31ae5e506efdacd5ef2401e5e5ccd13adc93cf6", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/f31ae5e506efdacd5ef2401e5e5ccd13adc93cf6", "committedDate": "2020-07-24T18:56:27Z", "message": "Make class to hold information about the database under test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5fe2cff8bab045576eeeb507b636b9b57dfd4fe4", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/5fe2cff8bab045576eeeb507b636b9b57dfd4fe4", "committedDate": "2020-07-24T20:34:47Z", "message": "Add mostly complete deltion and iteration tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9cdc4b244bfa8387097c04ea3667e2f600192d16", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/9cdc4b244bfa8387097c04ea3667e2f600192d16", "committedDate": "2020-07-24T20:48:20Z", "message": "Rename classes to reflect which database schema version they correspond to"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "da5c33c7a0e8c5983c55971c1a6cc722befaf2bc", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/da5c33c7a0e8c5983c55971c1a6cc722befaf2bc", "committedDate": "2020-07-24T21:00:32Z", "message": "Add statistics to new datanode store to match old implementation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2e122108122ff59913aa564c7672d43f5721cf82", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/2e122108122ff59913aa564c7672d43f5721cf82", "committedDate": "2020-07-28T00:18:40Z", "message": "Start outline of prefix speific solution\n\nAdd table implemention (WIP) that warns when keys are added without prefix, or table is scanned without filtering for prefix."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "acd0ca45c5162442fd3b7fdde17719b0fe4387b3", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/acd0ca45c5162442fd3b7fdde17719b0fe4387b3", "committedDate": "2020-07-28T13:19:12Z", "message": "Switch key type of deleted blocks table to String"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c18658f604d17534835635903bdcc4cfb0580530", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/c18658f604d17534835635903bdcc4cfb0580530", "committedDate": "2020-07-28T13:40:38Z", "message": "Update block deleting code to use #deleted# prefix in the deleted blocks table\n\nPasses unit tests for TestblockDeletingService."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "464223c5d23d0c8697381bfb199e9e0ee0ca7082", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/464223c5d23d0c8697381bfb199e9e0ee0ca7082", "committedDate": "2020-07-28T14:39:19Z", "message": "Fix test container contents so it has 2 of each block type"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a62f4731ccc858210f8197466507a4edee565dcc", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/a62f4731ccc858210f8197466507a4edee565dcc", "committedDate": "2020-07-28T14:39:49Z", "message": "Iteration using key filters passes on schema version 1 format"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d2faaa6fe1676dcfc9170d24323c4cd12c4b1e82", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/d2faaa6fe1676dcfc9170d24323c4cd12c4b1e82", "committedDate": "2020-07-28T15:19:49Z", "message": "Merge branch 'caller-specifies-prefix-solution' into fix-prefix-bugs\n\n* caller-specifies-prefix-solution:\n  Iteration using key filters passes on schema version 1 format\n  Fix test container contents so it has 2 of each block type\n  Update block deleting code to use #deleted# prefix in the deleted blocks table\n  Switch key type of deleted blocks table to String\n  Start outline of prefix speific solution"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "de8a6acf09d8c8b2779436b6a47a1234cea5e287", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/de8a6acf09d8c8b2779436b6a47a1234cea5e287", "committedDate": "2020-07-28T15:20:11Z", "message": "Merge branch 'fix-prefix-bugs' into HDDS-3869\n\n* fix-prefix-bugs:\n  Iteration using key filters passes on schema version 1 format\n  Fix test container contents so it has 2 of each block type\n  Update block deleting code to use #deleted# prefix in the deleted blocks table\n  Switch key type of deleted blocks table to String\n  Start outline of prefix speific solution"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "97a64c728a73d1162deb7ff225916b974242da01", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/97a64c728a73d1162deb7ff225916b974242da01", "committedDate": "2020-07-28T16:04:23Z", "message": "Fix warnings and update documentation for unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6abdae88f8ab7b1ba79826b0a5c016d3c292efee", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/6abdae88f8ab7b1ba79826b0a5c016d3c292efee", "committedDate": "2020-07-28T16:13:45Z", "message": "Update name and documentation header of unit tests for schema version 1"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "af24c6fd3b4b5486a0957d3dcd8f6271b2c41faf", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/af24c6fd3b4b5486a0957d3dcd8f6271b2c41faf", "committedDate": "2020-07-28T16:38:41Z", "message": "Fix improper usages of key prefix filters\n\nFix representation exposure in MetadataKeyFilter class that allowed callers to\nmodify private static instances of filters, affecting all others holding\nreferences to those values.\n\nRemove instances of manual prefix filter creation, and replace them with\nidentical calls to existing getters in MetadataKeyFilter."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "98ad1084a84167e80de282ed99a137abf88ea41a", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/98ad1084a84167e80de282ed99a137abf88ea41a", "committedDate": "2020-07-28T20:50:15Z", "message": "Initial attempt at block deleting service tests\n\nTests currently fail with a storage exception."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "53d42efaba93dbb69f839eb43b0dda2fcd67330a", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/53d42efaba93dbb69f839eb43b0dda2fcd67330a", "committedDate": "2020-07-28T21:55:02Z", "message": "Fix bug where codecs were not added for multiple DBDefinitions of default table"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e92af54c50d5ad5ee4e212063afaf6c4daf7c2d0", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/e92af54c50d5ad5ee4e212063afaf6c4daf7c2d0", "committedDate": "2020-07-29T13:43:07Z", "message": "Fix copy paste error in key filter getter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2274dbee33bb4d55c5bd1af6b08bf96408dbcc0e", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/2274dbee33bb4d55c5bd1af6b08bf96408dbcc0e", "committedDate": "2020-07-29T13:43:50Z", "message": "Fix the bytes used value in the test database and update the tests accordingly"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b61138a5de7d2dd144e3332b4a102aa3646a550d", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/b61138a5de7d2dd144e3332b4a102aa3646a550d", "committedDate": "2020-07-29T17:51:57Z", "message": "Block deleting service runs and deletes blocks"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c39bd71277b989a185ff5433660bc7201c99a911", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/c39bd71277b989a185ff5433660bc7201c99a911", "committedDate": "2020-07-30T14:10:15Z", "message": "All tests pass except for key count after delete\n\nIssue is documented, and related to\nKeyValueContainerUtil.initializeUsedBytesAndBlockCount() not counting\ndeleting blocks as part of the key count.\nThis will be fixed once it is confirmed if the current implementation is\nincorrect."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a2402b988d73763fe78513f58b189b05233d70a0", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/a2402b988d73763fe78513f58b189b05233d70a0", "committedDate": "2020-07-30T16:11:21Z", "message": "Remove unused imported from TestSchemaOneBackwardsCompatibility"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "953f98afa09f3f3950e9d519f216c67133206a57", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/953f98afa09f3f3950e9d519f216c67133206a57", "committedDate": "2020-07-30T16:12:04Z", "message": "Create initial implementation of SchemaOneDeletedBlocksTable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "584e565958626cad0aee59f5beae90e8382147d6", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/584e565958626cad0aee59f5beae90e8382147d6", "committedDate": "2020-07-30T18:08:45Z", "message": "Remove explicit usages of #deleted# prefix\n\nThis prepares these classes to use the new table implementation that will not require this."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "77f67379b4e9888642599857f1ba4c7f43f20bb7", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/77f67379b4e9888642599857f1ba4c7f43f20bb7", "committedDate": "2020-07-30T20:59:04Z", "message": "Switch from inheritance to composition relationship to TypedTable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "457ba132f63f5e30a073f8ad49db29c19d37685a", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/457ba132f63f5e30a073f8ad49db29c19d37685a", "committedDate": "2020-07-30T21:40:00Z", "message": "Fix errors with new table implementation and usages"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "674e89f8e9f6ffd4d2f9c68353366f0104d6e934", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/674e89f8e9f6ffd4d2f9c68353366f0104d6e934", "committedDate": "2020-07-30T22:10:14Z", "message": "Swap user passed prefixes with #deleted# prefix for key range queries\n\nAllowing user passed prefixes in addition to the #deleted# prefix could cause\nprefix collisions and return results that are not actually deleted blocks."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2d3623c625aa0f818eee20787ad9adef04bf7f2f", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/2d3623c625aa0f818eee20787ad9adef04bf7f2f", "committedDate": "2020-07-31T13:32:55Z", "message": "Change MetadataKeyFilters#getNormalKeyFilter to MetadataKeyFilters#getUnprefixedKeyFilter\n\nThe new method ignores keys that begin with #. The previous implementation\nrequired hard coding all prefixes used. This is no\nlonger effective now that schema version 1 will use the #deleted# prefix\ninternally to make up for a deleted blocks table not exisitng. The old\nimplementation returned these keys, even though they were prefixed."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e725a5b4f8abef6c3ebe773c3f160d8798e4464d", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/e725a5b4f8abef6c3ebe773c3f160d8798e4464d", "committedDate": "2020-07-31T15:54:43Z", "message": "Remove unused variable in DatanodeStoreSchemaOneImpl"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bd1d4b43fff2a51ba5b2c17cd6d6b0306d9a6996", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/bd1d4b43fff2a51ba5b2c17cd6d6b0306d9a6996", "committedDate": "2020-07-31T15:55:00Z", "message": "Make KeyValueContainerUtil include #deleting# blocks when setting key count\n\nUnit tests are updated to reflect this change, which is now consistent with how\nBlockDeletingService handles key count."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a3c64636b55eb000b0664da6acfa6b584c1b1859", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/a3c64636b55eb000b0664da6acfa6b584c1b1859", "committedDate": "2020-07-31T18:06:37Z", "message": "Update documentation and fix warnings in newly created of modified classes."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2f2ed9051192879ff15d65e55c507c88aaecdba8", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/2f2ed9051192879ff15d65e55c507c88aaecdba8", "committedDate": "2020-07-31T21:59:06Z", "message": "Merge branch 'master' into HDDS-3869\n\n* master: (55 commits)\n  HDDS-4052. Remove master/slave terminology from Ozone (#1281)\n  HDDS-4047. OzoneManager met NPE exception while getServiceList (#1277)\n  HDDS-3990. Test Kubernetes examples with acceptance tests (#1223)\n  HDDS-4045. Add more ignore rules to the RAT ignore list (#1273)\n  HDDS-3970. Enabling TestStorageContainerManager with all failures addressed (#1257)\n  HDDS-4033. Make the acceptance test reports hierarchical (#1263)\n  HDDS-3423. Enabling TestContainerReplicationEndToEnd and addressing failures (#1260)\n  HDDS-4027. Suppress ERROR message when SCM attempt to create additional pipelines. (#1265)\n  HDDS-4024. Avoid while loop too soon when exception happen (#1253)\n  HDDS-3809. Make number of open containers on a datanode a function of no of volumes reported by it. (#1081)\n  HDDS-4019. Show the storageDir while need init om or scm (#1248)\n  HDDS-3511. Fix javadoc comment in OmMetadataManager (#1247)\n  HDDS-4041. Ozone /conf endpoint triggers kerberos replay error when SPNEGO is enabled. (#1267)\n  HDDS-4031. Run shell tests in CI (#1261)\n  HDDS-4038. Eliminate GitHub check warnings (#1268)\n  HDDS-4011. Update S3 related documentation. (#1245)\n  HDDS-4030. Remember the selected columns and make the X-axis scrollable in recon datanodes UI (#1259)\n  HDDS-4032. Run author check without docker (#1262)\n  HDDS-4026. Dir rename failed when sets 'ozone.om.enable.filesystem.paths' to true (#1256)\n  HDDS-4017. Acceptance check may run against wrong commit (#1249)\n  ..."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "21eaf712a7f03fd889453dbcacac299b8112111c", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/21eaf712a7f03fd889453dbcacac299b8112111c", "committedDate": "2020-07-31T22:02:10Z", "message": "Remove caching test for DBStoreBuilder\n\nThe new interface does not have this caching structure. The data needed from\nthe config file is always read from memory, not disk."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "862e374bab1e993063e4d35a7551370b442b41e1", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/862e374bab1e993063e4d35a7551370b442b41e1", "committedDate": "2020-07-31T22:14:16Z", "message": "Remove commented out code to fix the test DB isntance"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b6ad8161a80f6bcf8e3e63cfb6de70b236b60b20", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/b6ad8161a80f6bcf8e3e63cfb6de70b236b60b20", "committedDate": "2020-08-03T15:39:52Z", "message": "Add schema version specifier to code merged from master"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "709ad0891478e6bfe21783aa533c3fe57c73ac6a", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/709ad0891478e6bfe21783aa533c3fe57c73ac6a", "committedDate": "2020-08-03T15:40:27Z", "message": "Fix generics issue that prevented code form building"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2948cd0e79ad3c96207c0573947817f2526cadd6", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/2948cd0e79ad3c96207c0573947817f2526cadd6", "committedDate": "2020-08-03T15:49:02Z", "message": "Create ChunkInfoCodec for storing chunk info in the deleted blocks table"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eb0f8550cb73fff09ba9eede1ffd35c45dbabd86", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/eb0f8550cb73fff09ba9eede1ffd35c45dbabd86", "committedDate": "2020-08-03T16:03:08Z", "message": "Make datanode store classes use ChunkInfo as value in deleted blocks table"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b2a8b5497e476ef6058e54ba869126cd5f7f380c", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/b2a8b5497e476ef6058e54ba869126cd5f7f380c", "committedDate": "2020-08-03T17:36:18Z", "message": "Update block deleting service to get chunk info list to store in table before doing deletes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c05d42574b073d568a7d99566f91a9e223812df", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/9c05d42574b073d568a7d99566f91a9e223812df", "committedDate": "2020-08-03T19:42:07Z", "message": "Move corrected tabase values out of target and in to test resources"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e7ee02485b65a044198565a074924679b04f5b68", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/e7ee02485b65a044198565a074924679b04f5b68", "committedDate": "2020-08-03T21:06:58Z", "message": "Restore KeyValueBlockIterator from master branch\n\nThe \"fix\" that was applied earlier was unnecessary."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9832145964687efcc616d0921994746000ded7c0", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/9832145964687efcc616d0921994746000ded7c0", "committedDate": "2020-08-03T21:45:44Z", "message": "Add RocksDB database that is part of unit testing resources to rat exclude list"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "41d596216d94b5bc1e3898179b5892d852fa8d1b", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/41d596216d94b5bc1e3898179b5892d852fa8d1b", "committedDate": "2020-08-04T15:47:16Z", "message": "Add container reader fix from HDDS-4061"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6ad6d07dd862857ed9bcd72db12140c4eb4e8aaf", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/6ad6d07dd862857ed9bcd72db12140c4eb4e8aaf", "committedDate": "2020-08-04T19:11:48Z", "message": "Create protobuf, codec, and helper class for serializing lists of ChunkInfo objects"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7cef40a86b63de5a365e77aed5999d43c3d9725a", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/7cef40a86b63de5a365e77aed5999d43c3d9725a", "committedDate": "2020-08-04T19:26:23Z", "message": "Switch all uses of deleted blocks table to use new ChunkInfoList object as value"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "04ba6896120fddf50efb2bd867279372aac64555", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/04ba6896120fddf50efb2bd867279372aac64555", "committedDate": "2020-08-04T20:16:59Z", "message": "Add unit test to check that chunk information for blocks is preserved\n\nTestBlockDeletion unit tests are currently not starting for some reason."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f796bf0a66fbd6ae99965d6d51e99c7a6a3533b5", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/f796bf0a66fbd6ae99965d6d51e99c7a6a3533b5", "committedDate": "2020-08-05T15:06:31Z", "message": "Add and verify unit test for preserving chunk info after block deletion"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aa9f1f6acd36685d346a665f354ff80b51c67f64", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/aa9f1f6acd36685d346a665f354ff80b51c67f64", "committedDate": "2020-08-05T16:07:46Z", "message": "Add test for reading the chunk info lists from deleted blocks before and after upgrade\n\nThe test fails (as expected) because there is currently no support for handling\ndeleted blocks written with the pre upgrade code, where the value is the block\nID, not the chunk info list."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1e6bf3d663247a7e69b5e7d37e2756ff5d10b231", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/1e6bf3d663247a7e69b5e7d37e2756ff5d10b231", "committedDate": "2020-08-05T16:42:20Z", "message": "Add support for chunk information saved with deleted blocks in both pre and post upgrade versions of the code"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "956b150f668a22d5dd7979cb14e2ba762dbfde82", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/956b150f668a22d5dd7979cb14e2ba762dbfde82", "committedDate": "2020-08-05T17:25:37Z", "message": "Fix checkstyle violations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e442c6af9241b08aec75225ff084bf1c4fac99b7", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/e442c6af9241b08aec75225ff084bf1c4fac99b7", "committedDate": "2020-08-05T17:42:43Z", "message": "Fix bug in unit test where reading of post upgrade deletion blocks was skipped"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b73de108e1371937766eeddeb6e3685ac8abb434", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/b73de108e1371937766eeddeb6e3685ac8abb434", "committedDate": "2020-08-05T17:51:50Z", "message": "Merge branch 'test-chunkinfo-store' into store-chunkinfo-with-deleted-blocks\n\n* test-chunkinfo-store:\n  Fix bug in unit test where reading of post upgrade deletion blocks was skipped\n  Fix checkstyle violations\n  Add support for chunk information saved with deleted blocks in both pre and post upgrade versions of the code\n  Add test for reading the chunk info lists from deleted blocks before and after upgrade\n  Add and verify unit test for preserving chunk info after block deletion"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf20035ab3a8f243d8ff5e5e42ee797eb658ed9e", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/cf20035ab3a8f243d8ff5e5e42ee797eb658ed9e", "committedDate": "2020-08-05T17:52:15Z", "message": "Merge branch 'store-chunkinfo-with-deleted-blocks' into HDDS-3869\n\n* store-chunkinfo-with-deleted-blocks:\n  Fix bug in unit test where reading of post upgrade deletion blocks was skipped\n  Fix checkstyle violations\n  Add support for chunk information saved with deleted blocks in both pre and post upgrade versions of the code\n  Add test for reading the chunk info lists from deleted blocks before and after upgrade\n  Add and verify unit test for preserving chunk info after block deletion\n  Add unit test to check that chunk information for blocks is preserved\n  Switch all uses of deleted blocks table to use new ChunkInfoList object as value\n  Create protobuf, codec, and helper class for serializing lists of ChunkInfo objects\n  Update block deleting service to get chunk info list to store in table before doing deletes\n  Make datanode store classes use ChunkInfo as value in deleted blocks table\n  Create ChunkInfoCodec for storing chunk info in the deleted blocks table"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "11dbefc84f33f54d5ff64881f66050662332eda1", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/11dbefc84f33f54d5ff64881f66050662332eda1", "committedDate": "2020-07-16T13:09:00Z", "message": "Rename DatanodeStore implementations to indicate which table config they use"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4c61bd4dac7541331e50637e5cdce632054a1f2c", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/4c61bd4dac7541331e50637e5cdce632054a1f2c", "committedDate": "2020-07-16T13:09:00Z", "message": "Delete levelDB classes\n\nWe will not be implementing levelDB support"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f80d5434d2e08b66df22247d3fd332f605ed5baa", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/f80d5434d2e08b66df22247d3fd332f605ed5baa", "committedDate": "2020-07-16T13:09:00Z", "message": "Make ReferenceCountedDB use DatanodeStore\n\nMay change to a more generic interface if one is created."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ec0a0cdcb15a6681ae507f5cf1d7ffd4dfdaee67", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/ec0a0cdcb15a6681ae507f5cf1d7ffd4dfdaee67", "committedDate": "2020-07-16T13:09:00Z", "message": "Add schemaVersion field to yaml key value container data"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bc0c2c05a755eb05ffa230f6c99cf4e82269dcce", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/bc0c2c05a755eb05ffa230f6c99cf4e82269dcce", "committedDate": "2020-07-16T13:09:00Z", "message": "Fix types and add comments to one and two table db definitions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8ff6dfd218ea2873b4098627fe52f2c2215f5c8c", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/8ff6dfd218ea2873b4098627fe52f2c2215f5c8c", "committedDate": "2020-07-16T13:09:00Z", "message": "Create abstract layer for shared implementations between one and two column family datanode DBs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3cbefd105000a36f5b644e6d673d6a6185e61571", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/3cbefd105000a36f5b644e6d673d6a6185e61571", "committedDate": "2020-07-16T13:09:00Z", "message": "Create abstract datanodestore layer to support one and two table implementations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3c78ab87a9bae1b67220a1e62e02a01c1fb37e07", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/3c78ab87a9bae1b67220a1e62e02a01c1fb37e07", "committedDate": "2020-07-16T13:09:00Z", "message": "Use generic ConfigurationSource instead of more specific OzoneConfiguration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b228f883f08e5980b9d64bdc84ba85b15bea9ad4", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/b228f883f08e5980b9d64bdc84ba85b15bea9ad4", "committedDate": "2020-07-16T13:09:00Z", "message": "Outline process for determining how to set schema version"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7969bed1580492a8b3aecfb37904eaeb438f75ea", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/7969bed1580492a8b3aecfb37904eaeb438f75ea", "committedDate": "2020-07-16T13:09:00Z", "message": "Implement block data codec"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b028681dfbf7fc751346c1d0372104c68145b27e", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/b028681dfbf7fc751346c1d0372104c68145b27e", "committedDate": "2020-07-16T13:12:39Z", "message": "Add schemaVersion yaml field as string to .container options"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7c32b50b93783279a6f9eea316554f3f44c55413", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/7c32b50b93783279a6f9eea316554f3f44c55413", "committedDate": "2020-07-16T13:13:08Z", "message": "Make AbstractDatanodeStore DB created if missing\n\nThis behavior is identical to the code it is replacing."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "58d21abbf540f253fbcd46975637574d7f86f2d3", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/58d21abbf540f253fbcd46975637574d7f86f2d3", "committedDate": "2020-07-16T13:17:45Z", "message": "Add schemaVersion field to yaml key value container data"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5a5de40dfdc7ab9ee160cfc6cc194f5863243a63", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/5a5de40dfdc7ab9ee160cfc6cc194f5863243a63", "committedDate": "2020-07-16T13:20:04Z", "message": "Add schemaVersion yaml field as string to .container options"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d9082ee5da67859cdc8c76da661ed02d2bc2b952", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/d9082ee5da67859cdc8c76da661ed02d2bc2b952", "committedDate": "2020-07-16T13:20:07Z", "message": "Add flush and compact methods to datanodestore"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e0ad69b3c28965856e1dc8e239c658d25d62921c", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/e0ad69b3c28965856e1dc8e239c658d25d62921c", "committedDate": "2020-07-16T13:20:07Z", "message": "Replace old puts, gets, deletes and batch operations with the new versions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3398689cc193542ceecbb96a322f0d5c4f42f87e", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/3398689cc193542ceecbb96a322f0d5c4f42f87e", "committedDate": "2020-07-16T13:20:07Z", "message": "Begin changing interface usage in nonKeyValue or Test classes\n\nSome changes are pending furhter investigation and are not yet completed."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "75cc4f3c87adac9837a49a3b05d63db70686c0cb", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/75cc4f3c87adac9837a49a3b05d63db70686c0cb", "committedDate": "2020-07-16T13:20:07Z", "message": "Switch key type of block data table from string to long"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "78d82663f8d781ae6d2e80ed52f5f29f2debed7a", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/78d82663f8d781ae6d2e80ed52f5f29f2debed7a", "committedDate": "2020-07-16T13:22:33Z", "message": "Update gets, puts and batch operations in KeyValue classes to use new interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "90d534e959549431d9a41ccce0f0aa4adf5bf5ab", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/90d534e959549431d9a41ccce0f0aa4adf5bf5ab", "committedDate": "2020-07-16T13:38:01Z", "message": "Update get, put, delete, and write batch calls to use new interface in unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b94c1e0093692face52082b875dced0617380381", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/b94c1e0093692face52082b875dced0617380381", "committedDate": "2020-07-16T13:38:04Z", "message": "Add getRangeKVs and getSequentialRangeKVs methods to Table interface\n\nAdd implementations based on the original implementations in MetadataStore,\nexcept that these new versions will only operate on one table."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "84e6c1aceca423ed7e4c2f86d685568228e38412", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/84e6c1aceca423ed7e4c2f86d685568228e38412", "committedDate": "2020-07-16T13:38:04Z", "message": "Fix typo in method call to new interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2221896290b01f9520726519f1d4f040caf22efe", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/2221896290b01f9520726519f1d4f040caf22efe", "committedDate": "2020-07-16T13:38:04Z", "message": "Add separate flushLog() and flushDB() methods to new interface to preserve old functionality"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "add07ada86576b5f63d1d015165e51b221cc1392", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/add07ada86576b5f63d1d015165e51b221cc1392", "committedDate": "2020-07-16T13:38:04Z", "message": "Switch key type of block data table from long to string\n\nAlthough the block IDs are longs, the prefixes sometimes used requires the type\nto be a string."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f3985dc5e472875cbe848c900223cebd7c42a2e2", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/f3985dc5e472875cbe848c900223cebd7c42a2e2", "committedDate": "2020-07-16T13:38:59Z", "message": "Switch calls to sequential and non sequential getRangeKV() calls to the new interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6dcfc786febe107a6d7b2a20ea6a7f35efec96be", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/6dcfc786febe107a6d7b2a20ea6a7f35efec96be", "committedDate": "2020-07-16T13:41:02Z", "message": "Switch KeyValueBlockIterator to new iterator interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7761b944bb192903dcf9cedb64548d7691520e8d", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/7761b944bb192903dcf9cedb64548d7691520e8d", "committedDate": "2020-07-16T13:41:05Z", "message": "Make ContainerCache.getDB() not require the schema version\n\nIf it pulls a cached container, we don't need to set it up.\nIf it needs to create a new container, we always use the two table version."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f251c8fdd9b9121abe9c33927318de54039e7cd5", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/f251c8fdd9b9121abe9c33927318de54039e7cd5", "committedDate": "2020-07-16T13:41:05Z", "message": "Remove unused imports"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "580c6f5d3f17e0eda12e0b7e04e3b8566ffb8ae4", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/580c6f5d3f17e0eda12e0b7e04e3b8566ffb8ae4", "committedDate": "2020-07-16T13:41:05Z", "message": "Add new constructor to DBStore builder that allows specifying the DBDefintiion"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fc4041006f99dc0206e021612327400d76467768", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/fc4041006f99dc0206e021612327400d76467768", "committedDate": "2020-07-16T13:41:05Z", "message": "Add getDBLocation method for Datanode DBDefinition to use instead of location config key"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9b64ab4bedc06870e3ee505ee4ed59feaf0ad44f", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/9b64ab4bedc06870e3ee505ee4ed59feaf0ad44f", "committedDate": "2020-07-16T13:41:05Z", "message": "Update container cache test to provide a schema version when using the cache"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7d35d43d1f3e0187876cc858af20cd4a9d865512", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/7d35d43d1f3e0187876cc858af20cd4a9d865512", "committedDate": "2020-07-16T13:47:00Z", "message": "Update method header and fix string comparison for schema version"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3a722ffafc1c786b46b9bf3bcf8eaedbe7155a28", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/3a722ffafc1c786b46b9bf3bcf8eaedbe7155a28", "committedDate": "2020-07-16T13:47:02Z", "message": "Remove extra import in BlockManagerImpl that was accidentally added"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3496c11d7d08eb530ae53906d988a6295c209a0f", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/3496c11d7d08eb530ae53906d988a6295c209a0f", "committedDate": "2020-07-16T13:47:02Z", "message": "Re add schema version parameter to in BlockUtils after it was removed by the merge"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "584fb9a261f3fcc40d9f1e96b24f678e9b36127c", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/584fb9a261f3fcc40d9f1e96b24f678e9b36127c", "committedDate": "2020-07-16T13:47:02Z", "message": "Remove extra import that was already added"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf08fce3fee578eae7d08ebca53238ac562da370", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/cf08fce3fee578eae7d08ebca53238ac562da370", "committedDate": "2020-07-16T13:47:02Z", "message": "Fix bug where DB name was used instead of column family name"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a0d6182e16c0d87c0f523f6c8ac19db32a20929d", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/a0d6182e16c0d87c0f523f6c8ac19db32a20929d", "committedDate": "2020-07-16T13:47:02Z", "message": "Fix bugs with missing column families and mismatched ContainerCache params"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "65213cbe29adcc11d9fd2214b59a55f4f8287152", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/65213cbe29adcc11d9fd2214b59a55f4f8287152", "committedDate": "2020-07-16T13:47:02Z", "message": "Fix bug to allow null key to be used as start key in key range query"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "091f013acd42d2893a1dbff48e92197456fa96fb", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/091f013acd42d2893a1dbff48e92197456fa96fb", "committedDate": "2020-07-16T13:47:02Z", "message": "Make DeleteBlocksCommandHandler.deleteKeyValueContainerBlocks() use block data table instead of metadata table"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0df4c4f65ee8b396d115428c26ba3ea384881cd1", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/0df4c4f65ee8b396d115428c26ba3ea384881cd1", "committedDate": "2020-07-16T13:47:02Z", "message": "Fix null start key error that was missed in the original fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ba80ef481891423014252c49f43dccd5cf45699e", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/ba80ef481891423014252c49f43dccd5cf45699e", "committedDate": "2020-07-16T13:47:03Z", "message": "Remove early batch operation commit in BlockDeletingService"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8c4f37f4175982172ac5b2caba67250835cdfb3d", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/8c4f37f4175982172ac5b2caba67250835cdfb3d", "committedDate": "2020-07-16T13:47:03Z", "message": "Fix bug where .db file was created in wrong directory\n\nThe added method DBDefinition.getDBLocation() is now defined to return the parent directory of the .db file."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3be476a01a4e7190eaae7c34555a4190c6049933", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/3be476a01a4e7190eaae7c34555a4190c6049933", "committedDate": "2020-07-16T13:47:03Z", "message": "Fix bug where default column family was being registered twice"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "67ee0958b54c29c2a8b67bc1d530dea08516f1be", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/67ee0958b54c29c2a8b67bc1d530dea08516f1be", "committedDate": "2020-07-16T13:47:03Z", "message": "Make testContainerImportExport write to block and metadata tables where needed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "232347fb746b7ebe1db0b7964977f072cd2f77d4", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/232347fb746b7ebe1db0b7964977f072cd2f77d4", "committedDate": "2020-07-16T13:47:03Z", "message": "Minor readability change"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8feede3f78ddf6788d9a2b2c2b5847e6ff51f879", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/8feede3f78ddf6788d9a2b2c2b5847e6ff51f879", "committedDate": "2020-07-16T16:00:23Z", "message": "Switch KeyValueBlockIterator to new interface after patch using old interface was integrated"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9faca0270c92c2cdc7b7c530dcab609d99dbe36b", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/9faca0270c92c2cdc7b7c530dcab609d99dbe36b", "committedDate": "2020-07-16T16:04:34Z", "message": "Remove missed git artifacts after conflict"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "da05a25a2eb753514d7258743847bc13f0d716ef", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/da05a25a2eb753514d7258743847bc13f0d716ef", "committedDate": "2020-07-16T16:05:37Z", "message": "Switch merged in unit test to new interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ed668971db69514a42ad4e38f3bfb3b1fb8f8ac9", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/ed668971db69514a42ad4e38f3bfb3b1fb8f8ac9", "committedDate": "2020-07-16T16:12:01Z", "message": "Update merged unit test to use new interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c1b6e056c0591ad269882e2806c46a7d8012000d", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/c1b6e056c0591ad269882e2806c46a7d8012000d", "committedDate": "2020-07-16T16:22:15Z", "message": "Add schema version param to ContainerCache.getDB() after it was accidentally removed in merge"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "786b49c2bacaf243a3560ac6fdef0273fec016b0", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/786b49c2bacaf243a3560ac6fdef0273fec016b0", "committedDate": "2020-07-16T18:08:07Z", "message": "Remove duplicate method call introduced accidentally when resolving merge conflict"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "90dd3f0137b15e11a3ddc1b02dfd45994f84b0db", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/90dd3f0137b15e11a3ddc1b02dfd45994f84b0db", "committedDate": "2020-07-16T20:25:42Z", "message": "New iterator implementation passes existing unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3b8cdbb69ffdc18c026bae2c2b117d1504030fa0", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/3b8cdbb69ffdc18c026bae2c2b117d1504030fa0", "committedDate": "2020-07-17T13:40:36Z", "message": "Merge branch 'HDDS-3869' of https://github.com/errose28/hadoop-ozone into HDDS-3869"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dc13c5a9b7c21810525505345d3cfdbb41f9ef7a", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/dc13c5a9b7c21810525505345d3cfdbb41f9ef7a", "committedDate": "2020-07-17T14:10:52Z", "message": "All unit tests pass after allowing keys with deleted and deleting prefixes to be made"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a89787835150b2eb39f95fee660245f86f357a39", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/a89787835150b2eb39f95fee660245f86f357a39", "committedDate": "2020-07-17T16:21:05Z", "message": "Add more robust unit test with alternating key matches"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b5ca1574ca1d04de43bf8963d6a2ce210fbc8210", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/b5ca1574ca1d04de43bf8963d6a2ce210fbc8210", "committedDate": "2020-07-17T16:22:40Z", "message": "Remove seekToLast() from iterator interface, implementation, and tests\n\nIt will require additional work to implement correctly and was only being called by tests anyways."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "731e96c1c67545d6a729b726215e3057d8c4837b", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/731e96c1c67545d6a729b726215e3057d8c4837b", "committedDate": "2020-07-17T16:30:23Z", "message": "Add comments on added helper method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "378f27ca3f82a0837abd3d8dba217874eeec7e60", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/378f27ca3f82a0837abd3d8dba217874eeec7e60", "committedDate": "2020-07-17T16:31:51Z", "message": "Update comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3ef5841be91f4d21bbb8d2700fef604a3ec4d186", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/3ef5841be91f4d21bbb8d2700fef604a3ec4d186", "committedDate": "2020-07-17T16:40:14Z", "message": "Merge branch 'master' into HDDS-3976\n\n* master:\n  HDDS-3855. Add upgrade smoketest (#1142)\n  HDDS-3964. Ratis config key mismatch (#1204)\n  HDDS-3612. Allow mounting bucket under other volume (#1104)\n  HDDS-3926. OM Token Identifier table should use in-house serialization. (#1182)\n  HDDS-3824: OM read requests should make SCM#refreshPipeline outside BUCKET_LOCK (#1164)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "28dbbfe47989cd519dc8dba04d9cc05c681c20be", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/28dbbfe47989cd519dc8dba04d9cc05c681c20be", "committedDate": "2020-07-17T18:54:44Z", "message": "Fix checkstyle violations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "548da86696d9e3fbdcb60f1e51f96b6e107c78e6", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/548da86696d9e3fbdcb60f1e51f96b6e107c78e6", "committedDate": "2020-07-17T19:25:40Z", "message": "Merge remote-tracking branch 'upstream/master' into switch-to-new-interface\n\n* upstream/master:\n  HDDS-3855. Add upgrade smoketest (#1142)\n  HDDS-3964. Ratis config key mismatch (#1204)\n  HDDS-3612. Allow mounting bucket under other volume (#1104)\n  HDDS-3926. OM Token Identifier table should use in-house serialization. (#1182)\n  HDDS-3824: OM read requests should make SCM#refreshPipeline outside BUCKET_LOCK (#1164)\n  HDDS-3966. Disable flaky TestOMRatisSnapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1bb61649c8f4e40bb18e69f205f5d8f51fa9d835", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/1bb61649c8f4e40bb18e69f205f5d8f51fa9d835", "committedDate": "2020-07-17T19:28:23Z", "message": "Merge branch 'HDDS-3976' into switch-to-new-interface\n\n* HDDS-3976:\n  Fix checkstyle violations\n  Update comments\n  Add comments on added helper method\n  Remove seekToLast() from iterator interface, implementation, and tests\n  Add more robust unit test with alternating key matches\n  All unit tests pass after allowing keys with deleted and deleting prefixes to be made\n  New iterator implementation passes existing unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3d771f7aacbc14ca60ffa024f91faa6c74f94622", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/3d771f7aacbc14ca60ffa024f91faa6c74f94622", "committedDate": "2020-07-17T19:34:25Z", "message": "Move new key value block iterator implementation and tests to new interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "af8b4d43d6f2d0e64599bf909b724e3052bb2a75", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/af8b4d43d6f2d0e64599bf909b724e3052bb2a75", "committedDate": "2020-07-20T13:19:43Z", "message": "Merge branch 'master' into HDDS-3869\n\n* master:\n  HDDS-3984. Support filter and search the columns in recon UI (#1218)\n  HDDS-3806. Support recognize aws v2 Authorization header. (#1098)\n  HDDS-3955. Unable to list intermediate paths on keys created using S3G. (#1196)\n  HDDS-3741. Reload old OM state if Install Snapshot from Leader fails (#1129)\n  HDDS-3965. SCM failed to start up for duplicated pipeline detected. (#1210)\n  HDDS-3855. Add upgrade smoketest (#1142)\n  HDDS-3964. Ratis config key mismatch (#1204)\n  HDDS-3612. Allow mounting bucket under other volume (#1104)\n  HDDS-3926. OM Token Identifier table should use in-house serialization. (#1182)\n  HDDS-3824: OM read requests should make SCM#refreshPipeline outside BUCKET_LOCK (#1164)\n  HDDS-3966. Disable flaky TestOMRatisSnapshots"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf6e0c472d932c3e39cf0082b7b6eb52b8ffc5b6", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/cf6e0c472d932c3e39cf0082b7b6eb52b8ffc5b6", "committedDate": "2020-07-20T13:19:58Z", "message": "Merge branch 'HDDS-3869' into switch-to-new-interface\n\n* HDDS-3869:\n  HDDS-3984. Support filter and search the columns in recon UI (#1218)\n  HDDS-3806. Support recognize aws v2 Authorization header. (#1098)\n  HDDS-3955. Unable to list intermediate paths on keys created using S3G. (#1196)\n  HDDS-3741. Reload old OM state if Install Snapshot from Leader fails (#1129)\n  HDDS-3965. SCM failed to start up for duplicated pipeline detected. (#1210)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3d208ea220790e170a313e9f7cbeb5e8a8dd9f99", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/3d208ea220790e170a313e9f7cbeb5e8a8dd9f99", "committedDate": "2020-07-20T15:43:59Z", "message": "Import schema version when importing container data from export"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3b02529ed09fac25151d9adbe4b6ef79469a27c2", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/3b02529ed09fac25151d9adbe4b6ef79469a27c2", "committedDate": "2020-07-20T18:39:22Z", "message": "Move block delete to correct table and remove debugging print statement"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "257ee9ee290c28fc6a9cf1fa915fcfa3630ef6c4", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/257ee9ee290c28fc6a9cf1fa915fcfa3630ef6c4", "committedDate": "2020-07-20T19:22:51Z", "message": "Have block deleting service test look for #deleted# keys in metadata table"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "18f6a18d5ffd33fd296f028c4e701272cadf402b", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/18f6a18d5ffd33fd296f028c4e701272cadf402b", "committedDate": "2020-07-20T19:52:01Z", "message": "Merge branch 'master' into HDDS-3976\n\n* master:\n  HDDS-3984. Support filter and search the columns in recon UI (#1218)\n  HDDS-3806. Support recognize aws v2 Authorization header. (#1098)\n  HDDS-3955. Unable to list intermediate paths on keys created using S3G. (#1196)\n  HDDS-3741. Reload old OM state if Install Snapshot from Leader fails (#1129)\n  HDDS-3965. SCM failed to start up for duplicated pipeline detected. (#1210)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "19857237dafffa86af9fd6a601bf2993b1cf7b0a", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/19857237dafffa86af9fd6a601bf2993b1cf7b0a", "committedDate": "2020-07-20T20:48:36Z", "message": "Merge branch 'switch-to-new-interface' into HDDS-3869\n\n* switch-to-new-interface: (67 commits)\n  Have block deleting service test look for #deleted# keys in metadata table\n  Move block delete to correct table and remove debugging print statement\n  Import schema version when importing container data from export\n  Move new key value block iterator implementation and tests to new interface\n  Remove duplicate method call introduced accidentally when resolving merge conflict\n  Add schema version param to ContainerCache.getDB() after it was accidentally removed in merge\n  Update merged unit test to use new interface\n  Switch merged in unit test to new interface\n  Remove missed git artifacts after conflict\n  Switch KeyValueBlockIterator to new interface after patch using old interface was integrated\n  Minor readability change\n  Make testContainerImportExport write to block and metadata tables where needed\n  Fix bug where default column family was being registered twice\n  Fix bug where .db file was created in wrong directory\n  Remove early batch operation commit in BlockDeletingService\n  Fix null start key error that was missed in the original fix\n  Make DeleteBlocksCommandHandler.deleteKeyValueContainerBlocks() use block data table instead of metadata table\n  Fix bug to allow null key to be used as start key in key range query\n  Fix bugs with missing column families and mismatched ContainerCache params\n  Fix bug where DB name was used instead of column family name\n  ..."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "24d4d788d725d7248c6659ca374dc8326223042e", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/24d4d788d725d7248c6659ca374dc8326223042e", "committedDate": "2020-07-20T21:09:12Z", "message": "Add deleted blocks table to base level DB wrappers"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "444f75f9abe2e520610e9e626cf2d915db87ccf0", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/444f75f9abe2e520610e9e626cf2d915db87ccf0", "committedDate": "2020-07-21T13:32:12Z", "message": "Replace uses of #deleted# key prefix with access to new deleted blocks table\n\nStill need to do the more involved fix of carrying this out for\nTestKeyValueBlockIterator and MetadataKeyFilters."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ef4272493c47271b4b28e2bf73aae9a62a1971ae", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/ef4272493c47271b4b28e2bf73aae9a62a1971ae", "committedDate": "2020-07-21T13:35:07Z", "message": "New iterator implementation passes existing unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5e7e9df47d6def80e971b910b2bc676e17c62b09", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/5e7e9df47d6def80e971b910b2bc676e17c62b09", "committedDate": "2020-07-21T13:35:07Z", "message": "All unit tests pass after allowing keys with deleted and deleting prefixes to be made"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "54e83a6bb302a49821300bb6b1545521542b05a3", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/54e83a6bb302a49821300bb6b1545521542b05a3", "committedDate": "2020-07-21T13:35:07Z", "message": "Add more robust unit test with alternating key matches"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f19e77f7e896a047c77cddc90c6437944fde8fd8", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/f19e77f7e896a047c77cddc90c6437944fde8fd8", "committedDate": "2020-07-21T13:35:07Z", "message": "Remove seekToLast() from iterator interface, implementation, and tests\n\nIt will require additional work to implement correctly and was only being called by tests anyways."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6916e4ad3bfef17a3c0a083f112d3723e0613c14", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/6916e4ad3bfef17a3c0a083f112d3723e0613c14", "committedDate": "2020-07-21T13:35:07Z", "message": "Add comments on added helper method"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "27ec1fbc53975e6dfc62a066cb0e1f282d374614", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/27ec1fbc53975e6dfc62a066cb0e1f282d374614", "committedDate": "2020-07-21T13:35:07Z", "message": "Update comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7fe3dc4ab4a81381f3196ff32a6f5c12463ff4df", "author": {"user": {"login": "rakeshadr", "name": "Rakesh Radhakrishnan"}}, "url": "https://github.com/apache/ozone/commit/7fe3dc4ab4a81381f3196ff32a6f5c12463ff4df", "committedDate": "2020-07-21T13:35:07Z", "message": "HDDS-3824: OM read requests should make SCM#refreshPipeline outside BUCKET_LOCK (#1164)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8a9e8331302c1811e8c1f06c1f22f23b4d8bc3fa", "author": {"user": {"login": "prashantpogde", "name": null}}, "url": "https://github.com/apache/ozone/commit/8a9e8331302c1811e8c1f06c1f22f23b4d8bc3fa", "committedDate": "2020-07-21T13:35:07Z", "message": "HDDS-3926. OM Token Identifier table should use in-house serialization. (#1182)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "68493482bc84890096a4dd1e2e23dff83d9ad99c", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/68493482bc84890096a4dd1e2e23dff83d9ad99c", "committedDate": "2020-07-21T13:35:07Z", "message": "HDDS-3612. Allow mounting bucket under other volume (#1104)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "072da921ab23c5377fd4eb20b2f01de6da4a116b", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/072da921ab23c5377fd4eb20b2f01de6da4a116b", "committedDate": "2020-07-21T13:35:07Z", "message": "HDDS-3964. Ratis config key mismatch (#1204)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "223d59c503ab9c326e43fdb909bbe4a3f6fa25a1", "author": {"user": {"login": "adoroszlai", "name": "Doroszlai, Attila"}}, "url": "https://github.com/apache/ozone/commit/223d59c503ab9c326e43fdb909bbe4a3f6fa25a1", "committedDate": "2020-07-21T13:35:07Z", "message": "HDDS-3855. Add upgrade smoketest (#1142)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a231ff82c0be2d1ac6da3d1f1b01d5cdc314f453", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/a231ff82c0be2d1ac6da3d1f1b01d5cdc314f453", "committedDate": "2020-07-21T13:35:07Z", "message": "Fix checkstyle violations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8878e9da634551542588517b893d55b5ff3256ed", "author": {"user": {"login": "avijayanhwx", "name": null}}, "url": "https://github.com/apache/ozone/commit/8878e9da634551542588517b893d55b5ff3256ed", "committedDate": "2020-07-21T13:35:07Z", "message": "HDDS-3965. SCM failed to start up for duplicated pipeline detected. (#1210)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3f7b4eb51679a567c41dc7e69ac978de9d463cf3", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/3f7b4eb51679a567c41dc7e69ac978de9d463cf3", "committedDate": "2020-07-21T13:35:07Z", "message": "HDDS-3741. Reload old OM state if Install Snapshot from Leader fails (#1129)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7acfb125b63273776c056fac62362496216a9d44", "author": {"user": {"login": "bharatviswa504", "name": "Bharat Viswanadham"}}, "url": "https://github.com/apache/ozone/commit/7acfb125b63273776c056fac62362496216a9d44", "committedDate": "2020-07-21T13:35:08Z", "message": "HDDS-3955. Unable to list intermediate paths on keys created using S3G. (#1196)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "81cdb949fa89f17d4ea3e13bdbf1bd67b010b4a3", "author": {"user": {"login": "ChenSammi", "name": "Sammi Chen"}}, "url": "https://github.com/apache/ozone/commit/81cdb949fa89f17d4ea3e13bdbf1bd67b010b4a3", "committedDate": "2020-07-21T13:35:08Z", "message": "HDDS-3806. Support recognize aws v2 Authorization header. (#1098)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8517e380a1c759e7f0d941db8a0a1c1efa42fcdb", "author": {"user": {"login": "runitao", "name": "HuangTao"}}, "url": "https://github.com/apache/ozone/commit/8517e380a1c759e7f0d941db8a0a1c1efa42fcdb", "committedDate": "2020-07-21T13:35:08Z", "message": "HDDS-3984. Support filter and search the columns in recon UI (#1218)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fc2b8774f070f824a4c88d366241c5ba55076393", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/fc2b8774f070f824a4c88d366241c5ba55076393", "committedDate": "2020-07-21T13:35:08Z", "message": "Add schemaVersion field to yaml key value container data"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3f2dc9510fd01a3882ef581177ab364354293dd8", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/3f2dc9510fd01a3882ef581177ab364354293dd8", "committedDate": "2020-07-21T13:35:08Z", "message": "Add schemaVersion yaml field as string to .container options"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "85711f94cd4e3520ff0676958c0a7eb387e5b4d8", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/85711f94cd4e3520ff0676958c0a7eb387e5b4d8", "committedDate": "2020-07-21T13:35:08Z", "message": "Add flush and compact methods to datanodestore"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "43d315b84466d72dd961febc67f75d349308cc77", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/43d315b84466d72dd961febc67f75d349308cc77", "committedDate": "2020-07-21T13:35:08Z", "message": "Replace old puts, gets, deletes and batch operations with the new versions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bcfe27f3d929a785dbe63475489d2de171b67ab0", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/bcfe27f3d929a785dbe63475489d2de171b67ab0", "committedDate": "2020-07-21T13:35:08Z", "message": "Begin changing interface usage in nonKeyValue or Test classes\n\nSome changes are pending furhter investigation and are not yet completed."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6cdba76b69ad0026db640f23814e442f2286e961", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/6cdba76b69ad0026db640f23814e442f2286e961", "committedDate": "2020-07-21T13:35:08Z", "message": "Switch key type of block data table from string to long"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8bfffc8cd972191b7c94ea444a983394b702a2ae", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/8bfffc8cd972191b7c94ea444a983394b702a2ae", "committedDate": "2020-07-21T13:35:08Z", "message": "Update gets, puts and batch operations in KeyValue classes to use new interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c5b3f8b0c1dea86de1eb819ff486dab052057a81", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/c5b3f8b0c1dea86de1eb819ff486dab052057a81", "committedDate": "2020-07-21T13:36:08Z", "message": "Update get, put, delete, and write batch calls to use new interface in unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8569557ec289bd908aad187c0ebe5e4ef86e2db7", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/8569557ec289bd908aad187c0ebe5e4ef86e2db7", "committedDate": "2020-07-21T13:36:12Z", "message": "Add getRangeKVs and getSequentialRangeKVs methods to Table interface\n\nAdd implementations based on the original implementations in MetadataStore,\nexcept that these new versions will only operate on one table."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "30b15e7a065b4599ad3c229b04e7427393bc9378", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/30b15e7a065b4599ad3c229b04e7427393bc9378", "committedDate": "2020-07-21T13:36:12Z", "message": "Fix typo in method call to new interface"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9175db605d188d78e0333fc21c3cbf3779607754", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/9175db605d188d78e0333fc21c3cbf3779607754", "committedDate": "2020-07-21T13:36:12Z", "message": "Add separate flushLog() and flushDB() methods to new interface to preserve old functionality"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2666, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}