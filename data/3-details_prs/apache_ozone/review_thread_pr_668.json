{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg3MDgyNDMy", "number": 668, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxMTo0Mjo1MlrODp1i_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxMTo0OTo1N1rODxm7xg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1MTk1NTE3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "isResolved": true, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxMTo0Mjo1MlrOF5QfEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQwNzo0OTo1NFrOF-JWUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU4MzI0OA==", "bodyText": "Could this be simplified to just nodes.get(0)? In the else branch we already know the list is not empty, so I think we can just pick the first node in the list safely.", "url": "https://github.com/apache/ozone/pull/668#discussion_r395583248", "createdAt": "2020-03-20T11:42:52Z", "author": {"login": "sodonnel"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -315,6 +314,50 @@ DatanodeDetails fallBackPickNodes(\n     return results;\n   }\n \n+  private DatanodeDetails randomPick(List<DatanodeDetails> healthyNodes) {\n+    DatanodeDetails datanodeDetails;\n+    int firstNodeNdx = getRand().nextInt(healthyNodes.size());\n+    int secondNodeNdx = getRand().nextInt(healthyNodes.size());\n+\n+    // There is a possibility that both numbers will be same.\n+    // if that is so, we just return the node.\n+    if (firstNodeNdx == secondNodeNdx) {\n+      datanodeDetails = healthyNodes.get(firstNodeNdx);\n+    } else {\n+      DatanodeDetails firstNodeDetails = healthyNodes.get(firstNodeNdx);\n+      DatanodeDetails secondNodeDetails = healthyNodes.get(secondNodeNdx);\n+      datanodeDetails = nodeManager.getPipelinesCount(firstNodeDetails)\n+          >= nodeManager.getPipelinesCount(secondNodeDetails)\n+          ? secondNodeDetails : firstNodeDetails;\n+    }\n+    return datanodeDetails;\n+  }\n+\n+  private List<DatanodeDetails> getLowerLoadNodes(\n+      List<DatanodeDetails> nodes, int num) {\n+    int maxPipelineUsage = nodes.size() * heavyNodeCriteria /\n+        HddsProtos.ReplicationFactor.THREE.getNumber();\n+    return nodes.stream()\n+        // Skip the nodes which exceeds the load limit.\n+        .filter(p -> nodeManager.getPipelinesCount(p) < num - maxPipelineUsage)\n+        .collect(Collectors.toList());\n+  }\n+\n+  private DatanodeDetails lowerLoadPick(List<DatanodeDetails> healthyNodes) {\n+    int curPipelineCounts =  stateManager\n+        .getPipelines(HddsProtos.ReplicationType.RATIS).size();\n+    DatanodeDetails datanodeDetails;\n+    List<DatanodeDetails> nodes = getLowerLoadNodes(\n+        healthyNodes, curPipelineCounts);\n+    if (nodes.isEmpty()) {\n+      // random pick node if nodes load is at same level.\n+      datanodeDetails = randomPick(healthyNodes);\n+    } else {\n+      datanodeDetails = nodes.stream().findFirst().get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e8506dc31a14b2938a5bbdb8dcabd62a1f9546e4"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTYxMDAyNw==", "bodyText": "Actually, thinking about this more - should this pick a random node from LowerLoadNodes, rather than the first node? We call getLowerLoadNodes which returns a list of nodes where the overloaded nodes are removed, but I don't think the list is sorted in anyway. The healthyNodes will will probably be naturally in the same order each time it is generated from the NodeManager. This means this lowerLoadPick method might return the same node on each call until it is overloaded. Then it would be excluded and the next node would be picked and so on. It would probably be better if we picked a random node from the less loaded nodes. Or, sort the list by load and return the first one so we are always picked the node with the least node.", "url": "https://github.com/apache/ozone/pull/668#discussion_r395610027", "createdAt": "2020-03-20T12:41:41Z", "author": {"login": "sodonnel"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -315,6 +314,50 @@ DatanodeDetails fallBackPickNodes(\n     return results;\n   }\n \n+  private DatanodeDetails randomPick(List<DatanodeDetails> healthyNodes) {\n+    DatanodeDetails datanodeDetails;\n+    int firstNodeNdx = getRand().nextInt(healthyNodes.size());\n+    int secondNodeNdx = getRand().nextInt(healthyNodes.size());\n+\n+    // There is a possibility that both numbers will be same.\n+    // if that is so, we just return the node.\n+    if (firstNodeNdx == secondNodeNdx) {\n+      datanodeDetails = healthyNodes.get(firstNodeNdx);\n+    } else {\n+      DatanodeDetails firstNodeDetails = healthyNodes.get(firstNodeNdx);\n+      DatanodeDetails secondNodeDetails = healthyNodes.get(secondNodeNdx);\n+      datanodeDetails = nodeManager.getPipelinesCount(firstNodeDetails)\n+          >= nodeManager.getPipelinesCount(secondNodeDetails)\n+          ? secondNodeDetails : firstNodeDetails;\n+    }\n+    return datanodeDetails;\n+  }\n+\n+  private List<DatanodeDetails> getLowerLoadNodes(\n+      List<DatanodeDetails> nodes, int num) {\n+    int maxPipelineUsage = nodes.size() * heavyNodeCriteria /\n+        HddsProtos.ReplicationFactor.THREE.getNumber();\n+    return nodes.stream()\n+        // Skip the nodes which exceeds the load limit.\n+        .filter(p -> nodeManager.getPipelinesCount(p) < num - maxPipelineUsage)\n+        .collect(Collectors.toList());\n+  }\n+\n+  private DatanodeDetails lowerLoadPick(List<DatanodeDetails> healthyNodes) {\n+    int curPipelineCounts =  stateManager\n+        .getPipelines(HddsProtos.ReplicationType.RATIS).size();\n+    DatanodeDetails datanodeDetails;\n+    List<DatanodeDetails> nodes = getLowerLoadNodes(\n+        healthyNodes, curPipelineCounts);\n+    if (nodes.isEmpty()) {\n+      // random pick node if nodes load is at same level.\n+      datanodeDetails = randomPick(healthyNodes);\n+    } else {\n+      datanodeDetails = nodes.stream().findFirst().get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU4MzI0OA=="}, "originalCommit": {"oid": "e8506dc31a14b2938a5bbdb8dcabd62a1f9546e4"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTYxNTUxMg==", "bodyText": "When i fixed the suspected bug I mentioned above, and then ran the test. The nodes do appear to fill their piplines on a node by node basis and then the test failed as each node did not have at least the average pipelines.\nMaking this change got it to pass again:\ndatanodeDetails = nodes.get(getRand().nextInt(nodes.size()) );//stream().findFirst().get();\n\nBut it might be ever better if we sorted the list by pipeline count ascending and then took the first one, but it would be more expensive.", "url": "https://github.com/apache/ozone/pull/668#discussion_r395615512", "createdAt": "2020-03-20T12:52:47Z", "author": {"login": "sodonnel"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -315,6 +314,50 @@ DatanodeDetails fallBackPickNodes(\n     return results;\n   }\n \n+  private DatanodeDetails randomPick(List<DatanodeDetails> healthyNodes) {\n+    DatanodeDetails datanodeDetails;\n+    int firstNodeNdx = getRand().nextInt(healthyNodes.size());\n+    int secondNodeNdx = getRand().nextInt(healthyNodes.size());\n+\n+    // There is a possibility that both numbers will be same.\n+    // if that is so, we just return the node.\n+    if (firstNodeNdx == secondNodeNdx) {\n+      datanodeDetails = healthyNodes.get(firstNodeNdx);\n+    } else {\n+      DatanodeDetails firstNodeDetails = healthyNodes.get(firstNodeNdx);\n+      DatanodeDetails secondNodeDetails = healthyNodes.get(secondNodeNdx);\n+      datanodeDetails = nodeManager.getPipelinesCount(firstNodeDetails)\n+          >= nodeManager.getPipelinesCount(secondNodeDetails)\n+          ? secondNodeDetails : firstNodeDetails;\n+    }\n+    return datanodeDetails;\n+  }\n+\n+  private List<DatanodeDetails> getLowerLoadNodes(\n+      List<DatanodeDetails> nodes, int num) {\n+    int maxPipelineUsage = nodes.size() * heavyNodeCriteria /\n+        HddsProtos.ReplicationFactor.THREE.getNumber();\n+    return nodes.stream()\n+        // Skip the nodes which exceeds the load limit.\n+        .filter(p -> nodeManager.getPipelinesCount(p) < num - maxPipelineUsage)\n+        .collect(Collectors.toList());\n+  }\n+\n+  private DatanodeDetails lowerLoadPick(List<DatanodeDetails> healthyNodes) {\n+    int curPipelineCounts =  stateManager\n+        .getPipelines(HddsProtos.ReplicationType.RATIS).size();\n+    DatanodeDetails datanodeDetails;\n+    List<DatanodeDetails> nodes = getLowerLoadNodes(\n+        healthyNodes, curPipelineCounts);\n+    if (nodes.isEmpty()) {\n+      // random pick node if nodes load is at same level.\n+      datanodeDetails = randomPick(healthyNodes);\n+    } else {\n+      datanodeDetails = nodes.stream().findFirst().get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU4MzI0OA=="}, "originalCommit": {"oid": "e8506dc31a14b2938a5bbdb8dcabd62a1f9546e4"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjIyOTk3MA==", "bodyText": "Sorting the node list would be expensive for large cluster. That's the reason why I choose to do this 'water mark' filter for selecting nodes with lower load.\nI can def do a findAny() kinda thing for random pick. @sodonnel", "url": "https://github.com/apache/ozone/pull/668#discussion_r396229970", "createdAt": "2020-03-23T06:12:46Z", "author": {"login": "timmylicheng"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -315,6 +314,50 @@ DatanodeDetails fallBackPickNodes(\n     return results;\n   }\n \n+  private DatanodeDetails randomPick(List<DatanodeDetails> healthyNodes) {\n+    DatanodeDetails datanodeDetails;\n+    int firstNodeNdx = getRand().nextInt(healthyNodes.size());\n+    int secondNodeNdx = getRand().nextInt(healthyNodes.size());\n+\n+    // There is a possibility that both numbers will be same.\n+    // if that is so, we just return the node.\n+    if (firstNodeNdx == secondNodeNdx) {\n+      datanodeDetails = healthyNodes.get(firstNodeNdx);\n+    } else {\n+      DatanodeDetails firstNodeDetails = healthyNodes.get(firstNodeNdx);\n+      DatanodeDetails secondNodeDetails = healthyNodes.get(secondNodeNdx);\n+      datanodeDetails = nodeManager.getPipelinesCount(firstNodeDetails)\n+          >= nodeManager.getPipelinesCount(secondNodeDetails)\n+          ? secondNodeDetails : firstNodeDetails;\n+    }\n+    return datanodeDetails;\n+  }\n+\n+  private List<DatanodeDetails> getLowerLoadNodes(\n+      List<DatanodeDetails> nodes, int num) {\n+    int maxPipelineUsage = nodes.size() * heavyNodeCriteria /\n+        HddsProtos.ReplicationFactor.THREE.getNumber();\n+    return nodes.stream()\n+        // Skip the nodes which exceeds the load limit.\n+        .filter(p -> nodeManager.getPipelinesCount(p) < num - maxPipelineUsage)\n+        .collect(Collectors.toList());\n+  }\n+\n+  private DatanodeDetails lowerLoadPick(List<DatanodeDetails> healthyNodes) {\n+    int curPipelineCounts =  stateManager\n+        .getPipelines(HddsProtos.ReplicationType.RATIS).size();\n+    DatanodeDetails datanodeDetails;\n+    List<DatanodeDetails> nodes = getLowerLoadNodes(\n+        healthyNodes, curPipelineCounts);\n+    if (nodes.isEmpty()) {\n+      // random pick node if nodes load is at same level.\n+      datanodeDetails = randomPick(healthyNodes);\n+    } else {\n+      datanodeDetails = nodes.stream().findFirst().get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU4MzI0OA=="}, "originalCommit": {"oid": "e8506dc31a14b2938a5bbdb8dcabd62a1f9546e4"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc3NDMxNw==", "bodyText": "You could probably do a \"top N\" type of sort where you only keep the lowest loaded node, but it would require a bit of a refactor of getLowerLoadNodes, probably changing it to getLowestLodeNode. That would avoid a full sort and would not be much more expensive that the current code, eg:\nDatanodeDetails lowest = null;\nint lowestPipelineCount;\nfor (DatanodeDetails dn : nodes) {\n  int nodePipelines = node.getPipelineCount();\n  if (nodePipelines > limit) {\n    continue;\n  }\n  if ((lowest == null) || lowestPipelineCount > nodePipelines {\n    lowest = node;\n    lowestPipelineCount = nodePipelines;\n  }\n}\nreturn lowest;", "url": "https://github.com/apache/ozone/pull/668#discussion_r396774317", "createdAt": "2020-03-23T21:42:33Z", "author": {"login": "sodonnel"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -315,6 +314,50 @@ DatanodeDetails fallBackPickNodes(\n     return results;\n   }\n \n+  private DatanodeDetails randomPick(List<DatanodeDetails> healthyNodes) {\n+    DatanodeDetails datanodeDetails;\n+    int firstNodeNdx = getRand().nextInt(healthyNodes.size());\n+    int secondNodeNdx = getRand().nextInt(healthyNodes.size());\n+\n+    // There is a possibility that both numbers will be same.\n+    // if that is so, we just return the node.\n+    if (firstNodeNdx == secondNodeNdx) {\n+      datanodeDetails = healthyNodes.get(firstNodeNdx);\n+    } else {\n+      DatanodeDetails firstNodeDetails = healthyNodes.get(firstNodeNdx);\n+      DatanodeDetails secondNodeDetails = healthyNodes.get(secondNodeNdx);\n+      datanodeDetails = nodeManager.getPipelinesCount(firstNodeDetails)\n+          >= nodeManager.getPipelinesCount(secondNodeDetails)\n+          ? secondNodeDetails : firstNodeDetails;\n+    }\n+    return datanodeDetails;\n+  }\n+\n+  private List<DatanodeDetails> getLowerLoadNodes(\n+      List<DatanodeDetails> nodes, int num) {\n+    int maxPipelineUsage = nodes.size() * heavyNodeCriteria /\n+        HddsProtos.ReplicationFactor.THREE.getNumber();\n+    return nodes.stream()\n+        // Skip the nodes which exceeds the load limit.\n+        .filter(p -> nodeManager.getPipelinesCount(p) < num - maxPipelineUsage)\n+        .collect(Collectors.toList());\n+  }\n+\n+  private DatanodeDetails lowerLoadPick(List<DatanodeDetails> healthyNodes) {\n+    int curPipelineCounts =  stateManager\n+        .getPipelines(HddsProtos.ReplicationType.RATIS).size();\n+    DatanodeDetails datanodeDetails;\n+    List<DatanodeDetails> nodes = getLowerLoadNodes(\n+        healthyNodes, curPipelineCounts);\n+    if (nodes.isEmpty()) {\n+      // random pick node if nodes load is at same level.\n+      datanodeDetails = randomPick(healthyNodes);\n+    } else {\n+      datanodeDetails = nodes.stream().findFirst().get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU4MzI0OA=="}, "originalCommit": {"oid": "e8506dc31a14b2938a5bbdb8dcabd62a1f9546e4"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg1Nzg1MQ==", "bodyText": "I feel like the outcome will be similar tho. The current implementation should work just fine.", "url": "https://github.com/apache/ozone/pull/668#discussion_r397857851", "createdAt": "2020-03-25T13:36:30Z", "author": {"login": "timmylicheng"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -315,6 +314,50 @@ DatanodeDetails fallBackPickNodes(\n     return results;\n   }\n \n+  private DatanodeDetails randomPick(List<DatanodeDetails> healthyNodes) {\n+    DatanodeDetails datanodeDetails;\n+    int firstNodeNdx = getRand().nextInt(healthyNodes.size());\n+    int secondNodeNdx = getRand().nextInt(healthyNodes.size());\n+\n+    // There is a possibility that both numbers will be same.\n+    // if that is so, we just return the node.\n+    if (firstNodeNdx == secondNodeNdx) {\n+      datanodeDetails = healthyNodes.get(firstNodeNdx);\n+    } else {\n+      DatanodeDetails firstNodeDetails = healthyNodes.get(firstNodeNdx);\n+      DatanodeDetails secondNodeDetails = healthyNodes.get(secondNodeNdx);\n+      datanodeDetails = nodeManager.getPipelinesCount(firstNodeDetails)\n+          >= nodeManager.getPipelinesCount(secondNodeDetails)\n+          ? secondNodeDetails : firstNodeDetails;\n+    }\n+    return datanodeDetails;\n+  }\n+\n+  private List<DatanodeDetails> getLowerLoadNodes(\n+      List<DatanodeDetails> nodes, int num) {\n+    int maxPipelineUsage = nodes.size() * heavyNodeCriteria /\n+        HddsProtos.ReplicationFactor.THREE.getNumber();\n+    return nodes.stream()\n+        // Skip the nodes which exceeds the load limit.\n+        .filter(p -> nodeManager.getPipelinesCount(p) < num - maxPipelineUsage)\n+        .collect(Collectors.toList());\n+  }\n+\n+  private DatanodeDetails lowerLoadPick(List<DatanodeDetails> healthyNodes) {\n+    int curPipelineCounts =  stateManager\n+        .getPipelines(HddsProtos.ReplicationType.RATIS).size();\n+    DatanodeDetails datanodeDetails;\n+    List<DatanodeDetails> nodes = getLowerLoadNodes(\n+        healthyNodes, curPipelineCounts);\n+    if (nodes.isEmpty()) {\n+      // random pick node if nodes load is at same level.\n+      datanodeDetails = randomPick(healthyNodes);\n+    } else {\n+      datanodeDetails = nodes.stream().findFirst().get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU4MzI0OA=="}, "originalCommit": {"oid": "e8506dc31a14b2938a5bbdb8dcabd62a1f9546e4"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDE1MjY5Ng==", "bodyText": "I am fine with doing a \"random pick\" from the getLowerLoadNodes list or using the idea I had above to get the node with the lowest load each time. I think there are advantages to each of them. I believe we do need to go with one of those ideas, as picking the first one will not work well.\nThe random pick is simple, but it may not spread the load evenly every time.\nPicking the lowest one each time is slightly more complicated, but it does guarantee to always use the lowest load node first and will spread the load evenly for sure. However it is less random - eg if a new node joins the cluster, then it will be used for the next N pipelines until it reaches the same load as some others.", "url": "https://github.com/apache/ozone/pull/668#discussion_r400152696", "createdAt": "2020-03-30T12:30:12Z", "author": {"login": "sodonnel"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -315,6 +314,50 @@ DatanodeDetails fallBackPickNodes(\n     return results;\n   }\n \n+  private DatanodeDetails randomPick(List<DatanodeDetails> healthyNodes) {\n+    DatanodeDetails datanodeDetails;\n+    int firstNodeNdx = getRand().nextInt(healthyNodes.size());\n+    int secondNodeNdx = getRand().nextInt(healthyNodes.size());\n+\n+    // There is a possibility that both numbers will be same.\n+    // if that is so, we just return the node.\n+    if (firstNodeNdx == secondNodeNdx) {\n+      datanodeDetails = healthyNodes.get(firstNodeNdx);\n+    } else {\n+      DatanodeDetails firstNodeDetails = healthyNodes.get(firstNodeNdx);\n+      DatanodeDetails secondNodeDetails = healthyNodes.get(secondNodeNdx);\n+      datanodeDetails = nodeManager.getPipelinesCount(firstNodeDetails)\n+          >= nodeManager.getPipelinesCount(secondNodeDetails)\n+          ? secondNodeDetails : firstNodeDetails;\n+    }\n+    return datanodeDetails;\n+  }\n+\n+  private List<DatanodeDetails> getLowerLoadNodes(\n+      List<DatanodeDetails> nodes, int num) {\n+    int maxPipelineUsage = nodes.size() * heavyNodeCriteria /\n+        HddsProtos.ReplicationFactor.THREE.getNumber();\n+    return nodes.stream()\n+        // Skip the nodes which exceeds the load limit.\n+        .filter(p -> nodeManager.getPipelinesCount(p) < num - maxPipelineUsage)\n+        .collect(Collectors.toList());\n+  }\n+\n+  private DatanodeDetails lowerLoadPick(List<DatanodeDetails> healthyNodes) {\n+    int curPipelineCounts =  stateManager\n+        .getPipelines(HddsProtos.ReplicationType.RATIS).size();\n+    DatanodeDetails datanodeDetails;\n+    List<DatanodeDetails> nodes = getLowerLoadNodes(\n+        healthyNodes, curPipelineCounts);\n+    if (nodes.isEmpty()) {\n+      // random pick node if nodes load is at same level.\n+      datanodeDetails = randomPick(healthyNodes);\n+    } else {\n+      datanodeDetails = nodes.stream().findFirst().get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU4MzI0OA=="}, "originalCommit": {"oid": "e8506dc31a14b2938a5bbdb8dcabd62a1f9546e4"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDcwOTIwMw==", "bodyText": "@sodonnel I'm navigating thru another problem with chooseNodeFromNetworkTopology. Because topology only gives interface like networkTopology.chooseRandom(anchor.getNetworkLocation(), excluded), I have to move all higher load nodes into excluded and allow topology to pick one. So the original water mark cannot guarantee full load balance due to fall back logic I added.\nI could see your proposed sort will help picking the lowEST node. How could it help leverage to have higher load with topology? I'm still experimenting...", "url": "https://github.com/apache/ozone/pull/668#discussion_r400709203", "createdAt": "2020-03-31T07:49:54Z", "author": {"login": "timmylicheng"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -315,6 +314,50 @@ DatanodeDetails fallBackPickNodes(\n     return results;\n   }\n \n+  private DatanodeDetails randomPick(List<DatanodeDetails> healthyNodes) {\n+    DatanodeDetails datanodeDetails;\n+    int firstNodeNdx = getRand().nextInt(healthyNodes.size());\n+    int secondNodeNdx = getRand().nextInt(healthyNodes.size());\n+\n+    // There is a possibility that both numbers will be same.\n+    // if that is so, we just return the node.\n+    if (firstNodeNdx == secondNodeNdx) {\n+      datanodeDetails = healthyNodes.get(firstNodeNdx);\n+    } else {\n+      DatanodeDetails firstNodeDetails = healthyNodes.get(firstNodeNdx);\n+      DatanodeDetails secondNodeDetails = healthyNodes.get(secondNodeNdx);\n+      datanodeDetails = nodeManager.getPipelinesCount(firstNodeDetails)\n+          >= nodeManager.getPipelinesCount(secondNodeDetails)\n+          ? secondNodeDetails : firstNodeDetails;\n+    }\n+    return datanodeDetails;\n+  }\n+\n+  private List<DatanodeDetails> getLowerLoadNodes(\n+      List<DatanodeDetails> nodes, int num) {\n+    int maxPipelineUsage = nodes.size() * heavyNodeCriteria /\n+        HddsProtos.ReplicationFactor.THREE.getNumber();\n+    return nodes.stream()\n+        // Skip the nodes which exceeds the load limit.\n+        .filter(p -> nodeManager.getPipelinesCount(p) < num - maxPipelineUsage)\n+        .collect(Collectors.toList());\n+  }\n+\n+  private DatanodeDetails lowerLoadPick(List<DatanodeDetails> healthyNodes) {\n+    int curPipelineCounts =  stateManager\n+        .getPipelines(HddsProtos.ReplicationType.RATIS).size();\n+    DatanodeDetails datanodeDetails;\n+    List<DatanodeDetails> nodes = getLowerLoadNodes(\n+        healthyNodes, curPipelineCounts);\n+    if (nodes.isEmpty()) {\n+      // random pick node if nodes load is at same level.\n+      datanodeDetails = randomPick(healthyNodes);\n+    } else {\n+      datanodeDetails = nodes.stream().findFirst().get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU4MzI0OA=="}, "originalCommit": {"oid": "e8506dc31a14b2938a5bbdb8dcabd62a1f9546e4"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1MTk2NTYxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxMTo0NzowNVrOF5QllA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QwNjoxMzo0N1rOF53-YA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU4NDkxNg==", "bodyText": "I think a Java doc would be useful for this method to explain how it works. It seems to pick two random nodes and then return the one with the less load - is that correct?", "url": "https://github.com/apache/ozone/pull/668#discussion_r395584916", "createdAt": "2020-03-20T11:47:05Z", "author": {"login": "sodonnel"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -315,6 +314,50 @@ DatanodeDetails fallBackPickNodes(\n     return results;\n   }\n \n+  private DatanodeDetails randomPick(List<DatanodeDetails> healthyNodes) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e8506dc31a14b2938a5bbdb8dcabd62a1f9546e4"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjIzMDI0MA==", "bodyText": "Yea. Your understanding is correct. I will add doc as description.", "url": "https://github.com/apache/ozone/pull/668#discussion_r396230240", "createdAt": "2020-03-23T06:13:47Z", "author": {"login": "timmylicheng"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -315,6 +314,50 @@ DatanodeDetails fallBackPickNodes(\n     return results;\n   }\n \n+  private DatanodeDetails randomPick(List<DatanodeDetails> healthyNodes) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTU4NDkxNg=="}, "originalCommit": {"oid": "e8506dc31a14b2938a5bbdb8dcabd62a1f9546e4"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1MjE0NTkzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxMjo0ODo1NVrOF5SVXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxMzozNjo1MVrOF7bVWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTYxMzUzMg==", "bodyText": "I think there is a bug here. I put some debug in and ran the test you added as part of this change, and this method always returned an empty list.\nmaxPipelineUsage starts at 13, so we have:\n\"0 < 3 - 13\" -> \"0 < -10 \" -> false and all the nodes are filtered out.\n\nShould this be:\n .filter(p -> nodeManager.getPipelinesCount(p) < maxPipelineUsage - num)", "url": "https://github.com/apache/ozone/pull/668#discussion_r395613532", "createdAt": "2020-03-20T12:48:55Z", "author": {"login": "sodonnel"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -315,6 +314,50 @@ DatanodeDetails fallBackPickNodes(\n     return results;\n   }\n \n+  private DatanodeDetails randomPick(List<DatanodeDetails> healthyNodes) {\n+    DatanodeDetails datanodeDetails;\n+    int firstNodeNdx = getRand().nextInt(healthyNodes.size());\n+    int secondNodeNdx = getRand().nextInt(healthyNodes.size());\n+\n+    // There is a possibility that both numbers will be same.\n+    // if that is so, we just return the node.\n+    if (firstNodeNdx == secondNodeNdx) {\n+      datanodeDetails = healthyNodes.get(firstNodeNdx);\n+    } else {\n+      DatanodeDetails firstNodeDetails = healthyNodes.get(firstNodeNdx);\n+      DatanodeDetails secondNodeDetails = healthyNodes.get(secondNodeNdx);\n+      datanodeDetails = nodeManager.getPipelinesCount(firstNodeDetails)\n+          >= nodeManager.getPipelinesCount(secondNodeDetails)\n+          ? secondNodeDetails : firstNodeDetails;\n+    }\n+    return datanodeDetails;\n+  }\n+\n+  private List<DatanodeDetails> getLowerLoadNodes(\n+      List<DatanodeDetails> nodes, int num) {\n+    int maxPipelineUsage = nodes.size() * heavyNodeCriteria /\n+        HddsProtos.ReplicationFactor.THREE.getNumber();\n+    return nodes.stream()\n+        // Skip the nodes which exceeds the load limit.\n+        .filter(p -> nodeManager.getPipelinesCount(p) < num - maxPipelineUsage)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e8506dc31a14b2938a5bbdb8dcabd62a1f9546e4"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Nzg1ODEzNw==", "bodyText": "Good catch. I update it. Thanks!", "url": "https://github.com/apache/ozone/pull/668#discussion_r397858137", "createdAt": "2020-03-25T13:36:51Z", "author": {"login": "timmylicheng"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -315,6 +314,50 @@ DatanodeDetails fallBackPickNodes(\n     return results;\n   }\n \n+  private DatanodeDetails randomPick(List<DatanodeDetails> healthyNodes) {\n+    DatanodeDetails datanodeDetails;\n+    int firstNodeNdx = getRand().nextInt(healthyNodes.size());\n+    int secondNodeNdx = getRand().nextInt(healthyNodes.size());\n+\n+    // There is a possibility that both numbers will be same.\n+    // if that is so, we just return the node.\n+    if (firstNodeNdx == secondNodeNdx) {\n+      datanodeDetails = healthyNodes.get(firstNodeNdx);\n+    } else {\n+      DatanodeDetails firstNodeDetails = healthyNodes.get(firstNodeNdx);\n+      DatanodeDetails secondNodeDetails = healthyNodes.get(secondNodeNdx);\n+      datanodeDetails = nodeManager.getPipelinesCount(firstNodeDetails)\n+          >= nodeManager.getPipelinesCount(secondNodeDetails)\n+          ? secondNodeDetails : firstNodeDetails;\n+    }\n+    return datanodeDetails;\n+  }\n+\n+  private List<DatanodeDetails> getLowerLoadNodes(\n+      List<DatanodeDetails> nodes, int num) {\n+    int maxPipelineUsage = nodes.size() * heavyNodeCriteria /\n+        HddsProtos.ReplicationFactor.THREE.getNumber();\n+    return nodes.stream()\n+        // Skip the nodes which exceeds the load limit.\n+        .filter(p -> nodeManager.getPipelinesCount(p) < num - maxPipelineUsage)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTYxMzUzMg=="}, "originalCommit": {"oid": "e8506dc31a14b2938a5bbdb8dcabd62a1f9546e4"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MTY1MDMwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxMzoyMDoyMlrOF9pVLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yMVQwMzo1ODo1N1rOGIxXBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDE4NDYyMg==", "bodyText": "I am not sure if this calculation is correct. The reason is that the healthy node list is already filtered to include only nodes with fewer than heavyNodeCritera pipelines in filterViableNodes(). Therefore the size of the list passed into this method gets smaller as the nodes are used up and eventually it stops returning any nodes, even though there are nodes valid to return. From some debug messages I added:\n2020-03-30 14:18:31,682 INFO  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:chooseDatanodes(202)) - There is no topology\n2020-03-30 14:18:31,682 INFO  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:chooseNode(391)) - In chooseNode\n2020-03-30 14:18:31,682 INFO  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:getLowerLoadNodes(351)) - Max pipeline usage is: 8\n2020-03-30 14:18:31,682 INFO  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:lowerLoadPick(368)) - getLowerLoadNodes() returned empty list\n2020-03-30 14:18:31,682 INFO  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:chooseNode(391)) - In chooseNode\n2020-03-30 14:18:31,682 INFO  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:getLowerLoadNodes(351)) - Max pipeline usage is: 6\n2020-03-30 14:18:31,682 INFO  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:lowerLoadPick(368)) - getLowerLoadNodes() returned empty list\n2020-03-30 14:18:31,682 INFO  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:chooseNode(391)) - In chooseNode\n2020-03-30 14:18:31,683 INFO  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:getLowerLoadNodes(351)) - Max pipeline usage is: 5\n2020-03-30 14:18:31,683 INFO  pipeline.PipelinePlacementPolicy (PipelinePlacementPolicy.java:lowerLoadPick(368)) - getLowerLoadNodes() returned empty list\n2020-03-30 14:18:31,683 INFO  pipeline.PipelineStateManager (PipelineStateManager.java:addPipeline(54)) - Created pipeline Pipeline[ Id: 3da2deda-7839-4e5c-88e0-a613729b99fd, Nodes: 0cbe69da-7ef6-43ff-a11a-7ba716ec7c9c{ip: 242.96.90.116, host: localhost-242.96.90.116, networkLocation: /default-rack, certSerialId: null}b4d70058-fb84-410a-8ca8-cc4e8e944fae{ip: 28.158.147.87, host: localhost-28.158.147.87, networkLocation: /default-rack, certSerialId: null}03846f48-f6e5-4dba-97b2-5ab8a7be1564{ip: 128.200.5.118, host: localhost-128.200.5.118, networkLocation: /default-rack, certSerialId: null}, Type:RATIS, Factor:THREE, State:ALLOCATED, leaderId:null, CreationTimestamp2020-03-30T13:18:31.683Z]\n2020-03-30 14:18:31,683 INFO  pipeline.TestPipelinePlacementPolicy (TestPipelinePlacementPolicy.java:testPickLowestLoadAnchor(112)) - Pipeline count for this node is 5\n2020-03-30 14:18:31,683 INFO  pipeline.TestPipelinePlacementPolicy (TestPipelinePlacementPolicy.java:testPickLowestLoadAnchor(112)) - Pipeline count for this node is 5\n2020-03-30 14:18:31,683 INFO  pipeline.TestPipelinePlacementPolicy (TestPipelinePlacementPolicy.java:testPickLowestLoadAnchor(112)) - Pipeline count for this node is 5\n2020-03-30 14:18:31,683 INFO  pipeline.TestPipelinePlacementPolicy (TestPipelinePlacementPolicy.java:testPickLowestLoadAnchor(112)) - Pipeline count for this node is 3\n2020-03-30 14:18:31,683 INFO  pipeline.TestPipelinePlacementPolicy (TestPipelinePlacementPolicy.java:testPickLowestLoadAnchor(112)) - Pipeline count for this node is 2\n2020-03-30 14:18:31,683 INFO  pipeline.TestPipelinePlacementPolicy (TestPipelinePlacementPolicy.java:testPickLowestLoadAnchor(112)) - Pipeline count for this node is 2\n2020-03-30 14:18:31,683 INFO  pipeline.TestPipelinePlacementPolicy (TestPipelinePlacementPolicy.java:testPickLowestLoadAnchor(112)) - Pipeline count for this node is 3\n2020-03-30 14:18:31,684 INFO  pipeline.TestPipelinePlacementPolicy (TestPipelinePlacementPolicy.java:testPickLowestLoadAnchor(112)) - Pipeline count for this node is 2\n\nIf the nodes are filtered by load count in filterViableNodes, do we actually need this getLowerLoadNodes method?", "url": "https://github.com/apache/ozone/pull/668#discussion_r400184622", "createdAt": "2020-03-30T13:20:22Z", "author": {"login": "sodonnel"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -316,36 +315,75 @@ DatanodeDetails fallBackPickNodes(\n   }\n \n   /**\n-   * Find a node from the healthy list and return it after removing it from the\n-   * list that we are operating on.\n-   *\n-   * @param healthyNodes - Set of healthy nodes we can choose from.\n-   * @return chosen datanodDetails\n+   * Random pick two nodes and compare with the pipeline load.\n+   * Return the node with lower pipeline load.\n+   * @param healthyNodes healthy nodes\n+   * @return node\n    */\n-  @Override\n-  public DatanodeDetails chooseNode(\n-      List<DatanodeDetails> healthyNodes) {\n-    if (healthyNodes == null || healthyNodes.isEmpty()) {\n-      return null;\n-    }\n+  private DatanodeDetails randomPick(List<DatanodeDetails> healthyNodes) {\n+    DatanodeDetails datanodeDetails;\n     int firstNodeNdx = getRand().nextInt(healthyNodes.size());\n     int secondNodeNdx = getRand().nextInt(healthyNodes.size());\n \n-    DatanodeDetails datanodeDetails;\n     // There is a possibility that both numbers will be same.\n     // if that is so, we just return the node.\n     if (firstNodeNdx == secondNodeNdx) {\n       datanodeDetails = healthyNodes.get(firstNodeNdx);\n     } else {\n       DatanodeDetails firstNodeDetails = healthyNodes.get(firstNodeNdx);\n       DatanodeDetails secondNodeDetails = healthyNodes.get(secondNodeNdx);\n-      SCMNodeMetric firstNodeMetric =\n-          nodeManager.getNodeStat(firstNodeDetails);\n-      SCMNodeMetric secondNodeMetric =\n-          nodeManager.getNodeStat(secondNodeDetails);\n-      datanodeDetails = firstNodeMetric.isGreater(secondNodeMetric.get())\n-          ? firstNodeDetails : secondNodeDetails;\n+      datanodeDetails = nodeManager.getPipelinesCount(firstNodeDetails)\n+          >= nodeManager.getPipelinesCount(secondNodeDetails)\n+          ? secondNodeDetails : firstNodeDetails;\n     }\n+    return datanodeDetails;\n+  }\n+\n+  /**\n+   * Get a list of nodes with lower load than max pipeline number.\n+   */\n+  private List<DatanodeDetails> getLowerLoadNodes(\n+      List<DatanodeDetails> nodes, int num) {\n+    int maxPipelineUsage = nodes.size() * heavyNodeCriteria /", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "201869a7c92dd05f6b1d84675abc0a1ae0696819"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTg1MDUwMQ==", "bodyText": "Removed and updated with new method.", "url": "https://github.com/apache/ozone/pull/668#discussion_r411850501", "createdAt": "2020-04-21T03:58:57Z", "author": {"login": "timmylicheng"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -316,36 +315,75 @@ DatanodeDetails fallBackPickNodes(\n   }\n \n   /**\n-   * Find a node from the healthy list and return it after removing it from the\n-   * list that we are operating on.\n-   *\n-   * @param healthyNodes - Set of healthy nodes we can choose from.\n-   * @return chosen datanodDetails\n+   * Random pick two nodes and compare with the pipeline load.\n+   * Return the node with lower pipeline load.\n+   * @param healthyNodes healthy nodes\n+   * @return node\n    */\n-  @Override\n-  public DatanodeDetails chooseNode(\n-      List<DatanodeDetails> healthyNodes) {\n-    if (healthyNodes == null || healthyNodes.isEmpty()) {\n-      return null;\n-    }\n+  private DatanodeDetails randomPick(List<DatanodeDetails> healthyNodes) {\n+    DatanodeDetails datanodeDetails;\n     int firstNodeNdx = getRand().nextInt(healthyNodes.size());\n     int secondNodeNdx = getRand().nextInt(healthyNodes.size());\n \n-    DatanodeDetails datanodeDetails;\n     // There is a possibility that both numbers will be same.\n     // if that is so, we just return the node.\n     if (firstNodeNdx == secondNodeNdx) {\n       datanodeDetails = healthyNodes.get(firstNodeNdx);\n     } else {\n       DatanodeDetails firstNodeDetails = healthyNodes.get(firstNodeNdx);\n       DatanodeDetails secondNodeDetails = healthyNodes.get(secondNodeNdx);\n-      SCMNodeMetric firstNodeMetric =\n-          nodeManager.getNodeStat(firstNodeDetails);\n-      SCMNodeMetric secondNodeMetric =\n-          nodeManager.getNodeStat(secondNodeDetails);\n-      datanodeDetails = firstNodeMetric.isGreater(secondNodeMetric.get())\n-          ? firstNodeDetails : secondNodeDetails;\n+      datanodeDetails = nodeManager.getPipelinesCount(firstNodeDetails)\n+          >= nodeManager.getPipelinesCount(secondNodeDetails)\n+          ? secondNodeDetails : firstNodeDetails;\n     }\n+    return datanodeDetails;\n+  }\n+\n+  /**\n+   * Get a list of nodes with lower load than max pipeline number.\n+   */\n+  private List<DatanodeDetails> getLowerLoadNodes(\n+      List<DatanodeDetails> nodes, int num) {\n+    int maxPipelineUsage = nodes.size() * heavyNodeCriteria /", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDE4NDYyMg=="}, "originalCommit": {"oid": "201869a7c92dd05f6b1d84675abc0a1ae0696819"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMzQ0MDg4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxMTo0ODowN1rOGFK24Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQwNzoyNDoyNFrOGFtALg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODA3Mzk1Mw==", "bodyText": "Can you rename nodeOnOtherRack to nodesOnSameRack here please?", "url": "https://github.com/apache/ozone/pull/668#discussion_r408073953", "createdAt": "2020-04-14T11:48:07Z", "author": {"login": "sodonnel"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -373,13 +354,31 @@ protected DatanodeDetails chooseNodeBasedOnRackAwareness(\n       return null;\n     }\n \n-    for (DatanodeDetails node : healthyNodes) {\n-      if (excludedNodes.contains(node) ||\n-          anchor.getNetworkLocation().equals(node.getNetworkLocation())) {\n-        continue;\n-      } else {\n-        return node;\n-      }\n+    List<DatanodeDetails> nodesOnOtherRack = healthyNodes.stream().filter(\n+        p -> !excludedNodes.contains(p)\n+            && !anchor.getNetworkLocation().equals(p.getNetworkLocation()))\n+        .collect(Collectors.toList());\n+    if (!nodesOnOtherRack.isEmpty()) {\n+      return nodesOnOtherRack.get(0);\n+    }\n+    return null;\n+  }\n+\n+  @VisibleForTesting\n+  protected DatanodeDetails chooseNodeBasedOnSameRack(\n+      List<DatanodeDetails> healthyNodes,  List<DatanodeDetails> excludedNodes,\n+      NetworkTopology networkTopology, DatanodeDetails anchor) {\n+    Preconditions.checkArgument(networkTopology != null);\n+    if (checkAllNodesAreEqual(networkTopology)) {\n+      return null;\n+    }\n+\n+    List<DatanodeDetails> nodesOnOtherRack = healthyNodes.stream().filter(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1ebecf2505fd0588eb7054ff7f76fabf595eec09"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODYzMzM5MA==", "bodyText": "Updated", "url": "https://github.com/apache/ozone/pull/668#discussion_r408633390", "createdAt": "2020-04-15T07:24:24Z", "author": {"login": "timmylicheng"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -373,13 +354,31 @@ protected DatanodeDetails chooseNodeBasedOnRackAwareness(\n       return null;\n     }\n \n-    for (DatanodeDetails node : healthyNodes) {\n-      if (excludedNodes.contains(node) ||\n-          anchor.getNetworkLocation().equals(node.getNetworkLocation())) {\n-        continue;\n-      } else {\n-        return node;\n-      }\n+    List<DatanodeDetails> nodesOnOtherRack = healthyNodes.stream().filter(\n+        p -> !excludedNodes.contains(p)\n+            && !anchor.getNetworkLocation().equals(p.getNetworkLocation()))\n+        .collect(Collectors.toList());\n+    if (!nodesOnOtherRack.isEmpty()) {\n+      return nodesOnOtherRack.get(0);\n+    }\n+    return null;\n+  }\n+\n+  @VisibleForTesting\n+  protected DatanodeDetails chooseNodeBasedOnSameRack(\n+      List<DatanodeDetails> healthyNodes,  List<DatanodeDetails> excludedNodes,\n+      NetworkTopology networkTopology, DatanodeDetails anchor) {\n+    Preconditions.checkArgument(networkTopology != null);\n+    if (checkAllNodesAreEqual(networkTopology)) {\n+      return null;\n+    }\n+\n+    List<DatanodeDetails> nodesOnOtherRack = healthyNodes.stream().filter(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODA3Mzk1Mw=="}, "originalCommit": {"oid": "1ebecf2505fd0588eb7054ff7f76fabf595eec09"}, "originalPosition": 161}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzMzQ0NzEwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNFQxMTo0OTo1N1rOGFK6rA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQwNzoyNDozM1rOGFtAfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODA3NDkyNA==", "bodyText": "We should add a note to this Java doc that the returned list is sorted in order of pipeline count, starting with the lowest to highest.", "url": "https://github.com/apache/ozone/pull/668#discussion_r408074924", "createdAt": "2020-04-14T11:49:57Z", "author": {"login": "sodonnel"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -110,17 +97,11 @@ boolean meetCriteria(DatanodeDetails datanodeDetails, int nodesRequired) {\n         pipelineNumDeductable++;\n       }\n     }\n-    boolean meet = (nodeManager.getPipelinesCount(datanodeDetails)\n-        - pipelineNumDeductable) < heavyNodeCriteria;\n-    if (!meet && LOG.isDebugEnabled()) {\n-      LOG.debug(\"Pipeline Placement: can't place more pipeline on heavy \" +\n-          \"datanode\uff1a \" + datanodeDetails.getUuid().toString() +\n-          \" Heaviness: \" + nodeManager.getPipelinesCount(datanodeDetails) +\n-          \" limit: \" + heavyNodeCriteria);\n-    }\n-    return meet;\n+    return pipelines.size() - pipelineNumDeductable;\n   }\n \n+\n+\n   /**\n    * Filter out viable nodes based on\n    * 1. nodes that are healthy", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1ebecf2505fd0588eb7054ff7f76fabf595eec09"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODYzMzQ3MA==", "bodyText": "Updated.", "url": "https://github.com/apache/ozone/pull/668#discussion_r408633470", "createdAt": "2020-04-15T07:24:33Z", "author": {"login": "timmylicheng"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/pipeline/PipelinePlacementPolicy.java", "diffHunk": "@@ -110,17 +97,11 @@ boolean meetCriteria(DatanodeDetails datanodeDetails, int nodesRequired) {\n         pipelineNumDeductable++;\n       }\n     }\n-    boolean meet = (nodeManager.getPipelinesCount(datanodeDetails)\n-        - pipelineNumDeductable) < heavyNodeCriteria;\n-    if (!meet && LOG.isDebugEnabled()) {\n-      LOG.debug(\"Pipeline Placement: can't place more pipeline on heavy \" +\n-          \"datanode\uff1a \" + datanodeDetails.getUuid().toString() +\n-          \" Heaviness: \" + nodeManager.getPipelinesCount(datanodeDetails) +\n-          \" limit: \" + heavyNodeCriteria);\n-    }\n-    return meet;\n+    return pipelines.size() - pipelineNumDeductable;\n   }\n \n+\n+\n   /**\n    * Filter out viable nodes based on\n    * 1. nodes that are healthy", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODA3NDkyNA=="}, "originalCommit": {"oid": "1ebecf2505fd0588eb7054ff7f76fabf595eec09"}, "originalPosition": 60}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4820, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}