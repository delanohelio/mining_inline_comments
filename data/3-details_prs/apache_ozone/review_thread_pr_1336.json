{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY5MDQxNjkx", "number": 1336, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwODozOTo0NVrOEbdm_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMjoxODo1M1rOEdMhmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3MjMyMTI2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwODozOTo0NVrOHFaBeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwODozOTo0NVrOHFaBeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQzMTI4OQ==", "bodyText": "Can you please extract these 2 lines to a separate method (and replace other 2 duplicate fragments, too)?", "url": "https://github.com/apache/ozone/pull/1336#discussion_r475431289", "createdAt": "2020-08-24T08:39:45Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "diffHunk": "@@ -154,6 +159,16 @@ public BlockOutputStream(BlockID blockID,\n     this.bufferPool = bufferPool;\n     this.bytesPerChecksum = bytesPerChecksum;\n \n+    //number of buffers used before doing a flush\n+    currentBuffer = bufferPool.getCurrentBuffer();\n+    currentBufferRemaining =\n+        currentBuffer != null ? currentBuffer.remaining() : 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3MjM3NjIzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChunkBuffer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwODo1MzozN1rOHFahSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwODo1MzozN1rOHFahSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQzOTQzMg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                return put(buf, 0, 1);  }\n          \n          \n            \n                return put(buf, 0, 1);\n          \n          \n            \n              }", "url": "https://github.com/apache/ozone/pull/1336#discussion_r475439432", "createdAt": "2020-08-24T08:53:37Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChunkBuffer.java", "diffHunk": "@@ -88,6 +86,12 @@ default ChunkBuffer put(byte[] b) {\n     return put(ByteBuffer.wrap(b));\n   }\n \n+  /** Similar to {@link ByteBuffer#put(byte[])}. */\n+  default ChunkBuffer put(byte b) {\n+    byte[] buf = new byte[1];\n+    buf[0] = (byte) b;\n+    return put(buf, 0, 1);  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3MjM4MzQ4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/client/src/test/java/org/apache/hadoop/hdds/scm/storage/TestBlockOutputStreamCorrectness.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwODo1NTozMVrOHFalmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwODo1NTozMVrOHFalmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ0MDUzNw==", "bodyText": "\ud83c\udded\ud83c\uddfa", "url": "https://github.com/apache/ozone/pull/1336#discussion_r475440537", "createdAt": "2020-08-24T08:55:31Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/test/java/org/apache/hadoop/hdds/scm/storage/TestBlockOutputStreamCorrectness.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.storage;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.hdds.client.BlockID;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.MockDatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ChecksumType;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerCommandRequestProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerCommandResponseProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.GetCommittedBlockLengthResponseProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.PutBlockResponseProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.Result;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.Type;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationFactor;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationType;\n+import org.apache.hadoop.hdds.scm.XceiverClientManager;\n+import org.apache.hadoop.hdds.scm.XceiverClientReply;\n+import org.apache.hadoop.hdds.scm.XceiverClientSpi;\n+import org.apache.hadoop.hdds.scm.pipeline.Pipeline;\n+import org.apache.hadoop.hdds.scm.pipeline.Pipeline.Builder;\n+import org.apache.hadoop.hdds.scm.pipeline.Pipeline.PipelineState;\n+import org.apache.hadoop.hdds.scm.pipeline.PipelineID;\n+\n+import org.apache.ratis.thirdparty.com.google.protobuf.ByteString;\n+import org.jetbrains.annotations.NotNull;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+/**\n+ * UNIT test for BlockOutputStream.\n+ * <p>\n+ * Compares bytes written to the stream and received in the ChunkWriteRequests.\n+ */\n+public class TestBlockOutputStreamCorrectness {\n+\n+  private static final long SEED = 18480315L;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3MjQwMDAyOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/client/src/test/java/org/apache/hadoop/hdds/scm/storage/TestBlockOutputStreamCorrectness.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwODo1OTo1N1rOHFavsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwODo1OTo1N1rOHFavsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ0MzEyMg==", "bodyText": "Can we move this logic to MockPipeline for reuse?", "url": "https://github.com/apache/ozone/pull/1336#discussion_r475443122", "createdAt": "2020-08-24T08:59:57Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/test/java/org/apache/hadoop/hdds/scm/storage/TestBlockOutputStreamCorrectness.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.storage;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.hadoop.hdds.client.BlockID;\n+import org.apache.hadoop.hdds.protocol.DatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.MockDatanodeDetails;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ChecksumType;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerCommandRequestProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.ContainerCommandResponseProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.GetCommittedBlockLengthResponseProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.PutBlockResponseProto;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.Result;\n+import org.apache.hadoop.hdds.protocol.datanode.proto.ContainerProtos.Type;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationFactor;\n+import org.apache.hadoop.hdds.protocol.proto.HddsProtos.ReplicationType;\n+import org.apache.hadoop.hdds.scm.XceiverClientManager;\n+import org.apache.hadoop.hdds.scm.XceiverClientReply;\n+import org.apache.hadoop.hdds.scm.XceiverClientSpi;\n+import org.apache.hadoop.hdds.scm.pipeline.Pipeline;\n+import org.apache.hadoop.hdds.scm.pipeline.Pipeline.Builder;\n+import org.apache.hadoop.hdds.scm.pipeline.Pipeline.PipelineState;\n+import org.apache.hadoop.hdds.scm.pipeline.PipelineID;\n+\n+import org.apache.ratis.thirdparty.com.google.protobuf.ByteString;\n+import org.jetbrains.annotations.NotNull;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+/**\n+ * UNIT test for BlockOutputStream.\n+ * <p>\n+ * Compares bytes written to the stream and received in the ChunkWriteRequests.\n+ */\n+public class TestBlockOutputStreamCorrectness {\n+\n+  private static final long SEED = 18480315L;\n+\n+  private int writeUnitSize = 1;\n+\n+  @Test\n+  public void test() throws IOException {\n+\n+    final BufferPool bufferPool = new BufferPool(4 * 1024 * 1024, 32 / 4);\n+\n+    for (int block = 0; block < 10; block++) {\n+      BlockOutputStream outputStream =\n+          createBlockOutputStream(bufferPool);\n+\n+      Random random = new Random(SEED);\n+\n+      int max = 256 * 1024 * 1024 / writeUnitSize;\n+\n+      byte[] writeBuffer = new byte[writeUnitSize];\n+      for (int t = 0; t < max; t++) {\n+        if (writeUnitSize > 1) {\n+          for (int i = 0; i < writeBuffer.length; i++) {\n+            writeBuffer[i] = (byte) random.nextInt();\n+          }\n+          outputStream.write(writeBuffer, 0, writeBuffer.length);\n+        } else {\n+          outputStream.write((byte) random.nextInt());\n+        }\n+      }\n+      outputStream.close();\n+    }\n+  }\n+\n+  @NotNull\n+  private BlockOutputStream createBlockOutputStream(BufferPool bufferPool)\n+      throws IOException {\n+    List<DatanodeDetails> nodes = new ArrayList<>();\n+    nodes.add(MockDatanodeDetails.randomDatanodeDetails());\n+    nodes.add(MockDatanodeDetails.randomDatanodeDetails());\n+    nodes.add(MockDatanodeDetails.randomDatanodeDetails());\n+\n+    final Pipeline pipeline = new Builder()\n+        .setFactor(ReplicationFactor.THREE)\n+        .setType(ReplicationType.RATIS)\n+        .setState(PipelineState.OPEN)\n+        .setId(PipelineID.randomId())\n+        .setNodes(nodes)\n+        .build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3MjQyNDQwOnYy", "diffSide": "LEFT", "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChunkBuffer.java", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwOTowNToyNVrOHFa_Sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQxNDoyODowMFrOHK44TA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ0NzExNQ==", "bodyText": "Can you please also change TestChunkBuffer#runTestIncrementalChunkBuffer to explicitly create IncrementalChunkBuffer?  Currently it uses this factory method, and so with this patch it really tests ChunkBufferImplWithByteBuffer.", "url": "https://github.com/apache/ozone/pull/1336#discussion_r475447115", "createdAt": "2020-08-24T09:05:25Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChunkBuffer.java", "diffHunk": "@@ -44,9 +45,6 @@ static ChunkBuffer allocate(int capacity) {\n    *   When increment <= 0, entire buffer is allocated in the beginning.\n    */\n   static ChunkBuffer allocate(int capacity, int increment) {\n-    if (increment > 0 && increment < capacity) {\n-      return new IncrementalChunkBuffer(capacity, increment, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzIyMjgwMQ==", "bodyText": "Wow, nice catch.", "url": "https://github.com/apache/ozone/pull/1336#discussion_r477222801", "createdAt": "2020-08-26T11:16:23Z", "author": {"login": "elek"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChunkBuffer.java", "diffHunk": "@@ -44,9 +45,6 @@ static ChunkBuffer allocate(int capacity) {\n    *   When increment <= 0, entire buffer is allocated in the beginning.\n    */\n   static ChunkBuffer allocate(int capacity, int increment) {\n-    if (increment > 0 && increment < capacity) {\n-      return new IncrementalChunkBuffer(capacity, increment, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ0NzExNQ=="}, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzIyNjc0MQ==", "bodyText": "IncrementalChunkBuffer was added to address cases where ozone client were running into OOM with keys less than chunk size , as without this, the smallest buffer which will be allocated will always be equal to the chunk size(4MB by default).\nPlease see https://issues.apache.org/jira/browse/HDDS-2331 for more details.\nI would prefer to not remove this logic of incremental chunk buffer and may be hide it within an internal config.\n@elek , how much of perf gain we will have of we still do incremental buffer allocation?", "url": "https://github.com/apache/ozone/pull/1336#discussion_r477226741", "createdAt": "2020-08-26T11:23:56Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChunkBuffer.java", "diffHunk": "@@ -44,9 +45,6 @@ static ChunkBuffer allocate(int capacity) {\n    *   When increment <= 0, entire buffer is allocated in the beginning.\n    */\n   static ChunkBuffer allocate(int capacity, int increment) {\n-    if (increment > 0 && increment < capacity) {\n-      return new IncrementalChunkBuffer(capacity, increment, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ0NzExNQ=="}, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM1MzQyOQ==", "bodyText": "@elek , how much of perf gain we will have of we still do incremental buffer allocation?\n\nI can repeat the test to get exact numbers, but I couldn't get good performance without removing the incremental buffer. You can easily test it with the new unit test, if you write a lot of data with byte=1, it's still low.\n\nIncrementalChunkBuffer was added to address cases where ozone client were running into OOM with keys less than chunk size , as without this, the smallest buffer which will be allocated will always be equal to the chunk size(4MB by default).\n\nI think it's a valid (and important question), but as far as I see it's safe to remove the IncrementalByteBuffer. As far as I see the situation is slightly different since HDDS-2331. I tried to test this patch with the commands from the HDDS-2331:\nozone freon rk --numOfThreads 1 --numOfVolumes 1 --numOfBuckets 1 --replicationType RATIS --factor ONE --keySize 1048576 --numOfKeys 5120 --bufferSize 65536\n\nI couldn't reproduce the OOM.\nBased on my understanding:\n\nWe already have an increment by the ByteBuffer but size of the increment is 4MB (adding one more buffer when required)\n4MB seems to be acceptable even with many clients in the same JVM, especially if we can have acceptable performance.\n\nLet's say I have 100 Ozone clients (in the same JVM!!!) which write 1kb keys. I will have (4MB-1kb)  *100 overhead without the IncrementalChunkBuffer (as far as I understood). It's still <400MB in exchange for 30-100% performance gain. Sounds like a good deal.\nBut let me know if you see any problems here.\n\nLet's say the 400MB overhead is unacceptable (or my calculation was wrong and the overhead is higher ;-) )\n\nAs far as I see the BufferPool is created per key. I think it would be possible to set the buffer size to min(keySize, bufferSize). With this approach the first and only buffer of the BufferPool can have exactly the required size (which covers all the where the key size is < 4MB)", "url": "https://github.com/apache/ozone/pull/1336#discussion_r478353429", "createdAt": "2020-08-27T11:42:34Z", "author": {"login": "elek"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChunkBuffer.java", "diffHunk": "@@ -44,9 +45,6 @@ static ChunkBuffer allocate(int capacity) {\n    *   When increment <= 0, entire buffer is allocated in the beginning.\n    */\n   static ChunkBuffer allocate(int capacity, int increment) {\n-    if (increment > 0 && increment < capacity) {\n-      return new IncrementalChunkBuffer(capacity, increment, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ0NzExNQ=="}, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM3MjU3OQ==", "bodyText": "the situation is slightly different since HDDS-2331\n\nNote that default chunk size was 16MB at the time when HDDS-2331 was reported.  The benefit from IncrementalChunkBuffer is less now with 4MB default size.", "url": "https://github.com/apache/ozone/pull/1336#discussion_r478372579", "createdAt": "2020-08-27T12:15:03Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChunkBuffer.java", "diffHunk": "@@ -44,9 +45,6 @@ static ChunkBuffer allocate(int capacity) {\n    *   When increment <= 0, entire buffer is allocated in the beginning.\n    */\n   static ChunkBuffer allocate(int capacity, int increment) {\n-    if (increment > 0 && increment < capacity) {\n-      return new IncrementalChunkBuffer(capacity, increment, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ0NzExNQ=="}, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDA4NTYzOQ==", "bodyText": "I would suggest separating the two parts of the PR:\n\nreorganize the position calculation and allocation\nremove the usage of the Incremental buffer\n\nWhile we can continue to searching for the safest method to do 2 (or do something instead of the removal), we can merge the first part where we already have an agreement.", "url": "https://github.com/apache/ozone/pull/1336#discussion_r480085639", "createdAt": "2020-08-31T12:06:59Z", "author": {"login": "elek"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChunkBuffer.java", "diffHunk": "@@ -44,9 +45,6 @@ static ChunkBuffer allocate(int capacity) {\n    *   When increment <= 0, entire buffer is allocated in the beginning.\n    */\n   static ChunkBuffer allocate(int capacity, int increment) {\n-    if (increment > 0 && increment < capacity) {\n-      return new IncrementalChunkBuffer(capacity, increment, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ0NzExNQ=="}, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTE3OTcyNA==", "bodyText": "See #1374 about the 2nd.", "url": "https://github.com/apache/ozone/pull/1336#discussion_r481179724", "createdAt": "2020-09-01T14:28:00Z", "author": {"login": "elek"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/common/ChunkBuffer.java", "diffHunk": "@@ -44,9 +45,6 @@ static ChunkBuffer allocate(int capacity) {\n    *   When increment <= 0, entire buffer is allocated in the beginning.\n    */\n   static ChunkBuffer allocate(int capacity, int increment) {\n-    if (increment > 0 && increment < capacity) {\n-      return new IncrementalChunkBuffer(capacity, increment, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ0NzExNQ=="}, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3MjQ3NTc4OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwOToxOTo1MlrOHFbe9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwOToxOTo1MlrOHFbe9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ1NTIyMQ==", "bodyText": "For consistency, I think we should move the condition to allocateNewBuffer and rename it to allocateNewBufferIfNeeded.", "url": "https://github.com/apache/ozone/pull/1336#discussion_r475455221", "createdAt": "2020-08-24T09:19:52Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "diffHunk": "@@ -209,9 +224,16 @@ public IOException getIoException() {\n   @Override\n   public void write(int b) throws IOException {\n     checkOpen();\n-    byte[] buf = new byte[1];\n-    buf[0] = (byte) b;\n-    write(buf, 0, 1);\n+    if (currentBufferRemaining == 0) {\n+      allocateNewBuffer();\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3MjQ3NzgzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwOToyMDoyMFrOHFbgLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMjoxOTowNlrOHINsjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ1NTUzMg==", "bodyText": "For consistency, I think we should move the condition to a new method writeChunkIfNeeded.", "url": "https://github.com/apache/ozone/pull/1336#discussion_r475455532", "createdAt": "2020-08-24T09:20:20Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "diffHunk": "@@ -209,9 +224,16 @@ public IOException getIoException() {\n   @Override\n   public void write(int b) throws IOException {\n     checkOpen();\n-    byte[] buf = new byte[1];\n-    buf[0] = (byte) b;\n-    write(buf, 0, 1);\n+    if (currentBufferRemaining == 0) {\n+      allocateNewBuffer();\n+    }\n+    currentBuffer.put((byte) b);\n+    currentBufferRemaining--;\n+    if (currentBufferRemaining == 0) {\n+      writeChunk(currentBuffer);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzIyMzk0MA==", "bodyText": "Not sure if I understood. Why is it more consistent to move this 3 lines to a separated method?", "url": "https://github.com/apache/ozone/pull/1336#discussion_r477223940", "createdAt": "2020-08-26T11:18:32Z", "author": {"login": "elek"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "diffHunk": "@@ -209,9 +224,16 @@ public IOException getIoException() {\n   @Override\n   public void write(int b) throws IOException {\n     checkOpen();\n-    byte[] buf = new byte[1];\n-    buf[0] = (byte) b;\n-    write(buf, 0, 1);\n+    if (currentBufferRemaining == 0) {\n+      allocateNewBuffer();\n+    }\n+    currentBuffer.put((byte) b);\n+    currentBufferRemaining--;\n+    if (currentBufferRemaining == 0) {\n+      writeChunk(currentBuffer);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ1NTUzMg=="}, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI1MzM5NA==", "bodyText": "checkOpen() and doFlushOrWatchIfNeeded() calls are both concise.  I think moving the conditions for these two other calls would help make the level of abstraction in write() more similar.  But it's optional.", "url": "https://github.com/apache/ozone/pull/1336#discussion_r477253394", "createdAt": "2020-08-26T12:15:04Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "diffHunk": "@@ -209,9 +224,16 @@ public IOException getIoException() {\n   @Override\n   public void write(int b) throws IOException {\n     checkOpen();\n-    byte[] buf = new byte[1];\n-    buf[0] = (byte) b;\n-    write(buf, 0, 1);\n+    if (currentBufferRemaining == 0) {\n+      allocateNewBuffer();\n+    }\n+    currentBuffer.put((byte) b);\n+    currentBufferRemaining--;\n+    if (currentBufferRemaining == 0) {\n+      writeChunk(currentBuffer);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ1NTUzMg=="}, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM1ODk2Mw==", "bodyText": "Ok, I think I got the idea, but please double-check my commit as I am not sure ;-)\n9ab01a7", "url": "https://github.com/apache/ozone/pull/1336#discussion_r478358963", "createdAt": "2020-08-27T11:52:15Z", "author": {"login": "elek"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "diffHunk": "@@ -209,9 +224,16 @@ public IOException getIoException() {\n   @Override\n   public void write(int b) throws IOException {\n     checkOpen();\n-    byte[] buf = new byte[1];\n-    buf[0] = (byte) b;\n-    write(buf, 0, 1);\n+    if (currentBufferRemaining == 0) {\n+      allocateNewBuffer();\n+    }\n+    currentBuffer.put((byte) b);\n+    currentBufferRemaining--;\n+    if (currentBufferRemaining == 0) {\n+      writeChunk(currentBuffer);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ1NTUzMg=="}, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM3NTA1NA==", "bodyText": "Thanks for the update.", "url": "https://github.com/apache/ozone/pull/1336#discussion_r478375054", "createdAt": "2020-08-27T12:19:06Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "diffHunk": "@@ -209,9 +224,16 @@ public IOException getIoException() {\n   @Override\n   public void write(int b) throws IOException {\n     checkOpen();\n-    byte[] buf = new byte[1];\n-    buf[0] = (byte) b;\n-    write(buf, 0, 1);\n+    if (currentBufferRemaining == 0) {\n+      allocateNewBuffer();\n+    }\n+    currentBuffer.put((byte) b);\n+    currentBufferRemaining--;\n+    if (currentBufferRemaining == 0) {\n+      writeChunk(currentBuffer);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ1NTUzMg=="}, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 117}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk3MjU0NjE1OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwOTozMToyN1rOHFcLHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxMToxOTo0OVrOHHHelA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ2NjUyNg==", "bodyText": "Should we also update currentBufferRemaining?", "url": "https://github.com/apache/ozone/pull/1336#discussion_r475466526", "createdAt": "2020-08-24T09:31:27Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "diffHunk": "@@ -481,7 +516,7 @@ private void handleFlush(boolean close)\n     checkOpen();\n     // flush the last chunk data residing on the currentBuffer\n     if (totalDataFlushedLength < writtenDataLength) {\n-      final ChunkBuffer currentBuffer = bufferPool.getCurrentBuffer();\n+      currentBuffer = bufferPool.getCurrentBuffer();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzIyNDU5Ng==", "bodyText": "Yeah, I started to use the newly introduced method which is suggested by the first comment... (update current buffer + remaining)", "url": "https://github.com/apache/ozone/pull/1336#discussion_r477224596", "createdAt": "2020-08-26T11:19:49Z", "author": {"login": "elek"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "diffHunk": "@@ -481,7 +516,7 @@ private void handleFlush(boolean close)\n     checkOpen();\n     // flush the last chunk data residing on the currentBuffer\n     if (totalDataFlushedLength < writtenDataLength) {\n-      final ChunkBuffer currentBuffer = bufferPool.getCurrentBuffer();\n+      currentBuffer = bufferPool.getCurrentBuffer();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTQ2NjUyNg=="}, "originalCommit": {"oid": "dd99deb1ae7b7a8bb5eed17681c473bdfb410f30"}, "originalPosition": 199}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5MDQ5MzY5OnYy", "diffSide": "LEFT", "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QxMjoxODo1M1rOHINsIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QwOTo0NTozM1rOHMe4YQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM3NDk0NA==", "bodyText": "Note the condition is different here, might not be safe to replace with writeChunkIfNeeded().", "url": "https://github.com/apache/ozone/pull/1336#discussion_r478374944", "createdAt": "2020-08-27T12:18:53Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "diffHunk": "@@ -481,11 +514,9 @@ private void handleFlush(boolean close)\n     checkOpen();\n     // flush the last chunk data residing on the currentBuffer\n     if (totalDataFlushedLength < writtenDataLength) {\n-      final ChunkBuffer currentBuffer = bufferPool.getCurrentBuffer();\n+      refreshCurrentBuffer(bufferPool);\n       Preconditions.checkArgument(currentBuffer.position() > 0);\n-      if (currentBuffer.hasRemaining()) {\n-        writeChunk(currentBuffer);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ab01a7753b75009997cc33a338bbef282230687"}, "originalPosition": 233}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Mjg1MDkxMw==", "bodyText": "Thanks the help. I fully reverted these lines in #23ba2d1 and build is green again.", "url": "https://github.com/apache/ozone/pull/1336#discussion_r482850913", "createdAt": "2020-09-03T09:45:33Z", "author": {"login": "elek"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockOutputStream.java", "diffHunk": "@@ -481,11 +514,9 @@ private void handleFlush(boolean close)\n     checkOpen();\n     // flush the last chunk data residing on the currentBuffer\n     if (totalDataFlushedLength < writtenDataLength) {\n-      final ChunkBuffer currentBuffer = bufferPool.getCurrentBuffer();\n+      refreshCurrentBuffer(bufferPool);\n       Preconditions.checkArgument(currentBuffer.position() > 0);\n-      if (currentBuffer.hasRemaining()) {\n-        writeChunk(currentBuffer);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODM3NDk0NA=="}, "originalCommit": {"oid": "9ab01a7753b75009997cc33a338bbef282230687"}, "originalPosition": 233}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3898, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}