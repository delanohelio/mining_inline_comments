{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDczNjI0NzE3", "number": 1353, "reviewThreads": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwNDozMTo0N1rOEcaJMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNzo1NToxN1rOEfkTZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MjIzOTIxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConsts.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwNDozMTo0N1rOHG7cvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwMToxMzoyMVrOHHt9Ug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAyNzUxOQ==", "bodyText": "Can we avoid duplication and use the same String constant for both OM and SCM servlet endpoints?", "url": "https://github.com/apache/ozone/pull/1353#discussion_r477027519", "createdAt": "2020-08-26T04:31:47Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConsts.java", "diffHunk": "@@ -79,6 +79,10 @@\n   public static final String OZONE_USER = \"user\";\n   public static final String OZONE_REQUEST = \"request\";\n \n+  // SCM Http server endpoints\n+  public static final String OZONE_SCM_DB_CHECKPOINT_HTTP_ENDPOINT =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "33b4fd850dbcc2a363b989a0eafe6dd25998a837"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYxNTQ5Mg==", "bodyText": "done", "url": "https://github.com/apache/ozone/pull/1353#discussion_r477615492", "createdAt": "2020-08-26T22:04:52Z", "author": {"login": "prashantpogde"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConsts.java", "diffHunk": "@@ -79,6 +79,10 @@\n   public static final String OZONE_USER = \"user\";\n   public static final String OZONE_REQUEST = \"request\";\n \n+  // SCM Http server endpoints\n+  public static final String OZONE_SCM_DB_CHECKPOINT_HTTP_ENDPOINT =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAyNzUxOQ=="}, "originalCommit": {"oid": "33b4fd850dbcc2a363b989a0eafe6dd25998a837"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYxODQ2Mg==", "bodyText": "done", "url": "https://github.com/apache/ozone/pull/1353#discussion_r477618462", "createdAt": "2020-08-26T22:12:44Z", "author": {"login": "prashantpogde"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConsts.java", "diffHunk": "@@ -79,6 +79,10 @@\n   public static final String OZONE_USER = \"user\";\n   public static final String OZONE_REQUEST = \"request\";\n \n+  // SCM Http server endpoints\n+  public static final String OZONE_SCM_DB_CHECKPOINT_HTTP_ENDPOINT =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAyNzUxOQ=="}, "originalCommit": {"oid": "33b4fd850dbcc2a363b989a0eafe6dd25998a837"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzg1NTA1OA==", "bodyText": "yup, will upload new changes.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r477855058", "createdAt": "2020-08-27T01:13:21Z", "author": {"login": "prashantpogde"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConsts.java", "diffHunk": "@@ -79,6 +79,10 @@\n   public static final String OZONE_USER = \"user\";\n   public static final String OZONE_REQUEST = \"request\";\n \n+  // SCM Http server endpoints\n+  public static final String OZONE_SCM_DB_CHECKPOINT_HTTP_ENDPOINT =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAyNzUxOQ=="}, "originalCommit": {"oid": "33b4fd850dbcc2a363b989a0eafe6dd25998a837"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MjI0NDIzOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDBCheckpointServlet.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwNDozNDo1OVrOHG7f1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwMToxMzozM1rOHHt-wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAyODMwOQ==", "bodyText": "Most of the code between the OM and SCM servlet is common. We should be able to abstract out common logic between the two and have just one copy. How about a super DBCheckpointServlet class from which both the servlets inherit logic?", "url": "https://github.com/apache/ozone/pull/1353#discussion_r477028309", "createdAt": "2020-08-26T04:34:59Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDBCheckpointServlet.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.server;\n+\n+import javax.servlet.ServletException;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.scm.container.placement.metrics.SCMMetrics;\n+import org.apache.hadoop.hdds.utils.db.DBCheckpoint;\n+import org.apache.hadoop.hdds.utils.db.DBStore;\n+import org.apache.hadoop.hdfs.util.DataTransferThrottler;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+\n+import org.apache.commons.compress.archivers.ArchiveEntry;\n+import org.apache.commons.compress.archivers.ArchiveOutputStream;\n+import org.apache.commons.compress.archivers.tar.TarArchiveOutputStream;\n+import org.apache.commons.compress.compressors.CompressorException;\n+import org.apache.commons.compress.compressors.CompressorOutputStream;\n+import org.apache.commons.compress.compressors.CompressorStreamFactory;\n+import org.apache.commons.compress.utils.IOUtils;\n+import org.apache.commons.lang3.StringUtils;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_DB_CHECKPOINT_REQUEST_FLUSH;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Provides the current checkpoint Snapshot of the OM DB. (tar.gz)\n+ */\n+public class SCMDBCheckpointServlet extends HttpServlet {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SCMDBCheckpointServlet.class);\n+  private static final long serialVersionUID = 1L;\n+\n+  private transient StorageContainerManager scm;\n+  private transient DBStore scmDbStore;\n+  private transient SCMMetrics scmMetrics;\n+  private transient DataTransferThrottler throttler = null;\n+\n+  @Override\n+  public void init() throws ServletException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "33b4fd850dbcc2a363b989a0eafe6dd25998a837"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzg1NTQyNg==", "bodyText": "yup, I will upload new changes.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r477855426", "createdAt": "2020-08-27T01:13:33Z", "author": {"login": "prashantpogde"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDBCheckpointServlet.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.server;\n+\n+import javax.servlet.ServletException;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.scm.container.placement.metrics.SCMMetrics;\n+import org.apache.hadoop.hdds.utils.db.DBCheckpoint;\n+import org.apache.hadoop.hdds.utils.db.DBStore;\n+import org.apache.hadoop.hdfs.util.DataTransferThrottler;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+\n+import org.apache.commons.compress.archivers.ArchiveEntry;\n+import org.apache.commons.compress.archivers.ArchiveOutputStream;\n+import org.apache.commons.compress.archivers.tar.TarArchiveOutputStream;\n+import org.apache.commons.compress.compressors.CompressorException;\n+import org.apache.commons.compress.compressors.CompressorOutputStream;\n+import org.apache.commons.compress.compressors.CompressorStreamFactory;\n+import org.apache.commons.compress.utils.IOUtils;\n+import org.apache.commons.lang3.StringUtils;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_DB_CHECKPOINT_REQUEST_FLUSH;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Provides the current checkpoint Snapshot of the OM DB. (tar.gz)\n+ */\n+public class SCMDBCheckpointServlet extends HttpServlet {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SCMDBCheckpointServlet.class);\n+  private static final long serialVersionUID = 1L;\n+\n+  private transient StorageContainerManager scm;\n+  private transient DBStore scmDbStore;\n+  private transient SCMMetrics scmMetrics;\n+  private transient DataTransferThrottler throttler = null;\n+\n+  @Override\n+  public void init() throws ServletException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAyODMwOQ=="}, "originalCommit": {"oid": "33b4fd850dbcc2a363b989a0eafe6dd25998a837"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MjI0ODMwOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDBCheckpointServlet.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwNDozNzoxMFrOHG7iBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwMToxMzo0OVrOHHuAvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAyODg3MQ==", "bodyText": "Same here. All this code is common. We should avoid duplication.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r477028871", "createdAt": "2020-08-26T04:37:10Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDBCheckpointServlet.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.server;\n+\n+import javax.servlet.ServletException;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.scm.container.placement.metrics.SCMMetrics;\n+import org.apache.hadoop.hdds.utils.db.DBCheckpoint;\n+import org.apache.hadoop.hdds.utils.db.DBStore;\n+import org.apache.hadoop.hdfs.util.DataTransferThrottler;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+\n+import org.apache.commons.compress.archivers.ArchiveEntry;\n+import org.apache.commons.compress.archivers.ArchiveOutputStream;\n+import org.apache.commons.compress.archivers.tar.TarArchiveOutputStream;\n+import org.apache.commons.compress.compressors.CompressorException;\n+import org.apache.commons.compress.compressors.CompressorOutputStream;\n+import org.apache.commons.compress.compressors.CompressorStreamFactory;\n+import org.apache.commons.compress.utils.IOUtils;\n+import org.apache.commons.lang3.StringUtils;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_DB_CHECKPOINT_REQUEST_FLUSH;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Provides the current checkpoint Snapshot of the OM DB. (tar.gz)\n+ */\n+public class SCMDBCheckpointServlet extends HttpServlet {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SCMDBCheckpointServlet.class);\n+  private static final long serialVersionUID = 1L;\n+\n+  private transient StorageContainerManager scm;\n+  private transient DBStore scmDbStore;\n+  private transient SCMMetrics scmMetrics;\n+  private transient DataTransferThrottler throttler = null;\n+\n+  @Override\n+  public void init() throws ServletException {\n+\n+    scm = (StorageContainerManager) getServletContext()\n+        .getAttribute(OzoneConsts.SCM_CONTEXT_ATTRIBUTE);\n+\n+    if (scm == null) {\n+      LOG.error(\"Unable to initialize SCMDBCheckpointServlet. SCM is null\");\n+      return;\n+    }\n+\n+    scmDbStore = scm.getScmMetadataStore().getStore();\n+    scmMetrics= scm.getMetrics();\n+\n+    OzoneConfiguration configuration = scm.getConfiguration();\n+    long transferBandwidth = configuration.getLongBytes(\n+        ScmConfigKeys.SCM_DB_CHECKPOINT_TRANSFER_RATE_KEY,\n+        ScmConfigKeys.SCM_DB_CHECKPOINT_TRANSFER_RATE_DEFAULT);\n+\n+    if (transferBandwidth > 0) {\n+      throttler = new DataTransferThrottler(transferBandwidth);\n+    }\n+  }\n+\n+  /**\n+   * Process a GET request for the SCM DB checkpoint snapshot.\n+   *\n+   * @param request  The servlet request we are processing\n+   * @param response The servlet response we are creating\n+   */\n+  @Override\n+  public void doGet(HttpServletRequest request, HttpServletResponse response) {\n+\n+    LOG.info(\"Received request to obtain SCM DB checkpoint snapshot\");\n+    if (scmDbStore == null) {\n+      LOG.error(\n+          \"Unable to process metadata snapshot request. DB Store is null\");\n+      response.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);\n+      return;\n+    }\n+\n+    DBCheckpoint checkpoint = null;\n+    try {\n+\n+      boolean flush = false;\n+      String flushParam =\n+          request.getParameter(OZONE_DB_CHECKPOINT_REQUEST_FLUSH);\n+      if (StringUtils.isNotEmpty(flushParam)) {\n+        flush = Boolean.valueOf(flushParam);\n+      }\n+\n+      checkpoint = scmDbStore.getCheckpoint(flush);\n+      if (checkpoint == null || checkpoint.getCheckpointLocation() == null) {\n+        LOG.error(\"Unable to process metadata snapshot request. \" +\n+            \"Checkpoint request returned null.\");\n+        response.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);\n+        return;\n+      }\n+      scmMetrics.setLastCheckpointCreationTimeTaken(\n+          checkpoint.checkpointCreationTimeTaken());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "33b4fd850dbcc2a363b989a0eafe6dd25998a837"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzg1NTkzNQ==", "bodyText": "yup, I will upload new changes.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r477855935", "createdAt": "2020-08-27T01:13:49Z", "author": {"login": "prashantpogde"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/SCMDBCheckpointServlet.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm.server;\n+\n+import javax.servlet.ServletException;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.ScmConfigKeys;\n+import org.apache.hadoop.hdds.scm.container.placement.metrics.SCMMetrics;\n+import org.apache.hadoop.hdds.utils.db.DBCheckpoint;\n+import org.apache.hadoop.hdds.utils.db.DBStore;\n+import org.apache.hadoop.hdfs.util.DataTransferThrottler;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+\n+import org.apache.commons.compress.archivers.ArchiveEntry;\n+import org.apache.commons.compress.archivers.ArchiveOutputStream;\n+import org.apache.commons.compress.archivers.tar.TarArchiveOutputStream;\n+import org.apache.commons.compress.compressors.CompressorException;\n+import org.apache.commons.compress.compressors.CompressorOutputStream;\n+import org.apache.commons.compress.compressors.CompressorStreamFactory;\n+import org.apache.commons.compress.utils.IOUtils;\n+import org.apache.commons.lang3.StringUtils;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_DB_CHECKPOINT_REQUEST_FLUSH;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Provides the current checkpoint Snapshot of the OM DB. (tar.gz)\n+ */\n+public class SCMDBCheckpointServlet extends HttpServlet {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SCMDBCheckpointServlet.class);\n+  private static final long serialVersionUID = 1L;\n+\n+  private transient StorageContainerManager scm;\n+  private transient DBStore scmDbStore;\n+  private transient SCMMetrics scmMetrics;\n+  private transient DataTransferThrottler throttler = null;\n+\n+  @Override\n+  public void init() throws ServletException {\n+\n+    scm = (StorageContainerManager) getServletContext()\n+        .getAttribute(OzoneConsts.SCM_CONTEXT_ATTRIBUTE);\n+\n+    if (scm == null) {\n+      LOG.error(\"Unable to initialize SCMDBCheckpointServlet. SCM is null\");\n+      return;\n+    }\n+\n+    scmDbStore = scm.getScmMetadataStore().getStore();\n+    scmMetrics= scm.getMetrics();\n+\n+    OzoneConfiguration configuration = scm.getConfiguration();\n+    long transferBandwidth = configuration.getLongBytes(\n+        ScmConfigKeys.SCM_DB_CHECKPOINT_TRANSFER_RATE_KEY,\n+        ScmConfigKeys.SCM_DB_CHECKPOINT_TRANSFER_RATE_DEFAULT);\n+\n+    if (transferBandwidth > 0) {\n+      throttler = new DataTransferThrottler(transferBandwidth);\n+    }\n+  }\n+\n+  /**\n+   * Process a GET request for the SCM DB checkpoint snapshot.\n+   *\n+   * @param request  The servlet request we are processing\n+   * @param response The servlet response we are creating\n+   */\n+  @Override\n+  public void doGet(HttpServletRequest request, HttpServletResponse response) {\n+\n+    LOG.info(\"Received request to obtain SCM DB checkpoint snapshot\");\n+    if (scmDbStore == null) {\n+      LOG.error(\n+          \"Unable to process metadata snapshot request. DB Store is null\");\n+      response.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);\n+      return;\n+    }\n+\n+    DBCheckpoint checkpoint = null;\n+    try {\n+\n+      boolean flush = false;\n+      String flushParam =\n+          request.getParameter(OZONE_DB_CHECKPOINT_REQUEST_FLUSH);\n+      if (StringUtils.isNotEmpty(flushParam)) {\n+        flush = Boolean.valueOf(flushParam);\n+      }\n+\n+      checkpoint = scmDbStore.getCheckpoint(flush);\n+      if (checkpoint == null || checkpoint.getCheckpointLocation() == null) {\n+        LOG.error(\"Unable to process metadata snapshot request. \" +\n+            \"Checkpoint request returned null.\");\n+        response.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);\n+        return;\n+      }\n+      scmMetrics.setLastCheckpointCreationTimeTaken(\n+          checkpoint.checkpointCreationTimeTaken());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAyODg3MQ=="}, "originalCommit": {"oid": "33b4fd850dbcc2a363b989a0eafe6dd25998a837"}, "originalPosition": 129}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MjI1NDc5OnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/TestSCMDbCheckpointServlet.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwNDo0MTowMlrOHG7l2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwMToxNDo1MFrOHHuGdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAyOTg0OA==", "bodyText": "If we abstract out the common logic, we don't need 2 different test classes.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r477029848", "createdAt": "2020-08-26T04:41:02Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/TestSCMDbCheckpointServlet.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm;\n+\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletOutputStream;\n+import javax.servlet.WriteListener;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.UUID;\n+\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.container.placement.metrics.SCMMetrics;\n+import org.apache.hadoop.hdds.scm.server.SCMDBCheckpointServlet;\n+import org.apache.hadoop.hdds.utils.db.DBCheckpoint;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+\n+import org.apache.commons.io.FileUtils;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_ACL_ENABLED;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_OPEN_KEY_EXPIRE_THRESHOLD_SECONDS;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_DB_CHECKPOINT_REQUEST_FLUSH;\n+import static org.apache.hadoop.ozone.om.OMDBCheckpointServlet.writeOmDBCheckpointToStream;\n+import org.junit.After;\n+import org.junit.Assert;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.rules.Timeout;\n+import org.mockito.Matchers;\n+import static org.mockito.Mockito.doCallRealMethod;\n+import static org.mockito.Mockito.doNothing;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Class used for testing the OM DB Checkpoint provider servlet.\n+ */\n+public class TestSCMDbCheckpointServlet {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "33b4fd850dbcc2a363b989a0eafe6dd25998a837"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzg1NzM5OA==", "bodyText": "We can still keep two test cases to verify end to end testing for individual servlets.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r477857398", "createdAt": "2020-08-27T01:14:50Z", "author": {"login": "prashantpogde"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/TestSCMDbCheckpointServlet.java", "diffHunk": "@@ -0,0 +1,244 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm;\n+\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletOutputStream;\n+import javax.servlet.WriteListener;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.UUID;\n+\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.container.placement.metrics.SCMMetrics;\n+import org.apache.hadoop.hdds.scm.server.SCMDBCheckpointServlet;\n+import org.apache.hadoop.hdds.utils.db.DBCheckpoint;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+\n+import org.apache.commons.io.FileUtils;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_ACL_ENABLED;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_OPEN_KEY_EXPIRE_THRESHOLD_SECONDS;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_DB_CHECKPOINT_REQUEST_FLUSH;\n+import static org.apache.hadoop.ozone.om.OMDBCheckpointServlet.writeOmDBCheckpointToStream;\n+import org.junit.After;\n+import org.junit.Assert;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.rules.Timeout;\n+import org.mockito.Matchers;\n+import static org.mockito.Mockito.doCallRealMethod;\n+import static org.mockito.Mockito.doNothing;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Class used for testing the OM DB Checkpoint provider servlet.\n+ */\n+public class TestSCMDbCheckpointServlet {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAyOTg0OA=="}, "originalCommit": {"oid": "33b4fd850dbcc2a363b989a0eafe6dd25998a837"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MzUzNjg2OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/ScmConfigKeys.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQxMTozMDoxN1rOHHHzrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yN1QwMToxMTozMVrOHHtwQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzIyOTk5Nw==", "bodyText": "Can you please use Java based configuration API:\nhttps://cwiki.apache.org/confluence/display/HADOOP/Java-based+configuration+API", "url": "https://github.com/apache/ozone/pull/1353#discussion_r477229997", "createdAt": "2020-08-26T11:30:17Z", "author": {"login": "elek"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/ScmConfigKeys.java", "diffHunk": "@@ -352,6 +352,11 @@\n   public static final String HDDS_TRACING_ENABLED = \"hdds.tracing.enabled\";\n   public static final boolean HDDS_TRACING_ENABLED_DEFAULT = false;\n \n+  public static final String SCM_DB_CHECKPOINT_TRANSFER_RATE_KEY =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "33b4fd850dbcc2a363b989a0eafe6dd25998a837"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzg1MTcxNQ==", "bodyText": "yup. I will upload new changes.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r477851715", "createdAt": "2020-08-27T01:11:31Z", "author": {"login": "prashantpogde"}, "path": "hadoop-hdds/common/src/main/java/org/apache/hadoop/hdds/scm/ScmConfigKeys.java", "diffHunk": "@@ -352,6 +352,11 @@\n   public static final String HDDS_TRACING_ENABLED = \"hdds.tracing.enabled\";\n   public static final boolean HDDS_TRACING_ENABLED_DEFAULT = false;\n \n+  public static final String SCM_DB_CHECKPOINT_TRANSFER_RATE_KEY =", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzIyOTk5Nw=="}, "originalCommit": {"oid": "33b4fd850dbcc2a363b989a0eafe6dd25998a837"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzU4NjI5OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/metrics/SCMMetrics.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxOTo1NzozM1rOHJS1DA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOVQwMzo1Mjo0OFrOHJYqHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwNzcyNA==", "bodyText": "Can we make DBCheckpointMetrics as a super class, and move these metrics up to that class? Since the metric names are same in OM and SCM, we can maintain them in one place.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r479507724", "createdAt": "2020-08-28T19:57:33Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/metrics/SCMMetrics.java", "diffHunk": "@@ -52,6 +55,11 @@\n   @Metric private MutableCounterLong containerReportReadCount;\n   @Metric private MutableCounterLong containerReportWriteCount;\n \n+  @Metric private MutableGaugeLong lastCheckpointCreationTimeTaken;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84a16bc2994d5103ecacc9f1908e83736ae8b6dc"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUxOTUzOQ==", "bodyText": "I thought of making it a superclass but we can do it only once and in future if any other feature wanted to do it the same way it can't.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r479519539", "createdAt": "2020-08-28T20:26:56Z", "author": {"login": "prashantpogde"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/metrics/SCMMetrics.java", "diffHunk": "@@ -52,6 +55,11 @@\n   @Metric private MutableCounterLong containerReportReadCount;\n   @Metric private MutableCounterLong containerReportWriteCount;\n \n+  @Metric private MutableGaugeLong lastCheckpointCreationTimeTaken;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwNzcyNA=="}, "originalCommit": {"oid": "84a16bc2994d5103ecacc9f1908e83736ae8b6dc"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYwMzIyOA==", "bodyText": "I made DBCheckpointMetrics a field. jmx metric are still showing up as a field.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r479603228", "createdAt": "2020-08-29T03:52:48Z", "author": {"login": "prashantpogde"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/placement/metrics/SCMMetrics.java", "diffHunk": "@@ -52,6 +55,11 @@\n   @Metric private MutableCounterLong containerReportReadCount;\n   @Metric private MutableCounterLong containerReportWriteCount;\n \n+  @Metric private MutableGaugeLong lastCheckpointCreationTimeTaken;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwNzcyNA=="}, "originalCommit": {"oid": "84a16bc2994d5103ecacc9f1908e83736ae8b6dc"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzU5ODIwOnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/TestSCMDbCheckpointServlet.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMDowMjowN1rOHJS8TA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMDoyNzoxM1rOHJTjpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwOTU4MA==", "bodyText": "This test can be removed. It is already present in TestOMDbCheckpointServlet.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r479509580", "createdAt": "2020-08-28T20:02:07Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/TestSCMDbCheckpointServlet.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm;\n+\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletOutputStream;\n+import javax.servlet.WriteListener;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.UUID;\n+\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.container.placement.metrics.SCMMetrics;\n+import org.apache.hadoop.hdds.scm.server.SCMDBCheckpointServlet;\n+import org.apache.hadoop.hdds.utils.db.DBCheckpoint;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+\n+import org.apache.commons.io.FileUtils;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_ACL_ENABLED;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_OPEN_KEY_EXPIRE_THRESHOLD_SECONDS;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_DB_CHECKPOINT_REQUEST_FLUSH;\n+import static org.apache.hadoop.ozone.om.OMDBCheckpointServlet.writeOmDBCheckpointToStream;\n+import org.junit.After;\n+import org.junit.Assert;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.rules.Timeout;\n+import org.mockito.Matchers;\n+import static org.mockito.Mockito.doCallRealMethod;\n+import static org.mockito.Mockito.doNothing;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Class used for testing the OM DB Checkpoint provider servlet.\n+ */\n+public class TestSCMDbCheckpointServlet {\n+  private MiniOzoneCluster cluster = null;\n+  private SCMMetrics scmMetrics;\n+  private OzoneConfiguration conf;\n+  private String clusterId;\n+  private String scmId;\n+  private String omId;\n+\n+  @Rule\n+  public Timeout timeout = new Timeout(240000);\n+\n+  @Rule\n+  public TemporaryFolder folder = new TemporaryFolder();\n+  /**\n+   * Create a MiniDFSCluster for testing.\n+   * <p>\n+   * Ozone is made active by setting OZONE_ENABLED = true\n+   *\n+   * @throws IOException\n+   */\n+  @Before\n+  public void init() throws Exception {\n+    conf = new OzoneConfiguration();\n+    clusterId = UUID.randomUUID().toString();\n+    scmId = UUID.randomUUID().toString();\n+    omId = UUID.randomUUID().toString();\n+    conf.setBoolean(OZONE_ACL_ENABLED, true);\n+    conf.setInt(OZONE_OPEN_KEY_EXPIRE_THRESHOLD_SECONDS, 2);\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setClusterId(clusterId)\n+        .setScmId(scmId)\n+        .setOmId(omId)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    scmMetrics = cluster.getStorageContainerManager().getMetrics();\n+  }\n+\n+  /**\n+   * Shutdown MiniDFSCluster.\n+   */\n+  @After\n+  public void shutdown() {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Test\n+  public void testDoGet() throws ServletException, IOException {\n+\n+    File tempFile = null;\n+    try {\n+      SCMDBCheckpointServlet scmDbCheckpointServletMock =\n+          mock(SCMDBCheckpointServlet.class);\n+\n+      doCallRealMethod().when(scmDbCheckpointServletMock).init();\n+      doCallRealMethod().when(scmDbCheckpointServletMock).initialize(\n+          cluster.getStorageContainerManager().getScmMetadataStore().getStore(),\n+          cluster.getStorageContainerManager().getMetrics());\n+\n+      HttpServletRequest requestMock = mock(HttpServletRequest.class);\n+      HttpServletResponse responseMock = mock(HttpServletResponse.class);\n+\n+      ServletContext servletContextMock = mock(ServletContext.class);\n+      when(scmDbCheckpointServletMock.getServletContext())\n+          .thenReturn(servletContextMock);\n+\n+      when(servletContextMock.getAttribute(OzoneConsts.SCM_CONTEXT_ATTRIBUTE))\n+          .thenReturn(cluster.getStorageContainerManager());\n+      when(requestMock.getParameter(OZONE_DB_CHECKPOINT_REQUEST_FLUSH))\n+          .thenReturn(\"true\");\n+      doNothing().when(responseMock).setContentType(\"application/x-tgz\");\n+      doNothing().when(responseMock).setHeader(Matchers.anyString(),\n+          Matchers.anyString());\n+\n+      tempFile = File.createTempFile(\"testDoGet_\" + System\n+          .currentTimeMillis(), \".tar.gz\");\n+\n+      FileOutputStream fileOutputStream = new FileOutputStream(tempFile);\n+      when(responseMock.getOutputStream()).thenReturn(\n+          new ServletOutputStream() {\n+            @Override\n+            public boolean isReady() {\n+              return true;\n+            }\n+\n+            @Override\n+            public void setWriteListener(WriteListener writeListener) {\n+            }\n+\n+            @Override\n+            public void write(int b) throws IOException {\n+              fileOutputStream.write(b);\n+            }\n+          });\n+\n+      doCallRealMethod().when(scmDbCheckpointServletMock).doGet(requestMock,\n+          responseMock);\n+\n+      scmDbCheckpointServletMock.init();\n+      long initialCheckpointCount = scmMetrics.getNumCheckpoints();\n+\n+      scmDbCheckpointServletMock.doGet(requestMock, responseMock);\n+\n+      Assert.assertTrue(tempFile.length() > 0);\n+      Assert.assertTrue(\n+          scmMetrics.getLastCheckpointCreationTimeTaken() > 0);\n+      Assert.assertTrue(\n+          scmMetrics.getLastCheckpointStreamingTimeTaken() > 0);\n+      Assert.assertTrue(scmMetrics.getNumCheckpoints() >\n+          initialCheckpointCount);\n+    } finally {\n+      FileUtils.deleteQuietly(tempFile);\n+    }\n+\n+  }\n+\n+  @Test\n+  public void testWriteCheckpointToOutputStream() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84a16bc2994d5103ecacc9f1908e83736ae8b6dc"}, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUxOTY1NQ==", "bodyText": "yup, will remove.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r479519655", "createdAt": "2020-08-28T20:27:13Z", "author": {"login": "prashantpogde"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/TestSCMDbCheckpointServlet.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm;\n+\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletOutputStream;\n+import javax.servlet.WriteListener;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.UUID;\n+\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.container.placement.metrics.SCMMetrics;\n+import org.apache.hadoop.hdds.scm.server.SCMDBCheckpointServlet;\n+import org.apache.hadoop.hdds.utils.db.DBCheckpoint;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+\n+import org.apache.commons.io.FileUtils;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_ACL_ENABLED;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_OPEN_KEY_EXPIRE_THRESHOLD_SECONDS;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_DB_CHECKPOINT_REQUEST_FLUSH;\n+import static org.apache.hadoop.ozone.om.OMDBCheckpointServlet.writeOmDBCheckpointToStream;\n+import org.junit.After;\n+import org.junit.Assert;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.rules.Timeout;\n+import org.mockito.Matchers;\n+import static org.mockito.Mockito.doCallRealMethod;\n+import static org.mockito.Mockito.doNothing;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Class used for testing the OM DB Checkpoint provider servlet.\n+ */\n+public class TestSCMDbCheckpointServlet {\n+  private MiniOzoneCluster cluster = null;\n+  private SCMMetrics scmMetrics;\n+  private OzoneConfiguration conf;\n+  private String clusterId;\n+  private String scmId;\n+  private String omId;\n+\n+  @Rule\n+  public Timeout timeout = new Timeout(240000);\n+\n+  @Rule\n+  public TemporaryFolder folder = new TemporaryFolder();\n+  /**\n+   * Create a MiniDFSCluster for testing.\n+   * <p>\n+   * Ozone is made active by setting OZONE_ENABLED = true\n+   *\n+   * @throws IOException\n+   */\n+  @Before\n+  public void init() throws Exception {\n+    conf = new OzoneConfiguration();\n+    clusterId = UUID.randomUUID().toString();\n+    scmId = UUID.randomUUID().toString();\n+    omId = UUID.randomUUID().toString();\n+    conf.setBoolean(OZONE_ACL_ENABLED, true);\n+    conf.setInt(OZONE_OPEN_KEY_EXPIRE_THRESHOLD_SECONDS, 2);\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setClusterId(clusterId)\n+        .setScmId(scmId)\n+        .setOmId(omId)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    scmMetrics = cluster.getStorageContainerManager().getMetrics();\n+  }\n+\n+  /**\n+   * Shutdown MiniDFSCluster.\n+   */\n+  @After\n+  public void shutdown() {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Test\n+  public void testDoGet() throws ServletException, IOException {\n+\n+    File tempFile = null;\n+    try {\n+      SCMDBCheckpointServlet scmDbCheckpointServletMock =\n+          mock(SCMDBCheckpointServlet.class);\n+\n+      doCallRealMethod().when(scmDbCheckpointServletMock).init();\n+      doCallRealMethod().when(scmDbCheckpointServletMock).initialize(\n+          cluster.getStorageContainerManager().getScmMetadataStore().getStore(),\n+          cluster.getStorageContainerManager().getMetrics());\n+\n+      HttpServletRequest requestMock = mock(HttpServletRequest.class);\n+      HttpServletResponse responseMock = mock(HttpServletResponse.class);\n+\n+      ServletContext servletContextMock = mock(ServletContext.class);\n+      when(scmDbCheckpointServletMock.getServletContext())\n+          .thenReturn(servletContextMock);\n+\n+      when(servletContextMock.getAttribute(OzoneConsts.SCM_CONTEXT_ATTRIBUTE))\n+          .thenReturn(cluster.getStorageContainerManager());\n+      when(requestMock.getParameter(OZONE_DB_CHECKPOINT_REQUEST_FLUSH))\n+          .thenReturn(\"true\");\n+      doNothing().when(responseMock).setContentType(\"application/x-tgz\");\n+      doNothing().when(responseMock).setHeader(Matchers.anyString(),\n+          Matchers.anyString());\n+\n+      tempFile = File.createTempFile(\"testDoGet_\" + System\n+          .currentTimeMillis(), \".tar.gz\");\n+\n+      FileOutputStream fileOutputStream = new FileOutputStream(tempFile);\n+      when(responseMock.getOutputStream()).thenReturn(\n+          new ServletOutputStream() {\n+            @Override\n+            public boolean isReady() {\n+              return true;\n+            }\n+\n+            @Override\n+            public void setWriteListener(WriteListener writeListener) {\n+            }\n+\n+            @Override\n+            public void write(int b) throws IOException {\n+              fileOutputStream.write(b);\n+            }\n+          });\n+\n+      doCallRealMethod().when(scmDbCheckpointServletMock).doGet(requestMock,\n+          responseMock);\n+\n+      scmDbCheckpointServletMock.init();\n+      long initialCheckpointCount = scmMetrics.getNumCheckpoints();\n+\n+      scmDbCheckpointServletMock.doGet(requestMock, responseMock);\n+\n+      Assert.assertTrue(tempFile.length() > 0);\n+      Assert.assertTrue(\n+          scmMetrics.getLastCheckpointCreationTimeTaken() > 0);\n+      Assert.assertTrue(\n+          scmMetrics.getLastCheckpointStreamingTimeTaken() > 0);\n+      Assert.assertTrue(scmMetrics.getNumCheckpoints() >\n+          initialCheckpointCount);\n+    } finally {\n+      FileUtils.deleteQuietly(tempFile);\n+    }\n+\n+  }\n+\n+  @Test\n+  public void testWriteCheckpointToOutputStream() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwOTU4MA=="}, "originalCommit": {"oid": "84a16bc2994d5103ecacc9f1908e83736ae8b6dc"}, "originalPosition": 184}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzYwMDczOnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/TestSCMDbCheckpointServlet.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMDowMzowMFrOHJS9vw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMDoyNzozMVrOHJTkDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwOTk1MQ==", "bodyText": "Class can be removed.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r479509951", "createdAt": "2020-08-28T20:03:00Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/TestSCMDbCheckpointServlet.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm;\n+\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletOutputStream;\n+import javax.servlet.WriteListener;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.UUID;\n+\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.container.placement.metrics.SCMMetrics;\n+import org.apache.hadoop.hdds.scm.server.SCMDBCheckpointServlet;\n+import org.apache.hadoop.hdds.utils.db.DBCheckpoint;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+\n+import org.apache.commons.io.FileUtils;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_ACL_ENABLED;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_OPEN_KEY_EXPIRE_THRESHOLD_SECONDS;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_DB_CHECKPOINT_REQUEST_FLUSH;\n+import static org.apache.hadoop.ozone.om.OMDBCheckpointServlet.writeOmDBCheckpointToStream;\n+import org.junit.After;\n+import org.junit.Assert;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.rules.Timeout;\n+import org.mockito.Matchers;\n+import static org.mockito.Mockito.doCallRealMethod;\n+import static org.mockito.Mockito.doNothing;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Class used for testing the OM DB Checkpoint provider servlet.\n+ */\n+public class TestSCMDbCheckpointServlet {\n+  private MiniOzoneCluster cluster = null;\n+  private SCMMetrics scmMetrics;\n+  private OzoneConfiguration conf;\n+  private String clusterId;\n+  private String scmId;\n+  private String omId;\n+\n+  @Rule\n+  public Timeout timeout = new Timeout(240000);\n+\n+  @Rule\n+  public TemporaryFolder folder = new TemporaryFolder();\n+  /**\n+   * Create a MiniDFSCluster for testing.\n+   * <p>\n+   * Ozone is made active by setting OZONE_ENABLED = true\n+   *\n+   * @throws IOException\n+   */\n+  @Before\n+  public void init() throws Exception {\n+    conf = new OzoneConfiguration();\n+    clusterId = UUID.randomUUID().toString();\n+    scmId = UUID.randomUUID().toString();\n+    omId = UUID.randomUUID().toString();\n+    conf.setBoolean(OZONE_ACL_ENABLED, true);\n+    conf.setInt(OZONE_OPEN_KEY_EXPIRE_THRESHOLD_SECONDS, 2);\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setClusterId(clusterId)\n+        .setScmId(scmId)\n+        .setOmId(omId)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    scmMetrics = cluster.getStorageContainerManager().getMetrics();\n+  }\n+\n+  /**\n+   * Shutdown MiniDFSCluster.\n+   */\n+  @After\n+  public void shutdown() {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Test\n+  public void testDoGet() throws ServletException, IOException {\n+\n+    File tempFile = null;\n+    try {\n+      SCMDBCheckpointServlet scmDbCheckpointServletMock =\n+          mock(SCMDBCheckpointServlet.class);\n+\n+      doCallRealMethod().when(scmDbCheckpointServletMock).init();\n+      doCallRealMethod().when(scmDbCheckpointServletMock).initialize(\n+          cluster.getStorageContainerManager().getScmMetadataStore().getStore(),\n+          cluster.getStorageContainerManager().getMetrics());\n+\n+      HttpServletRequest requestMock = mock(HttpServletRequest.class);\n+      HttpServletResponse responseMock = mock(HttpServletResponse.class);\n+\n+      ServletContext servletContextMock = mock(ServletContext.class);\n+      when(scmDbCheckpointServletMock.getServletContext())\n+          .thenReturn(servletContextMock);\n+\n+      when(servletContextMock.getAttribute(OzoneConsts.SCM_CONTEXT_ATTRIBUTE))\n+          .thenReturn(cluster.getStorageContainerManager());\n+      when(requestMock.getParameter(OZONE_DB_CHECKPOINT_REQUEST_FLUSH))\n+          .thenReturn(\"true\");\n+      doNothing().when(responseMock).setContentType(\"application/x-tgz\");\n+      doNothing().when(responseMock).setHeader(Matchers.anyString(),\n+          Matchers.anyString());\n+\n+      tempFile = File.createTempFile(\"testDoGet_\" + System\n+          .currentTimeMillis(), \".tar.gz\");\n+\n+      FileOutputStream fileOutputStream = new FileOutputStream(tempFile);\n+      when(responseMock.getOutputStream()).thenReturn(\n+          new ServletOutputStream() {\n+            @Override\n+            public boolean isReady() {\n+              return true;\n+            }\n+\n+            @Override\n+            public void setWriteListener(WriteListener writeListener) {\n+            }\n+\n+            @Override\n+            public void write(int b) throws IOException {\n+              fileOutputStream.write(b);\n+            }\n+          });\n+\n+      doCallRealMethod().when(scmDbCheckpointServletMock).doGet(requestMock,\n+          responseMock);\n+\n+      scmDbCheckpointServletMock.init();\n+      long initialCheckpointCount = scmMetrics.getNumCheckpoints();\n+\n+      scmDbCheckpointServletMock.doGet(requestMock, responseMock);\n+\n+      Assert.assertTrue(tempFile.length() > 0);\n+      Assert.assertTrue(\n+          scmMetrics.getLastCheckpointCreationTimeTaken() > 0);\n+      Assert.assertTrue(\n+          scmMetrics.getLastCheckpointStreamingTimeTaken() > 0);\n+      Assert.assertTrue(scmMetrics.getNumCheckpoints() >\n+          initialCheckpointCount);\n+    } finally {\n+      FileUtils.deleteQuietly(tempFile);\n+    }\n+\n+  }\n+\n+  @Test\n+  public void testWriteCheckpointToOutputStream() throws Exception {\n+\n+    FileInputStream fis = null;\n+    FileOutputStream fos = null;\n+\n+    try {\n+      String testDirName = folder.newFolder().getAbsolutePath();\n+      File file = new File(testDirName + \"/temp1.txt\");\n+      FileWriter writer = new FileWriter(file);\n+      writer.write(\"Test data 1\");\n+      writer.close();\n+\n+      file = new File(testDirName + \"/temp2.txt\");\n+      writer = new FileWriter(file);\n+      writer.write(\"Test data 2\");\n+      writer.close();\n+\n+      File outputFile =\n+          new File(Paths.get(testDirName, \"output_file.tgz\").toString());\n+      TestDBCheckpoint dbCheckpoint = new TestDBCheckpoint(\n+          Paths.get(testDirName));\n+      writeOmDBCheckpointToStream(dbCheckpoint,\n+          new FileOutputStream(outputFile));\n+      assertNotNull(outputFile);\n+    } finally {\n+      IOUtils.closeStream(fis);\n+      IOUtils.closeStream(fos);\n+    }\n+  }\n+}\n+\n+class TestDBCheckpoint implements DBCheckpoint {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84a16bc2994d5103ecacc9f1908e83736ae8b6dc"}, "originalPosition": 215}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUxOTc1OA==", "bodyText": "yup.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r479519758", "createdAt": "2020-08-28T20:27:31Z", "author": {"login": "prashantpogde"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/hdds/scm/TestSCMDbCheckpointServlet.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.scm;\n+\n+import javax.servlet.ServletContext;\n+import javax.servlet.ServletException;\n+import javax.servlet.ServletOutputStream;\n+import javax.servlet.WriteListener;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.FileWriter;\n+import java.io.IOException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.UUID;\n+\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.hdds.scm.container.placement.metrics.SCMMetrics;\n+import org.apache.hadoop.hdds.scm.server.SCMDBCheckpointServlet;\n+import org.apache.hadoop.hdds.utils.db.DBCheckpoint;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.ozone.MiniOzoneCluster;\n+import org.apache.hadoop.ozone.OzoneConsts;\n+\n+import org.apache.commons.io.FileUtils;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_ACL_ENABLED;\n+import static org.apache.hadoop.ozone.OzoneConfigKeys.OZONE_OPEN_KEY_EXPIRE_THRESHOLD_SECONDS;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_DB_CHECKPOINT_REQUEST_FLUSH;\n+import static org.apache.hadoop.ozone.om.OMDBCheckpointServlet.writeOmDBCheckpointToStream;\n+import org.junit.After;\n+import org.junit.Assert;\n+import static org.junit.Assert.assertNotNull;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.rules.Timeout;\n+import org.mockito.Matchers;\n+import static org.mockito.Mockito.doCallRealMethod;\n+import static org.mockito.Mockito.doNothing;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+/**\n+ * Class used for testing the OM DB Checkpoint provider servlet.\n+ */\n+public class TestSCMDbCheckpointServlet {\n+  private MiniOzoneCluster cluster = null;\n+  private SCMMetrics scmMetrics;\n+  private OzoneConfiguration conf;\n+  private String clusterId;\n+  private String scmId;\n+  private String omId;\n+\n+  @Rule\n+  public Timeout timeout = new Timeout(240000);\n+\n+  @Rule\n+  public TemporaryFolder folder = new TemporaryFolder();\n+  /**\n+   * Create a MiniDFSCluster for testing.\n+   * <p>\n+   * Ozone is made active by setting OZONE_ENABLED = true\n+   *\n+   * @throws IOException\n+   */\n+  @Before\n+  public void init() throws Exception {\n+    conf = new OzoneConfiguration();\n+    clusterId = UUID.randomUUID().toString();\n+    scmId = UUID.randomUUID().toString();\n+    omId = UUID.randomUUID().toString();\n+    conf.setBoolean(OZONE_ACL_ENABLED, true);\n+    conf.setInt(OZONE_OPEN_KEY_EXPIRE_THRESHOLD_SECONDS, 2);\n+    cluster = MiniOzoneCluster.newBuilder(conf)\n+        .setClusterId(clusterId)\n+        .setScmId(scmId)\n+        .setOmId(omId)\n+        .build();\n+    cluster.waitForClusterToBeReady();\n+    scmMetrics = cluster.getStorageContainerManager().getMetrics();\n+  }\n+\n+  /**\n+   * Shutdown MiniDFSCluster.\n+   */\n+  @After\n+  public void shutdown() {\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n+  }\n+\n+  @Test\n+  public void testDoGet() throws ServletException, IOException {\n+\n+    File tempFile = null;\n+    try {\n+      SCMDBCheckpointServlet scmDbCheckpointServletMock =\n+          mock(SCMDBCheckpointServlet.class);\n+\n+      doCallRealMethod().when(scmDbCheckpointServletMock).init();\n+      doCallRealMethod().when(scmDbCheckpointServletMock).initialize(\n+          cluster.getStorageContainerManager().getScmMetadataStore().getStore(),\n+          cluster.getStorageContainerManager().getMetrics());\n+\n+      HttpServletRequest requestMock = mock(HttpServletRequest.class);\n+      HttpServletResponse responseMock = mock(HttpServletResponse.class);\n+\n+      ServletContext servletContextMock = mock(ServletContext.class);\n+      when(scmDbCheckpointServletMock.getServletContext())\n+          .thenReturn(servletContextMock);\n+\n+      when(servletContextMock.getAttribute(OzoneConsts.SCM_CONTEXT_ATTRIBUTE))\n+          .thenReturn(cluster.getStorageContainerManager());\n+      when(requestMock.getParameter(OZONE_DB_CHECKPOINT_REQUEST_FLUSH))\n+          .thenReturn(\"true\");\n+      doNothing().when(responseMock).setContentType(\"application/x-tgz\");\n+      doNothing().when(responseMock).setHeader(Matchers.anyString(),\n+          Matchers.anyString());\n+\n+      tempFile = File.createTempFile(\"testDoGet_\" + System\n+          .currentTimeMillis(), \".tar.gz\");\n+\n+      FileOutputStream fileOutputStream = new FileOutputStream(tempFile);\n+      when(responseMock.getOutputStream()).thenReturn(\n+          new ServletOutputStream() {\n+            @Override\n+            public boolean isReady() {\n+              return true;\n+            }\n+\n+            @Override\n+            public void setWriteListener(WriteListener writeListener) {\n+            }\n+\n+            @Override\n+            public void write(int b) throws IOException {\n+              fileOutputStream.write(b);\n+            }\n+          });\n+\n+      doCallRealMethod().when(scmDbCheckpointServletMock).doGet(requestMock,\n+          responseMock);\n+\n+      scmDbCheckpointServletMock.init();\n+      long initialCheckpointCount = scmMetrics.getNumCheckpoints();\n+\n+      scmDbCheckpointServletMock.doGet(requestMock, responseMock);\n+\n+      Assert.assertTrue(tempFile.length() > 0);\n+      Assert.assertTrue(\n+          scmMetrics.getLastCheckpointCreationTimeTaken() > 0);\n+      Assert.assertTrue(\n+          scmMetrics.getLastCheckpointStreamingTimeTaken() > 0);\n+      Assert.assertTrue(scmMetrics.getNumCheckpoints() >\n+          initialCheckpointCount);\n+    } finally {\n+      FileUtils.deleteQuietly(tempFile);\n+    }\n+\n+  }\n+\n+  @Test\n+  public void testWriteCheckpointToOutputStream() throws Exception {\n+\n+    FileInputStream fis = null;\n+    FileOutputStream fos = null;\n+\n+    try {\n+      String testDirName = folder.newFolder().getAbsolutePath();\n+      File file = new File(testDirName + \"/temp1.txt\");\n+      FileWriter writer = new FileWriter(file);\n+      writer.write(\"Test data 1\");\n+      writer.close();\n+\n+      file = new File(testDirName + \"/temp2.txt\");\n+      writer = new FileWriter(file);\n+      writer.write(\"Test data 2\");\n+      writer.close();\n+\n+      File outputFile =\n+          new File(Paths.get(testDirName, \"output_file.tgz\").toString());\n+      TestDBCheckpoint dbCheckpoint = new TestDBCheckpoint(\n+          Paths.get(testDirName));\n+      writeOmDBCheckpointToStream(dbCheckpoint,\n+          new FileOutputStream(outputFile));\n+      assertNotNull(outputFile);\n+    } finally {\n+      IOUtils.closeStream(fis);\n+      IOUtils.closeStream(fos);\n+    }\n+  }\n+}\n+\n+class TestDBCheckpoint implements DBCheckpoint {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUwOTk1MQ=="}, "originalCommit": {"oid": "84a16bc2994d5103ecacc9f1908e83736ae8b6dc"}, "originalPosition": 215}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk5NzYwMTIyOnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOMDbCheckpointServlet.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMDowMzoxM1rOHJS-FA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMDoyODoyMlrOHJTlbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUxMDAzNg==", "bodyText": "Why do we need to increase timeout here?\nInstead, since only 1 of the 2 tests need the MiniOzoneCluster, we can change the cluster init to @BeforeClass or pull it into the test method. We can also add .setNumDatanodes(1) since we do not need 3 DNs. On my local host, I can see a significant perf improvement.\n(Similarly for the corresponding test)", "url": "https://github.com/apache/ozone/pull/1353#discussion_r479510036", "createdAt": "2020-08-28T20:03:13Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOMDbCheckpointServlet.java", "diffHunk": "@@ -70,7 +70,7 @@\n   private String omId;\n \n   @Rule\n-  public Timeout timeout = new Timeout(60000);\n+  public Timeout timeout = new Timeout(240000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "84a16bc2994d5103ecacc9f1908e83736ae8b6dc"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUyMDExMA==", "bodyText": "yup, will do. perhaps my mac is slow, it takes 2-3 minutes for mini ozone cluster to come up.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r479520110", "createdAt": "2020-08-28T20:28:22Z", "author": {"login": "prashantpogde"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOMDbCheckpointServlet.java", "diffHunk": "@@ -70,7 +70,7 @@\n   private String omId;\n \n   @Rule\n-  public Timeout timeout = new Timeout(60000);\n+  public Timeout timeout = new Timeout(240000);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUxMDAzNg=="}, "originalCommit": {"oid": "84a16bc2994d5103ecacc9f1908e83736ae8b6dc"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwMzY1NDk3OnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/DBCheckpointServlet.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxOTo1MjozMFrOHKHCTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxOTo1MjozMFrOHKHCTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDM2MzA4Ng==", "bodyText": "nit. writeOmDBCheckpointToStream --> writeDBCheckpointToStream", "url": "https://github.com/apache/ozone/pull/1353#discussion_r480363086", "createdAt": "2020-08-31T19:52:30Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/DBCheckpointServlet.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.utils;\n+\n+import javax.servlet.ServletException;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.apache.hadoop.hdds.utils.db.DBCheckpoint;\n+import org.apache.hadoop.hdds.utils.db.DBStore;\n+\n+import org.apache.commons.compress.archivers.ArchiveEntry;\n+import org.apache.commons.compress.archivers.ArchiveOutputStream;\n+import org.apache.commons.compress.archivers.tar.TarArchiveOutputStream;\n+import org.apache.commons.compress.compressors.CompressorException;\n+import org.apache.commons.compress.compressors.CompressorOutputStream;\n+import org.apache.commons.compress.compressors.CompressorStreamFactory;\n+import org.apache.commons.compress.utils.IOUtils;\n+import org.apache.commons.lang3.StringUtils;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_DB_CHECKPOINT_REQUEST_FLUSH;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Provides the current checkpoint Snapshot of the OM/SCM DB. (tar.gz)\n+ */\n+public class DBCheckpointServlet extends HttpServlet {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(DBCheckpointServlet.class);\n+  private static final long serialVersionUID = 1L;\n+\n+  private transient DBStore dbStore;\n+  private transient DBCheckpointMetrics dbMetrics;\n+\n+  public void initialize(DBStore store, DBCheckpointMetrics metrics)\n+      throws ServletException {\n+\n+    dbStore = store;\n+    dbMetrics = metrics;\n+    if (dbStore == null) {\n+      LOG.error(\n+          \"Unable to set metadata snapshot request. DB Store is null\");\n+    }\n+  }\n+\n+  /**\n+   * Process a GET request for the DB checkpoint snapshot.\n+   *\n+   * @param request  The servlet request we are processing\n+   * @param response The servlet response we are creating\n+   */\n+  @Override\n+  public void doGet(HttpServletRequest request, HttpServletResponse response) {\n+\n+    LOG.info(\"Received request to obtain DB checkpoint snapshot\");\n+    if (dbStore == null) {\n+      LOG.error(\n+          \"Unable to process metadata snapshot request. DB Store is null\");\n+      response.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);\n+      return;\n+    }\n+\n+    DBCheckpoint checkpoint = null;\n+    try {\n+\n+      boolean flush = false;\n+      String flushParam =\n+          request.getParameter(OZONE_DB_CHECKPOINT_REQUEST_FLUSH);\n+      if (StringUtils.isNotEmpty(flushParam)) {\n+        flush = Boolean.valueOf(flushParam);\n+      }\n+\n+      checkpoint = dbStore.getCheckpoint(flush);\n+      if (checkpoint == null || checkpoint.getCheckpointLocation() == null) {\n+        LOG.error(\"Unable to process metadata snapshot request. \" +\n+            \"Checkpoint request returned null.\");\n+        response.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);\n+        return;\n+      }\n+      dbMetrics.setLastCheckpointCreationTimeTaken(\n+          checkpoint.checkpointCreationTimeTaken());\n+\n+      Path file = checkpoint.getCheckpointLocation().getFileName();\n+      if (file == null) {\n+        return;\n+      }\n+      response.setContentType(\"application/x-tgz\");\n+      response.setHeader(\"Content-Disposition\",\n+          \"attachment; filename=\\\"\" +\n+               file.toString() + \".tgz\\\"\");\n+\n+      Instant start = Instant.now();\n+      writeOmDBCheckpointToStream(checkpoint,\n+          response.getOutputStream());\n+      Instant end = Instant.now();\n+\n+      long duration = Duration.between(start, end).toMillis();\n+      LOG.info(\"Time taken to write the checkpoint to response output \" +\n+          \"stream: {} milliseconds\", duration);\n+      dbMetrics.setLastCheckpointStreamingTimeTaken(duration);\n+      dbMetrics.incNumCheckpoints();\n+    } catch (Exception e) {\n+      LOG.error(\n+          \"Unable to process metadata snapshot request. \", e);\n+      response.setStatus(HttpServletResponse.SC_INTERNAL_SERVER_ERROR);\n+      dbMetrics.incNumCheckpointFails();\n+    } finally {\n+      if (checkpoint != null) {\n+        try {\n+          checkpoint.cleanupCheckpoint();\n+        } catch (IOException e) {\n+          LOG.error(\"Error trying to clean checkpoint at {} .\",\n+              checkpoint.getCheckpointLocation().toString());\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Write DB Checkpoint to an output stream as a compressed file (tgz).\n+   *\n+   * @param checkpoint  checkpoint file\n+   * @param destination desination output stream.\n+   * @throws IOException\n+   */\n+  public static void writeOmDBCheckpointToStream(DBCheckpoint checkpoint,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "937212a11988918e8a98d1ac750211ac231ced3f"}, "originalPosition": 155}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAwMzY1OTQ5OnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OMMetrics.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxOTo1Mzo0N1rOHKHE7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxOTo1Mzo0N1rOHKHE7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDM2Mzc1Nw==", "bodyText": "nit. move method down (after constructor)", "url": "https://github.com/apache/ozone/pull/1353#discussion_r480363757", "createdAt": "2020-08-31T19:53:47Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OMMetrics.java", "diffHunk": "@@ -137,7 +129,14 @@\n   private @Metric MutableCounterLong numListMultipartUploadFails;\n   private @Metric MutableCounterLong numListMultipartUploads;\n \n+  private DBCheckpointMetrics dbCheckpointMetrics;\n+\n+  public DBCheckpointMetrics getDBCheckpointMetrics() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "937212a11988918e8a98d1ac750211ac231ced3f"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxMDE1ODUyOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/DBCheckpointServlet.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMDozODo0OVrOHLHW9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMTowNTo1MFrOHLIOYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQxNjk0OA==", "bodyText": "There are many overlaps between OMDBCheckpointServlet and OMDBCheckpointServlet. Do we have a followup JIRA to refactor OMDBCheckpointServlet to use OMDBCheckpointServlet?", "url": "https://github.com/apache/ozone/pull/1353#discussion_r481416948", "createdAt": "2020-09-01T20:38:49Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/DBCheckpointServlet.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.utils;\n+\n+import javax.servlet.ServletException;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.apache.hadoop.hdds.utils.db.DBCheckpoint;\n+import org.apache.hadoop.hdds.utils.db.DBStore;\n+\n+import org.apache.commons.compress.archivers.ArchiveEntry;\n+import org.apache.commons.compress.archivers.ArchiveOutputStream;\n+import org.apache.commons.compress.archivers.tar.TarArchiveOutputStream;\n+import org.apache.commons.compress.compressors.CompressorException;\n+import org.apache.commons.compress.compressors.CompressorOutputStream;\n+import org.apache.commons.compress.compressors.CompressorStreamFactory;\n+import org.apache.commons.compress.utils.IOUtils;\n+import org.apache.commons.lang3.StringUtils;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_DB_CHECKPOINT_REQUEST_FLUSH;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Provides the current checkpoint Snapshot of the OM/SCM DB. (tar.gz)\n+ */\n+public class DBCheckpointServlet extends HttpServlet {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "831127056f4cec0a81c86b966d17dbe95a99835f"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQzMTEzOQ==", "bodyText": "I have refactored OMDBCheckpointServlet in this request itself.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r481431139", "createdAt": "2020-09-01T21:05:50Z", "author": {"login": "prashantpogde"}, "path": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/utils/DBCheckpointServlet.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hdds.utils;\n+\n+import javax.servlet.ServletException;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.time.Duration;\n+import java.time.Instant;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.apache.hadoop.hdds.utils.db.DBCheckpoint;\n+import org.apache.hadoop.hdds.utils.db.DBStore;\n+\n+import org.apache.commons.compress.archivers.ArchiveEntry;\n+import org.apache.commons.compress.archivers.ArchiveOutputStream;\n+import org.apache.commons.compress.archivers.tar.TarArchiveOutputStream;\n+import org.apache.commons.compress.compressors.CompressorException;\n+import org.apache.commons.compress.compressors.CompressorOutputStream;\n+import org.apache.commons.compress.compressors.CompressorStreamFactory;\n+import org.apache.commons.compress.utils.IOUtils;\n+import org.apache.commons.lang3.StringUtils;\n+import static org.apache.hadoop.ozone.OzoneConsts.OZONE_DB_CHECKPOINT_REQUEST_FLUSH;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * Provides the current checkpoint Snapshot of the OM/SCM DB. (tar.gz)\n+ */\n+public class DBCheckpointServlet extends HttpServlet {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQxNjk0OA=="}, "originalCommit": {"oid": "831127056f4cec0a81c86b966d17dbe95a99835f"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxNTE1MTIxOnYy", "diffSide": "RIGHT", "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNzowMToyM1rOHL4onA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMi0xOVQwOToyOToxN1rOIoOHKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyNDI4NA==", "bodyText": "Passing half initialized this to another constructor can cause tricky timing related bugs that are hard to debug at runtime. For example, the metrics are registered on line 368 after line 335.\nBased on the servlet code, we can minimize the dependency with just MetaStore and the DBCheckPointMetrics here, and set/get them into/from the webAppContext.\ninitialize(scm.getScmMetadataStore().getStore(),\n    scm.getMetrics().getDBCheckpointMetrics());", "url": "https://github.com/apache/ozone/pull/1353#discussion_r482224284", "createdAt": "2020-09-02T17:01:23Z", "author": {"login": "xiaoyuyao"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java", "diffHunk": "@@ -333,7 +332,7 @@ public StorageContainerManager(OzoneConfiguration conf,\n         eventQueue);\n     blockProtocolServer = new SCMBlockProtocolServer(conf, this);\n     clientProtocolServer = new SCMClientProtocolServer(conf, this);\n-    httpServer = new StorageContainerManagerHttpServer(conf);\n+    httpServer = new StorageContainerManagerHttpServer(conf, this);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "831127056f4cec0a81c86b966d17dbe95a99835f"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3MzkyNjY5NQ==", "bodyText": "Hi Xiaoyu, thanks for the comment. I get the point that we should avoid passing half inited objects to other constructors. But I didn't quite get this:\n\nBased on the servlet code, we can minimize the dependency with just MetaStore and the DBCheckPointMetrics here, and set/get them into/from the webAppContext.\n\nCould you elaborate a bit on this? Thanks!", "url": "https://github.com/apache/ozone/pull/1353#discussion_r573926695", "createdAt": "2021-02-10T17:26:22Z", "author": {"login": "smengcl"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java", "diffHunk": "@@ -333,7 +332,7 @@ public StorageContainerManager(OzoneConfiguration conf,\n         eventQueue);\n     blockProtocolServer = new SCMBlockProtocolServer(conf, this);\n     clientProtocolServer = new SCMClientProtocolServer(conf, this);\n-    httpServer = new StorageContainerManagerHttpServer(conf);\n+    httpServer = new StorageContainerManagerHttpServer(conf, this);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyNDI4NA=="}, "originalCommit": {"oid": "831127056f4cec0a81c86b966d17dbe95a99835f"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU3OTA0NTE2Mg==", "bodyText": "@xiaoyuyao I decided to move httpServer = new StorageContainerManagerHttpServer(conf, this); from SCM constructor to start() -- the same approach as we already do for OM:\n\n  \n    \n      ozone/hadoop-ozone/ozone-manager/src/main/java/org/apache/hadoop/ozone/om/OzoneManager.java\n    \n    \n         Line 1187\n      in\n      d964cc9\n    \n    \n    \n    \n\n        \n          \n           httpServer = new OzoneManagerHttpServer(configuration, this); \n        \n    \n  \n\n\nThis should avoid the race condition. Pls take a look when you are available. :)\nI have tested locally that http://127.0.0.1:9876/dbCheckpoint downloads the SCM DB archive for me.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r579045162", "createdAt": "2021-02-19T09:29:17Z", "author": {"login": "smengcl"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/server/StorageContainerManager.java", "diffHunk": "@@ -333,7 +332,7 @@ public StorageContainerManager(OzoneConfiguration conf,\n         eventQueue);\n     blockProtocolServer = new SCMBlockProtocolServer(conf, this);\n     clientProtocolServer = new SCMClientProtocolServer(conf, this);\n-    httpServer = new StorageContainerManagerHttpServer(conf);\n+    httpServer = new StorageContainerManagerHttpServer(conf, this);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjIyNDI4NA=="}, "originalCommit": {"oid": "831127056f4cec0a81c86b966d17dbe95a99835f"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAxNTM2MTAwOnYy", "diffSide": "RIGHT", "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOMDbCheckpointServlet.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQxNzo1NToxN1rOHL6xaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wM1QyMTowNDozN1rOHM3cSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI1OTMwNA==", "bodyText": "Can we revert the timeout change?", "url": "https://github.com/apache/ozone/pull/1353#discussion_r482259304", "createdAt": "2020-09-02T17:55:17Z", "author": {"login": "vivekratnavel"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOMDbCheckpointServlet.java", "diffHunk": "@@ -62,15 +62,15 @@\n  * Class used for testing the OM DB Checkpoint provider servlet.\n  */\n public class TestOMDbCheckpointServlet {\n-  private MiniOzoneCluster cluster = null;\n-  private OMMetrics omMetrics;\n-  private OzoneConfiguration conf;\n-  private String clusterId;\n-  private String scmId;\n-  private String omId;\n+  private static MiniOzoneCluster cluster = null;\n+  private static OMMetrics omMetrics;\n+  private static OzoneConfiguration conf;\n+  private static String clusterId;\n+  private static String scmId;\n+  private static String omId;\n \n   @Rule\n-  public Timeout timeout = new Timeout(60000);\n+  public Timeout timeout = new Timeout(240000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "831127056f4cec0a81c86b966d17dbe95a99835f"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MzI1MzMyMg==", "bodyText": "on my laptop, it takes 2-3 minutes for Ozone cluster to come up and the test fails.", "url": "https://github.com/apache/ozone/pull/1353#discussion_r483253322", "createdAt": "2020-09-03T21:04:37Z", "author": {"login": "prashantpogde"}, "path": "hadoop-ozone/integration-test/src/test/java/org/apache/hadoop/ozone/om/TestOMDbCheckpointServlet.java", "diffHunk": "@@ -62,15 +62,15 @@\n  * Class used for testing the OM DB Checkpoint provider servlet.\n  */\n public class TestOMDbCheckpointServlet {\n-  private MiniOzoneCluster cluster = null;\n-  private OMMetrics omMetrics;\n-  private OzoneConfiguration conf;\n-  private String clusterId;\n-  private String scmId;\n-  private String omId;\n+  private static MiniOzoneCluster cluster = null;\n+  private static OMMetrics omMetrics;\n+  private static OzoneConfiguration conf;\n+  private static String clusterId;\n+  private static String scmId;\n+  private static String omId;\n \n   @Rule\n-  public Timeout timeout = new Timeout(60000);\n+  public Timeout timeout = new Timeout(240000);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjI1OTMwNA=="}, "originalCommit": {"oid": "831127056f4cec0a81c86b966d17dbe95a99835f"}, "originalPosition": 34}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3923, "cost": 1, "resetAt": "2021-11-12T11:57:46Z"}}}