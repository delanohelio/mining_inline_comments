{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDEwMjYxNTU5", "number": 881, "title": "HDDS-3081. Replication manager should detect and correct containers which don't meet the replication policy", "bodyText": "What changes were proposed in this pull request?\nNow we have network topology available in Ozone, we need the ability to identify and correct containers where the replicas do not meet the replication policy. For network aware clusters, the rule is that the replicas must be on at least two racks. In the future, it is possible there will be other policies, eg node groups and so on.\nIdeally, the existing ReplicationManager in SCM should be extended to detect, and correct these mis-replicated containers.\nThis change starts by adding a new method to the PlacementPolicy interface:\nContainerPlacementStatus validateContainerPlacement(\n      List<DatanodeDetails> dns, int replicas);\n\nThis accepts the list of DNs currently hosting the replicas for a container, and the desired replication factor. It return an instance of ContainerPlacementStatus which has two methods indicating if the policy is met and if not, how many more replicas are needed:\nboolean isPolicySatisfied();\nint additionalReplicaRequired();\n\nCurrently, all placement policies extend SCMCommonPlacementPolicy and there is a default impelementation there, which handles both rack aware and non rack aware clusters. If we have further policies in the future (eg node groups, upgrade domain) this can be overridden in new subclasses.\nReplication manager can then call:\nplacementPolicy.validateContainerPlacement(List<DatanodeDetails>, replicationFactor)\n\nTo get the information it needs to check the container.\nFor over replicated containers, when removing replicas, it is important to also ensure removing the container does not violate the policy, so some changes will be needed around the over-replicated code path too.\nWhat is the link to the Apache JIRA\nhttps://issues.apache.org/jira/browse/HDDS-3081\nHow was this patch tested?\nNew unit tests", "createdAt": "2020-04-28T17:35:23Z", "url": "https://github.com/apache/ozone/pull/881", "merged": true, "mergeCommit": {"oid": "f81ce3a334e9cc434dc7e048aea88b1041f4f3e1"}, "closed": true, "closedAt": "2020-05-13T09:50:03Z", "author": {"login": "sodonnel"}, "timelineItems": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABccG6BXAH2gAyNDEwMjYxNTU5OjYyMDRkMGFmYmNlY2M1ZjAxYjMyZmVlMzNiYTU1YzMxMjg5MmQ4ZDU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcgnPplgFqTQxMDIzOTM0NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "6204d0afbcecc5f01b32fee33ba55c312892d8d5", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/6204d0afbcecc5f01b32fee33ba55c312892d8d5", "committedDate": "2020-04-28T16:58:46Z", "message": "Extend interface to allow for checking container placement status"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cd482fda23d5f0d43fc2243da71343d6ed75b739", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/cd482fda23d5f0d43fc2243da71343d6ed75b739", "committedDate": "2020-04-28T17:28:53Z", "message": "Handle under replicated containers caused by mis-replication"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dbdca02d45d80ce82359ef2f3ccdff042e4b3402", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/dbdca02d45d80ce82359ef2f3ccdff042e4b3402", "committedDate": "2020-04-28T19:55:47Z", "message": "Fixed failing unit test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03c70fd069e8c7ff8aaa315eb90f53118d5797f5", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/03c70fd069e8c7ff8aaa315eb90f53118d5797f5", "committedDate": "2020-04-29T09:27:59Z", "message": "retrigger build with empty commit"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f11797daf91c60436147eb82bd014839a923e5a3", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/f11797daf91c60436147eb82bd014839a923e5a3", "committedDate": "2020-04-29T12:30:24Z", "message": "Ensure handleOverReplicatedContainer correctly deals with mis-replicated blocks"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAzODg1Nzg4", "url": "https://github.com/apache/ozone/pull/881#pullrequestreview-403885788", "createdAt": "2020-04-30T21:19:55Z", "commit": {"oid": "f11797daf91c60436147eb82bd014839a923e5a3"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQyMToxOTo1NVrOGO6vpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0zMFQyMjoxMTo1M1rOGO8Hxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODI5NTcxOQ==", "bodyText": "nit. toplogy -> topology", "url": "https://github.com/apache/ozone/pull/881#discussion_r418295719", "createdAt": "2020-04-30T21:19:55Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/SCMCommonPlacementPolicy.java", "diffHunk": "@@ -198,4 +200,65 @@ public boolean hasEnoughSpace(DatanodeDetails datanodeDetails,\n    */\n   public abstract DatanodeDetails chooseNode(\n       List<DatanodeDetails> healthyNodes);\n+\n+  /**\n+   * Default implementation for basic placement policies that do not have a\n+   * placement policy. If the policy has not network toplogy this method should", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f11797daf91c60436147eb82bd014839a923e5a3"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODI5NTk5NQ==", "bodyText": "nit. space after 'maxLevel'.", "url": "https://github.com/apache/ozone/pull/881#discussion_r418295995", "createdAt": "2020-04-30T21:20:31Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/SCMCommonPlacementPolicy.java", "diffHunk": "@@ -198,4 +200,65 @@ public boolean hasEnoughSpace(DatanodeDetails datanodeDetails,\n    */\n   public abstract DatanodeDetails chooseNode(\n       List<DatanodeDetails> healthyNodes);\n+\n+  /**\n+   * Default implementation for basic placement policies that do not have a\n+   * placement policy. If the policy has not network toplogy this method should\n+   * return null.\n+   * @return The networkTopology for the policy or null if none is configured.\n+   */\n+  protected NetworkTopology getNetworkTopology() {\n+    return null;\n+  }\n+\n+  /**\n+   * Default implementation to return the number of racks containers should span\n+   * to meet the placement policy. For simple policies that are not rack aware\n+   * we return 1, from this default implementation.\n+   * should have\n+   * @return The number of racks containers should span to meet the policy\n+   */\n+  protected int getRequiredRackCount() {\n+    return 1;\n+  }\n+\n+  /**\n+   * This default implementation handles rack aware policies and non rack\n+   * aware policies. If a future placement policy needs to check more than racks\n+   * to validate the policy (eg node groups, HDFS like upgrade domain) this\n+   * method should be overridden in the sub class.\n+   * This method requires that subclasses which implement rack aware policies\n+   * override the default method getRequiredRackCount and getNetworkTopology.\n+   * @param dns List of datanodes holding a replica of the container\n+   * @param replicas The expected number of replicas\n+   * @return ContainerPlacementStatus indicating if the placement policy is\n+   *         met or not. Not this only considers the rack count and not the\n+   *         number of replicas.\n+   */\n+  @Override\n+  public ContainerPlacementStatus validateContainerPlacement(\n+      List<DatanodeDetails> dns, int replicas) {\n+    NetworkTopology topology = getNetworkTopology();\n+    int requiredRacks = getRequiredRackCount();\n+    if (topology == null || replicas == 1 || requiredRacks == 1) {\n+      // placement is always satisfied if there is at least one DN.\n+      return new ContainerPlacementStatusDefault(dns.size(), 1, 1);\n+    }\n+    // We have a network topology so calculate if it is satisfied or not.\n+    int numRacks = 1;\n+    final int maxLevel = topology.getMaxLevel();\n+    // The leaf nodes are all at max level, so the number of nodes at\n+    // leafLevel - 1 is the rack count\n+    numRacks = topology.getNumOfNodes(maxLevel- 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f11797daf91c60436147eb82bd014839a923e5a3"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODI5NzEwMQ==", "bodyText": "At this point if we know that the placement is satisfied, do we need to create a class that has to \"compute\" that again? Can we use an inline object here?", "url": "https://github.com/apache/ozone/pull/881#discussion_r418297101", "createdAt": "2020-04-30T21:23:03Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/SCMCommonPlacementPolicy.java", "diffHunk": "@@ -198,4 +200,65 @@ public boolean hasEnoughSpace(DatanodeDetails datanodeDetails,\n    */\n   public abstract DatanodeDetails chooseNode(\n       List<DatanodeDetails> healthyNodes);\n+\n+  /**\n+   * Default implementation for basic placement policies that do not have a\n+   * placement policy. If the policy has not network toplogy this method should\n+   * return null.\n+   * @return The networkTopology for the policy or null if none is configured.\n+   */\n+  protected NetworkTopology getNetworkTopology() {\n+    return null;\n+  }\n+\n+  /**\n+   * Default implementation to return the number of racks containers should span\n+   * to meet the placement policy. For simple policies that are not rack aware\n+   * we return 1, from this default implementation.\n+   * should have\n+   * @return The number of racks containers should span to meet the policy\n+   */\n+  protected int getRequiredRackCount() {\n+    return 1;\n+  }\n+\n+  /**\n+   * This default implementation handles rack aware policies and non rack\n+   * aware policies. If a future placement policy needs to check more than racks\n+   * to validate the policy (eg node groups, HDFS like upgrade domain) this\n+   * method should be overridden in the sub class.\n+   * This method requires that subclasses which implement rack aware policies\n+   * override the default method getRequiredRackCount and getNetworkTopology.\n+   * @param dns List of datanodes holding a replica of the container\n+   * @param replicas The expected number of replicas\n+   * @return ContainerPlacementStatus indicating if the placement policy is\n+   *         met or not. Not this only considers the rack count and not the\n+   *         number of replicas.\n+   */\n+  @Override\n+  public ContainerPlacementStatus validateContainerPlacement(\n+      List<DatanodeDetails> dns, int replicas) {\n+    NetworkTopology topology = getNetworkTopology();\n+    int requiredRacks = getRequiredRackCount();\n+    if (topology == null || replicas == 1 || requiredRacks == 1) {\n+      // placement is always satisfied if there is at least one DN.\n+      return new ContainerPlacementStatusDefault(dns.size(), 1, 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f11797daf91c60436147eb82bd014839a923e5a3"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODI5ODcwMg==", "bodyText": "Should we be doing this getNumOfNodes logN operation for every container in a run of the Replication Manager? Doesn't this stay fairly static?", "url": "https://github.com/apache/ozone/pull/881#discussion_r418298702", "createdAt": "2020-04-30T21:26:28Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/SCMCommonPlacementPolicy.java", "diffHunk": "@@ -198,4 +200,65 @@ public boolean hasEnoughSpace(DatanodeDetails datanodeDetails,\n    */\n   public abstract DatanodeDetails chooseNode(\n       List<DatanodeDetails> healthyNodes);\n+\n+  /**\n+   * Default implementation for basic placement policies that do not have a\n+   * placement policy. If the policy has not network toplogy this method should\n+   * return null.\n+   * @return The networkTopology for the policy or null if none is configured.\n+   */\n+  protected NetworkTopology getNetworkTopology() {\n+    return null;\n+  }\n+\n+  /**\n+   * Default implementation to return the number of racks containers should span\n+   * to meet the placement policy. For simple policies that are not rack aware\n+   * we return 1, from this default implementation.\n+   * should have\n+   * @return The number of racks containers should span to meet the policy\n+   */\n+  protected int getRequiredRackCount() {\n+    return 1;\n+  }\n+\n+  /**\n+   * This default implementation handles rack aware policies and non rack\n+   * aware policies. If a future placement policy needs to check more than racks\n+   * to validate the policy (eg node groups, HDFS like upgrade domain) this\n+   * method should be overridden in the sub class.\n+   * This method requires that subclasses which implement rack aware policies\n+   * override the default method getRequiredRackCount and getNetworkTopology.\n+   * @param dns List of datanodes holding a replica of the container\n+   * @param replicas The expected number of replicas\n+   * @return ContainerPlacementStatus indicating if the placement policy is\n+   *         met or not. Not this only considers the rack count and not the\n+   *         number of replicas.\n+   */\n+  @Override\n+  public ContainerPlacementStatus validateContainerPlacement(\n+      List<DatanodeDetails> dns, int replicas) {\n+    NetworkTopology topology = getNetworkTopology();\n+    int requiredRacks = getRequiredRackCount();\n+    if (topology == null || replicas == 1 || requiredRacks == 1) {\n+      // placement is always satisfied if there is at least one DN.\n+      return new ContainerPlacementStatusDefault(dns.size(), 1, 1);\n+    }\n+    // We have a network topology so calculate if it is satisfied or not.\n+    int numRacks = 1;\n+    final int maxLevel = topology.getMaxLevel();\n+    // The leaf nodes are all at max level, so the number of nodes at\n+    // leafLevel - 1 is the rack count\n+    numRacks = topology.getNumOfNodes(maxLevel- 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f11797daf91c60436147eb82bd014839a923e5a3"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMxMTAwNA==", "bodyText": "nit. Can be new ArrayList<>(source).", "url": "https://github.com/apache/ozone/pull/881#discussion_r418311004", "createdAt": "2020-04-30T21:54:26Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java", "diffHunk": "@@ -512,25 +523,61 @@ private void handleUnderReplicatedContainer(final ContainerInfo container,\n       if (source.size() > 0) {\n         final int replicationFactor = container\n             .getReplicationFactor().getNumber();\n-        final int delta = replicationFactor - getReplicaCount(id, replicas);\n+        // Want to check if the container is mis-replicated after considering\n+        // inflight add and delete.\n+        // Create a new list from source (healthy replicas minus pending delete)\n+        List<DatanodeDetails> targetReplicas = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f11797daf91c60436147eb82bd014839a923e5a3"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMxNTg1MQ==", "bodyText": "If delta > 0, do we still need to validate the placement status? Cannot we just choose a DN in accordance to policy and issue the replication command? In that case, this logic can be executed only if we are \"misreplicated\" and not \"underreplicated\".", "url": "https://github.com/apache/ozone/pull/881#discussion_r418315851", "createdAt": "2020-04-30T22:05:59Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java", "diffHunk": "@@ -512,25 +523,61 @@ private void handleUnderReplicatedContainer(final ContainerInfo container,\n       if (source.size() > 0) {\n         final int replicationFactor = container\n             .getReplicationFactor().getNumber();\n-        final int delta = replicationFactor - getReplicaCount(id, replicas);\n+        // Want to check if the container is mis-replicated after considering\n+        // inflight add and delete.\n+        // Create a new list from source (healthy replicas minus pending delete)\n+        List<DatanodeDetails> targetReplicas = new ArrayList<>();\n+        targetReplicas.addAll(source);\n+        // Then add any pending additions\n+        targetReplicas.addAll(replicationInFlight);\n+\n+        int delta = replicationFactor - getReplicaCount(id, replicas);\n+        final int additionalRacks", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f11797daf91c60436147eb82bd014839a923e5a3"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODMxODI3OA==", "bodyText": "Can we use something like findAny() here instead of breaking from the loop after first iteration? 'excess' is always > 0 here.", "url": "https://github.com/apache/ozone/pull/881#discussion_r418318278", "createdAt": "2020-04-30T22:11:53Z", "author": {"login": "avijayanhwx"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java", "diffHunk": "@@ -582,17 +629,71 @@ private void handleOverReplicatedContainer(final ContainerInfo container,\n           .filter(r -> !compareState(container.getState(), r.getState()))\n           .collect(Collectors.toList());\n \n-      //Move the unhealthy replicas to the front of eligible replicas to delete\n-      eligibleReplicas.removeAll(unhealthyReplicas);\n-      eligibleReplicas.addAll(0, unhealthyReplicas);\n-\n-      for (int i = 0; i < excess; i++) {\n-        sendDeleteCommand(container,\n-            eligibleReplicas.get(i).getDatanodeDetails(), true);\n+      // If there are unhealthy replicas, then we should remove them even if it\n+      // makes the container violate the placement policy, as excess unhealthy\n+      // containers are not really useful. It will be corrected later as a\n+      // mis-replicated container will be seen as under-replicated.\n+      for (ContainerReplica r : unhealthyReplicas) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f11797daf91c60436147eb82bd014839a923e5a3"}, "originalPosition": 146}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a05ab1bdfd661b8a703c43faf629637e3e2e1fe2", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/a05ab1bdfd661b8a703c43faf629637e3e2e1fe2", "committedDate": "2020-05-01T12:29:19Z", "message": "Address review comments from Aravindan"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "59489f5ea6ddb973be77afd19f079b4ac118b6e0", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/59489f5ea6ddb973be77afd19f079b4ac118b6e0", "committedDate": "2020-05-01T14:21:37Z", "message": "Fixed failing unit and checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db29a6f2f90d8835623373c692d530da33ae17cd", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/db29a6f2f90d8835623373c692d530da33ae17cd", "committedDate": "2020-05-04T16:53:40Z", "message": "Remove rack references inside replication manager"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA1NzM5NDI2", "url": "https://github.com/apache/ozone/pull/881#pullrequestreview-405739426", "createdAt": "2020-05-05T12:30:06Z", "commit": {"oid": "db29a6f2f90d8835623373c692d530da33ae17cd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxMjozMDowN1rOGQnF-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxMjozMDowN1rOGQnF-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MDkwNQ==", "bodyText": "Considering that we have the NetworkTopology inside NodeManager, do we need this method?\nIt is used only in the validateContainerPlacement method where we can use NodeManager also.", "url": "https://github.com/apache/ozone/pull/881#discussion_r420070905", "createdAt": "2020-05-05T12:30:07Z", "author": {"login": "fapifta"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/SCMCommonPlacementPolicy.java", "diffHunk": "@@ -198,4 +211,69 @@ public boolean hasEnoughSpace(DatanodeDetails datanodeDetails,\n    */\n   public abstract DatanodeDetails chooseNode(\n       List<DatanodeDetails> healthyNodes);\n+\n+  /**\n+   * Default implementation for basic placement policies that do not have a\n+   * placement policy. If the policy has not network topology this method should\n+   * return null.\n+   * @return The networkTopology for the policy or null if none is configured.\n+   */\n+  protected NetworkTopology getNetworkTopology() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db29a6f2f90d8835623373c692d530da33ae17cd"}, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA1NzM5NTQ1", "url": "https://github.com/apache/ozone/pull/881#pullrequestreview-405739545", "createdAt": "2020-05-05T12:30:15Z", "commit": {"oid": "db29a6f2f90d8835623373c692d530da33ae17cd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxMjozMDoxNVrOGQnGSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxMjozMDoxNVrOGQnGSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MDk4NQ==", "bodyText": "This part of the method is clearly dealing with non-topology-aware stuff, and returns, while the rest is dealing with topology-aware stuff. Why we don't use polymorphism here to separate the two? I understand that this requires copying the topology-aware stuff to two places, but that is an other problem we have to deal with at one point.\nSo I believe this if should be the default implementation in this class, and the rest should be inside the rack aware and pipeline placement policy.\nThis would render the getRequiredRackCount to be unnecessary as well, as here it would be possible to just remove the expression yielding into true always for non-topology-aware policies, while the topology aware ones can have and use the constant in their implementation.", "url": "https://github.com/apache/ozone/pull/881#discussion_r420070985", "createdAt": "2020-05-05T12:30:15Z", "author": {"login": "fapifta"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/SCMCommonPlacementPolicy.java", "diffHunk": "@@ -198,4 +211,69 @@ public boolean hasEnoughSpace(DatanodeDetails datanodeDetails,\n    */\n   public abstract DatanodeDetails chooseNode(\n       List<DatanodeDetails> healthyNodes);\n+\n+  /**\n+   * Default implementation for basic placement policies that do not have a\n+   * placement policy. If the policy has not network topology this method should\n+   * return null.\n+   * @return The networkTopology for the policy or null if none is configured.\n+   */\n+  protected NetworkTopology getNetworkTopology() {\n+    return null;\n+  }\n+\n+  /**\n+   * Default implementation to return the number of racks containers should span\n+   * to meet the placement policy. For simple policies that are not rack aware\n+   * we return 1, from this default implementation.\n+   * should have\n+   * @return The number of racks containers should span to meet the policy\n+   */\n+  protected int getRequiredRackCount() {\n+    return 1;\n+  }\n+\n+  /**\n+   * This default implementation handles rack aware policies and non rack\n+   * aware policies. If a future placement policy needs to check more than racks\n+   * to validate the policy (eg node groups, HDFS like upgrade domain) this\n+   * method should be overridden in the sub class.\n+   * This method requires that subclasses which implement rack aware policies\n+   * override the default method getRequiredRackCount and getNetworkTopology.\n+   * @param dns List of datanodes holding a replica of the container\n+   * @param replicas The expected number of replicas\n+   * @return ContainerPlacementStatus indicating if the placement policy is\n+   *         met or not. Not this only considers the rack count and not the\n+   *         number of replicas.\n+   */\n+  @Override\n+  public ContainerPlacementStatus validateContainerPlacement(\n+      List<DatanodeDetails> dns, int replicas) {\n+    NetworkTopology topology = getNetworkTopology();\n+    int requiredRacks = getRequiredRackCount();\n+    if (topology == null || replicas == 1 || requiredRacks == 1) {\n+      if (dns.size() > 0) {\n+        // placement is always satisfied if there is at least one DN.\n+        return validPlacement;\n+      } else {\n+        return invalidPlacement;\n+      }\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db29a6f2f90d8835623373c692d530da33ae17cd"}, "originalPosition": 80}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA1NzM5NjMw", "url": "https://github.com/apache/ozone/pull/881#pullrequestreview-405739630", "createdAt": "2020-05-05T12:30:22Z", "commit": {"oid": "db29a6f2f90d8835623373c692d530da33ae17cd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxMjozMDoyMlrOGQnGig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxMjozMDoyMlrOGQnGig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTA1MA==", "bodyText": "I understand that this is here to check if we have less than 2 racks with replicas, and if so, consider the container to be under replicated.\nI am unsure though if we want to ensure that a container is replicated to exactly 2 racks, it does not seems to be the case in the rack aware placement policy, but if so, then mis-replication should be checked somewhere else also to handle the case when a container is replicated to 3 racks due to for example removing a rack from config and place the nodes in a rack to other racks. (Probably this can go to isContainerOverReplicated(), if we want to check for this case.)", "url": "https://github.com/apache/ozone/pull/881#discussion_r420071050", "createdAt": "2020-05-05T12:30:22Z", "author": {"login": "fapifta"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java", "diffHunk": "@@ -393,8 +396,11 @@ private boolean isContainerHealthy(final ContainerInfo container,\n    */\n   private boolean isContainerUnderReplicated(final ContainerInfo container,\n       final Set<ContainerReplica> replicas) {\n+    boolean misReplicated = !getPlacementStatus(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db29a6f2f90d8835623373c692d530da33ae17cd"}, "originalPosition": 30}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA1NzM5NzE1", "url": "https://github.com/apache/ozone/pull/881#pullrequestreview-405739715", "createdAt": "2020-05-05T12:30:29Z", "commit": {"oid": "db29a6f2f90d8835623373c692d530da33ae17cd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxMjozMDozMFrOGQnG1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxMjozMDozMFrOGQnG1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTEyNw==", "bodyText": "This method in reality gives back the additional rack count required, not the additional replicas required.\nThis is why we need to check against the replication factor and the replica count difference also, while we provide the replication factor to the validation method, so it would be able to return the number of replicas really required.\nLet see the different cases:\n\nnon-topology aware placement policies: if replica count == replication factor we are good, otherwise the required additional replica count is replication factor - replica count, and we need that many extra replicas later from chooseDataNodes.\na topology aware placement policy with replication factor of 1 in case it is under replicated we already miss the block\na topology aware placement policy with replication factor of 2 in case it is under replicated either we miss the block, or we have it in one rack, and chooseDatanodes will give us the required one in a different rack.\ntopology aware placement policies with replication factor of 3 (or more):\n\nwhen we have 1 healthy replica, we need 2 additional replicas anyways, it is not really interesting in which rack.\nwhen we have 2 healthy replica, we need 1 additional replica from either one of the racks in which we already have a replica or in an other rack instead if we have the two replica in the same rack, this the policy already handles as it uses the excluded node list to anchor to proper racks in chooseDatanodes().\nwhen we have 3 healthy replica, but all three is in the same rack, then additionalReplicaRequired should yield to 1 still, and chooseDataNode should select a new set of DataNodes correctly, consisting of the nodes already have the replica, and one additional in a different rack. Which makes the container over replicated, so one excess replica will be removed later, after replication finished.\n\n\n\nSo if the additionalReplicaRequired works as the name suggests, we should not need to check replication factor and replica count here, but just rely on the value given back by the method.", "url": "https://github.com/apache/ozone/pull/881#discussion_r420071127", "createdAt": "2020-05-05T12:30:30Z", "author": {"login": "fapifta"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java", "diffHunk": "@@ -512,25 +523,60 @@ private void handleUnderReplicatedContainer(final ContainerInfo container,\n       if (source.size() > 0) {\n         final int replicationFactor = container\n             .getReplicationFactor().getNumber();\n-        final int delta = replicationFactor - getReplicaCount(id, replicas);\n+        // Want to check if the container is mis-replicated after considering\n+        // inflight add and delete.\n+        // Create a new list from source (healthy replicas minus pending delete)\n+        List<DatanodeDetails> targetReplicas = new ArrayList<>(source);\n+        // Then add any pending additions\n+        targetReplicas.addAll(replicationInFlight);\n+\n+        int delta = replicationFactor - getReplicaCount(id, replicas);\n+        final ContainerPlacementStatus placementStatus =\n+            containerPlacement.validateContainerPlacement(\n+                targetReplicas, replicationFactor);\n+        final int misRepDelta = placementStatus.additionalReplicaRequired();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db29a6f2f90d8835623373c692d530da33ae17cd"}, "originalPosition": 67}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA1NzM5NzYw", "url": "https://github.com/apache/ozone/pull/881#pullrequestreview-405739760", "createdAt": "2020-05-05T12:30:34Z", "commit": {"oid": "db29a6f2f90d8835623373c692d530da33ae17cd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxMjozMDozNFrOGQnG_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxMjozMDozNFrOGQnG_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTE2Nw==", "bodyText": "Do we add the in-flight replications twice here this way, or I am missing something?", "url": "https://github.com/apache/ozone/pull/881#discussion_r420071167", "createdAt": "2020-05-05T12:30:34Z", "author": {"login": "fapifta"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java", "diffHunk": "@@ -512,25 +523,60 @@ private void handleUnderReplicatedContainer(final ContainerInfo container,\n       if (source.size() > 0) {\n         final int replicationFactor = container\n             .getReplicationFactor().getNumber();\n-        final int delta = replicationFactor - getReplicaCount(id, replicas);\n+        // Want to check if the container is mis-replicated after considering\n+        // inflight add and delete.\n+        // Create a new list from source (healthy replicas minus pending delete)\n+        List<DatanodeDetails> targetReplicas = new ArrayList<>(source);\n+        // Then add any pending additions\n+        targetReplicas.addAll(replicationInFlight);\n+\n+        int delta = replicationFactor - getReplicaCount(id, replicas);\n+        final ContainerPlacementStatus placementStatus =\n+            containerPlacement.validateContainerPlacement(\n+                targetReplicas, replicationFactor);\n+        final int misRepDelta = placementStatus.additionalReplicaRequired();\n+        final int replicasNeeded\n+            = delta < misRepDelta ? misRepDelta : delta;\n+\n         final List<DatanodeDetails> excludeList = replicas.stream()\n             .map(ContainerReplica::getDatanodeDetails)\n             .collect(Collectors.toList());\n+        excludeList.addAll(replicationInFlight);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db29a6f2f90d8835623373c692d530da33ae17cd"}, "originalPosition": 74}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA1NzM5ODA5", "url": "https://github.com/apache/ozone/pull/881#pullrequestreview-405739809", "createdAt": "2020-05-05T12:30:38Z", "commit": {"oid": "db29a6f2f90d8835623373c692d530da33ae17cd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxMjozMDozOFrOGQnHKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxMjozMDozOFrOGQnHKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDA3MTIxMQ==", "bodyText": "The rack-aware policy's chooseDatanodes() method we use ensures that based on the excluded nodes list we get back a proper list, I believe we do not need to check it twice Are you aware of any cases where this is not true? Based on my understanding of the logic there I would expect all future placement policies to do so, as other existing ones implicitly do so now. If necessary we can add this to the contract of the interface, though this is an interesting property of the rack-aware policy that would worth some re-thinking but that is out of scope for now.", "url": "https://github.com/apache/ozone/pull/881#discussion_r420071211", "createdAt": "2020-05-05T12:30:38Z", "author": {"login": "fapifta"}, "path": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java", "diffHunk": "@@ -512,25 +523,60 @@ private void handleUnderReplicatedContainer(final ContainerInfo container,\n       if (source.size() > 0) {\n         final int replicationFactor = container\n             .getReplicationFactor().getNumber();\n-        final int delta = replicationFactor - getReplicaCount(id, replicas);\n+        // Want to check if the container is mis-replicated after considering\n+        // inflight add and delete.\n+        // Create a new list from source (healthy replicas minus pending delete)\n+        List<DatanodeDetails> targetReplicas = new ArrayList<>(source);\n+        // Then add any pending additions\n+        targetReplicas.addAll(replicationInFlight);\n+\n+        int delta = replicationFactor - getReplicaCount(id, replicas);\n+        final ContainerPlacementStatus placementStatus =\n+            containerPlacement.validateContainerPlacement(\n+                targetReplicas, replicationFactor);\n+        final int misRepDelta = placementStatus.additionalReplicaRequired();\n+        final int replicasNeeded\n+            = delta < misRepDelta ? misRepDelta : delta;\n+\n         final List<DatanodeDetails> excludeList = replicas.stream()\n             .map(ContainerReplica::getDatanodeDetails)\n             .collect(Collectors.toList());\n+        excludeList.addAll(replicationInFlight);\n         List<InflightAction> actionList = inflightReplication.get(id);\n         if (actionList != null) {\n           actionList.stream().map(r -> r.datanode)\n               .forEach(excludeList::add);\n         }\n         final List<DatanodeDetails> selectedDatanodes = containerPlacement\n-            .chooseDatanodes(excludeList, null, delta,\n+            .chooseDatanodes(excludeList, null, replicasNeeded,\n                 container.getUsedBytes());\n-\n-        LOG.info(\"Container {} is under replicated. Expected replica count\" +\n-                \" is {}, but found {}.\", id, replicationFactor,\n-            replicationFactor - delta);\n-\n-        for (DatanodeDetails datanode : selectedDatanodes) {\n-          sendReplicateCommand(container, datanode, source);\n+        if (delta > 0) {\n+          LOG.info(\"Container {} is under replicated. Expected replica count\" +\n+                  \" is {}, but found {}.\", id, replicationFactor,\n+              replicationFactor - delta);\n+        }\n+        int newMisRepDelta = misRepDelta;\n+        if (misRepDelta > 0) {\n+          LOG.info(\"Container: {}. {}\",\n+              id, placementStatus.misReplicatedReason());\n+          // Check if the new target nodes (original plus newly selected nodes)\n+          // makes the placement policy valid.\n+          targetReplicas.addAll(selectedDatanodes);\n+          newMisRepDelta = containerPlacement.validateContainerPlacement(\n+              targetReplicas, replicationFactor).additionalReplicaRequired();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db29a6f2f90d8835623373c692d530da33ae17cd"}, "originalPosition": 104}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8ca5723cd4e6e72cb18a8125d7be243cd623936c", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/8ca5723cd4e6e72cb18a8125d7be243cd623936c", "committedDate": "2020-05-05T15:39:10Z", "message": "Address comments from Pifta"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "332939031984154c0c54bf5cded748aa3723893e", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/332939031984154c0c54bf5cded748aa3723893e", "committedDate": "2020-05-05T17:17:11Z", "message": "Fixed failing unit test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b44f67b314b8216fa32c3816e129de6ec9b48491", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/b44f67b314b8216fa32c3816e129de6ec9b48491", "committedDate": "2020-05-06T11:04:48Z", "message": "Further javadoc changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f69b724bdc84295cbf7b8faba038229dd27da948", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/f69b724bdc84295cbf7b8faba038229dd27da948", "committedDate": "2020-05-06T15:07:28Z", "message": "Change method additionalReplicaRequired to misReplicationCount"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9cee52e6e421b5aa408cf0c1cd5b88b13906ef9a", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/9cee52e6e421b5aa408cf0c1cd5b88b13906ef9a", "committedDate": "2020-05-07T10:57:52Z", "message": "Trigger build"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "adf8bcd1f1ee7fe763d8cb4924482aaf860f61c0", "author": {"user": null}, "url": "https://github.com/apache/ozone/commit/adf8bcd1f1ee7fe763d8cb4924482aaf860f61c0", "committedDate": "2020-05-07T13:00:54Z", "message": "Trigger build"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEwMjM5MzQ0", "url": "https://github.com/apache/ozone/pull/881#pullrequestreview-410239344", "createdAt": "2020-05-12T16:55:03Z", "commit": {"oid": "adf8bcd1f1ee7fe763d8cb4924482aaf860f61c0"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3207, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}