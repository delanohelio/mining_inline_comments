{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM2MjI1MDI5", "number": 1685, "title": "HDDS-4552. Read data from chunk into ByteBuffer[] instead of single ByteBuffer.", "bodyText": "What changes were proposed in this pull request?\nWhen a ReadChunk operation is performed, all the data to be read from one chunk is read into a single ByteBuffer.\n#ChunkUtils#readData()\npublic static void readData(File file, ByteBuffer buf,\n    long offset, long len, VolumeIOStats volumeIOStats)\n    throws StorageContainerException {\n  .....\n  try {\n    bytesRead = processFileExclusively(path, () -> {\n      try (FileChannel channel = open(path, READ_OPTIONS, NO_ATTRIBUTES);\n           FileLock ignored = channel.lock(offset, len, true)) {\n        return channel.read(buf, offset);\n      } catch (IOException e) {\n        throw new UncheckedIOException(e);\n      }\n    });\n  } catch (UncheckedIOException e) {\n    throw wrapInStorageContainerException(e.getCause());\n  }\n  .....\n  .....\n\nThis Jira proposes to read the data from the channel and put it into an array of ByteBuffers for optimizing reads. For example, currently we hold onto the buffer until the chunkInputStream is closed or the last chunk byte is read (which can lead upto 4MB of data being cached in memory per ChunkInputStream). If we have smaller buffers, they can be released sooner, thus helping to optimize memory utilization (HDDS-4553). This Jira is a pre-requisite for optimizing client memory utilization.\nWe propose to add ReadChunk version to the ReadChunkRequestProto to determine if the response should have all the chunk data as a single ByteString (V0) or as a list of ByteStrings (V1). Default version will be V0. Older clients will get data back as a single ByteString to maintain wire compatibility.\nFor new clients, data will be returned as a list of ByteStrings with each ByteString having the capacity equal to its number of bytes per checksum. This is done so that checksum verification is uncomplicated and doesn't require extra buffer copying. For chunks with no checksum, the buffer capacity will be set to a configurable default.\nWhat is the link to the Apache JIRA\nhttps://issues.apache.org/jira/browse/HDDS-4552\nHow was this patch tested?\nAdded unit tests. Working on adding more unit tests.", "createdAt": "2020-12-10T20:14:09Z", "url": "https://github.com/apache/ozone/pull/1685", "merged": true, "mergeCommit": {"oid": "4136d47868e59fd19ccf21b953b0df9494d9aca9"}, "closed": true, "closedAt": "2021-03-18T23:30:41Z", "author": {"login": "hanishakoneru"}, "timelineItems": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdk526ygBqjQwOTcyMjU4NzE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABeEpUOWgFqTYxNjMxMTg3Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cb7f8f143c182225ce8e4cd569833b8c7e214ddd", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/cb7f8f143c182225ce8e4cd569833b8c7e214ddd", "committedDate": "2020-12-10T19:50:47Z", "message": "HDDS-4552. Read data from chunk into ByteBuffer[] instead of single ByteBuffer."}, "afterCommit": {"oid": "6320079208cdc9aeacd4c11e71b5b9b67832f767", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/6320079208cdc9aeacd4c11e71b5b9b67832f767", "committedDate": "2020-12-10T21:02:49Z", "message": "HDDS-4552. Read data from chunk into ByteBuffer[] instead of single ByteBuffer."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bdfe9cb37e951acae4bbab3f10803fc894dd79e2", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/bdfe9cb37e951acae4bbab3f10803fc894dd79e2", "committedDate": "2020-12-10T23:10:41Z", "message": "build fix"}, "afterCommit": {"oid": "577ddad3313d19ce7d4fe3dfec7565ce68623e27", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/577ddad3313d19ce7d4fe3dfec7565ce68623e27", "committedDate": "2020-12-10T23:56:31Z", "message": "CI fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU2MjAwNzA2", "url": "https://github.com/apache/ozone/pull/1685#pullrequestreview-556200706", "createdAt": "2020-12-21T08:20:29Z", "commit": {"oid": "7819426ccaf6dabe3d90ddb6f37733fb4989e30f"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQwODoyMDoyOVrOIJP1XQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQwODo0Mzo0MFrOIJQbuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjU2NzUxNw==", "bodyText": "LOG.info -> LOG.warn?", "url": "https://github.com/apache/ozone/pull/1685#discussion_r546567517", "createdAt": "2020-12-21T08:20:29Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/BlockInputStream.java", "diffHunk": "@@ -134,7 +135,12 @@ public synchronized void initialize() throws IOException {\n     try {\n       chunks = getChunkInfos();\n     } catch (ContainerNotFoundException ioEx) {\n-      refreshPipeline(ioEx);\n+      LOG.info(\"Unable to read information for block {} from pipeline {}: {}.\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7819426ccaf6dabe3d90ddb6f37733fb4989e30f"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjU3NzMzNg==", "bodyText": "This will do an additional buffer copy here. Let's see if we can explore anything here to avoid buffer copy here:\nhttps://developers.google.com/protocol-buffers/docs/reference/java/com/google/protobuf/UnsafeByteOperations", "url": "https://github.com/apache/ozone/pull/1685#discussion_r546577336", "createdAt": "2020-12-21T08:43:40Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -301,36 +348,38 @@ private synchronized void readChunkFromContainer(int len) throws IOException {\n       startByteIndex = chunkPosition;\n     } else {\n       // Start reading the chunk from the last chunkPosition onwards.\n-      startByteIndex = bufferOffset + bufferLength;\n+      startByteIndex = bufferOffsetWrtChunkData + bufferLength;\n     }\n \n-    // bufferOffset and bufferLength are updated below, but if read fails\n+    // bufferOffsetWrtChunkData and bufferLength are updated after the data\n+    // is read from Container and put into the buffers, but if read fails\n     // and is retried, we need the previous position.  Position is reset after\n     // successful read in adjustBufferPosition()\n     storePosition();\n \n+    long adjustedBuffersOffset, adjustedBuffersLen;\n     if (verifyChecksum) {\n-      // Update the bufferOffset and bufferLength as per the checksum\n-      // boundary requirement.\n-      computeChecksumBoundaries(startByteIndex, len);\n+      // Adjust the chunk offset and length to include required checksum\n+      // boundaries\n+      Pair<Long, Long> adjustedOffsetAndLength =\n+          computeChecksumBoundaries(startByteIndex, len);\n+      adjustedBuffersOffset = adjustedOffsetAndLength.getLeft();\n+      adjustedBuffersLen = adjustedOffsetAndLength.getRight();\n     } else {\n       // Read from the startByteIndex\n-      bufferOffset = startByteIndex;\n-      bufferLength = len;\n+      adjustedBuffersOffset = startByteIndex;\n+      adjustedBuffersLen = len;\n     }\n \n     // Adjust the chunkInfo so that only the required bytes are read from\n     // the chunk.\n     final ChunkInfo adjustedChunkInfo = ChunkInfo.newBuilder(chunkInfo)\n-        .setOffset(bufferOffset + chunkInfo.getOffset())\n-        .setLen(bufferLength)\n+        .setOffset(adjustedBuffersOffset + chunkInfo.getOffset())\n+        .setLen(adjustedBuffersLen)\n         .build();\n \n-    ByteString byteString = readChunk(adjustedChunkInfo);\n-\n-    buffers = byteString.asReadOnlyByteBufferList();\n-    bufferIndex = 0;\n-    allocated = true;\n+    byte[] chunkData = readChunk(adjustedChunkInfo).toByteArray();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7819426ccaf6dabe3d90ddb6f37733fb4989e30f"}, "originalPosition": 216}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7819426ccaf6dabe3d90ddb6f37733fb4989e30f", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/7819426ccaf6dabe3d90ddb6f37733fb4989e30f", "committedDate": "2020-12-12T00:26:27Z", "message": "Move BlockInputStream#handleReadError to ChunkInputStream so that\nif partial reads from ChunkInputStream have succeeded, then new\nacquired clients can start reading from that point onwards."}, "afterCommit": {"oid": "e3f550c6c20e573e22046ba5eba371ea8c9ae6a9", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/e3f550c6c20e573e22046ba5eba371ea8c9ae6a9", "committedDate": "2021-02-18T20:27:59Z", "message": "Unit tests for InputStreams"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTk2MjQwMDE3", "url": "https://github.com/apache/ozone/pull/1685#pullrequestreview-596240017", "createdAt": "2021-02-23T11:34:06Z", "commit": {"oid": "853bfdf8b4a061a5d1c85db18965223bfbf1d353"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "853bfdf8b4a061a5d1c85db18965223bfbf1d353", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/853bfdf8b4a061a5d1c85db18965223bfbf1d353", "committedDate": "2021-02-19T20:59:13Z", "message": "CI fix 3"}, "afterCommit": {"oid": "3b5253efdb566de2e02237390d54335274ba94e3", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/3b5253efdb566de2e02237390d54335274ba94e3", "committedDate": "2021-03-10T20:25:13Z", "message": "Set ReadChunkVersion explicitly"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjExODQzNTUx", "url": "https://github.com/apache/ozone/pull/1685#pullrequestreview-611843551", "createdAt": "2021-03-15T05:50:27Z", "commit": {"oid": "ca9a5318b2da6edbf76ee9cbeabfb7d2e6c1cb33"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xNVQwNTo1MDoyN1rOI2i2zQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xNVQwNjowMToxNVrOI2jDig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDA2NTEwMQ==", "bodyText": "Let's -> \"Let's say\"", "url": "https://github.com/apache/ozone/pull/1685#discussion_r594065101", "createdAt": "2021-03-15T05:50:27Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -66,13 +76,21 @@\n \n   // Index of the buffers corresponding to the current position of the buffers\n   private int bufferIndex;\n+  // bufferOffsets[i] stores the index of the first data byte in buffer i\n+  // (buffers.get(i)) w.r.t first byte in the buffers.\n+  // Let's each buffer has a capacity of 40 bytes. The bufferOffset for", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca9a5318b2da6edbf76ee9cbeabfb7d2e6c1cb33"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDA2NTI0MQ==", "bodyText": "Unintended change?", "url": "https://github.com/apache/ozone/pull/1685#discussion_r594065241", "createdAt": "2021-03-15T05:50:59Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -87,7 +105,8 @@\n   ChunkInputStream(ChunkInfo chunkInfo, BlockID blockId,\n       XceiverClientFactory xceiverClientFactory,\n       Supplier<Pipeline> pipelineSupplier,\n-      boolean verifyChecksum, Token<? extends TokenIdentifier> token) {\n+      boolean verifyChecksum,\n+      Token<? extends TokenIdentifier> token) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca9a5318b2da6edbf76ee9cbeabfb7d2e6c1cb33"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NDA2ODM2Mg==", "bodyText": "Let's not change the default for now. We can change once we do some tests and analyze performance .", "url": "https://github.com/apache/ozone/pull/1685#discussion_r594068362", "createdAt": "2021-03-15T06:01:15Z", "author": {"login": "bshashikant"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/OzoneClientConfig.java", "diffHunk": "@@ -109,13 +109,13 @@\n   private String checksumType = ChecksumType.CRC32.name();\n \n   @Config(key = \"bytes.per.checksum\",\n-      defaultValue = \"1MB\",\n+      defaultValue = \"256KB\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca9a5318b2da6edbf76ee9cbeabfb7d2e6c1cb33"}, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0493e0e8e6fe043570eb266966b512dd1c8c0ddb", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/0493e0e8e6fe043570eb266966b512dd1c8c0ddb", "committedDate": "2021-03-17T17:49:16Z", "message": "Change default bytes per checksum to 256KB and reduce min bytes per checksum to 16KB"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f6b0f31a2c64fae2b5376a46b4decfbb383e0e30", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/f6b0f31a2c64fae2b5376a46b4decfbb383e0e30", "committedDate": "2021-03-17T17:49:56Z", "message": "1. Introduce ReadChunk Versions -\n   V0 for returning data as single ByteString (old format).\n   V1 for returning data as a list of ByteStrings, with each ByteString length = number of bytes per checksum.\n2. If chunk does not have checksums, then set buffer capacity to a default (64KB).\n3. Return data from chunk as a list of ByteBuffers instead of a single ByteBuffer."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03a3d82777731f0dcfce0ef2b849cb7cf027932d", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/03a3d82777731f0dcfce0ef2b849cb7cf027932d", "committedDate": "2021-03-17T17:49:56Z", "message": "Unit tests for InputStreams"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bb672c04160654dd4ff60c4126bdcbe9ebecaf93", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/bb672c04160654dd4ff60c4126bdcbe9ebecaf93", "committedDate": "2021-03-17T17:49:56Z", "message": "CI fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f19c58e91b8039cdcad958e8cecf8057c7ae734a", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/f19c58e91b8039cdcad958e8cecf8057c7ae734a", "committedDate": "2021-03-17T17:49:56Z", "message": "CI fixes 2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cc460c8b651f84467c320c51f37098b64e2337a2", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/cc460c8b651f84467c320c51f37098b64e2337a2", "committedDate": "2021-03-17T17:49:56Z", "message": "CI fix 3"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "710e999afe1c009fa98b8801bd5a402f36404b88", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/710e999afe1c009fa98b8801bd5a402f36404b88", "committedDate": "2021-03-17T17:49:56Z", "message": "Set ReadChunkVersion explicitly"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d4020effb5591967181a442e1246bfb1f69649b3", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/d4020effb5591967181a442e1246bfb1f69649b3", "committedDate": "2021-03-17T17:49:56Z", "message": "CI fixes + check if ReadChunkResponse hasData before calling getData"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6beb15d11a0f3733e5f3ed76403460f01f94e1ea", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/6beb15d11a0f3733e5f3ed76403460f01f94e1ea", "committedDate": "2021-03-17T17:49:56Z", "message": "Review comments and CI fixes"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a3d49a1e33540874c1d5eb18f8cc209b78af7519", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/a3d49a1e33540874c1d5eb18f8cc209b78af7519", "committedDate": "2021-03-15T19:53:40Z", "message": "Review comments and CI fixes"}, "afterCommit": {"oid": "6beb15d11a0f3733e5f3ed76403460f01f94e1ea", "author": {"user": {"login": "hanishakoneru", "name": "Hanisha Koneru"}}, "url": "https://github.com/apache/ozone/commit/6beb15d11a0f3733e5f3ed76403460f01f94e1ea", "committedDate": "2021-03-17T17:49:56Z", "message": "Review comments and CI fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjE1NzA1MTQ3", "url": "https://github.com/apache/ozone/pull/1685#pullrequestreview-615705147", "createdAt": "2021-03-18T17:52:52Z", "commit": {"oid": "6beb15d11a0f3733e5f3ed76403460f01f94e1ea"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NjE2MzExODcz", "url": "https://github.com/apache/ozone/pull/1685#pullrequestreview-616311873", "createdAt": "2021-03-19T11:41:24Z", "commit": {"oid": "6beb15d11a0f3733e5f3ed76403460f01f94e1ea"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xOVQxMTo0MToyNFrOI57X_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMy0xOVQxMTo0OTo0OFrOI57pBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzYxMjU0Mw==", "bodyText": "If read from first location fails and we have to fall back to the temp chunk file, this would cause exception.", "url": "https://github.com/apache/ozone/pull/1685#discussion_r597612543", "createdAt": "2021-03-19T11:41:24Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "diffHunk": "@@ -255,16 +287,16 @@ public ChunkBuffer readChunk(Container container, BlockID blockID,\n         if (file.exists()) {\n           long offset = info.getOffset() - chunkFileOffset;\n           Preconditions.checkState(offset >= 0);\n-          ChunkUtils.readData(file, data, offset, len, volumeIOStats);\n-          return ChunkBuffer.wrap(data);\n+          ChunkUtils.readData(file, dataBuffers, offset, len, volumeIOStats);\n+          return ChunkBuffer.wrap(Lists.newArrayList(dataBuffers));\n         }\n       } catch (StorageContainerException ex) {\n         //UNABLE TO FIND chunk is not a problem as we will try with the\n         //next possible location\n         if (ex.getResult() != UNABLE_TO_FIND_CHUNK) {\n           throw ex;\n         }\n-        data.clear();\n+        dataBuffers = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6beb15d11a0f3733e5f3ed76403460f01f94e1ea"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzYxMzQzOQ==", "bodyText": "This block seems to be duplicated from FilePerBlock....  Can it be extracted?", "url": "https://github.com/apache/ozone/pull/1685#discussion_r597613439", "createdAt": "2021-03-19T11:43:14Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/impl/FilePerChunkStrategy.java", "diffHunk": "@@ -222,7 +228,33 @@ public ChunkBuffer readChunk(Container container, BlockID blockID,\n     }\n \n     long len = info.getLen();\n-    ByteBuffer data = ByteBuffer.allocate((int) len);\n+\n+    long bufferCapacity = 0;\n+    if (info.isReadDataIntoSingleBuffer()) {\n+      // Older client - read all chunk data into one single buffer.\n+      bufferCapacity = len;\n+    } else {\n+      // Set buffer capacity to checksum boundary size so that each buffer\n+      // corresponds to one checksum. If checksum is NONE, then set buffer\n+      // capacity to default (OZONE_CHUNK_READ_BUFFER_DEFAULT_SIZE_KEY = 64KB).\n+      ChecksumData checksumData = info.getChecksumData();\n+\n+      if (checksumData != null) {\n+        if (checksumData.getChecksumType() ==\n+            ContainerProtos.ChecksumType.NONE) {\n+          bufferCapacity = defaultReadBufferCapacity;\n+        } else {\n+          bufferCapacity = checksumData.getBytesPerChecksum();\n+        }\n+      }\n+    }\n+    // If the buffer capacity is 0, set all the data into one ByteBuffer\n+    if (bufferCapacity == 0) {\n+      bufferCapacity = len;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6beb15d11a0f3733e5f3ed76403460f01f94e1ea"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU5NzYxNjkwMA==", "bodyText": "I think we should avoid streams on read/write path.  Earlier these were found to cause CPU usage hotspots.  See eg. HDDS-3702.\n(Also in few other instances below.)", "url": "https://github.com/apache/ozone/pull/1685#discussion_r597616900", "createdAt": "2021-03-19T11:49:48Z", "author": {"login": "adoroszlai"}, "path": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/storage/ChunkInputStream.java", "diffHunk": "@@ -364,7 +404,18 @@ protected ByteString readChunk(ChunkInfo readChunkInfo) throws IOException {\n       throw new IOException(\"Unexpected OzoneException: \" + e.toString(), e);\n     }\n \n-    return readChunkResponse.getData();\n+    if (readChunkResponse.hasData()) {\n+      return readChunkResponse.getData().asReadOnlyByteBufferList();\n+    } else if (readChunkResponse.hasDataBuffers()) {\n+      List<ByteString> buffersList = readChunkResponse.getDataBuffers()\n+          .getBuffersList();\n+      return buffersList.stream()\n+          .map(ByteString::asReadOnlyByteBuffer)\n+          .collect(Collectors.toList());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6beb15d11a0f3733e5f3ed76403460f01f94e1ea"}, "originalPosition": 206}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2088, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}