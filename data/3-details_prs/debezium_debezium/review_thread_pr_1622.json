{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM2NjYwNzAw", "number": 1622, "reviewThreads": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTowMDozNVrOEG6UWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMToxNjo1OVrOEJnCaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjgyMzkzOnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTowMDozNVrOGmAC3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMjo1OToyNVrOGmCz4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ5OTgwNw==", "bodyText": "I feel like this and the following two paragraphs apply to both Kafka worker or individual connector setups and probably shouldn't be indented under the second option.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442499807", "createdAt": "2020-06-18T21:00:35Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n++\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU0NTEyMg==", "bodyText": "Sure. I removed the plus signs that indent those paragraphs. Fixed in the next commit.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442545122", "createdAt": "2020-06-18T22:59:25Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n++\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ5OTgwNw=="}, "originalCommit": null, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1Njg0NDM2OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTowNzozM1rOGmAP0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMjo1OTowMlrOGmCzdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwMzEyMg==", "bodyText": "Looks like this is a typo, should be endif::community[]?", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442503122", "createdAt": "2020-06-18T21:07:33Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -199,3 +358,5 @@ Note that some details around Kafka Connect converters have slightly changed sin\n \n For a complete example of using Avro as the message format for {prodname} data change events,\n please see the https://github.com/debezium/debezium-examples/tree/master/tutorial#using-mysql-and-the-avro-message-format[MySQL and the Avro message format] tutorial example.\n+\n+endif::[community]", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 363}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU0NTAxNA==", "bodyText": "Yes, that's a typo. Fixed in the next commit.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442545014", "createdAt": "2020-06-18T22:59:02Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -199,3 +358,5 @@ Note that some details around Kafka Connect converters have slightly changed sin\n \n For a complete example of using Avro as the message format for {prodname} data change events,\n please see the https://github.com/debezium/debezium-examples/tree/master/tutorial#using-mysql-and-the-avro-message-format[MySQL and the Avro message format] tutorial example.\n+\n+endif::[community]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwMzEyMg=="}, "originalCommit": null, "originalPosition": 363}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODM4NDk2OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwOTo1NTo0NVrOGmPFnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxODo0ODowMlrOGmeT5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc0NjI2OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            [id=\"overview-of-deploying-a-debezium-connector-that-uses-avro-serializatioin\"]\n          \n          \n            \n            [id=\"overview-of-deploying-a-debezium-connector-that-uses-avro-serialization\"]", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442746269", "createdAt": "2020-06-19T09:55:45Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. \n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Including schemas causes the messages to  be very verbose. If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the message keys and values by using link:https://avro.apache.org/[Apache Avro].\n+The Avro binary format is compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure. Avro's schema evolution mechanism makes it possible to evolve the schemas over time, which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables. Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema. Avro serialization makes it easier for consumers to adapt to the changing schema.\n \n-[NOTE]\n+ifdef::product[]\n+[IMPORTANT]\n ====\n-The Apicurio project also provides a JSON converter that can be used with the Apicurio registry.\n-This combines the advantage of less verbose messages (as messages do not contain the schema information themselves, but only a schema id)\n-with human-readable JSON.\n+Using Avro to serialize message keys and values is a Technology Preview feature. Technology Preview features are not supported with Red Hat production service-level agreements (SLAs) and might not be functionally complete; therefore, Red Hat does not recommend implementing any Technology Preview features in production environments. This Technology Preview feature provides early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process. For more information about support scope, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].\n ====\n+endif::product[]\n \n-Another option is using the Confluent schema registry, which is described further below.\n+// Type: concept\n+// Title: About the {registry}\n+[id=\"about-the-registry\"]\n+== About the {registry-name-full}\n \n-== Technical Information\n+ifdef::community[]\n+The link:https://github.com/Apicurio/apicurio-registry[{registry}] open-source project provides several components that work with Avro:\n+endif::community[]\n \n-A system that wants to use Avro serialization needs to complete two steps:\n+ifdef::product[]\n+{LinkServiceRegistryGetStart}[{registry-name-full}] provides several components that work with Avro:\n+endif::product[]\n \n-* Deploy an https://github.com/Apicurio/apicurio-registry[Apicurio API/Schema Registry] instance\n-* Install the Avro converter from https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka's _libs_ directory or directly into a plug-in directory\n-* Use thes the following properties to configure Apache Connect instance\n+* An Avro converter that you can configure in Kafka Connect workers. This converter maps Kafka Connect schemas to Avro schemas. The converter then uses the Avro schemas to serialize the message keys and values into Avro's compact binary form.\n \n-[source]\n+* An API/Schema registry that tracks:\n++\n+** Avro schemas that are used in Kafka topics\n+** Where the Avro converter sends the generated Avro schemas\n+\n++\n+Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n+This makes each message even smaller. For an I/O bound system like Kafka, this means more total throughput of the producers and consumers.\n+\n+* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n+Any Kafka consumer applications that you write to consume change event records can use Avro Serdes to deserialize the change event records.\n+\n+To use the {registry} with {prodname}, you must add {registry} converters and their dependencies to the Kafka Connect container image that you are using for running {prodname}.\n+\n+[NOTE]\n+====\n+The {registry} project also provides a JSON converter. This converter combines the advantage of less verbose messages with human-readable JSON. Messages do not contain the schema information themselves, but only a schema ID.\n+====\n+\n+ifdef::community[]\n+Another option is using the Confluent schema registry, which is described later.\n+endif::community[]\n+\n+// Type: concept\n+// Title: Overview of deploying a {prodname} connector that uses Avro serialization\n+[id=\"overview-of-deploying-a-debezium-connector-that-uses-avro-serializatioin\"]", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5NTY4Nw==", "bodyText": "Just noting here that I fixed this in the source. I will commit all the updates in one operation.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442995687", "createdAt": "2020-06-19T18:48:02Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. \n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Including schemas causes the messages to  be very verbose. If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the message keys and values by using link:https://avro.apache.org/[Apache Avro].\n+The Avro binary format is compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure. Avro's schema evolution mechanism makes it possible to evolve the schemas over time, which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables. Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema. Avro serialization makes it easier for consumers to adapt to the changing schema.\n \n-[NOTE]\n+ifdef::product[]\n+[IMPORTANT]\n ====\n-The Apicurio project also provides a JSON converter that can be used with the Apicurio registry.\n-This combines the advantage of less verbose messages (as messages do not contain the schema information themselves, but only a schema id)\n-with human-readable JSON.\n+Using Avro to serialize message keys and values is a Technology Preview feature. Technology Preview features are not supported with Red Hat production service-level agreements (SLAs) and might not be functionally complete; therefore, Red Hat does not recommend implementing any Technology Preview features in production environments. This Technology Preview feature provides early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process. For more information about support scope, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].\n ====\n+endif::product[]\n \n-Another option is using the Confluent schema registry, which is described further below.\n+// Type: concept\n+// Title: About the {registry}\n+[id=\"about-the-registry\"]\n+== About the {registry-name-full}\n \n-== Technical Information\n+ifdef::community[]\n+The link:https://github.com/Apicurio/apicurio-registry[{registry}] open-source project provides several components that work with Avro:\n+endif::community[]\n \n-A system that wants to use Avro serialization needs to complete two steps:\n+ifdef::product[]\n+{LinkServiceRegistryGetStart}[{registry-name-full}] provides several components that work with Avro:\n+endif::product[]\n \n-* Deploy an https://github.com/Apicurio/apicurio-registry[Apicurio API/Schema Registry] instance\n-* Install the Avro converter from https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka's _libs_ directory or directly into a plug-in directory\n-* Use thes the following properties to configure Apache Connect instance\n+* An Avro converter that you can configure in Kafka Connect workers. This converter maps Kafka Connect schemas to Avro schemas. The converter then uses the Avro schemas to serialize the message keys and values into Avro's compact binary form.\n \n-[source]\n+* An API/Schema registry that tracks:\n++\n+** Avro schemas that are used in Kafka topics\n+** Where the Avro converter sends the generated Avro schemas\n+\n++\n+Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n+This makes each message even smaller. For an I/O bound system like Kafka, this means more total throughput of the producers and consumers.\n+\n+* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n+Any Kafka consumer applications that you write to consume change event records can use Avro Serdes to deserialize the change event records.\n+\n+To use the {registry} with {prodname}, you must add {registry} converters and their dependencies to the Kafka Connect container image that you are using for running {prodname}.\n+\n+[NOTE]\n+====\n+The {registry} project also provides a JSON converter. This converter combines the advantage of less verbose messages with human-readable JSON. Messages do not contain the schema information themselves, but only a schema ID.\n+====\n+\n+ifdef::community[]\n+Another option is using the Confluent schema registry, which is described later.\n+endif::community[]\n+\n+// Type: concept\n+// Title: Overview of deploying a {prodname} connector that uses Avro serialization\n+[id=\"overview-of-deploying-a-debezium-connector-that-uses-avro-serializatioin\"]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc0NjI2OQ=="}, "originalCommit": null, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODQzOTExOnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMDoxMzoyOFrOGmPnTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxOTo1MTo0MFrOGmftWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1NDg5Mw==", "bodyText": "Without the section before (which now is part of the Apicurio section), there's some context lacking why a registry is needed. How about adding something like this here:\nWhen using Apache Avro as a message format, a schema registry must be deployed which manages Avro message schemas and their versions. Options include the Apicurio API and Schema Registry and the Confluent Schema registry.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442754893", "createdAt": "2020-06-19T10:13:28Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. \n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Including schemas causes the messages to  be very verbose. If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the message keys and values by using link:https://avro.apache.org/[Apache Avro].\n+The Avro binary format is compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure. Avro's schema evolution mechanism makes it possible to evolve the schemas over time, which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables. Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema. Avro serialization makes it easier for consumers to adapt to the changing schema.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5ODg4NA==", "bodyText": "Before this point in the source file, no content is conditionalzed for just upstream nor just downstream. That is, all content is visible both upstream and downstream. But I think you are right that the doc needs to say that a registry is required and why. I added the following at the end of the section:\nTo use Apache Avro as the message format, you must deploy a schema registry that manages Avro message schemas and their versions.\nI left out your suggested second sentence because there is only one option downstream, and the next section in the source file describes it. Then a bit later, conditionalized for upstream only, the doc says you can use Confluent.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442998884", "createdAt": "2020-06-19T18:56:31Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. \n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Including schemas causes the messages to  be very verbose. If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the message keys and values by using link:https://avro.apache.org/[Apache Avro].\n+The Avro binary format is compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure. Avro's schema evolution mechanism makes it possible to evolve the schemas over time, which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables. Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema. Avro serialization makes it easier for consumers to adapt to the changing schema.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1NDg5Mw=="}, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAxODU4Nw==", "bodyText": "Sounds good.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443018587", "createdAt": "2020-06-19T19:51:40Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. \n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Including schemas causes the messages to  be very verbose. If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the message keys and values by using link:https://avro.apache.org/[Apache Avro].\n+The Avro binary format is compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure. Avro's schema evolution mechanism makes it possible to evolve the schemas over time, which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables. Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema. Avro serialization makes it easier for consumers to adapt to the changing schema.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1NDg5Mw=="}, "originalCommit": null, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODQ0MTI3OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMDoxNDowNVrOGmPomg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxOTowMDo1M1rOGmem-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1NTIyNg==", "bodyText": "A reference would be nice.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442755226", "createdAt": "2020-06-19T10:14:05Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. \n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Including schemas causes the messages to  be very verbose. If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the message keys and values by using link:https://avro.apache.org/[Apache Avro].\n+The Avro binary format is compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure. Avro's schema evolution mechanism makes it possible to evolve the schemas over time, which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables. Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema. Avro serialization makes it easier for consumers to adapt to the changing schema.\n \n-[NOTE]\n+ifdef::product[]\n+[IMPORTANT]\n ====\n-The Apicurio project also provides a JSON converter that can be used with the Apicurio registry.\n-This combines the advantage of less verbose messages (as messages do not contain the schema information themselves, but only a schema id)\n-with human-readable JSON.\n+Using Avro to serialize message keys and values is a Technology Preview feature. Technology Preview features are not supported with Red Hat production service-level agreements (SLAs) and might not be functionally complete; therefore, Red Hat does not recommend implementing any Technology Preview features in production environments. This Technology Preview feature provides early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process. For more information about support scope, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].\n ====\n+endif::product[]\n \n-Another option is using the Confluent schema registry, which is described further below.\n+// Type: concept\n+// Title: About the {registry}\n+[id=\"about-the-registry\"]\n+== About the {registry-name-full}\n \n-== Technical Information\n+ifdef::community[]\n+The link:https://github.com/Apicurio/apicurio-registry[{registry}] open-source project provides several components that work with Avro:\n+endif::community[]\n \n-A system that wants to use Avro serialization needs to complete two steps:\n+ifdef::product[]\n+{LinkServiceRegistryGetStart}[{registry-name-full}] provides several components that work with Avro:\n+endif::product[]\n \n-* Deploy an https://github.com/Apicurio/apicurio-registry[Apicurio API/Schema Registry] instance\n-* Install the Avro converter from https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka's _libs_ directory or directly into a plug-in directory\n-* Use thes the following properties to configure Apache Connect instance\n+* An Avro converter that you can configure in Kafka Connect workers. This converter maps Kafka Connect schemas to Avro schemas. The converter then uses the Avro schemas to serialize the message keys and values into Avro's compact binary form.\n \n-[source]\n+* An API/Schema registry that tracks:\n++\n+** Avro schemas that are used in Kafka topics\n+** Where the Avro converter sends the generated Avro schemas\n+\n++\n+Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n+This makes each message even smaller. For an I/O bound system like Kafka, this means more total throughput of the producers and consumers.\n+\n+* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n+Any Kafka consumer applications that you write to consume change event records can use Avro Serdes to deserialize the change event records.\n+\n+To use the {registry} with {prodname}, you must add {registry} converters and their dependencies to the Kafka Connect container image that you are using for running {prodname}.\n+\n+[NOTE]\n+====\n+The {registry} project also provides a JSON converter. This converter combines the advantage of less verbose messages with human-readable JSON. Messages do not contain the schema information themselves, but only a schema ID.\n+====\n+\n+ifdef::community[]\n+Another option is using the Confluent schema registry, which is described later.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAwMDU2OQ==", "bodyText": "Sure. I added one.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443000569", "createdAt": "2020-06-19T19:00:53Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. \n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Including schemas causes the messages to  be very verbose. If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the message keys and values by using link:https://avro.apache.org/[Apache Avro].\n+The Avro binary format is compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure. Avro's schema evolution mechanism makes it possible to evolve the schemas over time, which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables. Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema. Avro serialization makes it easier for consumers to adapt to the changing schema.\n \n-[NOTE]\n+ifdef::product[]\n+[IMPORTANT]\n ====\n-The Apicurio project also provides a JSON converter that can be used with the Apicurio registry.\n-This combines the advantage of less verbose messages (as messages do not contain the schema information themselves, but only a schema id)\n-with human-readable JSON.\n+Using Avro to serialize message keys and values is a Technology Preview feature. Technology Preview features are not supported with Red Hat production service-level agreements (SLAs) and might not be functionally complete; therefore, Red Hat does not recommend implementing any Technology Preview features in production environments. This Technology Preview feature provides early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process. For more information about support scope, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].\n ====\n+endif::product[]\n \n-Another option is using the Confluent schema registry, which is described further below.\n+// Type: concept\n+// Title: About the {registry}\n+[id=\"about-the-registry\"]\n+== About the {registry-name-full}\n \n-== Technical Information\n+ifdef::community[]\n+The link:https://github.com/Apicurio/apicurio-registry[{registry}] open-source project provides several components that work with Avro:\n+endif::community[]\n \n-A system that wants to use Avro serialization needs to complete two steps:\n+ifdef::product[]\n+{LinkServiceRegistryGetStart}[{registry-name-full}] provides several components that work with Avro:\n+endif::product[]\n \n-* Deploy an https://github.com/Apicurio/apicurio-registry[Apicurio API/Schema Registry] instance\n-* Install the Avro converter from https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka's _libs_ directory or directly into a plug-in directory\n-* Use thes the following properties to configure Apache Connect instance\n+* An Avro converter that you can configure in Kafka Connect workers. This converter maps Kafka Connect schemas to Avro schemas. The converter then uses the Avro schemas to serialize the message keys and values into Avro's compact binary form.\n \n-[source]\n+* An API/Schema registry that tracks:\n++\n+** Avro schemas that are used in Kafka topics\n+** Where the Avro converter sends the generated Avro schemas\n+\n++\n+Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n+This makes each message even smaller. For an I/O bound system like Kafka, this means more total throughput of the producers and consumers.\n+\n+* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n+Any Kafka consumer applications that you write to consume change event records can use Avro Serdes to deserialize the change event records.\n+\n+To use the {registry} with {prodname}, you must add {registry} converters and their dependencies to the Kafka Connect container image that you are using for running {prodname}.\n+\n+[NOTE]\n+====\n+The {registry} project also provides a JSON converter. This converter combines the advantage of less verbose messages with human-readable JSON. Messages do not contain the schema information themselves, but only a schema ID.\n+====\n+\n+ifdef::community[]\n+Another option is using the Confluent schema registry, which is described later.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1NTIyNg=="}, "originalCommit": null, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODQ0ODMzOnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMDoxNjozNlrOGmPtFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxOTo1MzoyOFrOGmfvpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1NjM3NA==", "bodyText": "Can you add a sentence after the listing like this: Alternatively, you can also configure specific connectors to use the Avro converter.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442756374", "createdAt": "2020-06-19T10:16:36Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. \n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Including schemas causes the messages to  be very verbose. If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the message keys and values by using link:https://avro.apache.org/[Apache Avro].\n+The Avro binary format is compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure. Avro's schema evolution mechanism makes it possible to evolve the schemas over time, which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables. Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema. Avro serialization makes it easier for consumers to adapt to the changing schema.\n \n-[NOTE]\n+ifdef::product[]\n+[IMPORTANT]\n ====\n-The Apicurio project also provides a JSON converter that can be used with the Apicurio registry.\n-This combines the advantage of less verbose messages (as messages do not contain the schema information themselves, but only a schema id)\n-with human-readable JSON.\n+Using Avro to serialize message keys and values is a Technology Preview feature. Technology Preview features are not supported with Red Hat production service-level agreements (SLAs) and might not be functionally complete; therefore, Red Hat does not recommend implementing any Technology Preview features in production environments. This Technology Preview feature provides early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process. For more information about support scope, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].\n ====\n+endif::product[]\n \n-Another option is using the Confluent schema registry, which is described further below.\n+// Type: concept\n+// Title: About the {registry}\n+[id=\"about-the-registry\"]\n+== About the {registry-name-full}\n \n-== Technical Information\n+ifdef::community[]\n+The link:https://github.com/Apicurio/apicurio-registry[{registry}] open-source project provides several components that work with Avro:\n+endif::community[]\n \n-A system that wants to use Avro serialization needs to complete two steps:\n+ifdef::product[]\n+{LinkServiceRegistryGetStart}[{registry-name-full}] provides several components that work with Avro:\n+endif::product[]\n \n-* Deploy an https://github.com/Apicurio/apicurio-registry[Apicurio API/Schema Registry] instance\n-* Install the Avro converter from https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka's _libs_ directory or directly into a plug-in directory\n-* Use thes the following properties to configure Apache Connect instance\n+* An Avro converter that you can configure in Kafka Connect workers. This converter maps Kafka Connect schemas to Avro schemas. The converter then uses the Avro schemas to serialize the message keys and values into Avro's compact binary form.\n \n-[source]\n+* An API/Schema registry that tracks:\n++\n+** Avro schemas that are used in Kafka topics\n+** Where the Avro converter sends the generated Avro schemas\n+\n++\n+Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n+This makes each message even smaller. For an I/O bound system like Kafka, this means more total throughput of the producers and consumers.\n+\n+* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n+Any Kafka consumer applications that you write to consume change event records can use Avro Serdes to deserialize the change event records.\n+\n+To use the {registry} with {prodname}, you must add {registry} converters and their dependencies to the Kafka Connect container image that you are using for running {prodname}.\n+\n+[NOTE]\n+====\n+The {registry} project also provides a JSON converter. This converter combines the advantage of less verbose messages with human-readable JSON. Messages do not contain the schema information themselves, but only a schema ID.\n+====\n+\n+ifdef::community[]\n+Another option is using the Confluent schema registry, which is described later.\n+endif::community[]\n+\n+// Type: concept\n+// Title: Overview of deploying a {prodname} connector that uses Avro serialization\n+[id=\"overview-of-deploying-a-debezium-connector-that-uses-avro-serializatioin\"]\n+== Overview\n+\n+To deploy a {prodname} connector that uses Avro serialization, there are three main tasks: \n+\n+ifdef::community[]\n+. Deploy an link:https://github.com/Apicurio/apicurio-registry[{registry-name-full}] instance.\n+endif::community[]\n+ifdef::product[]\n+. Deploy a link:{LinkServiceRegistryGetStart}[{registry-name-full}] instance by following the instructions in {NameServiceRegistryGetStart}.\n+endif::product[]\n+\n+ifdef::community[]\n+. Install the Avro converter from link:https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka Connect's _libs_ directory or directly into a plug-in directory.\n+endif::community[]\n+ifdef::product[]\n+. Install the Avro converter by downloading the {prodname} link:https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?product=red.hat.integration&downloadType=distributions[Service Registry Kafka Connect] zip file and extracting it into the {prodname} connector's directory.\n+endif::product[]\n+\n+. Configure a Kafka Connect instance with the following property settings: ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAwMTk4Mg==", "bodyText": "Can the word \"also\" be removed from the sentence that you want to add?\nAnd should this additional sentence be indented to align with the listing? I think so, but I'm not certain.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443001982", "createdAt": "2020-06-19T19:04:48Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. \n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Including schemas causes the messages to  be very verbose. If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the message keys and values by using link:https://avro.apache.org/[Apache Avro].\n+The Avro binary format is compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure. Avro's schema evolution mechanism makes it possible to evolve the schemas over time, which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables. Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema. Avro serialization makes it easier for consumers to adapt to the changing schema.\n \n-[NOTE]\n+ifdef::product[]\n+[IMPORTANT]\n ====\n-The Apicurio project also provides a JSON converter that can be used with the Apicurio registry.\n-This combines the advantage of less verbose messages (as messages do not contain the schema information themselves, but only a schema id)\n-with human-readable JSON.\n+Using Avro to serialize message keys and values is a Technology Preview feature. Technology Preview features are not supported with Red Hat production service-level agreements (SLAs) and might not be functionally complete; therefore, Red Hat does not recommend implementing any Technology Preview features in production environments. This Technology Preview feature provides early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process. For more information about support scope, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].\n ====\n+endif::product[]\n \n-Another option is using the Confluent schema registry, which is described further below.\n+// Type: concept\n+// Title: About the {registry}\n+[id=\"about-the-registry\"]\n+== About the {registry-name-full}\n \n-== Technical Information\n+ifdef::community[]\n+The link:https://github.com/Apicurio/apicurio-registry[{registry}] open-source project provides several components that work with Avro:\n+endif::community[]\n \n-A system that wants to use Avro serialization needs to complete two steps:\n+ifdef::product[]\n+{LinkServiceRegistryGetStart}[{registry-name-full}] provides several components that work with Avro:\n+endif::product[]\n \n-* Deploy an https://github.com/Apicurio/apicurio-registry[Apicurio API/Schema Registry] instance\n-* Install the Avro converter from https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka's _libs_ directory or directly into a plug-in directory\n-* Use thes the following properties to configure Apache Connect instance\n+* An Avro converter that you can configure in Kafka Connect workers. This converter maps Kafka Connect schemas to Avro schemas. The converter then uses the Avro schemas to serialize the message keys and values into Avro's compact binary form.\n \n-[source]\n+* An API/Schema registry that tracks:\n++\n+** Avro schemas that are used in Kafka topics\n+** Where the Avro converter sends the generated Avro schemas\n+\n++\n+Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n+This makes each message even smaller. For an I/O bound system like Kafka, this means more total throughput of the producers and consumers.\n+\n+* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n+Any Kafka consumer applications that you write to consume change event records can use Avro Serdes to deserialize the change event records.\n+\n+To use the {registry} with {prodname}, you must add {registry} converters and their dependencies to the Kafka Connect container image that you are using for running {prodname}.\n+\n+[NOTE]\n+====\n+The {registry} project also provides a JSON converter. This converter combines the advantage of less verbose messages with human-readable JSON. Messages do not contain the schema information themselves, but only a schema ID.\n+====\n+\n+ifdef::community[]\n+Another option is using the Confluent schema registry, which is described later.\n+endif::community[]\n+\n+// Type: concept\n+// Title: Overview of deploying a {prodname} connector that uses Avro serialization\n+[id=\"overview-of-deploying-a-debezium-connector-that-uses-avro-serializatioin\"]\n+== Overview\n+\n+To deploy a {prodname} connector that uses Avro serialization, there are three main tasks: \n+\n+ifdef::community[]\n+. Deploy an link:https://github.com/Apicurio/apicurio-registry[{registry-name-full}] instance.\n+endif::community[]\n+ifdef::product[]\n+. Deploy a link:{LinkServiceRegistryGetStart}[{registry-name-full}] instance by following the instructions in {NameServiceRegistryGetStart}.\n+endif::product[]\n+\n+ifdef::community[]\n+. Install the Avro converter from link:https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka Connect's _libs_ directory or directly into a plug-in directory.\n+endif::community[]\n+ifdef::product[]\n+. Install the Avro converter by downloading the {prodname} link:https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?product=red.hat.integration&downloadType=distributions[Service Registry Kafka Connect] zip file and extracting it into the {prodname} connector's directory.\n+endif::product[]\n+\n+. Configure a Kafka Connect instance with the following property settings: ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1NjM3NA=="}, "originalCommit": null, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAxOTE3Mw==", "bodyText": "Yes, you can remove the \"also\". Re indentiation, I don't know, perhaps even just in brackets after \"property settings (alternatively...)\".", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443019173", "createdAt": "2020-06-19T19:53:28Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. \n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Including schemas causes the messages to  be very verbose. If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the message keys and values by using link:https://avro.apache.org/[Apache Avro].\n+The Avro binary format is compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure. Avro's schema evolution mechanism makes it possible to evolve the schemas over time, which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables. Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema. Avro serialization makes it easier for consumers to adapt to the changing schema.\n \n-[NOTE]\n+ifdef::product[]\n+[IMPORTANT]\n ====\n-The Apicurio project also provides a JSON converter that can be used with the Apicurio registry.\n-This combines the advantage of less verbose messages (as messages do not contain the schema information themselves, but only a schema id)\n-with human-readable JSON.\n+Using Avro to serialize message keys and values is a Technology Preview feature. Technology Preview features are not supported with Red Hat production service-level agreements (SLAs) and might not be functionally complete; therefore, Red Hat does not recommend implementing any Technology Preview features in production environments. This Technology Preview feature provides early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process. For more information about support scope, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].\n ====\n+endif::product[]\n \n-Another option is using the Confluent schema registry, which is described further below.\n+// Type: concept\n+// Title: About the {registry}\n+[id=\"about-the-registry\"]\n+== About the {registry-name-full}\n \n-== Technical Information\n+ifdef::community[]\n+The link:https://github.com/Apicurio/apicurio-registry[{registry}] open-source project provides several components that work with Avro:\n+endif::community[]\n \n-A system that wants to use Avro serialization needs to complete two steps:\n+ifdef::product[]\n+{LinkServiceRegistryGetStart}[{registry-name-full}] provides several components that work with Avro:\n+endif::product[]\n \n-* Deploy an https://github.com/Apicurio/apicurio-registry[Apicurio API/Schema Registry] instance\n-* Install the Avro converter from https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka's _libs_ directory or directly into a plug-in directory\n-* Use thes the following properties to configure Apache Connect instance\n+* An Avro converter that you can configure in Kafka Connect workers. This converter maps Kafka Connect schemas to Avro schemas. The converter then uses the Avro schemas to serialize the message keys and values into Avro's compact binary form.\n \n-[source]\n+* An API/Schema registry that tracks:\n++\n+** Avro schemas that are used in Kafka topics\n+** Where the Avro converter sends the generated Avro schemas\n+\n++\n+Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n+This makes each message even smaller. For an I/O bound system like Kafka, this means more total throughput of the producers and consumers.\n+\n+* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n+Any Kafka consumer applications that you write to consume change event records can use Avro Serdes to deserialize the change event records.\n+\n+To use the {registry} with {prodname}, you must add {registry} converters and their dependencies to the Kafka Connect container image that you are using for running {prodname}.\n+\n+[NOTE]\n+====\n+The {registry} project also provides a JSON converter. This converter combines the advantage of less verbose messages with human-readable JSON. Messages do not contain the schema information themselves, but only a schema ID.\n+====\n+\n+ifdef::community[]\n+Another option is using the Confluent schema registry, which is described later.\n+endif::community[]\n+\n+// Type: concept\n+// Title: Overview of deploying a {prodname} connector that uses Avro serialization\n+[id=\"overview-of-deploying-a-debezium-connector-that-uses-avro-serializatioin\"]\n+== Overview\n+\n+To deploy a {prodname} connector that uses Avro serialization, there are three main tasks: \n+\n+ifdef::community[]\n+. Deploy an link:https://github.com/Apicurio/apicurio-registry[{registry-name-full}] instance.\n+endif::community[]\n+ifdef::product[]\n+. Deploy a link:{LinkServiceRegistryGetStart}[{registry-name-full}] instance by following the instructions in {NameServiceRegistryGetStart}.\n+endif::product[]\n+\n+ifdef::community[]\n+. Install the Avro converter from link:https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka Connect's _libs_ directory or directly into a plug-in directory.\n+endif::community[]\n+ifdef::product[]\n+. Install the Avro converter by downloading the {prodname} link:https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?product=red.hat.integration&downloadType=distributions[Service Registry Kafka Connect] zip file and extracting it into the {prodname} connector's directory.\n+endif::product[]\n+\n+. Configure a Kafka Connect instance with the following property settings: ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1NjM3NA=="}, "originalCommit": null, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODQ1ODU0OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMDoxOTo1N1rOGmPzoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxOTowODoyOFrOGmexhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1ODA0OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n          \n          \n            \n            In your environment, you might want to use a {prodname} pre-built container to deploy {prodname} connectors that use Avro serialization. Follow the procedure here to do that. In this procedure, you build a custom Kafka Connect container image for {prodname} which contains the Avro converter.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442758048", "createdAt": "2020-06-19T10:19:57Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAwMzI2OQ==", "bodyText": "Done.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443003269", "createdAt": "2020-06-19T19:08:28Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1ODA0OA=="}, "originalCommit": null, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODQ2MDgzOnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMDoyMDo0OFrOGmP1Bw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMDoyMDo0OFrOGmP1Bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1ODQwNw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            . Build a {prodname} container image that implements the Avro converter:\n          \n          \n            \n            . Build a {prodname} container image that contains the Avro converter:", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442758407", "createdAt": "2020-06-19T10:20:48Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.\n++\n+.. Create a new {registry} deployment by specifying the in-memory persistence option in one of the example custom resources, for example:\n++\n+`oc create -f https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/example-cr/in-memory.yaml`\n++\n+The in-memory deployment is not suitable for production. Use the Kafka Streams persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].\n+endif::product[]\n+\n+. Build a {prodname} container image that implements the Avro converter:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 188}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODQ2NTA5OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMDoyMjowMFrOGmP3jA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMDoxODoxN1rOGmgPqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1OTA1Mg==", "bodyText": "There's a variable defined for the version (in antora.yml in case of upstream docs): {apicurio-version}. Enable substitution for the listing in order to reference it.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442759052", "createdAt": "2020-06-19T10:22:00Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.\n++\n+.. Create a new {registry} deployment by specifying the in-memory persistence option in one of the example custom resources, for example:\n++\n+`oc create -f https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/example-cr/in-memory.yaml`\n++\n+The in-memory deployment is not suitable for production. Use the Kafka Streams persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].\n+endif::product[]\n+\n+. Build a {prodname} container image that implements the Avro converter:\n++\n+ifdef::community[]\n+.. Copy link:https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[`Dockerfile`] to a convenient location. This file has the following content: \n++\n+[listing,options=\"nowrap\"]\n+----\n+ARG DEBEZIUM_VERSION\n+FROM debezium/connect:$DEBEZIUM_VERSION\n+ENV KAFKA_CONNECT_DEBEZIUM_DIR=$KAFKA_CONNECT_PLUGINS_DIR/debezium-connector-mysql\n+ENV APICURIO_VERSION=1.1.2.Final", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAyNzM2OQ==", "bodyText": "Done. In next commit.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443027369", "createdAt": "2020-06-19T20:18:17Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.\n++\n+.. Create a new {registry} deployment by specifying the in-memory persistence option in one of the example custom resources, for example:\n++\n+`oc create -f https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/example-cr/in-memory.yaml`\n++\n+The in-memory deployment is not suitable for production. Use the Kafka Streams persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].\n+endif::product[]\n+\n+. Build a {prodname} container image that implements the Avro converter:\n++\n+ifdef::community[]\n+.. Copy link:https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[`Dockerfile`] to a convenient location. This file has the following content: \n++\n+[listing,options=\"nowrap\"]\n+----\n+ARG DEBEZIUM_VERSION\n+FROM debezium/connect:$DEBEZIUM_VERSION\n+ENV KAFKA_CONNECT_DEBEZIUM_DIR=$KAFKA_CONNECT_PLUGINS_DIR/debezium-connector-mysql\n+ENV APICURIO_VERSION=1.1.2.Final", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1OTA1Mg=="}, "originalCommit": null, "originalPosition": 198}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODQ2OTE3OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMDoyMzoyNlrOGmP6EA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMDoxODo0OFrOGmgQXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1OTY5Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            . Run a Kafka Connect image that is configured to use Avro:\n          \n          \n            \n            . Run the newly built Kafka Connect image, configuring it so it uses the Avro converter (or configure specific converters to do so):", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442759696", "createdAt": "2020-06-19T10:23:26Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.\n++\n+.. Create a new {registry} deployment by specifying the in-memory persistence option in one of the example custom resources, for example:\n++\n+`oc create -f https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/example-cr/in-memory.yaml`\n++\n+The in-memory deployment is not suitable for production. Use the Kafka Streams persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].\n+endif::product[]\n+\n+. Build a {prodname} container image that implements the Avro converter:\n++\n+ifdef::community[]\n+.. Copy link:https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[`Dockerfile`] to a convenient location. This file has the following content: \n++\n+[listing,options=\"nowrap\"]\n+----\n+ARG DEBEZIUM_VERSION\n+FROM debezium/connect:$DEBEZIUM_VERSION\n+ENV KAFKA_CONNECT_DEBEZIUM_DIR=$KAFKA_CONNECT_PLUGINS_DIR/debezium-connector-mysql\n+ENV APICURIO_VERSION=1.1.2.Final\n \n-Build a {prodname} image with Avro converter from https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[Dockerfile]:\n+RUN cd $KAFKA_CONNECT_DEBEZIUM_DIR &&\\\n+    curl https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/$APICURIO_VERSION/apicurio-registry-distro-connect-converter-$APICURIO_VERSION-converter.tar.gz | tar xzv\n+----\n \n-[source]\n-[subs=\"attributes\"]\n+.. Run the following command: \n++\n+[source,subs=\"attributes\"]\n ----\n docker build --build-arg DEBEZIUM_VERSION={debezium-docker-label} -t debezium/connect-apicurio:{debezium-docker-label} .\n ----\n-\n-Run a Kafka Connect image configured to use Avro:\n-\n-[source]\n-[subs=\"attributes\"]\n+endif::community[]\n+ifdef::product[]\n+.. Download the link:https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?product=red.hat.integration&downloadType=distributions[{registry} Kafka Connect] zip file. \n+.. Extract the content into the directory that contains the {prodname} connector that you are configuring to use Avro serialization. \n+.. Create a custom image for Kafka Connect. See link:{LinkCDCInstallOpenShift}[NameCDCInstallOpenShift] for an example of how to do this. Start with the `Dockerfile` in that example. Then add the {registry} converters to the connector directories. \n+endif::product[]\n+\n+. Run a Kafka Connect image that is configured to use Avro:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 225}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAwNTUyOA==", "bodyText": "I don't understand the last part of your suggestion.\nWhat do you mean by \"configure specific converters to do so\"? -- It sounds like this says that you can configure other converters to use the Avro converter. Is that what you mean?", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443005528", "createdAt": "2020-06-19T19:14:39Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.\n++\n+.. Create a new {registry} deployment by specifying the in-memory persistence option in one of the example custom resources, for example:\n++\n+`oc create -f https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/example-cr/in-memory.yaml`\n++\n+The in-memory deployment is not suitable for production. Use the Kafka Streams persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].\n+endif::product[]\n+\n+. Build a {prodname} container image that implements the Avro converter:\n++\n+ifdef::community[]\n+.. Copy link:https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[`Dockerfile`] to a convenient location. This file has the following content: \n++\n+[listing,options=\"nowrap\"]\n+----\n+ARG DEBEZIUM_VERSION\n+FROM debezium/connect:$DEBEZIUM_VERSION\n+ENV KAFKA_CONNECT_DEBEZIUM_DIR=$KAFKA_CONNECT_PLUGINS_DIR/debezium-connector-mysql\n+ENV APICURIO_VERSION=1.1.2.Final\n \n-Build a {prodname} image with Avro converter from https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[Dockerfile]:\n+RUN cd $KAFKA_CONNECT_DEBEZIUM_DIR &&\\\n+    curl https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/$APICURIO_VERSION/apicurio-registry-distro-connect-converter-$APICURIO_VERSION-converter.tar.gz | tar xzv\n+----\n \n-[source]\n-[subs=\"attributes\"]\n+.. Run the following command: \n++\n+[source,subs=\"attributes\"]\n ----\n docker build --build-arg DEBEZIUM_VERSION={debezium-docker-label} -t debezium/connect-apicurio:{debezium-docker-label} .\n ----\n-\n-Run a Kafka Connect image configured to use Avro:\n-\n-[source]\n-[subs=\"attributes\"]\n+endif::community[]\n+ifdef::product[]\n+.. Download the link:https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?product=red.hat.integration&downloadType=distributions[{registry} Kafka Connect] zip file. \n+.. Extract the content into the directory that contains the {prodname} connector that you are configuring to use Avro serialization. \n+.. Create a custom image for Kafka Connect. See link:{LinkCDCInstallOpenShift}[NameCDCInstallOpenShift] for an example of how to do this. Start with the `Dockerfile` in that example. Then add the {registry} converters to the connector directories. \n+endif::product[]\n+\n+. Run a Kafka Connect image that is configured to use Avro:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1OTY5Ng=="}, "originalCommit": null, "originalPosition": 225}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAxMzU5MA==", "bodyText": "Ah, sorry, I meant \"specific connectors\". Converters can be configured globally for the entire Connect worker or individually for connectors. As commented somewhere else, I think ideally we'd only discuss the latter.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443013590", "createdAt": "2020-06-19T19:37:25Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.\n++\n+.. Create a new {registry} deployment by specifying the in-memory persistence option in one of the example custom resources, for example:\n++\n+`oc create -f https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/example-cr/in-memory.yaml`\n++\n+The in-memory deployment is not suitable for production. Use the Kafka Streams persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].\n+endif::product[]\n+\n+. Build a {prodname} container image that implements the Avro converter:\n++\n+ifdef::community[]\n+.. Copy link:https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[`Dockerfile`] to a convenient location. This file has the following content: \n++\n+[listing,options=\"nowrap\"]\n+----\n+ARG DEBEZIUM_VERSION\n+FROM debezium/connect:$DEBEZIUM_VERSION\n+ENV KAFKA_CONNECT_DEBEZIUM_DIR=$KAFKA_CONNECT_PLUGINS_DIR/debezium-connector-mysql\n+ENV APICURIO_VERSION=1.1.2.Final\n \n-Build a {prodname} image with Avro converter from https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[Dockerfile]:\n+RUN cd $KAFKA_CONNECT_DEBEZIUM_DIR &&\\\n+    curl https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/$APICURIO_VERSION/apicurio-registry-distro-connect-converter-$APICURIO_VERSION-converter.tar.gz | tar xzv\n+----\n \n-[source]\n-[subs=\"attributes\"]\n+.. Run the following command: \n++\n+[source,subs=\"attributes\"]\n ----\n docker build --build-arg DEBEZIUM_VERSION={debezium-docker-label} -t debezium/connect-apicurio:{debezium-docker-label} .\n ----\n-\n-Run a Kafka Connect image configured to use Avro:\n-\n-[source]\n-[subs=\"attributes\"]\n+endif::community[]\n+ifdef::product[]\n+.. Download the link:https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?product=red.hat.integration&downloadType=distributions[{registry} Kafka Connect] zip file. \n+.. Extract the content into the directory that contains the {prodname} connector that you are configuring to use Avro serialization. \n+.. Create a custom image for Kafka Connect. See link:{LinkCDCInstallOpenShift}[NameCDCInstallOpenShift] for an example of how to do this. Start with the `Dockerfile` in that example. Then add the {registry} converters to the connector directories. \n+endif::product[]\n+\n+. Run a Kafka Connect image that is configured to use Avro:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1OTY5Ng=="}, "originalCommit": null, "originalPosition": 225}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAyNzU0OQ==", "bodyText": "Fixed for now. Will revisit to do better.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443027549", "createdAt": "2020-06-19T20:18:48Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.\n++\n+.. Create a new {registry} deployment by specifying the in-memory persistence option in one of the example custom resources, for example:\n++\n+`oc create -f https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/example-cr/in-memory.yaml`\n++\n+The in-memory deployment is not suitable for production. Use the Kafka Streams persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].\n+endif::product[]\n+\n+. Build a {prodname} container image that implements the Avro converter:\n++\n+ifdef::community[]\n+.. Copy link:https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[`Dockerfile`] to a convenient location. This file has the following content: \n++\n+[listing,options=\"nowrap\"]\n+----\n+ARG DEBEZIUM_VERSION\n+FROM debezium/connect:$DEBEZIUM_VERSION\n+ENV KAFKA_CONNECT_DEBEZIUM_DIR=$KAFKA_CONNECT_PLUGINS_DIR/debezium-connector-mysql\n+ENV APICURIO_VERSION=1.1.2.Final\n \n-Build a {prodname} image with Avro converter from https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[Dockerfile]:\n+RUN cd $KAFKA_CONNECT_DEBEZIUM_DIR &&\\\n+    curl https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/$APICURIO_VERSION/apicurio-registry-distro-connect-converter-$APICURIO_VERSION-converter.tar.gz | tar xzv\n+----\n \n-[source]\n-[subs=\"attributes\"]\n+.. Run the following command: \n++\n+[source,subs=\"attributes\"]\n ----\n docker build --build-arg DEBEZIUM_VERSION={debezium-docker-label} -t debezium/connect-apicurio:{debezium-docker-label} .\n ----\n-\n-Run a Kafka Connect image configured to use Avro:\n-\n-[source]\n-[subs=\"attributes\"]\n+endif::community[]\n+ifdef::product[]\n+.. Download the link:https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?product=red.hat.integration&downloadType=distributions[{registry} Kafka Connect] zip file. \n+.. Extract the content into the directory that contains the {prodname} connector that you are configuring to use Avro serialization. \n+.. Create a custom image for Kafka Connect. See link:{LinkCDCInstallOpenShift}[NameCDCInstallOpenShift] for an example of how to do this. Start with the `Dockerfile` in that example. Then add the {registry} converters to the connector directories. \n+endif::product[]\n+\n+. Run a Kafka Connect image that is configured to use Avro:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1OTY5Ng=="}, "originalCommit": null, "originalPosition": 225}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODQ5MjExOnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMDozMjoxOVrOGmQI5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxOTo1OTozNVrOGohDVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2MzQ5Mg==", "bodyText": "Is this the right resource for downstream?", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442763492", "createdAt": "2020-06-19T10:32:19Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAwNzAxNQ==", "bodyText": "I asked Steve McCarthy, to point me to the downstream resource and this is what he pointed me to. I don't know his GitHub user name. I'll find out and tag him here to confirm.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443007015", "createdAt": "2020-06-19T19:18:49Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2MzQ5Mg=="}, "originalCommit": null, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAxOTU1MA==", "bodyText": "Ok, let's clarify that. In particular using \"master\" seems really odd for product, I'd expect a fixed reference instead of a live branch.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443019550", "createdAt": "2020-06-19T19:54:31Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2MzQ5Mg=="}, "originalCommit": null, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzE2NjUzMQ==", "bodyText": "Adding @smccarthy-ie , to help us straighten this out.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443166531", "createdAt": "2020-06-20T23:10:01Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2MzQ5Mg=="}, "originalCommit": null, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzIyNzY5NQ==", "bodyText": "@TovaCohen @gunnarmorling, OK, some confusion around upstream/downstream product names. You were asking about Apicurio Registry (upstream) installation, but Service Registry is the downstream product :)\nThe latest downstream Service Registry docs show using the OperatorHub in the OpenShift UI. I don't know the command line equivalent.\n@jsenko Can you tell us what is the downstream equivalent of curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443227695", "createdAt": "2020-06-21T14:56:30Z", "author": {"login": "smccarthy-ie"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2MzQ5Mg=="}, "originalCommit": null, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEzNzc1MQ==", "bodyText": "I replaced these instructions with a cross-reference to follow the instructions in Getting Started with Service Registry, \"Installing Service Registry from the OperatorHub\".\n@gunnarmorling  Let me know if you think we also need to provide a command line alternative.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r445137751", "createdAt": "2020-06-24T19:59:35Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2MzQ5Mg=="}, "originalCommit": null, "originalPosition": 177}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODQ5Mjg5OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMDozMjozOFrOGmQJYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMDowMDo0MlrOGohFnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2MzYxOA==", "bodyText": "This surely should be a stable released version, not master?", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442763618", "createdAt": "2020-06-19T10:32:38Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAxMTc5OA==", "bodyText": "I will ask Steve McCarthy to confirm.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443011798", "createdAt": "2020-06-19T19:32:01Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2MzYxOA=="}, "originalCommit": null, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzE2NjUxNA==", "bodyText": "Adding @smccarthy-ie , to help us straighten this out.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443166514", "createdAt": "2020-06-20T23:09:38Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2MzYxOA=="}, "originalCommit": null, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMzMDM3OQ==", "bodyText": "Latest stable released version is 0.0.2 (tag name is the same)", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r444330379", "createdAt": "2020-06-23T15:53:22Z", "author": {"login": "jsenko"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2MzYxOA=="}, "originalCommit": null, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEzODMzMw==", "bodyText": "@gunnarmorling  I removed this instruction. Let me know if you want me to put back an updated command line.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r445138333", "createdAt": "2020-06-24T20:00:42Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2MzYxOA=="}, "originalCommit": null, "originalPosition": 179}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODQ5NDE5OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMDozMzowNFrOGmQKPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxOToyMTozN1rOGmfEOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2MzgzOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The in-memory deployment is not suitable for production. Use the Kafka Streams persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].\n          \n          \n            \n            The in-memory deployment is not suitable for production. Use the Apache Kafka persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442763839", "createdAt": "2020-06-19T10:33:04Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.\n++\n+.. Create a new {registry} deployment by specifying the in-memory persistence option in one of the example custom resources, for example:\n++\n+`oc create -f https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/example-cr/in-memory.yaml`\n++\n+The in-memory deployment is not suitable for production. Use the Kafka Streams persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAwODA1OQ==", "bodyText": "Done", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443008059", "createdAt": "2020-06-19T19:21:37Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.\n++\n+.. Create a new {registry} deployment by specifying the in-memory persistence option in one of the example custom resources, for example:\n++\n+`oc create -f https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/example-cr/in-memory.yaml`\n++\n+The in-memory deployment is not suitable for production. Use the Kafka Streams persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2MzgzOQ=="}, "originalCommit": null, "originalPosition": 185}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1ODUwNzkwOnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMDozODo0NlrOGmQTPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMDowMjowNVrOGohIgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2NjE0MA==", "bodyText": "Ok, so that's good to show this connector-specific configuration here. Taking a step back, there's still some inconsistency overall. The situation is this:\n\nAvro can be enabled on the Kafka Connect worker level (as shown above via \"Configure a Kafka Connect instance with the following property settings\", and also then via the environment variables in the community variant)\nAvro can be enabled for a specific connector (as shown here for product, it's currently not shown for the community variant)\n\nMy general recommendation would be to only show the second approach, enabling it for a specific connector, and not mention the worker-level approach, as it's less flexible.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442766140", "createdAt": "2020-06-19T10:38:46Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -112,19 +202,90 @@ docker run -it --rm --name connect \\\n     -e CONNECT_VALUE_CONVERTER_APICURIO_REGISTRY_GLOBAL-ID=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy \\\n     -p 8083:8083 debezium/connect-apicurio:{debezium-docker-label}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Follow the steps in the link:{LinkCDCGettingStarted}#deploying-kafka-connect[example of deploying Kafka Connect] in {NameCDCGettingStarted}.\n+.. Open the `examples/kafka-connect/kafka-connect-s2i-single-node-kafka.yaml` file that you used to deploy Kafka Connect.\n++\n+Before you can create the connector instance,\n+you must first enable connector resources in the `KafkaConnectS2I` Custom Resource (CR).\n+\n+.. In the `metadata.annotations` section, enable Kafka Connect to use connector resources.\n++\n+.kafka-connect-s2i-single-node-kafka.yaml\n+[source,yaml,options=\"nowrap\"]\n+----\n+apiVersion: kafka.strimzi.io/v1beta1\n+kind: KafkaConnectS2I\n+metadata:\n+  name: my-connect-cluster\n+  annotations:\n+    strimzi.io/use-connector-resources: \"true\"\n+spec:\n+   ...\n+----\n \n+.. Apply the updated `kafka-connect-s2i-single-node-kafka.yaml` file to update the `KafkaConnectS2I` CR:\n++\n+`oc apply -f kafka-connect-s2i-single-node-kafka.yaml`\n+\n+.. In the Kafka Connect CR that defines the connector, add the properties that are required by the Avro converter. The CR looks like this:\n++\n+.inventory-connector.yaml\n+[source,yaml,options=\"nowrap\"]\n+----\n+  apiVersion: kafka.strimzi.io/v1beta1\n+  kind: KafkaConnector\n+  metadata:\n+    name: inventory-connector  \n+    labels:\n+      strimzi.io/cluster: my-connect-cluster\n+  spec:\n+    class: io.debezium.connector.mysql.MySqlConnector\n+    tasksMax: 1  \n+    config:  \n+      database.hostname: mysql  \n+      database.port: 3306\n+      database.user: debezium\n+      database.password: dbz\n+      database.server.id: 184054  \n+      database.server.name: dbserver1  \n+      database.whitelist: inventory  \n+      database.history.kafka.bootstrap.servers: my-cluster-kafka-bootstrap:9092  \n+      database.history.kafka.topic: schema-changes.inventory  \n+      key.converter: io.apicurio.registry.utils.converter.AvroConverter\n+      key.converter.apicurio.registry.url: http://apicurio:8080/api\n+      key.converter.apicurio.registry.global-id: io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n+      value.converter: io.apicurio.registry.utils.converter.AvroConverter\n+      value.converter.apicurio.registry.url: http://apicurio:8080/api\n+      value.converter.apicurio.registry.global-id: io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 303}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAxMDYwNg==", "bodyText": "I don't know how to implement your recommendation - that is, I don't know how to change the doc. I'll commit the changes I made and then perhaps you can suggest the specific changes you want to see. I'll be happy to accept/commit your suggestions. This time around, I didn't Commit Suggestions because in the past Chris has recommended one commit with all the updates.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443010606", "createdAt": "2020-06-19T19:28:40Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -112,19 +202,90 @@ docker run -it --rm --name connect \\\n     -e CONNECT_VALUE_CONVERTER_APICURIO_REGISTRY_GLOBAL-ID=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy \\\n     -p 8083:8083 debezium/connect-apicurio:{debezium-docker-label}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Follow the steps in the link:{LinkCDCGettingStarted}#deploying-kafka-connect[example of deploying Kafka Connect] in {NameCDCGettingStarted}.\n+.. Open the `examples/kafka-connect/kafka-connect-s2i-single-node-kafka.yaml` file that you used to deploy Kafka Connect.\n++\n+Before you can create the connector instance,\n+you must first enable connector resources in the `KafkaConnectS2I` Custom Resource (CR).\n+\n+.. In the `metadata.annotations` section, enable Kafka Connect to use connector resources.\n++\n+.kafka-connect-s2i-single-node-kafka.yaml\n+[source,yaml,options=\"nowrap\"]\n+----\n+apiVersion: kafka.strimzi.io/v1beta1\n+kind: KafkaConnectS2I\n+metadata:\n+  name: my-connect-cluster\n+  annotations:\n+    strimzi.io/use-connector-resources: \"true\"\n+spec:\n+   ...\n+----\n \n+.. Apply the updated `kafka-connect-s2i-single-node-kafka.yaml` file to update the `KafkaConnectS2I` CR:\n++\n+`oc apply -f kafka-connect-s2i-single-node-kafka.yaml`\n+\n+.. In the Kafka Connect CR that defines the connector, add the properties that are required by the Avro converter. The CR looks like this:\n++\n+.inventory-connector.yaml\n+[source,yaml,options=\"nowrap\"]\n+----\n+  apiVersion: kafka.strimzi.io/v1beta1\n+  kind: KafkaConnector\n+  metadata:\n+    name: inventory-connector  \n+    labels:\n+      strimzi.io/cluster: my-connect-cluster\n+  spec:\n+    class: io.debezium.connector.mysql.MySqlConnector\n+    tasksMax: 1  \n+    config:  \n+      database.hostname: mysql  \n+      database.port: 3306\n+      database.user: debezium\n+      database.password: dbz\n+      database.server.id: 184054  \n+      database.server.name: dbserver1  \n+      database.whitelist: inventory  \n+      database.history.kafka.bootstrap.servers: my-cluster-kafka-bootstrap:9092  \n+      database.history.kafka.topic: schema-changes.inventory  \n+      key.converter: io.apicurio.registry.utils.converter.AvroConverter\n+      key.converter.apicurio.registry.url: http://apicurio:8080/api\n+      key.converter.apicurio.registry.global-id: io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n+      value.converter: io.apicurio.registry.utils.converter.AvroConverter\n+      value.converter.apicurio.registry.url: http://apicurio:8080/api\n+      value.converter.apicurio.registry.global-id: io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2NjE0MA=="}, "originalCommit": null, "originalPosition": 303}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAxOTk0NQ==", "bodyText": "Ok. Let me know once you're done from your side and I'll try and make that change myself.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443019945", "createdAt": "2020-06-19T19:55:41Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -112,19 +202,90 @@ docker run -it --rm --name connect \\\n     -e CONNECT_VALUE_CONVERTER_APICURIO_REGISTRY_GLOBAL-ID=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy \\\n     -p 8083:8083 debezium/connect-apicurio:{debezium-docker-label}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Follow the steps in the link:{LinkCDCGettingStarted}#deploying-kafka-connect[example of deploying Kafka Connect] in {NameCDCGettingStarted}.\n+.. Open the `examples/kafka-connect/kafka-connect-s2i-single-node-kafka.yaml` file that you used to deploy Kafka Connect.\n++\n+Before you can create the connector instance,\n+you must first enable connector resources in the `KafkaConnectS2I` Custom Resource (CR).\n+\n+.. In the `metadata.annotations` section, enable Kafka Connect to use connector resources.\n++\n+.kafka-connect-s2i-single-node-kafka.yaml\n+[source,yaml,options=\"nowrap\"]\n+----\n+apiVersion: kafka.strimzi.io/v1beta1\n+kind: KafkaConnectS2I\n+metadata:\n+  name: my-connect-cluster\n+  annotations:\n+    strimzi.io/use-connector-resources: \"true\"\n+spec:\n+   ...\n+----\n \n+.. Apply the updated `kafka-connect-s2i-single-node-kafka.yaml` file to update the `KafkaConnectS2I` CR:\n++\n+`oc apply -f kafka-connect-s2i-single-node-kafka.yaml`\n+\n+.. In the Kafka Connect CR that defines the connector, add the properties that are required by the Avro converter. The CR looks like this:\n++\n+.inventory-connector.yaml\n+[source,yaml,options=\"nowrap\"]\n+----\n+  apiVersion: kafka.strimzi.io/v1beta1\n+  kind: KafkaConnector\n+  metadata:\n+    name: inventory-connector  \n+    labels:\n+      strimzi.io/cluster: my-connect-cluster\n+  spec:\n+    class: io.debezium.connector.mysql.MySqlConnector\n+    tasksMax: 1  \n+    config:  \n+      database.hostname: mysql  \n+      database.port: 3306\n+      database.user: debezium\n+      database.password: dbz\n+      database.server.id: 184054  \n+      database.server.name: dbserver1  \n+      database.whitelist: inventory  \n+      database.history.kafka.bootstrap.servers: my-cluster-kafka-bootstrap:9092  \n+      database.history.kafka.topic: schema-changes.inventory  \n+      key.converter: io.apicurio.registry.utils.converter.AvroConverter\n+      key.converter.apicurio.registry.url: http://apicurio:8080/api\n+      key.converter.apicurio.registry.global-id: io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n+      value.converter: io.apicurio.registry.utils.converter.AvroConverter\n+      value.converter.apicurio.registry.url: http://apicurio:8080/api\n+      value.converter.apicurio.registry.global-id: io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2NjE0MA=="}, "originalCommit": null, "originalPosition": 303}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAyMDM4Ng==", "bodyText": "In general though, the idea is to solely show how the Avro converter is used at the connector level instead of showing how to enable it globally for the Kafka Connect worker node. That's already the case for the product variant. For upstream it'd mean to show the right curl POST command with the equivalent configuration.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r443020386", "createdAt": "2020-06-19T19:57:09Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -112,19 +202,90 @@ docker run -it --rm --name connect \\\n     -e CONNECT_VALUE_CONVERTER_APICURIO_REGISTRY_GLOBAL-ID=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy \\\n     -p 8083:8083 debezium/connect-apicurio:{debezium-docker-label}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Follow the steps in the link:{LinkCDCGettingStarted}#deploying-kafka-connect[example of deploying Kafka Connect] in {NameCDCGettingStarted}.\n+.. Open the `examples/kafka-connect/kafka-connect-s2i-single-node-kafka.yaml` file that you used to deploy Kafka Connect.\n++\n+Before you can create the connector instance,\n+you must first enable connector resources in the `KafkaConnectS2I` Custom Resource (CR).\n+\n+.. In the `metadata.annotations` section, enable Kafka Connect to use connector resources.\n++\n+.kafka-connect-s2i-single-node-kafka.yaml\n+[source,yaml,options=\"nowrap\"]\n+----\n+apiVersion: kafka.strimzi.io/v1beta1\n+kind: KafkaConnectS2I\n+metadata:\n+  name: my-connect-cluster\n+  annotations:\n+    strimzi.io/use-connector-resources: \"true\"\n+spec:\n+   ...\n+----\n \n+.. Apply the updated `kafka-connect-s2i-single-node-kafka.yaml` file to update the `KafkaConnectS2I` CR:\n++\n+`oc apply -f kafka-connect-s2i-single-node-kafka.yaml`\n+\n+.. In the Kafka Connect CR that defines the connector, add the properties that are required by the Avro converter. The CR looks like this:\n++\n+.inventory-connector.yaml\n+[source,yaml,options=\"nowrap\"]\n+----\n+  apiVersion: kafka.strimzi.io/v1beta1\n+  kind: KafkaConnector\n+  metadata:\n+    name: inventory-connector  \n+    labels:\n+      strimzi.io/cluster: my-connect-cluster\n+  spec:\n+    class: io.debezium.connector.mysql.MySqlConnector\n+    tasksMax: 1  \n+    config:  \n+      database.hostname: mysql  \n+      database.port: 3306\n+      database.user: debezium\n+      database.password: dbz\n+      database.server.id: 184054  \n+      database.server.name: dbserver1  \n+      database.whitelist: inventory  \n+      database.history.kafka.bootstrap.servers: my-cluster-kafka-bootstrap:9092  \n+      database.history.kafka.topic: schema-changes.inventory  \n+      key.converter: io.apicurio.registry.utils.converter.AvroConverter\n+      key.converter.apicurio.registry.url: http://apicurio:8080/api\n+      key.converter.apicurio.registry.global-id: io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n+      value.converter: io.apicurio.registry.utils.converter.AvroConverter\n+      value.converter.apicurio.registry.url: http://apicurio:8080/api\n+      value.converter.apicurio.registry.global-id: io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2NjE0MA=="}, "originalCommit": null, "originalPosition": 303}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEzOTA3Mw==", "bodyText": "I removed all references to workers. The doc now talks only about configuring a Debezium connector, upstream and downstream.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r445139073", "createdAt": "2020-06-24T20:02:05Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -112,19 +202,90 @@ docker run -it --rm --name connect \\\n     -e CONNECT_VALUE_CONVERTER_APICURIO_REGISTRY_GLOBAL-ID=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy \\\n     -p 8083:8083 debezium/connect-apicurio:{debezium-docker-label}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Follow the steps in the link:{LinkCDCGettingStarted}#deploying-kafka-connect[example of deploying Kafka Connect] in {NameCDCGettingStarted}.\n+.. Open the `examples/kafka-connect/kafka-connect-s2i-single-node-kafka.yaml` file that you used to deploy Kafka Connect.\n++\n+Before you can create the connector instance,\n+you must first enable connector resources in the `KafkaConnectS2I` Custom Resource (CR).\n+\n+.. In the `metadata.annotations` section, enable Kafka Connect to use connector resources.\n++\n+.kafka-connect-s2i-single-node-kafka.yaml\n+[source,yaml,options=\"nowrap\"]\n+----\n+apiVersion: kafka.strimzi.io/v1beta1\n+kind: KafkaConnectS2I\n+metadata:\n+  name: my-connect-cluster\n+  annotations:\n+    strimzi.io/use-connector-resources: \"true\"\n+spec:\n+   ...\n+----\n \n+.. Apply the updated `kafka-connect-s2i-single-node-kafka.yaml` file to update the `KafkaConnectS2I` CR:\n++\n+`oc apply -f kafka-connect-s2i-single-node-kafka.yaml`\n+\n+.. In the Kafka Connect CR that defines the connector, add the properties that are required by the Avro converter. The CR looks like this:\n++\n+.inventory-connector.yaml\n+[source,yaml,options=\"nowrap\"]\n+----\n+  apiVersion: kafka.strimzi.io/v1beta1\n+  kind: KafkaConnector\n+  metadata:\n+    name: inventory-connector  \n+    labels:\n+      strimzi.io/cluster: my-connect-cluster\n+  spec:\n+    class: io.debezium.connector.mysql.MySqlConnector\n+    tasksMax: 1  \n+    config:  \n+      database.hostname: mysql  \n+      database.port: 3306\n+      database.user: debezium\n+      database.password: dbz\n+      database.server.id: 184054  \n+      database.server.name: dbserver1  \n+      database.whitelist: inventory  \n+      database.history.kafka.bootstrap.servers: my-cluster-kafka-bootstrap:9092  \n+      database.history.kafka.topic: schema-changes.inventory  \n+      key.converter: io.apicurio.registry.utils.converter.AvroConverter\n+      key.converter.apicurio.registry.url: http://apicurio:8080/api\n+      key.converter.apicurio.registry.global-id: io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n+      value.converter: io.apicurio.registry.utils.converter.AvroConverter\n+      value.converter.apicurio.registry.url: http://apicurio:8080/api\n+      value.converter.apicurio.registry.global-id: io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2NjE0MA=="}, "originalCommit": null, "originalPosition": 303}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NzIyNTk2OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDowOToxM1rOGnigTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxNjoxMzoyMVrOGoZVew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExMjk3Mg==", "bodyText": "I don't think the tutorial link will work downstream, or would it? Rather link to the \"Getting Started\" documented for that?", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r444112972", "createdAt": "2020-06-23T10:09:13Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,94 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMxNjcyNA==", "bodyText": "At this point, I cannot publish a rendered version of this content on the Pantheon Staging site. The Avro content is hidden for the June 25th release. On June 25th, we publish the Stage version of the Debezium User Guide. So it cannot show the Avro serialization content.  I hope that's clear. Let me know if not!", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r444316724", "createdAt": "2020-06-23T15:33:20Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,94 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExMjk3Mg=="}, "originalCommit": null, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDc1MjgyOQ==", "bodyText": "Sorry, I'm not quite sure I'm following. I was referring to the link {prodname} {link-prefix}:{link-tutorial}[tutorial] which points to a resource (tutorial) which doesn't exist in downstream docs. Instead, the \"Getting Started\" guide should be linked in a conditional for downstream.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r444752829", "createdAt": "2020-06-24T09:06:46Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,94 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExMjk3Mg=="}, "originalCommit": null, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTAxMTMyMw==", "bodyText": "As for your question about the link to the tutorial - yes, that will work downstream. The downstream versions of these attributes are set to display the customer portal Debezium Getting Started Guide. I think that the clickable text of \"tutorial\" is okay as a generic term. But if you prefer to see \"Debezium Getting Started Guide\" as the clickable text, let me know. I would need to define a new document attribute to do that.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r445011323", "createdAt": "2020-06-24T16:13:21Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,94 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExMjk3Mg=="}, "originalCommit": null, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3NjQ0NTI5OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNDoyMDo1MFrOGo8_pQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMToyMDoxNVrOGqMOHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTU5NTU1Nw==", "bodyText": "@TovaCohen, I somehow had missed this before (thanks to @jcechace for pointing it out), here you're adding new S2I docs. While we agreed to only fully transition to the custom containers approach for the next release docs, why adding more S2I-flavoured this time, which we only will have to modify next time?", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r445595557", "createdAt": "2020-06-25T14:20:50Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -112,26 +194,98 @@ docker run -it --rm --name connect \\\n     -e CONNECT_VALUE_CONVERTER_APICURIO_REGISTRY_GLOBAL-ID=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy \\\n     -p 8083:8083 debezium/connect-apicurio:{debezium-docker-label}\n ----\n+endif::community[]\n+ifdef::product[]\n+\n+.. Follow the steps in the link:{LinkCDCGettingStarted}#deploying-kafka-connect[example of deploying Kafka Connect] in {NameCDCGettingStarted}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 244}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY0NTU4Mg==", "bodyText": "@gunnarmorling I agree that this is the wrong way to update the doc. But I don't know how to make it right. Please see my previous comment.  I recognize the inconsistency and the disconnect in the procedure. But I don't have any background with regard to OpenShift images. I plan to remedy that. I'll go do some reading now. But I need precise input from you or Jiri or someone to correct this documentation for this release.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r445645582", "createdAt": "2020-06-25T15:28:26Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -112,26 +194,98 @@ docker run -it --rm --name connect \\\n     -e CONNECT_VALUE_CONVERTER_APICURIO_REGISTRY_GLOBAL-ID=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy \\\n     -p 8083:8083 debezium/connect-apicurio:{debezium-docker-label}\n ----\n+endif::community[]\n+ifdef::product[]\n+\n+.. Follow the steps in the link:{LinkCDCGettingStarted}#deploying-kafka-connect[example of deploying Kafka Connect] in {NameCDCGettingStarted}.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTU5NTU1Nw=="}, "originalCommit": null, "originalPosition": 244}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5MzU5OA==", "bodyText": "Not only that... this procedure starts with Dockerfile (correct way of building custom Kafka Connect image) and then switches to s2i... this creates a mix between two working (although we want to promote only the Docker way) approaches...", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r446893598", "createdAt": "2020-06-29T11:20:15Z", "author": {"login": "jcechace"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -112,26 +194,98 @@ docker run -it --rm --name connect \\\n     -e CONNECT_VALUE_CONVERTER_APICURIO_REGISTRY_GLOBAL-ID=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy \\\n     -p 8083:8083 debezium/connect-apicurio:{debezium-docker-label}\n ----\n+endif::community[]\n+ifdef::product[]\n+\n+.. Follow the steps in the link:{LinkCDCGettingStarted}#deploying-kafka-connect[example of deploying Kafka Connect] in {NameCDCGettingStarted}.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTU5NTU1Nw=="}, "originalCommit": null, "originalPosition": 244}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4MDI4MjQxOnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNDoxMzo0MVrOGpif2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNDoxNTowMFrOGpiirg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIxMDAxMQ==", "bodyText": "This doesn't render (yet?) into working link", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r446210011", "createdAt": "2020-06-26T14:13:41Z", "author": {"login": "jcechace"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,94 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+ifdef::product[]\n+[IMPORTANT]\n+====\n+Using Avro to serialize record keys and values is a Technology Preview feature. Technology Preview features are not supported with Red Hat production service-level agreements (SLAs) and might not be functionally complete; therefore, Red Hat does not recommend implementing any Technology Preview features in production environments. This Technology Preview feature provides early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process. For more information about support scope, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].\n+====\n+endif::product[]\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+A {prodname} connector works in the Kafka Connect framework to capture each row-level change in a database by generating a change event record. For each change event record, the {prodname} connector does the following: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+. Applies configured transformations\n+. Serializes the record key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[Kafka Connect converters]\n+. Writes the record to the correct Kafka topic\n \n-== The Apicurio API and Schema Registry\n+You can specify converters for each individual {prodname} connector instance. Kafka Connect provides a JSON converter that serializes the record keys and values into JSON documents. The default behavior is that the JSON converter includes the record's message schema, which makes each record very verbose. The {link-prefix}:{link-tutorial}[{name-tutorial}] shows what the records look like when both payload and schemas are included. If you want records to be serialized with JSON, consider setting the following connector configuration properties to `false`: \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+* `key.converter.schemas.enable`\n+* `value.converter.schemas.enable`\n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Setting these properties to `false` excludes the verbose schema information from each record. \n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the record keys and values by using https://avro.apache.org/[Apache Avro]. The Avro binary format is compact and efficient. Avro schemas make it possible to ensure that each record has the correct structure. Avro's schema evolution mechanism enables schemas to evolve. This is essential for {prodname} connectors, which dynamically generate each record's schema to match the structure of the database table that was changed. Over time, change event records written to the same Kafka topic might have different versions of the same schema. Avro serialization makes it easier for change event record consumers to adapt to a changing record schema.\n \n-[NOTE]\n-====\n-The Apicurio project also provides a JSON converter that can be used with the Apicurio registry.\n-This combines the advantage of less verbose messages (as messages do not contain the schema information themselves, but only a schema id)\n-with human-readable JSON.\n-====\n+ifdef::community[]\n+To use Apache Avro serialization, you must deploy a schema registry that manages Avro message schemas and their versions. \n+Available options include the {registry-name-full} as well as the Confluent Schema Registry. Both are described here.\n+endif::community[]\n \n-Another option is using the Confluent schema registry, which is described further below.\n+ifdef::product[]\n+To use Apache Avro serialization, you must deploy a schema registry that manages Avro message schemas and their versions. For information about setting up this registry, see the documentation for  {LinkServiceRegistryGetStart}[{registry-name-full}].\n+endif::product[]\n \n-== Technical Information\n+// Type: concept\n+// Title: About the {registry}\n+[id=\"about-the-registry\"]\n+== About the {registry-name-full}\n \n-A system that wants to use Avro serialization needs to complete two steps:\n+ifdef::community[]\n+The link:https://github.com/Apicurio/apicurio-registry[{registry}] open-source project provides several components that work with Avro:\n+endif::community[]\n \n-* Deploy an https://github.com/Apicurio/apicurio-registry[Apicurio API/Schema Registry] instance\n-* Install the Avro converter from https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka's _libs_ directory or directly into a plug-in directory\n-* Use thes the following properties to configure Apache Connect instance\n+ifdef::product[]\n+{LinkServiceRegistryGetStart}[{registry-name-full}] provides several components that work with Avro:\n+endif::product[]\n \n-[source]\n+* An Avro converter that you can specify in {prodname} connector configurations. This converter maps Kafka Connect schemas to Avro schemas. The converter then uses the Avro schemas to serialize the record keys and values into Avro's compact binary form.\n+\n+* An API and schema registry that tracks:\n++\n+** Avro schemas that are used in Kafka topics\n+** Where the Avro converter sends the generated Avro schemas\n+\n++\n+Since the Avro schemas are stored in this registry, each record needs to contain only a tiny _schema identifier_.\n+This makes each record even smaller. For an I/O bound system like Kafka, this means more total throughput for producers and consumers.\n+\n+* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers. Kafka consumer applications that you write to consume change event records can use Avro Serdes to deserialize the change event records.\n+\n+To use the {registry} with {prodname}, add {registry} converters and their dependencies to the Kafka Connect container image that you are using for running a {prodname} connector.\n+\n+[NOTE]\n+====\n+The {registry} project also provides a JSON converter. This converter combines the advantage of less verbose messages with human-readable JSON. Messages do not contain the schema information themselves, but only a schema ID.\n+====\n+\n+// Type: concept\n+// Title: Overview of deploying a {prodname} connector that uses Avro serialization\n+[id=\"overview-of-deploying-a-debezium-connector-that-uses-avro-serialization\"]\n+== Deployment overview\n+\n+To deploy a {prodname} connector that uses Avro serialization, there are three main tasks: \n+\n+ifdef::community[]\n+. Deploy an link:https://github.com/Apicurio/apicurio-registry[{registry-name-full}] instance.\n+endif::community[]\n+ifdef::product[]\n+. Deploy a link:{LinkServiceRegistryGetStart}[{registry-name-full} instance by following the instructions in {NameServiceRegistryGetStart}].", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 126}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIxMDczNA==", "bodyText": "Right. This doc has not yet been published. It should not work yet.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r446210734", "createdAt": "2020-06-26T14:15:00Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,94 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+ifdef::product[]\n+[IMPORTANT]\n+====\n+Using Avro to serialize record keys and values is a Technology Preview feature. Technology Preview features are not supported with Red Hat production service-level agreements (SLAs) and might not be functionally complete; therefore, Red Hat does not recommend implementing any Technology Preview features in production environments. This Technology Preview feature provides early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process. For more information about support scope, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].\n+====\n+endif::product[]\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+A {prodname} connector works in the Kafka Connect framework to capture each row-level change in a database by generating a change event record. For each change event record, the {prodname} connector does the following: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+. Applies configured transformations\n+. Serializes the record key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[Kafka Connect converters]\n+. Writes the record to the correct Kafka topic\n \n-== The Apicurio API and Schema Registry\n+You can specify converters for each individual {prodname} connector instance. Kafka Connect provides a JSON converter that serializes the record keys and values into JSON documents. The default behavior is that the JSON converter includes the record's message schema, which makes each record very verbose. The {link-prefix}:{link-tutorial}[{name-tutorial}] shows what the records look like when both payload and schemas are included. If you want records to be serialized with JSON, consider setting the following connector configuration properties to `false`: \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+* `key.converter.schemas.enable`\n+* `value.converter.schemas.enable`\n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Setting these properties to `false` excludes the verbose schema information from each record. \n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the record keys and values by using https://avro.apache.org/[Apache Avro]. The Avro binary format is compact and efficient. Avro schemas make it possible to ensure that each record has the correct structure. Avro's schema evolution mechanism enables schemas to evolve. This is essential for {prodname} connectors, which dynamically generate each record's schema to match the structure of the database table that was changed. Over time, change event records written to the same Kafka topic might have different versions of the same schema. Avro serialization makes it easier for change event record consumers to adapt to a changing record schema.\n \n-[NOTE]\n-====\n-The Apicurio project also provides a JSON converter that can be used with the Apicurio registry.\n-This combines the advantage of less verbose messages (as messages do not contain the schema information themselves, but only a schema id)\n-with human-readable JSON.\n-====\n+ifdef::community[]\n+To use Apache Avro serialization, you must deploy a schema registry that manages Avro message schemas and their versions. \n+Available options include the {registry-name-full} as well as the Confluent Schema Registry. Both are described here.\n+endif::community[]\n \n-Another option is using the Confluent schema registry, which is described further below.\n+ifdef::product[]\n+To use Apache Avro serialization, you must deploy a schema registry that manages Avro message schemas and their versions. For information about setting up this registry, see the documentation for  {LinkServiceRegistryGetStart}[{registry-name-full}].\n+endif::product[]\n \n-== Technical Information\n+// Type: concept\n+// Title: About the {registry}\n+[id=\"about-the-registry\"]\n+== About the {registry-name-full}\n \n-A system that wants to use Avro serialization needs to complete two steps:\n+ifdef::community[]\n+The link:https://github.com/Apicurio/apicurio-registry[{registry}] open-source project provides several components that work with Avro:\n+endif::community[]\n \n-* Deploy an https://github.com/Apicurio/apicurio-registry[Apicurio API/Schema Registry] instance\n-* Install the Avro converter from https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka's _libs_ directory or directly into a plug-in directory\n-* Use thes the following properties to configure Apache Connect instance\n+ifdef::product[]\n+{LinkServiceRegistryGetStart}[{registry-name-full}] provides several components that work with Avro:\n+endif::product[]\n \n-[source]\n+* An Avro converter that you can specify in {prodname} connector configurations. This converter maps Kafka Connect schemas to Avro schemas. The converter then uses the Avro schemas to serialize the record keys and values into Avro's compact binary form.\n+\n+* An API and schema registry that tracks:\n++\n+** Avro schemas that are used in Kafka topics\n+** Where the Avro converter sends the generated Avro schemas\n+\n++\n+Since the Avro schemas are stored in this registry, each record needs to contain only a tiny _schema identifier_.\n+This makes each record even smaller. For an I/O bound system like Kafka, this means more total throughput for producers and consumers.\n+\n+* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers. Kafka consumer applications that you write to consume change event records can use Avro Serdes to deserialize the change event records.\n+\n+To use the {registry} with {prodname}, add {registry} converters and their dependencies to the Kafka Connect container image that you are using for running a {prodname} connector.\n+\n+[NOTE]\n+====\n+The {registry} project also provides a JSON converter. This converter combines the advantage of less verbose messages with human-readable JSON. Messages do not contain the schema information themselves, but only a schema ID.\n+====\n+\n+// Type: concept\n+// Title: Overview of deploying a {prodname} connector that uses Avro serialization\n+[id=\"overview-of-deploying-a-debezium-connector-that-uses-avro-serialization\"]\n+== Deployment overview\n+\n+To deploy a {prodname} connector that uses Avro serialization, there are three main tasks: \n+\n+ifdef::community[]\n+. Deploy an link:https://github.com/Apicurio/apicurio-registry[{registry-name-full}] instance.\n+endif::community[]\n+ifdef::product[]\n+. Deploy a link:{LinkServiceRegistryGetStart}[{registry-name-full} instance by following the instructions in {NameServiceRegistryGetStart}].", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIxMDAxMQ=="}, "originalCommit": null, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4NTEyMjMyOnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMToxNjo1OVrOGqMH5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMToxNjo1OVrOGqMH5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5MjAwNg==", "bodyText": "This section talks about Docker file, which means building a custom Kafka Connect image (which is correct), the issue comes in my next comment", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r446892006", "createdAt": "2020-06-29T11:16:59Z", "author": {"login": "jcechace"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +110,71 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n \n-== {prodname} Container Images\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+In your environment, you might want to use a provided {prodname} container to deploy {prodname} connectors that use Avro serializaion. Follow the procedure here to do that. In this procedure, you build a custom Kafka Connect container image for {prodname}, which uses the Avro converter. \n \n-[source,subs=\"+attributes\"]\n+.Prerequisites\n+\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n+\n+.Procedure\n+\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes+\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+Follow the instructions in \n+{LinkServiceRegistryGetStart}#installing-registry-operatorhub[NameServiceRegistryGetStart, Installing Service Registry from the OpenShift OperatorHub].\n+endif::product[]\n+\n+. Build a {prodname} container image that contains the Avro converter:\n++\n+ifdef::community[]\n+.. Copy link:https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[`Dockerfile`] to a convenient location. This file has the following content: \n++\n+[listing,subs=\"attributes\",options=\"nowrap\"]\n+----\n+ARG DEBEZIUM_VERSION\n+FROM debezium/connect:$DEBEZIUM_VERSION\n+ENV KAFKA_CONNECT_DEBEZIUM_DIR=$KAFKA_CONNECT_PLUGINS_DIR/debezium-connector-mysql\n+ENV APICURIO_VERSION={apicurio-version}\n \n-Build a {prodname} image with Avro converter from https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[Dockerfile]:\n+RUN cd $KAFKA_CONNECT_DEBEZIUM_DIR &&\\\n+    curl https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/$APICURIO_VERSION/apicurio-registry-distro-connect-converter-$APICURIO_VERSION-converter.tar.gz | tar xzv\n+----\n \n-[source]\n-[subs=\"attributes\"]\n+.. Run the following command: \n++\n+[source,subs=\"attributes+\"]\n ----\n docker build --build-arg DEBEZIUM_VERSION={debezium-docker-label} -t debezium/connect-apicurio:{debezium-docker-label} .\n ----\n-\n-Run a Kafka Connect image configured to use Avro:\n-\n-[source]\n-[subs=\"attributes\"]\n+endif::community[]\n+ifdef::product[]\n+.. Download the link:https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?product=red.hat.integration&downloadType=distributions[{registry} Kafka Connect] zip file. \n+.. Extract the content into the directory that contains the {prodname} connector that you are configuring to use Avro serialization. \n+.. Create a custom image for Kafka Connect. See link:{LinkCDCInstallOpenShift}[{NameCDCInstallOpenShift}, Creating a container image from the Kafka Connect base image] for an example of how to do this. Start with the `Dockerfile` in that example. Then add the {registry} converters to the connector directories. \n+endif::product[]", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 217}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4302, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}