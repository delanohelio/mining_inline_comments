{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU3OTUxMTI0", "number": 1720, "reviewThreads": {"totalCount": 20, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxMzo0NToyMlrOETQs2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNTowMjo0MFrOETS-yQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjMyMDI3OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxMzo0NToyMlrOG45J_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNTozOToxM1rOG4-bGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMwOTg4Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            {prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 10, 11, and 12 are supported. \n          \n          \n            \n            {prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 9.6, 10, 11, and 12 are supported. \n          \n      \n    \n    \n  \n\nThis may need to be conditionalized for downstream since technically downstream can only support PG 10+ while community supports PG 9.6+.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462309886", "createdAt": "2020-07-29T13:45:22Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -12,356 +14,156 @@ ifdef::community[]\n toc::[]\n endif::community[]\n \n-{prodname}'s PostgreSQL Connector can monitor and record row-level changes in the schemas of a PostgreSQL database.\n+{prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 10, 11, and 12 are supported. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM5NjE4NQ==", "bodyText": "Fixed in the next commit.\nI duplicated the paragraph and conditionalized and updated the one for community and conditionalized the original for product.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462396185", "createdAt": "2020-07-29T15:39:13Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -12,356 +14,156 @@ ifdef::community[]\n toc::[]\n endif::community[]\n \n-{prodname}'s PostgreSQL Connector can monitor and record row-level changes in the schemas of a PostgreSQL database.\n+{prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 10, 11, and 12 are supported. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMwOTg4Ng=="}, "originalCommit": null, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjMzNjI5OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxMzo0ODo0M1rOG45T5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNjozMDo1NlrOG5AlDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMxMjQyMQ==", "bodyText": "The reference here about installation is applicable in the community because we offer 3 differing decoders but I do question if the omission about the installation for downstream would be more appropriate since we only support the pgoutput logical decoding stream, which is provided by PostgreSQL natively?\nIf we do decide to make a change here and omit the installation portion of this statement, there are other places in the downstream visible parts of the documentation that refer to installation too which may need tweaks.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462312421", "createdAt": "2020-07-29T13:48:43Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -12,356 +14,156 @@ ifdef::community[]\n toc::[]\n endif::community[]\n \n-{prodname}'s PostgreSQL Connector can monitor and record row-level changes in the schemas of a PostgreSQL database.\n+{prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 10, 11, and 12 are supported. \n \n-The first time it connects to a PostgreSQL server/cluster, it reads a consistent snapshot of all of the schemas. When that snapshot is complete, the connector continuously streams the changes that were committed to PostgreSQL 9.6 or later and generates corresponding insert, update and delete events. All of the events for each table are recorded in a separate Kafka topic, where they can be easily consumed by applications and services.\n+The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic. \n \n+ifdef::product[]\n+Information and procedures for using a {prodname} PostgreSQL connector is organized as follows: \n+\n+* xref:overview-of-debezium-postgresql-connector[]\n+* xref:how-debezium-postgresql-connectors-work[]\n+* xref:descriptions-of-debezium-postgresql-connector-data-change-events[]\n+* xref:how-debezium-postgresql-connectors-map-data-types[]\n+* xref:setting-up-postgresql-to-run-a-debezium-connector[]\n+* xref:deploying-and-managing-debezium-postgresql-connectors[]\n+* xref:how-debezium-postgresql-connectors-handle-faults-and-problems[]\n+\n+endif::product[]\n \n+// Type: concept\n+// Title: Overview of {prodname} PostgreSQL connector \n+// ModuleID: overview-of-debezium-postgresql-connector\n [[postgresql-overview]]\n == Overview\n \n-PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was first introduced in version 9.4 and is a mechanism which allows the extraction of the changes which were committed to the transaction log and the processing of these changes in a user-friendly manner via the help of an https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. This output plug-in must be installed prior to running the PostgreSQL server and enabled together with a replication slot in order for clients to be able to consume the changes.\n+PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was introduced in version 9.4. It is a mechanism that allows the extraction of the changes that were committed to the transaction log and the processing of these changes in a user-friendly manner with the help of an link:https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. You must install this output plug-in and configure a replication slot that uses the output plug-in before running the PostgreSQL server. This enables clients to consume the changes. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQzMTUwMw==", "bodyText": "Of course. I might not have understood the implication when I started working on this content, but I certainly knew this by the end. I have updated all content so that it is clear that installation of a plug-in might be required only upstream. Downstream, there is no longer content related to installing a plug-in. So this is fixed in the next commit. (subject of course to your review/approval :-)", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462431503", "createdAt": "2020-07-29T16:30:56Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -12,356 +14,156 @@ ifdef::community[]\n toc::[]\n endif::community[]\n \n-{prodname}'s PostgreSQL Connector can monitor and record row-level changes in the schemas of a PostgreSQL database.\n+{prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 10, 11, and 12 are supported. \n \n-The first time it connects to a PostgreSQL server/cluster, it reads a consistent snapshot of all of the schemas. When that snapshot is complete, the connector continuously streams the changes that were committed to PostgreSQL 9.6 or later and generates corresponding insert, update and delete events. All of the events for each table are recorded in a separate Kafka topic, where they can be easily consumed by applications and services.\n+The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic. \n \n+ifdef::product[]\n+Information and procedures for using a {prodname} PostgreSQL connector is organized as follows: \n+\n+* xref:overview-of-debezium-postgresql-connector[]\n+* xref:how-debezium-postgresql-connectors-work[]\n+* xref:descriptions-of-debezium-postgresql-connector-data-change-events[]\n+* xref:how-debezium-postgresql-connectors-map-data-types[]\n+* xref:setting-up-postgresql-to-run-a-debezium-connector[]\n+* xref:deploying-and-managing-debezium-postgresql-connectors[]\n+* xref:how-debezium-postgresql-connectors-handle-faults-and-problems[]\n+\n+endif::product[]\n \n+// Type: concept\n+// Title: Overview of {prodname} PostgreSQL connector \n+// ModuleID: overview-of-debezium-postgresql-connector\n [[postgresql-overview]]\n == Overview\n \n-PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was first introduced in version 9.4 and is a mechanism which allows the extraction of the changes which were committed to the transaction log and the processing of these changes in a user-friendly manner via the help of an https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. This output plug-in must be installed prior to running the PostgreSQL server and enabled together with a replication slot in order for clients to be able to consume the changes.\n+PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was introduced in version 9.4. It is a mechanism that allows the extraction of the changes that were committed to the transaction log and the processing of these changes in a user-friendly manner with the help of an link:https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. You must install this output plug-in and configure a replication slot that uses the output plug-in before running the PostgreSQL server. This enables clients to consume the changes. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMxMjQyMQ=="}, "originalCommit": null, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjM1MTMzOnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxMzo1MTo1OFrOG45djQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNjozMjoyNVrOG5AorQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMxNDg5Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The connector produces a _change event_ for every row-level insert, update, and delete operation that was captured and sends change event records for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables of interest, and can react to every row-level event they receive from those topics.\n          \n          \n            \n            The connector produces a _change event_ for every row-level insert, update, and delete operation that was captured and sends change event records for each table in a separate Kafka topic. Client applications read the Kafka topics that correspond to the database tables of interest, and can react to every row-level event they receive from those topics.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462314893", "createdAt": "2020-07-29T13:51:58Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -12,356 +14,156 @@ ifdef::community[]\n toc::[]\n endif::community[]\n \n-{prodname}'s PostgreSQL Connector can monitor and record row-level changes in the schemas of a PostgreSQL database.\n+{prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 10, 11, and 12 are supported. \n \n-The first time it connects to a PostgreSQL server/cluster, it reads a consistent snapshot of all of the schemas. When that snapshot is complete, the connector continuously streams the changes that were committed to PostgreSQL 9.6 or later and generates corresponding insert, update and delete events. All of the events for each table are recorded in a separate Kafka topic, where they can be easily consumed by applications and services.\n+The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic. \n \n+ifdef::product[]\n+Information and procedures for using a {prodname} PostgreSQL connector is organized as follows: \n+\n+* xref:overview-of-debezium-postgresql-connector[]\n+* xref:how-debezium-postgresql-connectors-work[]\n+* xref:descriptions-of-debezium-postgresql-connector-data-change-events[]\n+* xref:how-debezium-postgresql-connectors-map-data-types[]\n+* xref:setting-up-postgresql-to-run-a-debezium-connector[]\n+* xref:deploying-and-managing-debezium-postgresql-connectors[]\n+* xref:how-debezium-postgresql-connectors-handle-faults-and-problems[]\n+\n+endif::product[]\n \n+// Type: concept\n+// Title: Overview of {prodname} PostgreSQL connector \n+// ModuleID: overview-of-debezium-postgresql-connector\n [[postgresql-overview]]\n == Overview\n \n-PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was first introduced in version 9.4 and is a mechanism which allows the extraction of the changes which were committed to the transaction log and the processing of these changes in a user-friendly manner via the help of an https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. This output plug-in must be installed prior to running the PostgreSQL server and enabled together with a replication slot in order for clients to be able to consume the changes.\n+PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was introduced in version 9.4. It is a mechanism that allows the extraction of the changes that were committed to the transaction log and the processing of these changes in a user-friendly manner with the help of an link:https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. You must install this output plug-in and configure a replication slot that uses the output plug-in before running the PostgreSQL server. This enables clients to consume the changes. \n \n-PostgreSQL connector contains two different parts which work together in order to be able to read and process server changes:\n+The PostgreSQL connector contains two main parts that work together to read and process server changes:\n \n ifdef::product[]\n-* A logical decoding output plug-in, which has to be installed and configured in the PostgreSQL server.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server.\n+* Java code (the actual Kafka Connect connector), which reads the changes produced by the plug-in by using PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_] and the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_].\n endif::product[]\n \n ifdef::community[]\n-* a logical decoding output plug-in which has to be installed and configured in the PostgreSQL server, one of\n-** https://github.com/debezium/postgres-decoderbufs[decoderbufs] (maintained by the {prodname} community, based on ProtoBuf)\n-** https://github.com/eulerto/wal2json[wal2json] (maintained by the wal2json community, based on JSON)\n-** pgoutput, the standard logical decoding plug-in in PostgreSQL 10+ (maintained by the Postgres community, used by Postgres itself for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]);\n-this plug-in is always present, meaning that no additional libraries must be installed,\n-and the {prodname} connector will interpret the raw replication event stream into change events directly.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the chosen plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server. The plug-in is one of these: \n+** link:https://github.com/debezium/postgres-decoderbufs[`decoderbufs`] - maintained by the {prodname} community, based on ProtoBuf.\n+** link:https://github.com/eulerto/wal2json[`wal2json`] - maintained by the wal2json community, based on JSON.\n+** `pgoutput`, which is the standard logical decoding plug-in in PostgreSQL 10+. It is maintained by the PostgreSQL community, and used by PostgreSQL itself for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. This plug-in is always present so no additional libraries need to be installed. The {prodname} connector interprets the raw replication event stream directly into change events.\n+* Java code (the actual Kafka Connect connector) that reads the changes produced by the chosen plug-in. It uses PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], by means of the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n endif::community[]\n \n-The connector then produces a _change event_ for every row-level insert, update, and delete operation that was received, recording all the change events for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables they're interested in following, and react to every row-level event it sees in those topics.\n+The connector produces a _change event_ for every row-level insert, update, and delete operation that was captured and sends change event records for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables of interest, and can react to every row-level event they receive from those topics.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQzMjQyOQ==", "bodyText": "Done in the next commit.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462432429", "createdAt": "2020-07-29T16:32:25Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -12,356 +14,156 @@ ifdef::community[]\n toc::[]\n endif::community[]\n \n-{prodname}'s PostgreSQL Connector can monitor and record row-level changes in the schemas of a PostgreSQL database.\n+{prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 10, 11, and 12 are supported. \n \n-The first time it connects to a PostgreSQL server/cluster, it reads a consistent snapshot of all of the schemas. When that snapshot is complete, the connector continuously streams the changes that were committed to PostgreSQL 9.6 or later and generates corresponding insert, update and delete events. All of the events for each table are recorded in a separate Kafka topic, where they can be easily consumed by applications and services.\n+The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic. \n \n+ifdef::product[]\n+Information and procedures for using a {prodname} PostgreSQL connector is organized as follows: \n+\n+* xref:overview-of-debezium-postgresql-connector[]\n+* xref:how-debezium-postgresql-connectors-work[]\n+* xref:descriptions-of-debezium-postgresql-connector-data-change-events[]\n+* xref:how-debezium-postgresql-connectors-map-data-types[]\n+* xref:setting-up-postgresql-to-run-a-debezium-connector[]\n+* xref:deploying-and-managing-debezium-postgresql-connectors[]\n+* xref:how-debezium-postgresql-connectors-handle-faults-and-problems[]\n+\n+endif::product[]\n \n+// Type: concept\n+// Title: Overview of {prodname} PostgreSQL connector \n+// ModuleID: overview-of-debezium-postgresql-connector\n [[postgresql-overview]]\n == Overview\n \n-PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was first introduced in version 9.4 and is a mechanism which allows the extraction of the changes which were committed to the transaction log and the processing of these changes in a user-friendly manner via the help of an https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. This output plug-in must be installed prior to running the PostgreSQL server and enabled together with a replication slot in order for clients to be able to consume the changes.\n+PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was introduced in version 9.4. It is a mechanism that allows the extraction of the changes that were committed to the transaction log and the processing of these changes in a user-friendly manner with the help of an link:https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. You must install this output plug-in and configure a replication slot that uses the output plug-in before running the PostgreSQL server. This enables clients to consume the changes. \n \n-PostgreSQL connector contains two different parts which work together in order to be able to read and process server changes:\n+The PostgreSQL connector contains two main parts that work together to read and process server changes:\n \n ifdef::product[]\n-* A logical decoding output plug-in, which has to be installed and configured in the PostgreSQL server.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server.\n+* Java code (the actual Kafka Connect connector), which reads the changes produced by the plug-in by using PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_] and the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_].\n endif::product[]\n \n ifdef::community[]\n-* a logical decoding output plug-in which has to be installed and configured in the PostgreSQL server, one of\n-** https://github.com/debezium/postgres-decoderbufs[decoderbufs] (maintained by the {prodname} community, based on ProtoBuf)\n-** https://github.com/eulerto/wal2json[wal2json] (maintained by the wal2json community, based on JSON)\n-** pgoutput, the standard logical decoding plug-in in PostgreSQL 10+ (maintained by the Postgres community, used by Postgres itself for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]);\n-this plug-in is always present, meaning that no additional libraries must be installed,\n-and the {prodname} connector will interpret the raw replication event stream into change events directly.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the chosen plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server. The plug-in is one of these: \n+** link:https://github.com/debezium/postgres-decoderbufs[`decoderbufs`] - maintained by the {prodname} community, based on ProtoBuf.\n+** link:https://github.com/eulerto/wal2json[`wal2json`] - maintained by the wal2json community, based on JSON.\n+** `pgoutput`, which is the standard logical decoding plug-in in PostgreSQL 10+. It is maintained by the PostgreSQL community, and used by PostgreSQL itself for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. This plug-in is always present so no additional libraries need to be installed. The {prodname} connector interprets the raw replication event stream directly into change events.\n+* Java code (the actual Kafka Connect connector) that reads the changes produced by the chosen plug-in. It uses PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], by means of the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n endif::community[]\n \n-The connector then produces a _change event_ for every row-level insert, update, and delete operation that was received, recording all the change events for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables they're interested in following, and react to every row-level event it sees in those topics.\n+The connector produces a _change event_ for every row-level insert, update, and delete operation that was captured and sends change event records for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables of interest, and can react to every row-level event they receive from those topics.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMxNDg5Mw=="}, "originalCommit": null, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjM2NzU3OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxMzo1NTozNlrOG45n7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNjozMzoyN1rOG5ArRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMxNzU1MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The connector is tolerant of failures. As the connector reads changes and produces events, it records the position in the WAL for each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart the connector continues reading the WAL where it last left off. This includes snapshots. If the connector stops during a snapshot, the connector begins a new snapshot when it restarts. \n          \n          \n            \n            The connector is tolerant of failures. As the connector reads changes and produces events, it records the WAL position for each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart the connector continues reading the WAL where it last left off. This includes snapshots. If the connector stops during a snapshot, the connector begins a new snapshot when it restarts.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462317550", "createdAt": "2020-07-29T13:55:36Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -12,356 +14,156 @@ ifdef::community[]\n toc::[]\n endif::community[]\n \n-{prodname}'s PostgreSQL Connector can monitor and record row-level changes in the schemas of a PostgreSQL database.\n+{prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 10, 11, and 12 are supported. \n \n-The first time it connects to a PostgreSQL server/cluster, it reads a consistent snapshot of all of the schemas. When that snapshot is complete, the connector continuously streams the changes that were committed to PostgreSQL 9.6 or later and generates corresponding insert, update and delete events. All of the events for each table are recorded in a separate Kafka topic, where they can be easily consumed by applications and services.\n+The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic. \n \n+ifdef::product[]\n+Information and procedures for using a {prodname} PostgreSQL connector is organized as follows: \n+\n+* xref:overview-of-debezium-postgresql-connector[]\n+* xref:how-debezium-postgresql-connectors-work[]\n+* xref:descriptions-of-debezium-postgresql-connector-data-change-events[]\n+* xref:how-debezium-postgresql-connectors-map-data-types[]\n+* xref:setting-up-postgresql-to-run-a-debezium-connector[]\n+* xref:deploying-and-managing-debezium-postgresql-connectors[]\n+* xref:how-debezium-postgresql-connectors-handle-faults-and-problems[]\n+\n+endif::product[]\n \n+// Type: concept\n+// Title: Overview of {prodname} PostgreSQL connector \n+// ModuleID: overview-of-debezium-postgresql-connector\n [[postgresql-overview]]\n == Overview\n \n-PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was first introduced in version 9.4 and is a mechanism which allows the extraction of the changes which were committed to the transaction log and the processing of these changes in a user-friendly manner via the help of an https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. This output plug-in must be installed prior to running the PostgreSQL server and enabled together with a replication slot in order for clients to be able to consume the changes.\n+PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was introduced in version 9.4. It is a mechanism that allows the extraction of the changes that were committed to the transaction log and the processing of these changes in a user-friendly manner with the help of an link:https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. You must install this output plug-in and configure a replication slot that uses the output plug-in before running the PostgreSQL server. This enables clients to consume the changes. \n \n-PostgreSQL connector contains two different parts which work together in order to be able to read and process server changes:\n+The PostgreSQL connector contains two main parts that work together to read and process server changes:\n \n ifdef::product[]\n-* A logical decoding output plug-in, which has to be installed and configured in the PostgreSQL server.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server.\n+* Java code (the actual Kafka Connect connector), which reads the changes produced by the plug-in by using PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_] and the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_].\n endif::product[]\n \n ifdef::community[]\n-* a logical decoding output plug-in which has to be installed and configured in the PostgreSQL server, one of\n-** https://github.com/debezium/postgres-decoderbufs[decoderbufs] (maintained by the {prodname} community, based on ProtoBuf)\n-** https://github.com/eulerto/wal2json[wal2json] (maintained by the wal2json community, based on JSON)\n-** pgoutput, the standard logical decoding plug-in in PostgreSQL 10+ (maintained by the Postgres community, used by Postgres itself for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]);\n-this plug-in is always present, meaning that no additional libraries must be installed,\n-and the {prodname} connector will interpret the raw replication event stream into change events directly.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the chosen plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server. The plug-in is one of these: \n+** link:https://github.com/debezium/postgres-decoderbufs[`decoderbufs`] - maintained by the {prodname} community, based on ProtoBuf.\n+** link:https://github.com/eulerto/wal2json[`wal2json`] - maintained by the wal2json community, based on JSON.\n+** `pgoutput`, which is the standard logical decoding plug-in in PostgreSQL 10+. It is maintained by the PostgreSQL community, and used by PostgreSQL itself for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. This plug-in is always present so no additional libraries need to be installed. The {prodname} connector interprets the raw replication event stream directly into change events.\n+* Java code (the actual Kafka Connect connector) that reads the changes produced by the chosen plug-in. It uses PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], by means of the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n endif::community[]\n \n-The connector then produces a _change event_ for every row-level insert, update, and delete operation that was received, recording all the change events for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables they're interested in following, and react to every row-level event it sees in those topics.\n+The connector produces a _change event_ for every row-level insert, update, and delete operation that was captured and sends change event records for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables of interest, and can react to every row-level event they receive from those topics.\n \n-PostgreSQL normally purges WAL segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, we start with a consistent view of all of the data, yet continue reading without having lost any of the changes made while the snapshot was taking place.\n+PostgreSQL normally purges write-ahead log (WAL) segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, the connector starts with a consistent view of all of the data, and does not omit any changes that were made while the snapshot was being taken.\n \n-The connector is also tolerant of failures. As the connector reads changes and produces events, it records the position in the write-ahead log with each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart it simply continues reading the WAL where it last left off. This includes snapshots: if the snapshot was not completed when the connector is stopped, upon restart it will begin a new snapshot.\n+The connector is tolerant of failures. As the connector reads changes and produces events, it records the position in the WAL for each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart the connector continues reading the WAL where it last left off. This includes snapshots. If the connector stops during a snapshot, the connector begins a new snapshot when it restarts. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQzMzA5Mw==", "bodyText": "Updated as you suggest in the next commit.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462433093", "createdAt": "2020-07-29T16:33:27Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -12,356 +14,156 @@ ifdef::community[]\n toc::[]\n endif::community[]\n \n-{prodname}'s PostgreSQL Connector can monitor and record row-level changes in the schemas of a PostgreSQL database.\n+{prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 10, 11, and 12 are supported. \n \n-The first time it connects to a PostgreSQL server/cluster, it reads a consistent snapshot of all of the schemas. When that snapshot is complete, the connector continuously streams the changes that were committed to PostgreSQL 9.6 or later and generates corresponding insert, update and delete events. All of the events for each table are recorded in a separate Kafka topic, where they can be easily consumed by applications and services.\n+The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic. \n \n+ifdef::product[]\n+Information and procedures for using a {prodname} PostgreSQL connector is organized as follows: \n+\n+* xref:overview-of-debezium-postgresql-connector[]\n+* xref:how-debezium-postgresql-connectors-work[]\n+* xref:descriptions-of-debezium-postgresql-connector-data-change-events[]\n+* xref:how-debezium-postgresql-connectors-map-data-types[]\n+* xref:setting-up-postgresql-to-run-a-debezium-connector[]\n+* xref:deploying-and-managing-debezium-postgresql-connectors[]\n+* xref:how-debezium-postgresql-connectors-handle-faults-and-problems[]\n+\n+endif::product[]\n \n+// Type: concept\n+// Title: Overview of {prodname} PostgreSQL connector \n+// ModuleID: overview-of-debezium-postgresql-connector\n [[postgresql-overview]]\n == Overview\n \n-PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was first introduced in version 9.4 and is a mechanism which allows the extraction of the changes which were committed to the transaction log and the processing of these changes in a user-friendly manner via the help of an https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. This output plug-in must be installed prior to running the PostgreSQL server and enabled together with a replication slot in order for clients to be able to consume the changes.\n+PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was introduced in version 9.4. It is a mechanism that allows the extraction of the changes that were committed to the transaction log and the processing of these changes in a user-friendly manner with the help of an link:https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. You must install this output plug-in and configure a replication slot that uses the output plug-in before running the PostgreSQL server. This enables clients to consume the changes. \n \n-PostgreSQL connector contains two different parts which work together in order to be able to read and process server changes:\n+The PostgreSQL connector contains two main parts that work together to read and process server changes:\n \n ifdef::product[]\n-* A logical decoding output plug-in, which has to be installed and configured in the PostgreSQL server.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server.\n+* Java code (the actual Kafka Connect connector), which reads the changes produced by the plug-in by using PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_] and the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_].\n endif::product[]\n \n ifdef::community[]\n-* a logical decoding output plug-in which has to be installed and configured in the PostgreSQL server, one of\n-** https://github.com/debezium/postgres-decoderbufs[decoderbufs] (maintained by the {prodname} community, based on ProtoBuf)\n-** https://github.com/eulerto/wal2json[wal2json] (maintained by the wal2json community, based on JSON)\n-** pgoutput, the standard logical decoding plug-in in PostgreSQL 10+ (maintained by the Postgres community, used by Postgres itself for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]);\n-this plug-in is always present, meaning that no additional libraries must be installed,\n-and the {prodname} connector will interpret the raw replication event stream into change events directly.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the chosen plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server. The plug-in is one of these: \n+** link:https://github.com/debezium/postgres-decoderbufs[`decoderbufs`] - maintained by the {prodname} community, based on ProtoBuf.\n+** link:https://github.com/eulerto/wal2json[`wal2json`] - maintained by the wal2json community, based on JSON.\n+** `pgoutput`, which is the standard logical decoding plug-in in PostgreSQL 10+. It is maintained by the PostgreSQL community, and used by PostgreSQL itself for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. This plug-in is always present so no additional libraries need to be installed. The {prodname} connector interprets the raw replication event stream directly into change events.\n+* Java code (the actual Kafka Connect connector) that reads the changes produced by the chosen plug-in. It uses PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], by means of the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n endif::community[]\n \n-The connector then produces a _change event_ for every row-level insert, update, and delete operation that was received, recording all the change events for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables they're interested in following, and react to every row-level event it sees in those topics.\n+The connector produces a _change event_ for every row-level insert, update, and delete operation that was captured and sends change event records for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables of interest, and can react to every row-level event they receive from those topics.\n \n-PostgreSQL normally purges WAL segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, we start with a consistent view of all of the data, yet continue reading without having lost any of the changes made while the snapshot was taking place.\n+PostgreSQL normally purges write-ahead log (WAL) segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, the connector starts with a consistent view of all of the data, and does not omit any changes that were made while the snapshot was being taken.\n \n-The connector is also tolerant of failures. As the connector reads changes and produces events, it records the position in the write-ahead log with each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart it simply continues reading the WAL where it last left off. This includes snapshots: if the snapshot was not completed when the connector is stopped, upon restart it will begin a new snapshot.\n+The connector is tolerant of failures. As the connector reads changes and produces events, it records the position in the WAL for each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart the connector continues reading the WAL where it last left off. This includes snapshots. If the connector stops during a snapshot, the connector begins a new snapshot when it restarts. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMxNzU1MA=="}, "originalCommit": null, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjM5NDA5OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDowMToxOVrOG454mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNjozNDoyM1rOG5AtZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMyMTgxOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            For more advanced uses, you can provide an implementation of the `io.debezium.connector.postgresql.spi.Snapshotter` interface. This interaces allows control of most of the aspects of how the connector performs snapshots. This includes whether or not to take a snapshot, the options for opening the snapshot transaction, and whether to take locks. \n          \n          \n            \n            For more advanced uses, you can provide an implementation of the `io.debezium.connector.postgresql.spi.Snapshotter` interface. This interface allows control of most of the aspects of how the connector performs snapshots. This includes whether or not to take a snapshot, the options for opening the snapshot transaction, and whether to take locks.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462321819", "createdAt": "2020-07-29T14:01:19Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -12,356 +14,156 @@ ifdef::community[]\n toc::[]\n endif::community[]\n \n-{prodname}'s PostgreSQL Connector can monitor and record row-level changes in the schemas of a PostgreSQL database.\n+{prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 10, 11, and 12 are supported. \n \n-The first time it connects to a PostgreSQL server/cluster, it reads a consistent snapshot of all of the schemas. When that snapshot is complete, the connector continuously streams the changes that were committed to PostgreSQL 9.6 or later and generates corresponding insert, update and delete events. All of the events for each table are recorded in a separate Kafka topic, where they can be easily consumed by applications and services.\n+The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic. \n \n+ifdef::product[]\n+Information and procedures for using a {prodname} PostgreSQL connector is organized as follows: \n+\n+* xref:overview-of-debezium-postgresql-connector[]\n+* xref:how-debezium-postgresql-connectors-work[]\n+* xref:descriptions-of-debezium-postgresql-connector-data-change-events[]\n+* xref:how-debezium-postgresql-connectors-map-data-types[]\n+* xref:setting-up-postgresql-to-run-a-debezium-connector[]\n+* xref:deploying-and-managing-debezium-postgresql-connectors[]\n+* xref:how-debezium-postgresql-connectors-handle-faults-and-problems[]\n+\n+endif::product[]\n \n+// Type: concept\n+// Title: Overview of {prodname} PostgreSQL connector \n+// ModuleID: overview-of-debezium-postgresql-connector\n [[postgresql-overview]]\n == Overview\n \n-PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was first introduced in version 9.4 and is a mechanism which allows the extraction of the changes which were committed to the transaction log and the processing of these changes in a user-friendly manner via the help of an https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. This output plug-in must be installed prior to running the PostgreSQL server and enabled together with a replication slot in order for clients to be able to consume the changes.\n+PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was introduced in version 9.4. It is a mechanism that allows the extraction of the changes that were committed to the transaction log and the processing of these changes in a user-friendly manner with the help of an link:https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. You must install this output plug-in and configure a replication slot that uses the output plug-in before running the PostgreSQL server. This enables clients to consume the changes. \n \n-PostgreSQL connector contains two different parts which work together in order to be able to read and process server changes:\n+The PostgreSQL connector contains two main parts that work together to read and process server changes:\n \n ifdef::product[]\n-* A logical decoding output plug-in, which has to be installed and configured in the PostgreSQL server.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server.\n+* Java code (the actual Kafka Connect connector), which reads the changes produced by the plug-in by using PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_] and the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_].\n endif::product[]\n \n ifdef::community[]\n-* a logical decoding output plug-in which has to be installed and configured in the PostgreSQL server, one of\n-** https://github.com/debezium/postgres-decoderbufs[decoderbufs] (maintained by the {prodname} community, based on ProtoBuf)\n-** https://github.com/eulerto/wal2json[wal2json] (maintained by the wal2json community, based on JSON)\n-** pgoutput, the standard logical decoding plug-in in PostgreSQL 10+ (maintained by the Postgres community, used by Postgres itself for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]);\n-this plug-in is always present, meaning that no additional libraries must be installed,\n-and the {prodname} connector will interpret the raw replication event stream into change events directly.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the chosen plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server. The plug-in is one of these: \n+** link:https://github.com/debezium/postgres-decoderbufs[`decoderbufs`] - maintained by the {prodname} community, based on ProtoBuf.\n+** link:https://github.com/eulerto/wal2json[`wal2json`] - maintained by the wal2json community, based on JSON.\n+** `pgoutput`, which is the standard logical decoding plug-in in PostgreSQL 10+. It is maintained by the PostgreSQL community, and used by PostgreSQL itself for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. This plug-in is always present so no additional libraries need to be installed. The {prodname} connector interprets the raw replication event stream directly into change events.\n+* Java code (the actual Kafka Connect connector) that reads the changes produced by the chosen plug-in. It uses PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], by means of the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n endif::community[]\n \n-The connector then produces a _change event_ for every row-level insert, update, and delete operation that was received, recording all the change events for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables they're interested in following, and react to every row-level event it sees in those topics.\n+The connector produces a _change event_ for every row-level insert, update, and delete operation that was captured and sends change event records for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables of interest, and can react to every row-level event they receive from those topics.\n \n-PostgreSQL normally purges WAL segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, we start with a consistent view of all of the data, yet continue reading without having lost any of the changes made while the snapshot was taking place.\n+PostgreSQL normally purges write-ahead log (WAL) segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, the connector starts with a consistent view of all of the data, and does not omit any changes that were made while the snapshot was being taken.\n \n-The connector is also tolerant of failures. As the connector reads changes and produces events, it records the position in the write-ahead log with each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart it simply continues reading the WAL where it last left off. This includes snapshots: if the snapshot was not completed when the connector is stopped, upon restart it will begin a new snapshot.\n+The connector is tolerant of failures. As the connector reads changes and produces events, it records the position in the WAL for each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart the connector continues reading the WAL where it last left off. This includes snapshots. If the connector stops during a snapshot, the connector begins a new snapshot when it restarts. \n \n ifdef::product[]\n [[postgresql-output-plugin]]\n-=== Logical decoding output plug-in\n-The `pgoutput` logical decoder is the only supported logical decoder in the Tecnhology Preview release of {prodname}.\n-\n-`pgoutput`, the standard logical decoding plug-in in PostgreSQL 10+, is maintained by the Postgres community, and is also used by Postgres for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication].\n-The `pgoutput` plug-in is always present, meaning that no additional libraries must be installed,\n-and the connector will interpret the raw replication event stream into change events directly.\n+.Logical decoding output plug-in\n+The `pgoutput` logical decoding plug-in, the only supported logical decoding plug-in in this {prodname} release, is the standard logical decoding plug-in in PostgreSQL 10+, it is maintained by the PostgreSQL community, and it is also used by PostgreSQL for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. The `pgoutput` plug-in is always present, meaning that no additional libraries must be installed. The connector directly interprets the raw replication event stream into change events records.\n endif::product[]\n \n [[postgresql-limitations]]\n [IMPORTANT]\n ====\n-The connector's functionality relies on PostgreSQL's logical decoding feature.\n-Please be aware of the following limitations which are also reflected by the connector:\n+The connector relies on and reflects the PostgreSQL logical decoding feature, which has the following limitations:\n+\n+* Logical decoding does not support DDL changes. This means that the connector is unable to report DDL change events back to consumers.\n+* Logical decoding replication slots are supported on only `primary` servers. When there is a cluster of PostgreSQL servers, the connector can run on only the active `primary` server. It cannot run on `hot` or `warm` standby replicas. If the `primary` server fails or is demoted, the connector stops. After the `primary` server has recovered, you can restart the connector. If a different PostgreSQL server has been promoted to `primary`, adjust the connector configuration before restarting the connector. \n \n-. Logical Decoding does not support DDL changes: this means that the connector is unable to report DDL change events back to consumers.\n-. Logical Decoding replication slots are only supported on `primary` servers: this means that when there is a cluster of PostgreSQL servers, the connector can only run on the active `primary` server. It cannot run on `hot` or `warm` standby replicas. If the `primary` server fails or is demoted, the connector will stop. Once the `primary` has recovered the connector can simply be restarted. If a different PostgreSQL server has been promoted to `primary`, the connector configuration must be adjusted before the connector is restarted. Make sure you read more about how the connector behaves {link-prefix}:{link-postgresql-connector}#postgresql-when-things-go-wrong[when things go wrong].\n+{link-prefix}:{link-postgresql-connector}#postgresql-when-things-go-wrong[ Behavior when things go wrong] describes what the connector does when there is a problem. \n ====\n \n [IMPORTANT]\n ====\n-{prodname} currently supports only databases with UTF-8 character encoding.\n-With a single byte character encoding it is not possible to correctly process strings containing extended ASCII code characters.\n+{prodname} currently supports databases with UTF-8 character encoding only.\n+With a single byte character encoding, it is not possible to correctly process strings that contain extended ASCII code characters.\n ====\n \n-[[setting-up-postgresql]]\n-== Setting up PostgreSQL\n-\n-ifdef::product[]\n-This release of {prodname} only supports the native pgoutput logical replication stream.\n-To set up PostgreSQL using pgoutput, you will need to enable a replication slot, and configure a user with sufficient privileges to perform the replication.\n-\n-=== Configuring the replication slot\n-\n-PostgreSQL's logical decoding uses replication slots.\n-\n-First, you configure the replication slot:\n-\n-.postgresql.conf\n+// Type: assembly\n+// ModuleID: how-debezium-postgresql-connectors-work\n+// Title: How {prodname} PostgreSQL connectors work\n+[[how-the-postgresql-connector-works]]\n+== How the connector works\n \n-[source]\n-----\n-wal_level=logical\n-max_wal_senders=1\n-max_replication_slots=1\n-----\n+To optimally configure and run a {prodname} PostgreSQL connector, it is helpful to understand how the connector performs snapshots, streams change events, determines Kafka topic names, and uses metadata. \n \n-* `wal_level` tells the server to use logical decoding with the write-ahead log\n-* `max_wal_senders` tells the server to use a maximum of 1 separate processes for processing WAL changes\n-* `max_replication_slots` tells the server to allow a maximum of 1 replication slots to be created for streaming WAL changes\n+ifdef::product[]\n+Details are in the following topics: \n \n-Replication slots are guaranteed to retain all WAL required for {prodname} even during {prodname} outages.\n-It is important for this reason to closely monitor replication slots to avoid too much disk consumption and other conditions that can happen such as catalog bloat if a replication slot stays unused for too long.\n-For more information, refer to the https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION-SLOTS[the Postgres documentation].\n+* xref:how-debezium-postgresql-connectors-perform-database-snapshots[]\n+* xref:how-debezium-postgresql-connectors-stream-change-event-records[]\n+* xref:postgresql-10-logical-decoding-support-pgoutput[]\n+* xref:default-names-of-kafka-topics-that-receive-debezium-change-event-records[]\n+* xref:metadata-in-debezium-change-event-records[]\n+* xref:debezium-connector-generated-events-that-represent-transaction-boundaries[]\n \n-[NOTE]\n-====\n-We recommend reading and understanding the https://www.postgresql.org/docs/current/static/wal-configuration.html[WAL configuration documentation] regarding the mechanics and configuration of the PostgreSQL write-ahead log.\n-====\n endif::product[]\n \n-ifdef::community[]\n-Before using the PostgreSQL connector to monitor the changes committed on a PostgreSQL server,\n-first decide which logical decoder method you intend to use.\n-If you plan *not* to use the native pgoutput logical replication stream support,\n-then you will need to install the logical decoding plug-in into the PostgreSQL server.\n-Afterward enable a replication slot, and configure a user with sufficient privileges to perform the replication.\n-\n-Note that if your database is hosted by a service such as https://www.heroku.com/postgres[Heroku Postgres] you may be unable to install the plug-in.\n-If so, and if you're using PostgreSQL 10+, you can use the pgoutput decoder support to monitor your database.\n-If that is not an option, you'll be unable to monitor your database with {prodname}.\n-\n-[[postgresql-on-amazon-rds]]\n-=== PostgreSQL on Amazon RDS\n-\n-It is possible to monitor PostgreSQL database running in https://aws.amazon.com/rds/[Amazon RDS]. To get it running you must fulfill the following conditions\n-\n-* The instance parameter `rds.logical_replication` is set to `1`.\n-* Verify that `wal_level` parameter is set to `logical` by running the query `SHOW wal_level` as DB master user; this might not be the case in multi-zone replication setups.\n-You cannot set this option manually, it is (https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html[automatically changed]) when the `rds.logical_replication` is set to `1`.\n-If the `wal_level` is not `logical` after the change above, it is probably because the instance has to be restarted due to the parameter group change. This happens accordingly to your maintenance window or can be done manually.\n-* Set `plugin.name` {prodname} parameter to `wal2json`.  You can skip this on PostgreSQL 10+ if you wish to use pgoutput logical replication stream support.\n-* Use database master account for replication as RDS currently does not support setting of `REPLICATION` privilege for another account.\n-\n-[IMPORTANT]\n-====\n-You should make sure to use the latest versions of Postgres 9.6, 10 or 11 on Amazon RDS.\n-Otherwise, older versions of the wal2json plug-in may be installed\n-(see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html[the official documentation] for the exact wal2json versions installed on Amazon RDS).\n-In that case, replication messages received from the database may not carry complete information about type constraints like length or scale or `NULL`/`NOT NULL`,\n-which in turn might cause creation of messages with an inconsistent schema for a short period of time in case of changes to a column's definition.\n-\n-As of January 2019, the following Postgres versions on RDS come with an up-to-date version of wal2json and thus should be used:\n-\n-* Postgres 9.6: 9.6.10 and newer\n-* Postgres 10: 10.5 and newer\n-* Postgres 11: any version\n-====\n-\n-\n-[[postgresql-output-plugin]]\n-=== Installing the Logical Decoding Output Plug-in\n-\n-[TIP]\n-====\n-Also see {link-prefix}:{link-postgresql-plugins}[Logical Decoding Output Plug-in Installation for PostgreSQL] for more detailed instructions of setting up and testing logical decoding plug-ins.\n-====\n-\n-[NOTE]\n-====\n-As of {prodname} 0.10, the connector supports PostgreSQL 10+ logical replication streaming using _pgoutput_.\n-This means that a logical decoding output plug-in is no longer necessary and changes can be emitted directly from the replication stream by the connector.\n-====\n+// Type: concept\n+// ModuleID: how-debezium-postgresql-connectors-perform-database-snapshots\n+// Title: How {prodname} PostgreSQL connectors perform database snapshots\n+[[postgresql-snapshots]]\n+=== Snapshots\n \n-As of PostgreSQL 9.4, the only way to read changes to the write-ahead-log is to first install a logical decoding output plug-in. Plugins are written in C, compiled, and installed on the machine which runs the PostgreSQL server. Plugins use  a number of PostgreSQL specific APIs, as described by the https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_PostgreSQL documentation_].\n+Most PostgreSQL servers are configured to not retain the complete history of the database in the WAL segments. This means that the PostgreSQL connector would be unable to see the entire history of the database by reading only the WAL. Consequently, the first time that the connector starts, it performs an initial _consistent snapshot_ of the database. The default behavior for performing a snapshot consists of the following steps. You can change this behavior by setting the {link-prefix}:{link-postgresql-connector}#postgresql-property-snapshot-mode[`snapshot.mode` connector configuration property] to a value other than `initial`. \n \n-The PostgreSQL connector works with one of {prodname}'s supported logical decoding plug-ins to encode the changes in either https://github.com/google/protobuf[_Protobuf format_] or http://www.json.org/[_JSON_] format.\n-See the documentation of your chosen plug-in (https://github.com/debezium/postgres-decoderbufs/blob/master/README.md[_protobuf_], https://github.com/eulerto/wal2json/blob/master/README.md[_wal2json_]) to learn more about the plug-in's requirements, limitations, and how to compile it.\n+. Start a transaction with a link:https://www.postgresql.org/docs/current/static/sql-set-transaction.html[SERIALIZABLE, READ ONLY, DEFERRABLE] isolation level to ensure that subsequent reads in this transaction are against a single consistent version of the data. Any changes to the data due to subsequent `INSERT`, `UPDATE`, and `DELETE` operations by other clients are not visible to this transaction.\n+. Obtain an `ACCESS SHARE MODE` lock on each of the tables being tracked to ensure that no structural changes can occur to any of the tables while the snapshot is taking place. These locks do not prevent table `INSERT`, `UPDATE` and `DELETE` operations from taking place during the snapshot. \n++\n+_This step is omitted when `snapshot.mode` is set to `exported`, which allows the connector to perform a lock-free snapshot_.\n+. Read the current position in the server's transaction log.\n+. Scan the database tables and schemas, generate a `READ` event for each row and write that event to the appropriate table-specific Kafka topic.\n+. Commit the transaction.\n+. Record the successful completion of the snapshot in the connector offsets.\n \n-For simplicity, {prodname} also provides a Docker image based on a vanilla PostgreSQL server image on top of which it compiles and installs the plug-ins. We recommend https://github.com/debezium/docker-images/tree/master/postgres/9.6[_using this image_] as an example of the detailed steps required for the installation.\n+If the connector fails, is rebalanced, or stops after Step 1 begins but before Step 6 completes, upon restart the connector begins a new snapshot. After the connector completes its initial snapshot, the PostgreSQL connector continues streaming from the position that it read in step 3. This ensures that the connector does not miss any updates. If the connector stops again for any reason, upon restart, the connector continues streaming changes from where it previously left off.\n \n [WARNING]\n ====\n-The {prodname} logical decoding plug-ins have only been installed and tested on _Linux_ machines.\n-For Windows and other operating systems it may require different installation steps.\n-====\n-\n-[[postgresql-differences-between-plugins]]\n-==== Differences Between Plug-ins\n-\n-The plug-ins' behavior is not completely same for all cases.\n-So far these differences have been identified:\n-\n-* wal2json plug-in is not able to process quoted identifiers (https://github.com/eulerto/wal2json/issues/35[issue])\n-* wal2json and decoderbufs plug-ins emit events for tables without primary keys\n-* wal2json plug-in does not support special values (`NaN` or `infinity`) for floating point types\n-* wal2json should be used with setting the `schema.refresh.mode` connector option to `columns_diff_exclude_unchanged_toast`;\n-otherwise, when receiving a change event for a row containing an unchanged TOAST column, no field for that column is contained in the emitted change event's `after` structure.\n-This is because wal2json's messages do not contain a field for such a column.\n-\n-The requirement for adding this is tracked under the wal2json https://github.com/eulerto/wal2json/issues/98[issue 98].\n-See the documentation of `columns_diff_exclude_unchanged_toast` further below for implications of using it.\n-\n-* pgoutput plug-in does not emit all the events for tables without primary keys, it only emits inserts\n-\n-All up-to-date differences are tracked in a test suite https://github.com/debezium/debezium/blob/master/debezium-connector-postgres/src/test/java/io/debezium/connector/postgresql/DecoderDifferences.java[Java class].\n-\n-\n-[[postgresql-server-configuration]]\n-=== Configuring the PostgreSQL Server\n-\n-If you are using one of the supported {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-ins] (i.e. not pgoutput) and it has been installed,\n-configure the server to load the plug-in at startup:\n-\n-.postgresql.conf\n-[source,properties]\n-----\n-# MODULES\n-shared_preload_libraries = 'decoderbufs,wal2json' // <1>\n-----\n-<1> tells the server that it should load at startup the `decoderbufs` and `wal2json` logical decoding plug-ins (the names of the plug-ins are set in https://github.com/debezium/postgres-decoderbufs/blob/v0.3.0/Makefile[_Protobuf_] and https://github.com/eulerto/wal2json/blob/master/Makefile[_wal2json_] Makefiles)\n-\n-Next is to configure the replication slot regardless of the decoder being used:\n-\n-.postgresql.conf\n-[source,properties]\n-----\n-# REPLICATION\n-wal_level = logical             // <1>\n-max_wal_senders = 1             // <2>\n-max_replication_slots = 1       // <3>\n-----\n-<1> tells the server that it should use logical decoding with the write-ahead log\n-<2> tells the server that it should use a maximum of `1` separate processes for processing WAL changes\n-<3> tells the server that it should allow a maximum of `1` replication slots to be created for streaming WAL changes\n-\n-{prodname} uses PostgreSQL's logical decoding, which uses replication slots.\n-Replication slots are guaranteed to retain all WAL required for {prodname} even during {prodname} outages.\n-It is important for this reason to closely monitor replication slots to avoid too much disk consumption and other conditions that can happen such as catalog bloat if a replication slot stays unused for too long.\n-For more information please see the official Postgres docs on https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION-SLOTS[this subject].\n-\n-If you are working with a `synchronous_commit` setting other than `on`,\n-the recommendation is to set `wal_writer_delay` to a value such as 10 ms to achieve a low latency of change events.\n-Otherwise, its default value is applied, which adds a latency of about 200 ms.\n-\n-[TIP]\n-====\n-We strongly recommend reading and understanding https://www.postgresql.org/docs/current/static/wal-configuration.html[the official documentation] regarding the mechanics and configuration of the PostgreSQL write-ahead log.\n-====\n-endif::community[]\n-\n-[[postgresql-permissions]]\n-=== Setting up Permissions\n-\n-Next, configure a database user who can perform replications.\n-\n-Replication can only be performed by a database user that has appropriate permissions and only for a configured number of hosts.\n-\n-In order to give a user replication permissions, define a PostgreSQL role that has _at least_ the `REPLICATION` and `LOGIN` permissions. For example:\n-\n-[source,sql]\n-----\n-CREATE ROLE name REPLICATION LOGIN;\n-----\n-\n-[NOTE]\n-====\n-Superusers have by default both of the above roles.\n+It is strongly recommended that you configure a PostgreSQL connector to set `snapshot.mode` to `exported`. The `initial`, `initial only` and `always` modes can lose a few events while a connector switches from performing the snapshot to streaming change event records when a database is under heavy load.\n+This is a known issue and the affected snapshot modes will be reworked to use `exported` mode internally (link:https://issues.redhat.com/browse/DBZ-2337[DBZ-2337]).\n ====\n \n-Finally, configure the PostgreSQL server to allow replication to take place between the server machine and the host on which the PostgreSQL connector is running:\n-\n-.pg_hba.conf\n-[source]\n-----\n-local   replication     <youruser>                          trust   // <1>\n-host    replication     <youruser>  127.0.0.1/32            trust   // <2>\n-host    replication     <youruser>  ::1/128                 trust   // <3>\n-----\n-<1> Tells the server to allow replication for `<youruser>` locally (i.e. on the server machine)\n-<2> Tells the server to allow `<youruser>` on `localhost` to receive replication changes using `IPV4`\n-<3> Tells the server to allow `<youruser>` on `localhost` to receive replication changes using `IPV6`\n+[id=\"snapshot-mode-settings\"]\n+.Settings for `snapshot.mode` connector configuration property\n+[cols=\"20%a,80%a\",options=\"header\"]\n+|===\n+|Setting\n+|Description\n \n-[NOTE]\n-====\n-See https://www.postgresql.org/docs/current/static/datatype-net-types.html[_the PostgreSQL documentation_] for more information on network masks.\n-====\n+|`always`\n+|The connector always performs a snapshot when it starts. After the snapshot completes, the connector continues streaming changes from step 3 in the above sequence. This mode is useful in these situations: + \n+ \n+* It is known that some WAL segments have been deleted and are no longer available. +\n+* After a cluster failure, a new primary has been promoted. The `always` snapshot mode ensures that the connector does not miss any changes that were made after the new primary had been promoted but before the connector was restarted on the new primary.\n \n-ifdef::community[]\n-[[supported-postgresql-topologies]]\n-== Supported PostgreSQL Topologies\n+|`never`\n+|The connector never performs snapshots. When a connector is configured this way, its behavior when it starts is as follows. If there is a previously stored LSN in the Kafka offsets topic, the connector continues streaming changes from that position. If no LSN has been stored, the connector starts streaming changes from the point in time when the PostgreSQL logical replication slot was created on the server. The `never` snapshot mode is useful only when you know all data of interest is still reflected in the WAL.\n \n-The PostgreSQL connector can be used with a standalone PostgreSQL server or with a cluster of PostgreSQL servers.\n+|`initial only`\n+|The connector performs a database snapshot and stops before streaming any change event records. If the connector had started but did not complete a snapshot before stopping, the connector restarts the snapshot process and stops when the snapshot completes.\n \n-As mentioned {link-prefix}:{link-postgresql-connector}#postgresql-limitations[in the beginning], PostgreSQL (for all versions <= 12) only supports logical replication slots on `primary` servers. This means that a replica in a PostgreSQL cluster cannot be configured for logical replication, and consequently that the {prodname} PostgreSQL Connector can only connect and communicate with the primary server. Should this server fail, the connector will stop. When the cluster is repaired, if the original primary server is once again promoted to `primary`, the connector can simply be restarted. However, if a different PostgreSQL server _with the plug-in and proper configuration_ is promoted to `primary`, the connector configuration must be changed to point to the new `primary` server and then can be restarted.\n-endif::community[]\n+|`exported`\n+|The connector performs a database snapshot based on the point in time when the replication slot was created. This mode is an excellent way to perform a snapshot in a lock-free way.\n \n-[[postgresql-wal-disk-space]]\n-=== WAL Disk Space Consumption\n-In certain cases, it is possible that PostgreSQL disk space consumed by WAL files either experiences spikes or increases out of usual proportions.\n-There are three potential reasons that explain the situation:\n-\n- * {prodname} regularly confirms LSN of processed events to the database.\n-This is visible as `confirmed_flush_lsn` in the `pg_replication_slots` slots table.\n-The database is responsible for reclaiming the disk space and the WAL size can be calculated from `restart_lsn` of the same table.\n-So if the `confirmed_flush_lsn` is regularly increasing and `restart_lsn` lags then the database does need to reclaim the space.\n-Disk space is usually reclaimed in batch blocks so this is expected behavior and no action on a user's side is necessary.\n- * There are many updates in a monitored database but only a minuscule amount relates to the monitored table(s) and/or schema(s).\n-This situation can be easily solved by enabling periodic heartbeat events using `heartbeat.interval.ms` configuration option.\n- * The PostgreSQL instance contains multiple databases where one of them is a high-traffic database.\n- {prodname} monitors another database that is low-traffic in comparison to the other one.\n- {prodname} then cannot confirm the LSN as replication slots work per-database and {prodname} is not invoked.\n- As WAL is shared by all databases it tends to grow until an event is emitted by the database monitored by {prodname}.\n-\n-To overcome the third cause it is necessary to\n-\n- * enable periodic heartbeat record generation using the `heartbeat.interval.ms` configuration option\n- * regularly emit change events from the database tracked by {prodname}\n ifdef::community[]\n- ** In the case of `wal2json` decoder plug-in, it is sufficient to generate empty events.\n- This can be achieved for example by truncating an empty temporary table.\n- ** For other decoder plug-ins, it is recommended to create a supplementary table that is not monitored by {prodname}.\n+|`custom`\n+|The `custom` snapshot mode lets you inject your own implementation of the `io.debezium.connector.postgresql.spi.Snapshotter` interface. Set the `snapshot.custom.class` configuration property to the class on the classpath of your Kafka Connect cluster or included in the JAR if using the `EmbeddedEngine`. For more details, see {link-prefix}:{link-postgresql-connector}#postgresql-custom-snapshot[custom snapshotter SPI].\n endif::community[]\n \n-A separate process would then periodically update the table (either inserting a new event or updating the same row all over).\n-PostgreSQL then will invoke {prodname} which will confirm the latest LSN and allow the database to reclaim the WAL space.\n-This task can be automated by means of the `heartbeat.action.query` connector option (see below).\n-\n-[TIP]\n-====\n-For users on AWS RDS with Postgres, a similar situation to the third cause may occur on an idle environment,\n-since AWS RDS makes writes to its own system tables not visible to the useres on a frequent basis (5 minutes).\n-Again regularly emitting events will solve the problem.\n-====\n-\n-[[how-the-postgresql-connector-works]]\n-=== How the PostgreSQL connector works\n-\n-[[postgresql-snapshots]]\n-==== Snapshots\n-\n-Most PostgreSQL servers are configured to not retain the complete history of the database in the WAL segments, so the PostgreSQL connector would be unable to see the entire history of the database by simply reading the WAL. So, by default the connector will upon first startup perform an initial _consistent snapshot_ of the database. Each snapshot consists of the following steps (when using the builtin snapshot modes, *custom* snapshot modes may override this):\n-\n-1. Start a transaction with a https://www.postgresql.org/docs/current/static/sql-set-transaction.html[SERIALIZABLE, READ ONLY, DEFERRABLE] isolation level to ensure that all subsequent reads within this transaction are done against a single consistent version of the data. Any changes to the data due to subsequent `INSERT`, `UPDATE`, and `DELETE` operations by other clients will not be visible to this transaction.\n-2. Obtain a `ACCESS SHARE MODE` lock on each of the monitored tables to ensure that no structural changes can occur to any of the tables while the snapshot is taking place. Note that these locks do not prevent table `INSERTS`, `UPDATES` and `DELETES` from taking place during the operation.  _This step is omitted when using the exported snapshot mode to allow for a lock-free snapshots_.\n-3. Read the current position in the server's transaction log.\n-4. Scan all of the database tables and schemas, and generate a `READ` event for each row and write that event to the appropriate table-specific Kafka topic.\n-5. Commit the transaction.\n-6. Record the successful completion of the snapshot in the connector offsets.\n-\n-If the connector fails, is rebalanced, or stops after Step 1 begins but before Step 6 completes, upon restart the connector will begin a new snapshot. Once the connector does complete its initial snapshot, the PostgreSQL connector then continues streaming from the position read during step 3, ensuring that it does not miss any updates. If the connector stops again for any reason, upon restart it will simply continue streaming changes from where it previously left off.\n-\n-A second snapshot mode allows the connector to perform snapshots *always*. This behavior tells the connector to _always_ perform a snapshot when it starts up, and after the snapshot completes to continue streaming changes from step 3 in the above sequence. This mode can be used in cases when it is known that some WAL segments have been deleted and are no longer available, or in case of a cluster failure after a new primary has been promoted so that the connector does not miss any potential changes that could have taken place after the new primary had been promoted but before the connector was restarted on the new primary.\n-\n-The third snapshot mode instructs the connector to *never* performs snapshots. When a new connector is configured this way, if will either continue streaming changes from a previous stored offset or it will start from the point in time when the PostgreSQL logical replication slot was first created on the server. Note that this mode is useful only when you know all data of interest is still reflected in the WAL.\n-\n-The fourth snapshot mode, *initial only*, will perform a database snapshot and then stop before streaming any other changes. If the connector had started but did not complete a snapshot before stopping, the connector will restart the snapshot process and stop once the snapshot completes.\n-\n-The fifth snapshot mode, *exported*, will perform a database snapshot based on the point in time when the replication slot was created.  This mode is an excellent way to perform a snapshot in a lock-free way.\n-\n-[WARNING]\n-====\n-It is strongly recommended to use *exported* mode as the *initial (only)* and *always* modes can lose few events while switching from snapshot to streaming mode when database is under heavy load.\n-This is a known issue and the affected modes will be reworked to use *exported* mode under the hood (https://issues.redhat.com/browse/DBZ-2337[DBZ-2337]).\n-====\n+|===\n \n ifdef::community[]\n-The final snapshot mode, *custom*, allows the user to inject their own implementation of the `io.debezium.connector.postgresql.spi.Snapshotter` interface via the `snapshot.custom.class` configuration property, with the class on the classpath of your Kafka Connect cluster (or included in the JAR if using the `EmbeddedEngine`). For more details, see the {link-prefix}:{link-postgresql-connector}#postgresql-custom-snapshot[Custom Snapshot] section.\n-\n-\n [[postgresql-custom-snapshot]]\n-=== Custom Snapshotter SPI\n+=== Custom snapshotter SPI\n \n-For more advanced usages, the user can provide an implementation of the `io.debezium.connector.postgresql.spi.Snapshotter` interface. This interfaces allows control of most of the aspects of how snapshots operate, such as whether to take a snapshot or not and the way the options used to open the snapshot transaction or take locks.\n+For more advanced uses, you can provide an implementation of the `io.debezium.connector.postgresql.spi.Snapshotter` interface. This interaces allows control of most of the aspects of how the connector performs snapshots. This includes whether or not to take a snapshot, the options for opening the snapshot transaction, and whether to take locks. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 451}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQzMzYzNw==", "bodyText": "Fixed in the next commit.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462433637", "createdAt": "2020-07-29T16:34:23Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -12,356 +14,156 @@ ifdef::community[]\n toc::[]\n endif::community[]\n \n-{prodname}'s PostgreSQL Connector can monitor and record row-level changes in the schemas of a PostgreSQL database.\n+{prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 10, 11, and 12 are supported. \n \n-The first time it connects to a PostgreSQL server/cluster, it reads a consistent snapshot of all of the schemas. When that snapshot is complete, the connector continuously streams the changes that were committed to PostgreSQL 9.6 or later and generates corresponding insert, update and delete events. All of the events for each table are recorded in a separate Kafka topic, where they can be easily consumed by applications and services.\n+The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic. \n \n+ifdef::product[]\n+Information and procedures for using a {prodname} PostgreSQL connector is organized as follows: \n+\n+* xref:overview-of-debezium-postgresql-connector[]\n+* xref:how-debezium-postgresql-connectors-work[]\n+* xref:descriptions-of-debezium-postgresql-connector-data-change-events[]\n+* xref:how-debezium-postgresql-connectors-map-data-types[]\n+* xref:setting-up-postgresql-to-run-a-debezium-connector[]\n+* xref:deploying-and-managing-debezium-postgresql-connectors[]\n+* xref:how-debezium-postgresql-connectors-handle-faults-and-problems[]\n+\n+endif::product[]\n \n+// Type: concept\n+// Title: Overview of {prodname} PostgreSQL connector \n+// ModuleID: overview-of-debezium-postgresql-connector\n [[postgresql-overview]]\n == Overview\n \n-PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was first introduced in version 9.4 and is a mechanism which allows the extraction of the changes which were committed to the transaction log and the processing of these changes in a user-friendly manner via the help of an https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. This output plug-in must be installed prior to running the PostgreSQL server and enabled together with a replication slot in order for clients to be able to consume the changes.\n+PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was introduced in version 9.4. It is a mechanism that allows the extraction of the changes that were committed to the transaction log and the processing of these changes in a user-friendly manner with the help of an link:https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. You must install this output plug-in and configure a replication slot that uses the output plug-in before running the PostgreSQL server. This enables clients to consume the changes. \n \n-PostgreSQL connector contains two different parts which work together in order to be able to read and process server changes:\n+The PostgreSQL connector contains two main parts that work together to read and process server changes:\n \n ifdef::product[]\n-* A logical decoding output plug-in, which has to be installed and configured in the PostgreSQL server.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server.\n+* Java code (the actual Kafka Connect connector), which reads the changes produced by the plug-in by using PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_] and the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_].\n endif::product[]\n \n ifdef::community[]\n-* a logical decoding output plug-in which has to be installed and configured in the PostgreSQL server, one of\n-** https://github.com/debezium/postgres-decoderbufs[decoderbufs] (maintained by the {prodname} community, based on ProtoBuf)\n-** https://github.com/eulerto/wal2json[wal2json] (maintained by the wal2json community, based on JSON)\n-** pgoutput, the standard logical decoding plug-in in PostgreSQL 10+ (maintained by the Postgres community, used by Postgres itself for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]);\n-this plug-in is always present, meaning that no additional libraries must be installed,\n-and the {prodname} connector will interpret the raw replication event stream into change events directly.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the chosen plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server. The plug-in is one of these: \n+** link:https://github.com/debezium/postgres-decoderbufs[`decoderbufs`] - maintained by the {prodname} community, based on ProtoBuf.\n+** link:https://github.com/eulerto/wal2json[`wal2json`] - maintained by the wal2json community, based on JSON.\n+** `pgoutput`, which is the standard logical decoding plug-in in PostgreSQL 10+. It is maintained by the PostgreSQL community, and used by PostgreSQL itself for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. This plug-in is always present so no additional libraries need to be installed. The {prodname} connector interprets the raw replication event stream directly into change events.\n+* Java code (the actual Kafka Connect connector) that reads the changes produced by the chosen plug-in. It uses PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], by means of the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n endif::community[]\n \n-The connector then produces a _change event_ for every row-level insert, update, and delete operation that was received, recording all the change events for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables they're interested in following, and react to every row-level event it sees in those topics.\n+The connector produces a _change event_ for every row-level insert, update, and delete operation that was captured and sends change event records for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables of interest, and can react to every row-level event they receive from those topics.\n \n-PostgreSQL normally purges WAL segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, we start with a consistent view of all of the data, yet continue reading without having lost any of the changes made while the snapshot was taking place.\n+PostgreSQL normally purges write-ahead log (WAL) segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, the connector starts with a consistent view of all of the data, and does not omit any changes that were made while the snapshot was being taken.\n \n-The connector is also tolerant of failures. As the connector reads changes and produces events, it records the position in the write-ahead log with each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart it simply continues reading the WAL where it last left off. This includes snapshots: if the snapshot was not completed when the connector is stopped, upon restart it will begin a new snapshot.\n+The connector is tolerant of failures. As the connector reads changes and produces events, it records the position in the WAL for each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart the connector continues reading the WAL where it last left off. This includes snapshots. If the connector stops during a snapshot, the connector begins a new snapshot when it restarts. \n \n ifdef::product[]\n [[postgresql-output-plugin]]\n-=== Logical decoding output plug-in\n-The `pgoutput` logical decoder is the only supported logical decoder in the Tecnhology Preview release of {prodname}.\n-\n-`pgoutput`, the standard logical decoding plug-in in PostgreSQL 10+, is maintained by the Postgres community, and is also used by Postgres for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication].\n-The `pgoutput` plug-in is always present, meaning that no additional libraries must be installed,\n-and the connector will interpret the raw replication event stream into change events directly.\n+.Logical decoding output plug-in\n+The `pgoutput` logical decoding plug-in, the only supported logical decoding plug-in in this {prodname} release, is the standard logical decoding plug-in in PostgreSQL 10+, it is maintained by the PostgreSQL community, and it is also used by PostgreSQL for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. The `pgoutput` plug-in is always present, meaning that no additional libraries must be installed. The connector directly interprets the raw replication event stream into change events records.\n endif::product[]\n \n [[postgresql-limitations]]\n [IMPORTANT]\n ====\n-The connector's functionality relies on PostgreSQL's logical decoding feature.\n-Please be aware of the following limitations which are also reflected by the connector:\n+The connector relies on and reflects the PostgreSQL logical decoding feature, which has the following limitations:\n+\n+* Logical decoding does not support DDL changes. This means that the connector is unable to report DDL change events back to consumers.\n+* Logical decoding replication slots are supported on only `primary` servers. When there is a cluster of PostgreSQL servers, the connector can run on only the active `primary` server. It cannot run on `hot` or `warm` standby replicas. If the `primary` server fails or is demoted, the connector stops. After the `primary` server has recovered, you can restart the connector. If a different PostgreSQL server has been promoted to `primary`, adjust the connector configuration before restarting the connector. \n \n-. Logical Decoding does not support DDL changes: this means that the connector is unable to report DDL change events back to consumers.\n-. Logical Decoding replication slots are only supported on `primary` servers: this means that when there is a cluster of PostgreSQL servers, the connector can only run on the active `primary` server. It cannot run on `hot` or `warm` standby replicas. If the `primary` server fails or is demoted, the connector will stop. Once the `primary` has recovered the connector can simply be restarted. If a different PostgreSQL server has been promoted to `primary`, the connector configuration must be adjusted before the connector is restarted. Make sure you read more about how the connector behaves {link-prefix}:{link-postgresql-connector}#postgresql-when-things-go-wrong[when things go wrong].\n+{link-prefix}:{link-postgresql-connector}#postgresql-when-things-go-wrong[ Behavior when things go wrong] describes what the connector does when there is a problem. \n ====\n \n [IMPORTANT]\n ====\n-{prodname} currently supports only databases with UTF-8 character encoding.\n-With a single byte character encoding it is not possible to correctly process strings containing extended ASCII code characters.\n+{prodname} currently supports databases with UTF-8 character encoding only.\n+With a single byte character encoding, it is not possible to correctly process strings that contain extended ASCII code characters.\n ====\n \n-[[setting-up-postgresql]]\n-== Setting up PostgreSQL\n-\n-ifdef::product[]\n-This release of {prodname} only supports the native pgoutput logical replication stream.\n-To set up PostgreSQL using pgoutput, you will need to enable a replication slot, and configure a user with sufficient privileges to perform the replication.\n-\n-=== Configuring the replication slot\n-\n-PostgreSQL's logical decoding uses replication slots.\n-\n-First, you configure the replication slot:\n-\n-.postgresql.conf\n+// Type: assembly\n+// ModuleID: how-debezium-postgresql-connectors-work\n+// Title: How {prodname} PostgreSQL connectors work\n+[[how-the-postgresql-connector-works]]\n+== How the connector works\n \n-[source]\n-----\n-wal_level=logical\n-max_wal_senders=1\n-max_replication_slots=1\n-----\n+To optimally configure and run a {prodname} PostgreSQL connector, it is helpful to understand how the connector performs snapshots, streams change events, determines Kafka topic names, and uses metadata. \n \n-* `wal_level` tells the server to use logical decoding with the write-ahead log\n-* `max_wal_senders` tells the server to use a maximum of 1 separate processes for processing WAL changes\n-* `max_replication_slots` tells the server to allow a maximum of 1 replication slots to be created for streaming WAL changes\n+ifdef::product[]\n+Details are in the following topics: \n \n-Replication slots are guaranteed to retain all WAL required for {prodname} even during {prodname} outages.\n-It is important for this reason to closely monitor replication slots to avoid too much disk consumption and other conditions that can happen such as catalog bloat if a replication slot stays unused for too long.\n-For more information, refer to the https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION-SLOTS[the Postgres documentation].\n+* xref:how-debezium-postgresql-connectors-perform-database-snapshots[]\n+* xref:how-debezium-postgresql-connectors-stream-change-event-records[]\n+* xref:postgresql-10-logical-decoding-support-pgoutput[]\n+* xref:default-names-of-kafka-topics-that-receive-debezium-change-event-records[]\n+* xref:metadata-in-debezium-change-event-records[]\n+* xref:debezium-connector-generated-events-that-represent-transaction-boundaries[]\n \n-[NOTE]\n-====\n-We recommend reading and understanding the https://www.postgresql.org/docs/current/static/wal-configuration.html[WAL configuration documentation] regarding the mechanics and configuration of the PostgreSQL write-ahead log.\n-====\n endif::product[]\n \n-ifdef::community[]\n-Before using the PostgreSQL connector to monitor the changes committed on a PostgreSQL server,\n-first decide which logical decoder method you intend to use.\n-If you plan *not* to use the native pgoutput logical replication stream support,\n-then you will need to install the logical decoding plug-in into the PostgreSQL server.\n-Afterward enable a replication slot, and configure a user with sufficient privileges to perform the replication.\n-\n-Note that if your database is hosted by a service such as https://www.heroku.com/postgres[Heroku Postgres] you may be unable to install the plug-in.\n-If so, and if you're using PostgreSQL 10+, you can use the pgoutput decoder support to monitor your database.\n-If that is not an option, you'll be unable to monitor your database with {prodname}.\n-\n-[[postgresql-on-amazon-rds]]\n-=== PostgreSQL on Amazon RDS\n-\n-It is possible to monitor PostgreSQL database running in https://aws.amazon.com/rds/[Amazon RDS]. To get it running you must fulfill the following conditions\n-\n-* The instance parameter `rds.logical_replication` is set to `1`.\n-* Verify that `wal_level` parameter is set to `logical` by running the query `SHOW wal_level` as DB master user; this might not be the case in multi-zone replication setups.\n-You cannot set this option manually, it is (https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html[automatically changed]) when the `rds.logical_replication` is set to `1`.\n-If the `wal_level` is not `logical` after the change above, it is probably because the instance has to be restarted due to the parameter group change. This happens accordingly to your maintenance window or can be done manually.\n-* Set `plugin.name` {prodname} parameter to `wal2json`.  You can skip this on PostgreSQL 10+ if you wish to use pgoutput logical replication stream support.\n-* Use database master account for replication as RDS currently does not support setting of `REPLICATION` privilege for another account.\n-\n-[IMPORTANT]\n-====\n-You should make sure to use the latest versions of Postgres 9.6, 10 or 11 on Amazon RDS.\n-Otherwise, older versions of the wal2json plug-in may be installed\n-(see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html[the official documentation] for the exact wal2json versions installed on Amazon RDS).\n-In that case, replication messages received from the database may not carry complete information about type constraints like length or scale or `NULL`/`NOT NULL`,\n-which in turn might cause creation of messages with an inconsistent schema for a short period of time in case of changes to a column's definition.\n-\n-As of January 2019, the following Postgres versions on RDS come with an up-to-date version of wal2json and thus should be used:\n-\n-* Postgres 9.6: 9.6.10 and newer\n-* Postgres 10: 10.5 and newer\n-* Postgres 11: any version\n-====\n-\n-\n-[[postgresql-output-plugin]]\n-=== Installing the Logical Decoding Output Plug-in\n-\n-[TIP]\n-====\n-Also see {link-prefix}:{link-postgresql-plugins}[Logical Decoding Output Plug-in Installation for PostgreSQL] for more detailed instructions of setting up and testing logical decoding plug-ins.\n-====\n-\n-[NOTE]\n-====\n-As of {prodname} 0.10, the connector supports PostgreSQL 10+ logical replication streaming using _pgoutput_.\n-This means that a logical decoding output plug-in is no longer necessary and changes can be emitted directly from the replication stream by the connector.\n-====\n+// Type: concept\n+// ModuleID: how-debezium-postgresql-connectors-perform-database-snapshots\n+// Title: How {prodname} PostgreSQL connectors perform database snapshots\n+[[postgresql-snapshots]]\n+=== Snapshots\n \n-As of PostgreSQL 9.4, the only way to read changes to the write-ahead-log is to first install a logical decoding output plug-in. Plugins are written in C, compiled, and installed on the machine which runs the PostgreSQL server. Plugins use  a number of PostgreSQL specific APIs, as described by the https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_PostgreSQL documentation_].\n+Most PostgreSQL servers are configured to not retain the complete history of the database in the WAL segments. This means that the PostgreSQL connector would be unable to see the entire history of the database by reading only the WAL. Consequently, the first time that the connector starts, it performs an initial _consistent snapshot_ of the database. The default behavior for performing a snapshot consists of the following steps. You can change this behavior by setting the {link-prefix}:{link-postgresql-connector}#postgresql-property-snapshot-mode[`snapshot.mode` connector configuration property] to a value other than `initial`. \n \n-The PostgreSQL connector works with one of {prodname}'s supported logical decoding plug-ins to encode the changes in either https://github.com/google/protobuf[_Protobuf format_] or http://www.json.org/[_JSON_] format.\n-See the documentation of your chosen plug-in (https://github.com/debezium/postgres-decoderbufs/blob/master/README.md[_protobuf_], https://github.com/eulerto/wal2json/blob/master/README.md[_wal2json_]) to learn more about the plug-in's requirements, limitations, and how to compile it.\n+. Start a transaction with a link:https://www.postgresql.org/docs/current/static/sql-set-transaction.html[SERIALIZABLE, READ ONLY, DEFERRABLE] isolation level to ensure that subsequent reads in this transaction are against a single consistent version of the data. Any changes to the data due to subsequent `INSERT`, `UPDATE`, and `DELETE` operations by other clients are not visible to this transaction.\n+. Obtain an `ACCESS SHARE MODE` lock on each of the tables being tracked to ensure that no structural changes can occur to any of the tables while the snapshot is taking place. These locks do not prevent table `INSERT`, `UPDATE` and `DELETE` operations from taking place during the snapshot. \n++\n+_This step is omitted when `snapshot.mode` is set to `exported`, which allows the connector to perform a lock-free snapshot_.\n+. Read the current position in the server's transaction log.\n+. Scan the database tables and schemas, generate a `READ` event for each row and write that event to the appropriate table-specific Kafka topic.\n+. Commit the transaction.\n+. Record the successful completion of the snapshot in the connector offsets.\n \n-For simplicity, {prodname} also provides a Docker image based on a vanilla PostgreSQL server image on top of which it compiles and installs the plug-ins. We recommend https://github.com/debezium/docker-images/tree/master/postgres/9.6[_using this image_] as an example of the detailed steps required for the installation.\n+If the connector fails, is rebalanced, or stops after Step 1 begins but before Step 6 completes, upon restart the connector begins a new snapshot. After the connector completes its initial snapshot, the PostgreSQL connector continues streaming from the position that it read in step 3. This ensures that the connector does not miss any updates. If the connector stops again for any reason, upon restart, the connector continues streaming changes from where it previously left off.\n \n [WARNING]\n ====\n-The {prodname} logical decoding plug-ins have only been installed and tested on _Linux_ machines.\n-For Windows and other operating systems it may require different installation steps.\n-====\n-\n-[[postgresql-differences-between-plugins]]\n-==== Differences Between Plug-ins\n-\n-The plug-ins' behavior is not completely same for all cases.\n-So far these differences have been identified:\n-\n-* wal2json plug-in is not able to process quoted identifiers (https://github.com/eulerto/wal2json/issues/35[issue])\n-* wal2json and decoderbufs plug-ins emit events for tables without primary keys\n-* wal2json plug-in does not support special values (`NaN` or `infinity`) for floating point types\n-* wal2json should be used with setting the `schema.refresh.mode` connector option to `columns_diff_exclude_unchanged_toast`;\n-otherwise, when receiving a change event for a row containing an unchanged TOAST column, no field for that column is contained in the emitted change event's `after` structure.\n-This is because wal2json's messages do not contain a field for such a column.\n-\n-The requirement for adding this is tracked under the wal2json https://github.com/eulerto/wal2json/issues/98[issue 98].\n-See the documentation of `columns_diff_exclude_unchanged_toast` further below for implications of using it.\n-\n-* pgoutput plug-in does not emit all the events for tables without primary keys, it only emits inserts\n-\n-All up-to-date differences are tracked in a test suite https://github.com/debezium/debezium/blob/master/debezium-connector-postgres/src/test/java/io/debezium/connector/postgresql/DecoderDifferences.java[Java class].\n-\n-\n-[[postgresql-server-configuration]]\n-=== Configuring the PostgreSQL Server\n-\n-If you are using one of the supported {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-ins] (i.e. not pgoutput) and it has been installed,\n-configure the server to load the plug-in at startup:\n-\n-.postgresql.conf\n-[source,properties]\n-----\n-# MODULES\n-shared_preload_libraries = 'decoderbufs,wal2json' // <1>\n-----\n-<1> tells the server that it should load at startup the `decoderbufs` and `wal2json` logical decoding plug-ins (the names of the plug-ins are set in https://github.com/debezium/postgres-decoderbufs/blob/v0.3.0/Makefile[_Protobuf_] and https://github.com/eulerto/wal2json/blob/master/Makefile[_wal2json_] Makefiles)\n-\n-Next is to configure the replication slot regardless of the decoder being used:\n-\n-.postgresql.conf\n-[source,properties]\n-----\n-# REPLICATION\n-wal_level = logical             // <1>\n-max_wal_senders = 1             // <2>\n-max_replication_slots = 1       // <3>\n-----\n-<1> tells the server that it should use logical decoding with the write-ahead log\n-<2> tells the server that it should use a maximum of `1` separate processes for processing WAL changes\n-<3> tells the server that it should allow a maximum of `1` replication slots to be created for streaming WAL changes\n-\n-{prodname} uses PostgreSQL's logical decoding, which uses replication slots.\n-Replication slots are guaranteed to retain all WAL required for {prodname} even during {prodname} outages.\n-It is important for this reason to closely monitor replication slots to avoid too much disk consumption and other conditions that can happen such as catalog bloat if a replication slot stays unused for too long.\n-For more information please see the official Postgres docs on https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION-SLOTS[this subject].\n-\n-If you are working with a `synchronous_commit` setting other than `on`,\n-the recommendation is to set `wal_writer_delay` to a value such as 10 ms to achieve a low latency of change events.\n-Otherwise, its default value is applied, which adds a latency of about 200 ms.\n-\n-[TIP]\n-====\n-We strongly recommend reading and understanding https://www.postgresql.org/docs/current/static/wal-configuration.html[the official documentation] regarding the mechanics and configuration of the PostgreSQL write-ahead log.\n-====\n-endif::community[]\n-\n-[[postgresql-permissions]]\n-=== Setting up Permissions\n-\n-Next, configure a database user who can perform replications.\n-\n-Replication can only be performed by a database user that has appropriate permissions and only for a configured number of hosts.\n-\n-In order to give a user replication permissions, define a PostgreSQL role that has _at least_ the `REPLICATION` and `LOGIN` permissions. For example:\n-\n-[source,sql]\n-----\n-CREATE ROLE name REPLICATION LOGIN;\n-----\n-\n-[NOTE]\n-====\n-Superusers have by default both of the above roles.\n+It is strongly recommended that you configure a PostgreSQL connector to set `snapshot.mode` to `exported`. The `initial`, `initial only` and `always` modes can lose a few events while a connector switches from performing the snapshot to streaming change event records when a database is under heavy load.\n+This is a known issue and the affected snapshot modes will be reworked to use `exported` mode internally (link:https://issues.redhat.com/browse/DBZ-2337[DBZ-2337]).\n ====\n \n-Finally, configure the PostgreSQL server to allow replication to take place between the server machine and the host on which the PostgreSQL connector is running:\n-\n-.pg_hba.conf\n-[source]\n-----\n-local   replication     <youruser>                          trust   // <1>\n-host    replication     <youruser>  127.0.0.1/32            trust   // <2>\n-host    replication     <youruser>  ::1/128                 trust   // <3>\n-----\n-<1> Tells the server to allow replication for `<youruser>` locally (i.e. on the server machine)\n-<2> Tells the server to allow `<youruser>` on `localhost` to receive replication changes using `IPV4`\n-<3> Tells the server to allow `<youruser>` on `localhost` to receive replication changes using `IPV6`\n+[id=\"snapshot-mode-settings\"]\n+.Settings for `snapshot.mode` connector configuration property\n+[cols=\"20%a,80%a\",options=\"header\"]\n+|===\n+|Setting\n+|Description\n \n-[NOTE]\n-====\n-See https://www.postgresql.org/docs/current/static/datatype-net-types.html[_the PostgreSQL documentation_] for more information on network masks.\n-====\n+|`always`\n+|The connector always performs a snapshot when it starts. After the snapshot completes, the connector continues streaming changes from step 3 in the above sequence. This mode is useful in these situations: + \n+ \n+* It is known that some WAL segments have been deleted and are no longer available. +\n+* After a cluster failure, a new primary has been promoted. The `always` snapshot mode ensures that the connector does not miss any changes that were made after the new primary had been promoted but before the connector was restarted on the new primary.\n \n-ifdef::community[]\n-[[supported-postgresql-topologies]]\n-== Supported PostgreSQL Topologies\n+|`never`\n+|The connector never performs snapshots. When a connector is configured this way, its behavior when it starts is as follows. If there is a previously stored LSN in the Kafka offsets topic, the connector continues streaming changes from that position. If no LSN has been stored, the connector starts streaming changes from the point in time when the PostgreSQL logical replication slot was created on the server. The `never` snapshot mode is useful only when you know all data of interest is still reflected in the WAL.\n \n-The PostgreSQL connector can be used with a standalone PostgreSQL server or with a cluster of PostgreSQL servers.\n+|`initial only`\n+|The connector performs a database snapshot and stops before streaming any change event records. If the connector had started but did not complete a snapshot before stopping, the connector restarts the snapshot process and stops when the snapshot completes.\n \n-As mentioned {link-prefix}:{link-postgresql-connector}#postgresql-limitations[in the beginning], PostgreSQL (for all versions <= 12) only supports logical replication slots on `primary` servers. This means that a replica in a PostgreSQL cluster cannot be configured for logical replication, and consequently that the {prodname} PostgreSQL Connector can only connect and communicate with the primary server. Should this server fail, the connector will stop. When the cluster is repaired, if the original primary server is once again promoted to `primary`, the connector can simply be restarted. However, if a different PostgreSQL server _with the plug-in and proper configuration_ is promoted to `primary`, the connector configuration must be changed to point to the new `primary` server and then can be restarted.\n-endif::community[]\n+|`exported`\n+|The connector performs a database snapshot based on the point in time when the replication slot was created. This mode is an excellent way to perform a snapshot in a lock-free way.\n \n-[[postgresql-wal-disk-space]]\n-=== WAL Disk Space Consumption\n-In certain cases, it is possible that PostgreSQL disk space consumed by WAL files either experiences spikes or increases out of usual proportions.\n-There are three potential reasons that explain the situation:\n-\n- * {prodname} regularly confirms LSN of processed events to the database.\n-This is visible as `confirmed_flush_lsn` in the `pg_replication_slots` slots table.\n-The database is responsible for reclaiming the disk space and the WAL size can be calculated from `restart_lsn` of the same table.\n-So if the `confirmed_flush_lsn` is regularly increasing and `restart_lsn` lags then the database does need to reclaim the space.\n-Disk space is usually reclaimed in batch blocks so this is expected behavior and no action on a user's side is necessary.\n- * There are many updates in a monitored database but only a minuscule amount relates to the monitored table(s) and/or schema(s).\n-This situation can be easily solved by enabling periodic heartbeat events using `heartbeat.interval.ms` configuration option.\n- * The PostgreSQL instance contains multiple databases where one of them is a high-traffic database.\n- {prodname} monitors another database that is low-traffic in comparison to the other one.\n- {prodname} then cannot confirm the LSN as replication slots work per-database and {prodname} is not invoked.\n- As WAL is shared by all databases it tends to grow until an event is emitted by the database monitored by {prodname}.\n-\n-To overcome the third cause it is necessary to\n-\n- * enable periodic heartbeat record generation using the `heartbeat.interval.ms` configuration option\n- * regularly emit change events from the database tracked by {prodname}\n ifdef::community[]\n- ** In the case of `wal2json` decoder plug-in, it is sufficient to generate empty events.\n- This can be achieved for example by truncating an empty temporary table.\n- ** For other decoder plug-ins, it is recommended to create a supplementary table that is not monitored by {prodname}.\n+|`custom`\n+|The `custom` snapshot mode lets you inject your own implementation of the `io.debezium.connector.postgresql.spi.Snapshotter` interface. Set the `snapshot.custom.class` configuration property to the class on the classpath of your Kafka Connect cluster or included in the JAR if using the `EmbeddedEngine`. For more details, see {link-prefix}:{link-postgresql-connector}#postgresql-custom-snapshot[custom snapshotter SPI].\n endif::community[]\n \n-A separate process would then periodically update the table (either inserting a new event or updating the same row all over).\n-PostgreSQL then will invoke {prodname} which will confirm the latest LSN and allow the database to reclaim the WAL space.\n-This task can be automated by means of the `heartbeat.action.query` connector option (see below).\n-\n-[TIP]\n-====\n-For users on AWS RDS with Postgres, a similar situation to the third cause may occur on an idle environment,\n-since AWS RDS makes writes to its own system tables not visible to the useres on a frequent basis (5 minutes).\n-Again regularly emitting events will solve the problem.\n-====\n-\n-[[how-the-postgresql-connector-works]]\n-=== How the PostgreSQL connector works\n-\n-[[postgresql-snapshots]]\n-==== Snapshots\n-\n-Most PostgreSQL servers are configured to not retain the complete history of the database in the WAL segments, so the PostgreSQL connector would be unable to see the entire history of the database by simply reading the WAL. So, by default the connector will upon first startup perform an initial _consistent snapshot_ of the database. Each snapshot consists of the following steps (when using the builtin snapshot modes, *custom* snapshot modes may override this):\n-\n-1. Start a transaction with a https://www.postgresql.org/docs/current/static/sql-set-transaction.html[SERIALIZABLE, READ ONLY, DEFERRABLE] isolation level to ensure that all subsequent reads within this transaction are done against a single consistent version of the data. Any changes to the data due to subsequent `INSERT`, `UPDATE`, and `DELETE` operations by other clients will not be visible to this transaction.\n-2. Obtain a `ACCESS SHARE MODE` lock on each of the monitored tables to ensure that no structural changes can occur to any of the tables while the snapshot is taking place. Note that these locks do not prevent table `INSERTS`, `UPDATES` and `DELETES` from taking place during the operation.  _This step is omitted when using the exported snapshot mode to allow for a lock-free snapshots_.\n-3. Read the current position in the server's transaction log.\n-4. Scan all of the database tables and schemas, and generate a `READ` event for each row and write that event to the appropriate table-specific Kafka topic.\n-5. Commit the transaction.\n-6. Record the successful completion of the snapshot in the connector offsets.\n-\n-If the connector fails, is rebalanced, or stops after Step 1 begins but before Step 6 completes, upon restart the connector will begin a new snapshot. Once the connector does complete its initial snapshot, the PostgreSQL connector then continues streaming from the position read during step 3, ensuring that it does not miss any updates. If the connector stops again for any reason, upon restart it will simply continue streaming changes from where it previously left off.\n-\n-A second snapshot mode allows the connector to perform snapshots *always*. This behavior tells the connector to _always_ perform a snapshot when it starts up, and after the snapshot completes to continue streaming changes from step 3 in the above sequence. This mode can be used in cases when it is known that some WAL segments have been deleted and are no longer available, or in case of a cluster failure after a new primary has been promoted so that the connector does not miss any potential changes that could have taken place after the new primary had been promoted but before the connector was restarted on the new primary.\n-\n-The third snapshot mode instructs the connector to *never* performs snapshots. When a new connector is configured this way, if will either continue streaming changes from a previous stored offset or it will start from the point in time when the PostgreSQL logical replication slot was first created on the server. Note that this mode is useful only when you know all data of interest is still reflected in the WAL.\n-\n-The fourth snapshot mode, *initial only*, will perform a database snapshot and then stop before streaming any other changes. If the connector had started but did not complete a snapshot before stopping, the connector will restart the snapshot process and stop once the snapshot completes.\n-\n-The fifth snapshot mode, *exported*, will perform a database snapshot based on the point in time when the replication slot was created.  This mode is an excellent way to perform a snapshot in a lock-free way.\n-\n-[WARNING]\n-====\n-It is strongly recommended to use *exported* mode as the *initial (only)* and *always* modes can lose few events while switching from snapshot to streaming mode when database is under heavy load.\n-This is a known issue and the affected modes will be reworked to use *exported* mode under the hood (https://issues.redhat.com/browse/DBZ-2337[DBZ-2337]).\n-====\n+|===\n \n ifdef::community[]\n-The final snapshot mode, *custom*, allows the user to inject their own implementation of the `io.debezium.connector.postgresql.spi.Snapshotter` interface via the `snapshot.custom.class` configuration property, with the class on the classpath of your Kafka Connect cluster (or included in the JAR if using the `EmbeddedEngine`). For more details, see the {link-prefix}:{link-postgresql-connector}#postgresql-custom-snapshot[Custom Snapshot] section.\n-\n-\n [[postgresql-custom-snapshot]]\n-=== Custom Snapshotter SPI\n+=== Custom snapshotter SPI\n \n-For more advanced usages, the user can provide an implementation of the `io.debezium.connector.postgresql.spi.Snapshotter` interface. This interfaces allows control of most of the aspects of how snapshots operate, such as whether to take a snapshot or not and the way the options used to open the snapshot transaction or take locks.\n+For more advanced uses, you can provide an implementation of the `io.debezium.connector.postgresql.spi.Snapshotter` interface. This interaces allows control of most of the aspects of how the connector performs snapshots. This includes whether or not to take a snapshot, the options for opening the snapshot transaction, and whether to take locks. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMyMTgxOQ=="}, "originalCommit": null, "originalPosition": 451}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjM5ODIwOnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDowMjoxNFrOG457Sg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDowMjoxNFrOG457Sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMyMjUwNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * can be used as an alternative to takFIRST BUILD OF SPLIT FILE: ing a lock\n          \n          \n            \n                 * can be used as an alternative to taking a lock\n          \n      \n    \n    \n  \n\nThis line change looks suspicious.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462322506", "createdAt": "2020-07-29T14:02:14Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -397,7 +199,7 @@ public interface Snapshotter {\n \n     /**\n      * @return true if when creating a slot, a snapshot should be exported, which\n-     * can be used as an alternative to taking a lock\n+     * can be used as an alternative to takFIRST BUILD OF SPLIT FILE: ing a lock", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 479}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjQwODA3OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDowNDoyOFrOG46BaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDowNDoyOFrOG46BaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMyNDA3Mg==", "bodyText": "Since downstream we only support pgoutput, the last sentence in this section may want to omit the part about protobufs/json as the output stream is neither for this native plugin.  The format call-out is appropriate for community however.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462324072", "createdAt": "2020-07-29T14:04:28Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -443,115 +245,221 @@ public interface Snapshotter {\n }\n ----\n \n-All of the builtin snapshot modes are implemented in terms of this interface as well.\n endif::community[]\n \n+// Type: concept\n+// ModuleID: how-debezium-postgresql-connectors-stream-change-event-records\n+// Title: How {prodname} PostgreSQL connectors stream change event records\n [[postgresql-streaming-changes]]\n-==== Streaming Changes\n+=== Streaming changes\n+\n+The PostgreSQL connector typically spends the vast majority of its time streaming changes from the PostgreSQL server to which it is connected. This mechanism relies on link:https://www.postgresql.org/docs/current/static/protocol-replication.html[_PostgreSQL's replication protocol_]. This protocol enables clients to receive changes from the server as they are committed in the server's transaction log at certain positions, which are referred to as Log Sequence Numbers (LSNs).\n \n-The PostgreSQL connector will typically spend the vast majority of its time streaming changes from the PostgreSQL server to which it is connected. This mechanism relies on https://www.postgresql.org/docs/current/static/protocol-replication.html[_PostgreSQL's replication protocol_] where the client can receive changes from the server as they are committed in the server's transaction log at certain positions (also known as `Log Sequence Numbers` or in short LSNs).\n+Whenever the server commits a transaction, a separate server process invokes a callback function from the {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-in]. This function processes the changes from the transaction, converts them to a specific format (Protobuf or JSON in the case of {prodname} plug-in) and writes them on an output stream, which can then be consumed by clients.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 500}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjQ5MTc3OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDoyMToxMFrOG460nQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDoyMToxMFrOG460nQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMzNzE4MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    \"lsn\": \"24023128\",J\n          \n          \n            \n                    \"lsn\": \"24023128\",", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462337181", "createdAt": "2020-07-29T14:21:10Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -443,115 +245,221 @@ public interface Snapshotter {\n }\n ----\n \n-All of the builtin snapshot modes are implemented in terms of this interface as well.\n endif::community[]\n \n+// Type: concept\n+// ModuleID: how-debezium-postgresql-connectors-stream-change-event-records\n+// Title: How {prodname} PostgreSQL connectors stream change event records\n [[postgresql-streaming-changes]]\n-==== Streaming Changes\n+=== Streaming changes\n+\n+The PostgreSQL connector typically spends the vast majority of its time streaming changes from the PostgreSQL server to which it is connected. This mechanism relies on link:https://www.postgresql.org/docs/current/static/protocol-replication.html[_PostgreSQL's replication protocol_]. This protocol enables clients to receive changes from the server as they are committed in the server's transaction log at certain positions, which are referred to as Log Sequence Numbers (LSNs).\n \n-The PostgreSQL connector will typically spend the vast majority of its time streaming changes from the PostgreSQL server to which it is connected. This mechanism relies on https://www.postgresql.org/docs/current/static/protocol-replication.html[_PostgreSQL's replication protocol_] where the client can receive changes from the server as they are committed in the server's transaction log at certain positions (also known as `Log Sequence Numbers` or in short LSNs).\n+Whenever the server commits a transaction, a separate server process invokes a callback function from the {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-in]. This function processes the changes from the transaction, converts them to a specific format (Protobuf or JSON in the case of {prodname} plug-in) and writes them on an output stream, which can then be consumed by clients.\n \n-Whenever the server commits a transaction, a separate server process invokes a callback function from the {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-in]. This function processes the changes from the transaction, converts them to a specific format (Protobuf or JSON in the case of {prodname} plug-in) and writes them on an output stream which can then be consumed by clients.\n+The {prodname} PostgreSQL connector acts as a PostgreSQL client. When the connector receives changes it transforms the events into {prodname} _create_, _update_, or _delete_ events that include the LSN of the event. The PostgreSQL connector forwards these change events in records to the Kafka Connect framework, which is running in the same process. The Kafka Connect process asynchronously writes the change event records in the same order in which they were generated to the appropriate Kafka topic. \n \n-The PostgreSQL connector acts as a PostgreSQL client, and when it receives these changes it transforms the events into {prodname} _create_, _update_, or _delete_ events that include the LSN position of the event. The PostgreSQL connector forwards these change events to the Kafka Connect framework (running in the same process), which then asynchronously writes them in the same order to the appropriate Kafka topic. Kafka Connect uses the term _offset_ for the source-specific position information that {prodname} includes with each event, and Kafka Connect periodically records the most recent offset in another Kafka topic.\n+Periodically, Kafka Connect records the most recent _offset_ in another Kafka topic. The offset indicates source-specific position information that {prodname} includes with each event. For the PostgreSQL connector, the LSN recorded in each change event is the offset.\n \n-When Kafka Connect gracefully shuts down, it stops the connectors, flushes all events to Kafka, and records the last offset received from each connector. Upon restart, Kafka Connect reads the last recorded offset for each connector, and starts the connector from that point. The PostgreSQL connector uses the LSN recorded in each change event as the offset, so that upon restart the connector requests the PostgreSQL server send it the events starting just after that position.\n+When Kafka Connect gracefully shuts down, it stops the connectors, flushes all event records to Kafka, and records the last offset received from each connector. When Kafka Connect restarts, it reads the last recorded offset for each connector, and starts each connector at its last recorded offset. When the connector restarts, it sends a request to the PostgreSQL server to send the events starting just after that position. \n \n [NOTE]\n ====\n-The PostgreSQL connector retrieves the schema information as part of the events sent by the logical decoder plug-in.\n-The only exception is the information about which columns compose the primary key, as this information is obtained from the JDBC metadata (side channel).\n-If the primary key definition of a table changes (by adding, removing or renaming PK columns),\n-then there exists a slight risk of an unfortunate timing when the primary key information from JDBC\n-will not be synchronized with the change data in the logical decoding event and a small amount of messages will be created with an inconsistent key structure.\n-If this happens then a restart of the connector and a reprocessing of the messages will fix the issue.\n-To prevent the issue completely it is recommended to synchronize updates to the primary key structure with {prodname} roughly using following sequence of operations:\n-\n-* Put the database or an application into a read-only mode\n-* Let {prodname} process all remaining events\n-* Stop {prodname}\n-* Update the primary key definition\n-* Put the database or the application into read/write state and start {prodname} again\n+The PostgreSQL connector retrieves schema information as part of the events sent by the logical decoding plug-in. However, the connector does not retrieve information about which columns compose the primary key. The connector obtains this information from the JDBC metadata (side channel). If the primary key definition of a table changes (by adding, removing or renaming primary key columns), there is a tiny period of time when the primary key information from JDBC is not synchronized with the change event that the logical decoding plug-in generates. During this tiny period, a message could be created with an inconsistent key structure. To prevent this inconsistency, update primary key structures as follows: \n+\n+. Put the database or an application into a read-only mode.\n+. Let {prodname} process all remaining events.\n+. Stop {prodname}.\n+. Update the primary key definition in the relevant table.\n+. Put the database or the application into read/write mode.\n+. Restart {prodname}.\n ====\n \n+// Type: concept\n+// ModuleID: postgresql-10-logical-decoding-support-pgoutput\n [[postgresql-pgoutput]]\n-==== PostgreSQL 10+ Logical Decoding Support (pgoutput)\n+=== PostgreSQL 10+ logical decoding support (`pgoutput`)\n \n-As of PostgreSQL 10+, a new logical replication stream mode was introduced, called _pgoutput_.  This logical replication stream mode is natively supported by PostgreSQL,\n-which means that this connector can consume that replication stream\n-without the need for additional plug-ins being installed.\n-This is particularly valuable for environments where installation of plug-ins is not supported or allowed.\n+As of PostgreSQL 10+, a new logical replication stream mode was introduced, called `pgoutput`. This logical replication stream mode is natively supported by PostgreSQL,\n+which means that a {prodname} PostgreSQL connector can consume that replication stream\n+without the need for additional plug-ins.\n+This is particularly valuable for environments where installation of plug-ins is not supported or not allowed.\n \n See {link-prefix}:{link-postgresql-connector}#setting-up-postgresql[Setting up PostgreSQL] for more details.\n \n+// Type: concept\n+// ModuleID: default-names-of-kafka-topics-that-receive-debezium-change-event-records\n+// Title: Default names of Kafka topics that receive {prodname} change event records\n [[postgresql-topic-names]]\n-==== Topics Names\n+=== Topics names\n \n-The PostgreSQL connector writes events for all insert, update, and delete operations on a single table to a single Kafka topic. By default, the Kafka topic name is _serverName_._schemaName_._tableName_ where _serverName_ is the logical name of the connector as specified with the `database.server.name` configuration property, _schemaName_ is the name of the database schema where the operation occurred, and _tableName_ is the name of the database table on which the operation occurred.\n+The PostgreSQL connector writes events for all insert, update, and delete operations on a single table to a single Kafka topic. By default, the Kafka topic name is _serverName_._schemaName_._tableName_ where: \n \n-For example, consider a PostgreSQL installation with a `postgres` database and an `inventory` schema that contains four tables: `products`, `products_on_hand`, `customers`, and `orders`. If the connector monitoring this database were given a logical server name of `fulfillment`, then the connector would produce events on these four Kafka topics:\n+* _serverName_ is the logical name of the connector as specified with the `database.server.name` connector configuration property.\n+* _schemaName_ is the name of the database schema where the operation occurred.\n+* _tableName_ is the name of the database table in which the operation occurred.\n+\n+For example, suppose that `fulfillment` is the logical server name in the configuration for a connector that is capturing changes in a PostgreSQL installation that has a `postgres` database and an `inventory` schema that contains four tables: `products`, `products_on_hand`, `customers`, and `orders`. The connector would stream records to these four Kafka topics:\n \n * `fulfillment.inventory.products`\n * `fulfillment.inventory.products_on_hand`\n * `fulfillment.inventory.customers`\n * `fulfillment.inventory.orders`\n \n-If on the other hand the tables were not part of a specific schema but rather created in the default `public` PostgreSQL schema, then the name of the Kafka topics would be:\n+Now suppose that the tables are not part of a specific schema but were created in the default `public` PostgreSQL schema. The names of the Kafka topics would be:\n \n * `fulfillment.public.products`\n * `fulfillment.public.products_on_hand`\n * `fulfillment.public.customers`\n * `fulfillment.public.orders`\n \n+// Type: concept\n+// ModuleID: metadata-in-debezium-change-event-records\n+// Title: Metadata in {prodname} change event records\n [[postgresql-meta-information]]\n-==== Meta Information\n+=== Meta information\n \n-Each `record` produced by the PostgreSQL connector has, in addition to the {link-prefix}:{link-postgresql-connector}#postgresql-events[_database event_], some meta-information about where the event occurred on the server, the name of the source partition and the name of the Kafka topic and partition where the event should be placed:\n+In addition to a {link-prefix}:{link-postgresql-connector}#postgresql-events[_database change event_], each record produced by a PostgreSQL connector contains some metadata. Metadata includes where the event occurred on the server, the name of the source partition and the name of the Kafka topic and partition where the event should go, for example: \n \n [source,json,indent=0]\n ----\n    \"sourcePartition\": {\n         \"server\": \"fulfillment\"\n     },\n     \"sourceOffset\": {\n-        \"lsn\": \"24023128\",\n+        \"lsn\": \"24023128\",J", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 600}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjQ5NzE2OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDoyMjoxNFrOG463_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDoyMjoxNFrOG463_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjMzODA0NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            {prodname} can generate events that represent transaction boundaries and that enrich change data event messages. For every transaction `BEGIN` and `END`, {prodname} generates tan event that contains: \n          \n          \n            \n            {prodname} can generate events that represent transaction boundaries and that enrich change data event messages. For every transaction `BEGIN` and `END`, {prodname} generates an event that contains:", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462338044", "createdAt": "2020-07-29T14:22:14Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -443,115 +245,221 @@ public interface Snapshotter {\n }\n ----\n \n-All of the builtin snapshot modes are implemented in terms of this interface as well.\n endif::community[]\n \n+// Type: concept\n+// ModuleID: how-debezium-postgresql-connectors-stream-change-event-records\n+// Title: How {prodname} PostgreSQL connectors stream change event records\n [[postgresql-streaming-changes]]\n-==== Streaming Changes\n+=== Streaming changes\n+\n+The PostgreSQL connector typically spends the vast majority of its time streaming changes from the PostgreSQL server to which it is connected. This mechanism relies on link:https://www.postgresql.org/docs/current/static/protocol-replication.html[_PostgreSQL's replication protocol_]. This protocol enables clients to receive changes from the server as they are committed in the server's transaction log at certain positions, which are referred to as Log Sequence Numbers (LSNs).\n \n-The PostgreSQL connector will typically spend the vast majority of its time streaming changes from the PostgreSQL server to which it is connected. This mechanism relies on https://www.postgresql.org/docs/current/static/protocol-replication.html[_PostgreSQL's replication protocol_] where the client can receive changes from the server as they are committed in the server's transaction log at certain positions (also known as `Log Sequence Numbers` or in short LSNs).\n+Whenever the server commits a transaction, a separate server process invokes a callback function from the {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-in]. This function processes the changes from the transaction, converts them to a specific format (Protobuf or JSON in the case of {prodname} plug-in) and writes them on an output stream, which can then be consumed by clients.\n \n-Whenever the server commits a transaction, a separate server process invokes a callback function from the {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-in]. This function processes the changes from the transaction, converts them to a specific format (Protobuf or JSON in the case of {prodname} plug-in) and writes them on an output stream which can then be consumed by clients.\n+The {prodname} PostgreSQL connector acts as a PostgreSQL client. When the connector receives changes it transforms the events into {prodname} _create_, _update_, or _delete_ events that include the LSN of the event. The PostgreSQL connector forwards these change events in records to the Kafka Connect framework, which is running in the same process. The Kafka Connect process asynchronously writes the change event records in the same order in which they were generated to the appropriate Kafka topic. \n \n-The PostgreSQL connector acts as a PostgreSQL client, and when it receives these changes it transforms the events into {prodname} _create_, _update_, or _delete_ events that include the LSN position of the event. The PostgreSQL connector forwards these change events to the Kafka Connect framework (running in the same process), which then asynchronously writes them in the same order to the appropriate Kafka topic. Kafka Connect uses the term _offset_ for the source-specific position information that {prodname} includes with each event, and Kafka Connect periodically records the most recent offset in another Kafka topic.\n+Periodically, Kafka Connect records the most recent _offset_ in another Kafka topic. The offset indicates source-specific position information that {prodname} includes with each event. For the PostgreSQL connector, the LSN recorded in each change event is the offset.\n \n-When Kafka Connect gracefully shuts down, it stops the connectors, flushes all events to Kafka, and records the last offset received from each connector. Upon restart, Kafka Connect reads the last recorded offset for each connector, and starts the connector from that point. The PostgreSQL connector uses the LSN recorded in each change event as the offset, so that upon restart the connector requests the PostgreSQL server send it the events starting just after that position.\n+When Kafka Connect gracefully shuts down, it stops the connectors, flushes all event records to Kafka, and records the last offset received from each connector. When Kafka Connect restarts, it reads the last recorded offset for each connector, and starts each connector at its last recorded offset. When the connector restarts, it sends a request to the PostgreSQL server to send the events starting just after that position. \n \n [NOTE]\n ====\n-The PostgreSQL connector retrieves the schema information as part of the events sent by the logical decoder plug-in.\n-The only exception is the information about which columns compose the primary key, as this information is obtained from the JDBC metadata (side channel).\n-If the primary key definition of a table changes (by adding, removing or renaming PK columns),\n-then there exists a slight risk of an unfortunate timing when the primary key information from JDBC\n-will not be synchronized with the change data in the logical decoding event and a small amount of messages will be created with an inconsistent key structure.\n-If this happens then a restart of the connector and a reprocessing of the messages will fix the issue.\n-To prevent the issue completely it is recommended to synchronize updates to the primary key structure with {prodname} roughly using following sequence of operations:\n-\n-* Put the database or an application into a read-only mode\n-* Let {prodname} process all remaining events\n-* Stop {prodname}\n-* Update the primary key definition\n-* Put the database or the application into read/write state and start {prodname} again\n+The PostgreSQL connector retrieves schema information as part of the events sent by the logical decoding plug-in. However, the connector does not retrieve information about which columns compose the primary key. The connector obtains this information from the JDBC metadata (side channel). If the primary key definition of a table changes (by adding, removing or renaming primary key columns), there is a tiny period of time when the primary key information from JDBC is not synchronized with the change event that the logical decoding plug-in generates. During this tiny period, a message could be created with an inconsistent key structure. To prevent this inconsistency, update primary key structures as follows: \n+\n+. Put the database or an application into a read-only mode.\n+. Let {prodname} process all remaining events.\n+. Stop {prodname}.\n+. Update the primary key definition in the relevant table.\n+. Put the database or the application into read/write mode.\n+. Restart {prodname}.\n ====\n \n+// Type: concept\n+// ModuleID: postgresql-10-logical-decoding-support-pgoutput\n [[postgresql-pgoutput]]\n-==== PostgreSQL 10+ Logical Decoding Support (pgoutput)\n+=== PostgreSQL 10+ logical decoding support (`pgoutput`)\n \n-As of PostgreSQL 10+, a new logical replication stream mode was introduced, called _pgoutput_.  This logical replication stream mode is natively supported by PostgreSQL,\n-which means that this connector can consume that replication stream\n-without the need for additional plug-ins being installed.\n-This is particularly valuable for environments where installation of plug-ins is not supported or allowed.\n+As of PostgreSQL 10+, a new logical replication stream mode was introduced, called `pgoutput`. This logical replication stream mode is natively supported by PostgreSQL,\n+which means that a {prodname} PostgreSQL connector can consume that replication stream\n+without the need for additional plug-ins.\n+This is particularly valuable for environments where installation of plug-ins is not supported or not allowed.\n \n See {link-prefix}:{link-postgresql-connector}#setting-up-postgresql[Setting up PostgreSQL] for more details.\n \n+// Type: concept\n+// ModuleID: default-names-of-kafka-topics-that-receive-debezium-change-event-records\n+// Title: Default names of Kafka topics that receive {prodname} change event records\n [[postgresql-topic-names]]\n-==== Topics Names\n+=== Topics names\n \n-The PostgreSQL connector writes events for all insert, update, and delete operations on a single table to a single Kafka topic. By default, the Kafka topic name is _serverName_._schemaName_._tableName_ where _serverName_ is the logical name of the connector as specified with the `database.server.name` configuration property, _schemaName_ is the name of the database schema where the operation occurred, and _tableName_ is the name of the database table on which the operation occurred.\n+The PostgreSQL connector writes events for all insert, update, and delete operations on a single table to a single Kafka topic. By default, the Kafka topic name is _serverName_._schemaName_._tableName_ where: \n \n-For example, consider a PostgreSQL installation with a `postgres` database and an `inventory` schema that contains four tables: `products`, `products_on_hand`, `customers`, and `orders`. If the connector monitoring this database were given a logical server name of `fulfillment`, then the connector would produce events on these four Kafka topics:\n+* _serverName_ is the logical name of the connector as specified with the `database.server.name` connector configuration property.\n+* _schemaName_ is the name of the database schema where the operation occurred.\n+* _tableName_ is the name of the database table in which the operation occurred.\n+\n+For example, suppose that `fulfillment` is the logical server name in the configuration for a connector that is capturing changes in a PostgreSQL installation that has a `postgres` database and an `inventory` schema that contains four tables: `products`, `products_on_hand`, `customers`, and `orders`. The connector would stream records to these four Kafka topics:\n \n * `fulfillment.inventory.products`\n * `fulfillment.inventory.products_on_hand`\n * `fulfillment.inventory.customers`\n * `fulfillment.inventory.orders`\n \n-If on the other hand the tables were not part of a specific schema but rather created in the default `public` PostgreSQL schema, then the name of the Kafka topics would be:\n+Now suppose that the tables are not part of a specific schema but were created in the default `public` PostgreSQL schema. The names of the Kafka topics would be:\n \n * `fulfillment.public.products`\n * `fulfillment.public.products_on_hand`\n * `fulfillment.public.customers`\n * `fulfillment.public.orders`\n \n+// Type: concept\n+// ModuleID: metadata-in-debezium-change-event-records\n+// Title: Metadata in {prodname} change event records\n [[postgresql-meta-information]]\n-==== Meta Information\n+=== Meta information\n \n-Each `record` produced by the PostgreSQL connector has, in addition to the {link-prefix}:{link-postgresql-connector}#postgresql-events[_database event_], some meta-information about where the event occurred on the server, the name of the source partition and the name of the Kafka topic and partition where the event should be placed:\n+In addition to a {link-prefix}:{link-postgresql-connector}#postgresql-events[_database change event_], each record produced by a PostgreSQL connector contains some metadata. Metadata includes where the event occurred on the server, the name of the source partition and the name of the Kafka topic and partition where the event should go, for example: \n \n [source,json,indent=0]\n ----\n    \"sourcePartition\": {\n         \"server\": \"fulfillment\"\n     },\n     \"sourceOffset\": {\n-        \"lsn\": \"24023128\",\n+        \"lsn\": \"24023128\",J\n         \"txId\": \"555\",\n         \"ts_ms\": \"1482918357011\"\n     },\n     \"kafkaPartition\": null\n ----\n \n-The PostgreSQL connector uses only 1 Kafka Connect _partition_ and it places the generated events into 1 Kafka partition. Therefore, the name of the `sourcePartition` will always default to the name of the `database.server.name` configuration property, while the `kafkaPartition` has the value `null` which means that the connector does not use a specific Kafka partition.\n+* `sourcePartition` always defaults to the setting of the `database.server.name` connector configuration property. \n+\n+* `sourceOffset` contains information about the location of the server where the event occurred:\n+\n+** `lsn` represents the PostgreSQL https://www.postgresql.org/docs/current/static/datatype-pg-lsn.html[Log Sequence Number] or `offset` in the transaction log.\n+** `txId` represents the identifier of the server transaction that caused the event.\n+** `ts_ms` represents the server time at which the transaction was committed in the form of the number of microseconds since the epoch. \n+* `kafkaPartition` with a setting of `null` means that the connector does not use a specific Kafka partition. The PostgreSQL connector uses only one Kafka Connect partition and it places the generated events into one Kafka partition. \n+\n+// Type: concept\n+// ModuleID: debezium-connector-generated-events-that-represent-transaction-boundaries\n+// Title: {prodname} connector-generated events that represent transaction boundaries\n+[[postgresql-transaction-metadata]]\n+=== Transaction metadata\n+\n+{prodname} can generate events that represent transaction boundaries and that enrich change data event messages. For every transaction `BEGIN` and `END`, {prodname} generates tan event that contains: ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 623}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjUyMzEyOnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDoyNzozMlrOG47IFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDoyNzozMlrOG47IFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM0MjE2Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This can lead to unexpected conflicts if the logical server name, a schema name, or a table name contains invalid characters, and the only characters that  distinguish names from one another are invalid and thus replaced with underscores.\n          \n          \n            \n            This can lead to unexpected conflicts if the logical server name, a schema name, or a table name contains invalid characters, and the only characters that distinguish names from one another are invalid and thus replaced with underscores.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462342167", "createdAt": "2020-07-29T14:27:32Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -443,115 +245,221 @@ public interface Snapshotter {\n }\n ----\n \n-All of the builtin snapshot modes are implemented in terms of this interface as well.\n endif::community[]\n \n+// Type: concept\n+// ModuleID: how-debezium-postgresql-connectors-stream-change-event-records\n+// Title: How {prodname} PostgreSQL connectors stream change event records\n [[postgresql-streaming-changes]]\n-==== Streaming Changes\n+=== Streaming changes\n+\n+The PostgreSQL connector typically spends the vast majority of its time streaming changes from the PostgreSQL server to which it is connected. This mechanism relies on link:https://www.postgresql.org/docs/current/static/protocol-replication.html[_PostgreSQL's replication protocol_]. This protocol enables clients to receive changes from the server as they are committed in the server's transaction log at certain positions, which are referred to as Log Sequence Numbers (LSNs).\n \n-The PostgreSQL connector will typically spend the vast majority of its time streaming changes from the PostgreSQL server to which it is connected. This mechanism relies on https://www.postgresql.org/docs/current/static/protocol-replication.html[_PostgreSQL's replication protocol_] where the client can receive changes from the server as they are committed in the server's transaction log at certain positions (also known as `Log Sequence Numbers` or in short LSNs).\n+Whenever the server commits a transaction, a separate server process invokes a callback function from the {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-in]. This function processes the changes from the transaction, converts them to a specific format (Protobuf or JSON in the case of {prodname} plug-in) and writes them on an output stream, which can then be consumed by clients.\n \n-Whenever the server commits a transaction, a separate server process invokes a callback function from the {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-in]. This function processes the changes from the transaction, converts them to a specific format (Protobuf or JSON in the case of {prodname} plug-in) and writes them on an output stream which can then be consumed by clients.\n+The {prodname} PostgreSQL connector acts as a PostgreSQL client. When the connector receives changes it transforms the events into {prodname} _create_, _update_, or _delete_ events that include the LSN of the event. The PostgreSQL connector forwards these change events in records to the Kafka Connect framework, which is running in the same process. The Kafka Connect process asynchronously writes the change event records in the same order in which they were generated to the appropriate Kafka topic. \n \n-The PostgreSQL connector acts as a PostgreSQL client, and when it receives these changes it transforms the events into {prodname} _create_, _update_, or _delete_ events that include the LSN position of the event. The PostgreSQL connector forwards these change events to the Kafka Connect framework (running in the same process), which then asynchronously writes them in the same order to the appropriate Kafka topic. Kafka Connect uses the term _offset_ for the source-specific position information that {prodname} includes with each event, and Kafka Connect periodically records the most recent offset in another Kafka topic.\n+Periodically, Kafka Connect records the most recent _offset_ in another Kafka topic. The offset indicates source-specific position information that {prodname} includes with each event. For the PostgreSQL connector, the LSN recorded in each change event is the offset.\n \n-When Kafka Connect gracefully shuts down, it stops the connectors, flushes all events to Kafka, and records the last offset received from each connector. Upon restart, Kafka Connect reads the last recorded offset for each connector, and starts the connector from that point. The PostgreSQL connector uses the LSN recorded in each change event as the offset, so that upon restart the connector requests the PostgreSQL server send it the events starting just after that position.\n+When Kafka Connect gracefully shuts down, it stops the connectors, flushes all event records to Kafka, and records the last offset received from each connector. When Kafka Connect restarts, it reads the last recorded offset for each connector, and starts each connector at its last recorded offset. When the connector restarts, it sends a request to the PostgreSQL server to send the events starting just after that position. \n \n [NOTE]\n ====\n-The PostgreSQL connector retrieves the schema information as part of the events sent by the logical decoder plug-in.\n-The only exception is the information about which columns compose the primary key, as this information is obtained from the JDBC metadata (side channel).\n-If the primary key definition of a table changes (by adding, removing or renaming PK columns),\n-then there exists a slight risk of an unfortunate timing when the primary key information from JDBC\n-will not be synchronized with the change data in the logical decoding event and a small amount of messages will be created with an inconsistent key structure.\n-If this happens then a restart of the connector and a reprocessing of the messages will fix the issue.\n-To prevent the issue completely it is recommended to synchronize updates to the primary key structure with {prodname} roughly using following sequence of operations:\n-\n-* Put the database or an application into a read-only mode\n-* Let {prodname} process all remaining events\n-* Stop {prodname}\n-* Update the primary key definition\n-* Put the database or the application into read/write state and start {prodname} again\n+The PostgreSQL connector retrieves schema information as part of the events sent by the logical decoding plug-in. However, the connector does not retrieve information about which columns compose the primary key. The connector obtains this information from the JDBC metadata (side channel). If the primary key definition of a table changes (by adding, removing or renaming primary key columns), there is a tiny period of time when the primary key information from JDBC is not synchronized with the change event that the logical decoding plug-in generates. During this tiny period, a message could be created with an inconsistent key structure. To prevent this inconsistency, update primary key structures as follows: \n+\n+. Put the database or an application into a read-only mode.\n+. Let {prodname} process all remaining events.\n+. Stop {prodname}.\n+. Update the primary key definition in the relevant table.\n+. Put the database or the application into read/write mode.\n+. Restart {prodname}.\n ====\n \n+// Type: concept\n+// ModuleID: postgresql-10-logical-decoding-support-pgoutput\n [[postgresql-pgoutput]]\n-==== PostgreSQL 10+ Logical Decoding Support (pgoutput)\n+=== PostgreSQL 10+ logical decoding support (`pgoutput`)\n \n-As of PostgreSQL 10+, a new logical replication stream mode was introduced, called _pgoutput_.  This logical replication stream mode is natively supported by PostgreSQL,\n-which means that this connector can consume that replication stream\n-without the need for additional plug-ins being installed.\n-This is particularly valuable for environments where installation of plug-ins is not supported or allowed.\n+As of PostgreSQL 10+, a new logical replication stream mode was introduced, called `pgoutput`. This logical replication stream mode is natively supported by PostgreSQL,\n+which means that a {prodname} PostgreSQL connector can consume that replication stream\n+without the need for additional plug-ins.\n+This is particularly valuable for environments where installation of plug-ins is not supported or not allowed.\n \n See {link-prefix}:{link-postgresql-connector}#setting-up-postgresql[Setting up PostgreSQL] for more details.\n \n+// Type: concept\n+// ModuleID: default-names-of-kafka-topics-that-receive-debezium-change-event-records\n+// Title: Default names of Kafka topics that receive {prodname} change event records\n [[postgresql-topic-names]]\n-==== Topics Names\n+=== Topics names\n \n-The PostgreSQL connector writes events for all insert, update, and delete operations on a single table to a single Kafka topic. By default, the Kafka topic name is _serverName_._schemaName_._tableName_ where _serverName_ is the logical name of the connector as specified with the `database.server.name` configuration property, _schemaName_ is the name of the database schema where the operation occurred, and _tableName_ is the name of the database table on which the operation occurred.\n+The PostgreSQL connector writes events for all insert, update, and delete operations on a single table to a single Kafka topic. By default, the Kafka topic name is _serverName_._schemaName_._tableName_ where: \n \n-For example, consider a PostgreSQL installation with a `postgres` database and an `inventory` schema that contains four tables: `products`, `products_on_hand`, `customers`, and `orders`. If the connector monitoring this database were given a logical server name of `fulfillment`, then the connector would produce events on these four Kafka topics:\n+* _serverName_ is the logical name of the connector as specified with the `database.server.name` connector configuration property.\n+* _schemaName_ is the name of the database schema where the operation occurred.\n+* _tableName_ is the name of the database table in which the operation occurred.\n+\n+For example, suppose that `fulfillment` is the logical server name in the configuration for a connector that is capturing changes in a PostgreSQL installation that has a `postgres` database and an `inventory` schema that contains four tables: `products`, `products_on_hand`, `customers`, and `orders`. The connector would stream records to these four Kafka topics:\n \n * `fulfillment.inventory.products`\n * `fulfillment.inventory.products_on_hand`\n * `fulfillment.inventory.customers`\n * `fulfillment.inventory.orders`\n \n-If on the other hand the tables were not part of a specific schema but rather created in the default `public` PostgreSQL schema, then the name of the Kafka topics would be:\n+Now suppose that the tables are not part of a specific schema but were created in the default `public` PostgreSQL schema. The names of the Kafka topics would be:\n \n * `fulfillment.public.products`\n * `fulfillment.public.products_on_hand`\n * `fulfillment.public.customers`\n * `fulfillment.public.orders`\n \n+// Type: concept\n+// ModuleID: metadata-in-debezium-change-event-records\n+// Title: Metadata in {prodname} change event records\n [[postgresql-meta-information]]\n-==== Meta Information\n+=== Meta information\n \n-Each `record` produced by the PostgreSQL connector has, in addition to the {link-prefix}:{link-postgresql-connector}#postgresql-events[_database event_], some meta-information about where the event occurred on the server, the name of the source partition and the name of the Kafka topic and partition where the event should be placed:\n+In addition to a {link-prefix}:{link-postgresql-connector}#postgresql-events[_database change event_], each record produced by a PostgreSQL connector contains some metadata. Metadata includes where the event occurred on the server, the name of the source partition and the name of the Kafka topic and partition where the event should go, for example: \n \n [source,json,indent=0]\n ----\n    \"sourcePartition\": {\n         \"server\": \"fulfillment\"\n     },\n     \"sourceOffset\": {\n-        \"lsn\": \"24023128\",\n+        \"lsn\": \"24023128\",J\n         \"txId\": \"555\",\n         \"ts_ms\": \"1482918357011\"\n     },\n     \"kafkaPartition\": null\n ----\n \n-The PostgreSQL connector uses only 1 Kafka Connect _partition_ and it places the generated events into 1 Kafka partition. Therefore, the name of the `sourcePartition` will always default to the name of the `database.server.name` configuration property, while the `kafkaPartition` has the value `null` which means that the connector does not use a specific Kafka partition.\n+* `sourcePartition` always defaults to the setting of the `database.server.name` connector configuration property. \n+\n+* `sourceOffset` contains information about the location of the server where the event occurred:\n+\n+** `lsn` represents the PostgreSQL https://www.postgresql.org/docs/current/static/datatype-pg-lsn.html[Log Sequence Number] or `offset` in the transaction log.\n+** `txId` represents the identifier of the server transaction that caused the event.\n+** `ts_ms` represents the server time at which the transaction was committed in the form of the number of microseconds since the epoch. \n+* `kafkaPartition` with a setting of `null` means that the connector does not use a specific Kafka partition. The PostgreSQL connector uses only one Kafka Connect partition and it places the generated events into one Kafka partition. \n+\n+// Type: concept\n+// ModuleID: debezium-connector-generated-events-that-represent-transaction-boundaries\n+// Title: {prodname} connector-generated events that represent transaction boundaries\n+[[postgresql-transaction-metadata]]\n+=== Transaction metadata\n+\n+{prodname} can generate events that represent transaction boundaries and that enrich change data event messages. For every transaction `BEGIN` and `END`, {prodname} generates tan event that contains: \n+\n+* `status` - `BEGIN` or `END`\n+* `id` - string representation of unique transaction identifier\n+* `event_count` (for `END` events) - total number of events emitted by the transaction\n+* `data_collections` (for `END` events) - an array of pairs of `data_collection` and `event_count` that provides the number of events emitted by changes originating from given data collection\n+\n+.Examples\n+\n+[source,json,indent=0,subs=\"attributes\"]\n+----\n+{\n+  \"status\": \"BEGIN\",\n+  \"id\": \"571\",\n+  \"event_count\": null,\n+  \"data_collections\": null\n+}\n+\n+{\n+  \"status\": \"END\",\n+  \"id\": \"571\",\n+  \"event_count\": 2,\n+  \"data_collections\": [\n+    {\n+      \"data_collection\": \"s1.a\",\n+      \"event_count\": 1\n+    },\n+    {\n+      \"data_collection\": \"s2.a\",\n+      \"event_count\": 1\n+    }\n+  ]\n+}\n+----\n+\n+Transaction events are written to the topic named `_database.server.name_.transaction`.\n+\n+.Change data event enrichment\n+\n+When transaction metadata is enabled the data message `Envelope` is enriched with a new `transaction` field.\n+This field provides information about every event in the form of a composite of fields:\n+\n+* `id` - string representation of unique transaction identifier\n+* `total_order` - absolute position of the event among all events generated by the transaction\n+* `data_collection_order` - the per-data collection position of the event among all events that were emitted by the transaction\n \n-The `sourceOffset` portion of the message contains information about the location of the server where the event occurred:\n+Following is an example of a message:\n \n-* `lsn` represents the PostgreSQL https://www.postgresql.org/docs/current/static/datatype-pg-lsn.html[_log sequence number_] or `offset` in the transaction log\n-* `txId` represents the identifier of the server transaction which caused the event\n-* `ts_ms` represents the number of microseconds since Unix Epoch as the server time at which the transaction was committed\n+[source,json,indent=0,subs=\"attributes\"]\n+----\n+{\n+  \"before\": null,\n+  \"after\": {\n+    \"pk\": \"2\",\n+    \"aa\": \"1\"\n+  },\n+  \"source\": {\n+...\n+  },\n+  \"op\": \"c\",\n+  \"ts_ms\": \"1580390884335\",\n+  \"transaction\": {\n+    \"id\": \"571\",\n+    \"total_order\": \"1\",\n+    \"data_collection_order\": \"1\"\n+  }\n+}\n+----\n \n+// Type: assembly\n+// ModuleID: descriptions-of-debezium-postgresql-connector-data-change-events\n+// Title: Descriptions of {prodname} PostgreSQL connector data change events\n [[postgresql-events]]\n-==== Events\n+== Data change events\n \n-All data change events produced by the PostgreSQL connector have a key and a value, although the structure of the key and value depend on the table from which the change events originated (see {link-prefix}:{link-postgresql-connector}#postgresql-topic-names[Topic names]).\n+Each data change event produced by the PostgreSQL connector has a key and a value. The structure of the key and value depends on the table from which the change event originates. The default behavior is that the connector streams change event records to {link-prefix}:{link-postgresql-connector}#postgresql-topic-names[topic names] that are the same as the event's originating table.\n \n [NOTE]\n ====\n-Starting with Kafka 0.10, Kafka can optionally record with the message key and value the {link-kafka-docs}.html#upgrade_10_performance_impact[_timestamp_] at which the message was created (recorded by the producer) or written to the log by Kafka.\n+Starting with Kafka 0.10, Kafka can optionally record the event key and value with the {link-kafka-docs}.html#upgrade_10_performance_impact[_timestamp_] at which the message was created (recorded by the producer) or written to the log by Kafka.\n ====\n \n [WARNING]\n ====\n-The PostgreSQL connector ensures that all Kafka Connect _schema names_ are http://avro.apache.org/docs/current/spec.html#names[valid Avro schema names]. This means that the logical server name must start with Latin letters or an underscore (e.g., [a-z,A-Z,\\_]), and the remaining characters in the logical server name and all characters in the schema and table names must be Latin letters, digits, or an underscore (e.g., [a-z,A-Z,0-9,\\_]). If not, then all invalid characters will automatically be replaced with an underscore character.\n+The PostgreSQL connector ensures that all Kafka Connect _schema names_ are http://avro.apache.org/docs/current/spec.html#names[valid Avro schema names]. This means that the logical server name must start with a Latin letter or an underscore, that is, a-z, A-Z, or \\_. Each remaining character in the logical server name and each character in the schema and table names must be a Latin letter, a digit, or an underscore, that is, a-z, A-Z, 0-9, or \\_. If there is an invalid character it is replaced with an underscore character.\n \n-This can lead to unexpected conflicts when the logical server name, schema names, and table names contain other characters, and the only distinguishing characters between table full names are invalid and thus replaced with underscores.\n+This can lead to unexpected conflicts if the logical server name, a schema name, or a table name contains invalid characters, and the only characters that  distinguish names from one another are invalid and thus replaced with underscores.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 718}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjUzMjMzOnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDoyOToyMlrOG47OBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDoyOToyMlrOG47OBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM0MzY4NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            For a given table, the change event's key has a structure that contains a field for each column in the primary key of the table at the time the event was created. Alternatively, if the table has `REPLICA IDENTITY` set to `FULL` or `USING INDEX` there is a field for each unique key constraint. \n          \n          \n            \n            The change event key for a given table has a structure that contains a field for each column in the primary key of the table at the time the event was created. Alternatively, if the table has `REPLICA IDENTITY` set to `FULL` or `USING INDEX` there is a field for each unique key constraint.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462343684", "createdAt": "2020-07-29T14:29:22Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -443,115 +245,221 @@ public interface Snapshotter {\n }\n ----\n \n-All of the builtin snapshot modes are implemented in terms of this interface as well.\n endif::community[]\n \n+// Type: concept\n+// ModuleID: how-debezium-postgresql-connectors-stream-change-event-records\n+// Title: How {prodname} PostgreSQL connectors stream change event records\n [[postgresql-streaming-changes]]\n-==== Streaming Changes\n+=== Streaming changes\n+\n+The PostgreSQL connector typically spends the vast majority of its time streaming changes from the PostgreSQL server to which it is connected. This mechanism relies on link:https://www.postgresql.org/docs/current/static/protocol-replication.html[_PostgreSQL's replication protocol_]. This protocol enables clients to receive changes from the server as they are committed in the server's transaction log at certain positions, which are referred to as Log Sequence Numbers (LSNs).\n \n-The PostgreSQL connector will typically spend the vast majority of its time streaming changes from the PostgreSQL server to which it is connected. This mechanism relies on https://www.postgresql.org/docs/current/static/protocol-replication.html[_PostgreSQL's replication protocol_] where the client can receive changes from the server as they are committed in the server's transaction log at certain positions (also known as `Log Sequence Numbers` or in short LSNs).\n+Whenever the server commits a transaction, a separate server process invokes a callback function from the {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-in]. This function processes the changes from the transaction, converts them to a specific format (Protobuf or JSON in the case of {prodname} plug-in) and writes them on an output stream, which can then be consumed by clients.\n \n-Whenever the server commits a transaction, a separate server process invokes a callback function from the {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-in]. This function processes the changes from the transaction, converts them to a specific format (Protobuf or JSON in the case of {prodname} plug-in) and writes them on an output stream which can then be consumed by clients.\n+The {prodname} PostgreSQL connector acts as a PostgreSQL client. When the connector receives changes it transforms the events into {prodname} _create_, _update_, or _delete_ events that include the LSN of the event. The PostgreSQL connector forwards these change events in records to the Kafka Connect framework, which is running in the same process. The Kafka Connect process asynchronously writes the change event records in the same order in which they were generated to the appropriate Kafka topic. \n \n-The PostgreSQL connector acts as a PostgreSQL client, and when it receives these changes it transforms the events into {prodname} _create_, _update_, or _delete_ events that include the LSN position of the event. The PostgreSQL connector forwards these change events to the Kafka Connect framework (running in the same process), which then asynchronously writes them in the same order to the appropriate Kafka topic. Kafka Connect uses the term _offset_ for the source-specific position information that {prodname} includes with each event, and Kafka Connect periodically records the most recent offset in another Kafka topic.\n+Periodically, Kafka Connect records the most recent _offset_ in another Kafka topic. The offset indicates source-specific position information that {prodname} includes with each event. For the PostgreSQL connector, the LSN recorded in each change event is the offset.\n \n-When Kafka Connect gracefully shuts down, it stops the connectors, flushes all events to Kafka, and records the last offset received from each connector. Upon restart, Kafka Connect reads the last recorded offset for each connector, and starts the connector from that point. The PostgreSQL connector uses the LSN recorded in each change event as the offset, so that upon restart the connector requests the PostgreSQL server send it the events starting just after that position.\n+When Kafka Connect gracefully shuts down, it stops the connectors, flushes all event records to Kafka, and records the last offset received from each connector. When Kafka Connect restarts, it reads the last recorded offset for each connector, and starts each connector at its last recorded offset. When the connector restarts, it sends a request to the PostgreSQL server to send the events starting just after that position. \n \n [NOTE]\n ====\n-The PostgreSQL connector retrieves the schema information as part of the events sent by the logical decoder plug-in.\n-The only exception is the information about which columns compose the primary key, as this information is obtained from the JDBC metadata (side channel).\n-If the primary key definition of a table changes (by adding, removing or renaming PK columns),\n-then there exists a slight risk of an unfortunate timing when the primary key information from JDBC\n-will not be synchronized with the change data in the logical decoding event and a small amount of messages will be created with an inconsistent key structure.\n-If this happens then a restart of the connector and a reprocessing of the messages will fix the issue.\n-To prevent the issue completely it is recommended to synchronize updates to the primary key structure with {prodname} roughly using following sequence of operations:\n-\n-* Put the database or an application into a read-only mode\n-* Let {prodname} process all remaining events\n-* Stop {prodname}\n-* Update the primary key definition\n-* Put the database or the application into read/write state and start {prodname} again\n+The PostgreSQL connector retrieves schema information as part of the events sent by the logical decoding plug-in. However, the connector does not retrieve information about which columns compose the primary key. The connector obtains this information from the JDBC metadata (side channel). If the primary key definition of a table changes (by adding, removing or renaming primary key columns), there is a tiny period of time when the primary key information from JDBC is not synchronized with the change event that the logical decoding plug-in generates. During this tiny period, a message could be created with an inconsistent key structure. To prevent this inconsistency, update primary key structures as follows: \n+\n+. Put the database or an application into a read-only mode.\n+. Let {prodname} process all remaining events.\n+. Stop {prodname}.\n+. Update the primary key definition in the relevant table.\n+. Put the database or the application into read/write mode.\n+. Restart {prodname}.\n ====\n \n+// Type: concept\n+// ModuleID: postgresql-10-logical-decoding-support-pgoutput\n [[postgresql-pgoutput]]\n-==== PostgreSQL 10+ Logical Decoding Support (pgoutput)\n+=== PostgreSQL 10+ logical decoding support (`pgoutput`)\n \n-As of PostgreSQL 10+, a new logical replication stream mode was introduced, called _pgoutput_.  This logical replication stream mode is natively supported by PostgreSQL,\n-which means that this connector can consume that replication stream\n-without the need for additional plug-ins being installed.\n-This is particularly valuable for environments where installation of plug-ins is not supported or allowed.\n+As of PostgreSQL 10+, a new logical replication stream mode was introduced, called `pgoutput`. This logical replication stream mode is natively supported by PostgreSQL,\n+which means that a {prodname} PostgreSQL connector can consume that replication stream\n+without the need for additional plug-ins.\n+This is particularly valuable for environments where installation of plug-ins is not supported or not allowed.\n \n See {link-prefix}:{link-postgresql-connector}#setting-up-postgresql[Setting up PostgreSQL] for more details.\n \n+// Type: concept\n+// ModuleID: default-names-of-kafka-topics-that-receive-debezium-change-event-records\n+// Title: Default names of Kafka topics that receive {prodname} change event records\n [[postgresql-topic-names]]\n-==== Topics Names\n+=== Topics names\n \n-The PostgreSQL connector writes events for all insert, update, and delete operations on a single table to a single Kafka topic. By default, the Kafka topic name is _serverName_._schemaName_._tableName_ where _serverName_ is the logical name of the connector as specified with the `database.server.name` configuration property, _schemaName_ is the name of the database schema where the operation occurred, and _tableName_ is the name of the database table on which the operation occurred.\n+The PostgreSQL connector writes events for all insert, update, and delete operations on a single table to a single Kafka topic. By default, the Kafka topic name is _serverName_._schemaName_._tableName_ where: \n \n-For example, consider a PostgreSQL installation with a `postgres` database and an `inventory` schema that contains four tables: `products`, `products_on_hand`, `customers`, and `orders`. If the connector monitoring this database were given a logical server name of `fulfillment`, then the connector would produce events on these four Kafka topics:\n+* _serverName_ is the logical name of the connector as specified with the `database.server.name` connector configuration property.\n+* _schemaName_ is the name of the database schema where the operation occurred.\n+* _tableName_ is the name of the database table in which the operation occurred.\n+\n+For example, suppose that `fulfillment` is the logical server name in the configuration for a connector that is capturing changes in a PostgreSQL installation that has a `postgres` database and an `inventory` schema that contains four tables: `products`, `products_on_hand`, `customers`, and `orders`. The connector would stream records to these four Kafka topics:\n \n * `fulfillment.inventory.products`\n * `fulfillment.inventory.products_on_hand`\n * `fulfillment.inventory.customers`\n * `fulfillment.inventory.orders`\n \n-If on the other hand the tables were not part of a specific schema but rather created in the default `public` PostgreSQL schema, then the name of the Kafka topics would be:\n+Now suppose that the tables are not part of a specific schema but were created in the default `public` PostgreSQL schema. The names of the Kafka topics would be:\n \n * `fulfillment.public.products`\n * `fulfillment.public.products_on_hand`\n * `fulfillment.public.customers`\n * `fulfillment.public.orders`\n \n+// Type: concept\n+// ModuleID: metadata-in-debezium-change-event-records\n+// Title: Metadata in {prodname} change event records\n [[postgresql-meta-information]]\n-==== Meta Information\n+=== Meta information\n \n-Each `record` produced by the PostgreSQL connector has, in addition to the {link-prefix}:{link-postgresql-connector}#postgresql-events[_database event_], some meta-information about where the event occurred on the server, the name of the source partition and the name of the Kafka topic and partition where the event should be placed:\n+In addition to a {link-prefix}:{link-postgresql-connector}#postgresql-events[_database change event_], each record produced by a PostgreSQL connector contains some metadata. Metadata includes where the event occurred on the server, the name of the source partition and the name of the Kafka topic and partition where the event should go, for example: \n \n [source,json,indent=0]\n ----\n    \"sourcePartition\": {\n         \"server\": \"fulfillment\"\n     },\n     \"sourceOffset\": {\n-        \"lsn\": \"24023128\",\n+        \"lsn\": \"24023128\",J\n         \"txId\": \"555\",\n         \"ts_ms\": \"1482918357011\"\n     },\n     \"kafkaPartition\": null\n ----\n \n-The PostgreSQL connector uses only 1 Kafka Connect _partition_ and it places the generated events into 1 Kafka partition. Therefore, the name of the `sourcePartition` will always default to the name of the `database.server.name` configuration property, while the `kafkaPartition` has the value `null` which means that the connector does not use a specific Kafka partition.\n+* `sourcePartition` always defaults to the setting of the `database.server.name` connector configuration property. \n+\n+* `sourceOffset` contains information about the location of the server where the event occurred:\n+\n+** `lsn` represents the PostgreSQL https://www.postgresql.org/docs/current/static/datatype-pg-lsn.html[Log Sequence Number] or `offset` in the transaction log.\n+** `txId` represents the identifier of the server transaction that caused the event.\n+** `ts_ms` represents the server time at which the transaction was committed in the form of the number of microseconds since the epoch. \n+* `kafkaPartition` with a setting of `null` means that the connector does not use a specific Kafka partition. The PostgreSQL connector uses only one Kafka Connect partition and it places the generated events into one Kafka partition. \n+\n+// Type: concept\n+// ModuleID: debezium-connector-generated-events-that-represent-transaction-boundaries\n+// Title: {prodname} connector-generated events that represent transaction boundaries\n+[[postgresql-transaction-metadata]]\n+=== Transaction metadata\n+\n+{prodname} can generate events that represent transaction boundaries and that enrich change data event messages. For every transaction `BEGIN` and `END`, {prodname} generates tan event that contains: \n+\n+* `status` - `BEGIN` or `END`\n+* `id` - string representation of unique transaction identifier\n+* `event_count` (for `END` events) - total number of events emitted by the transaction\n+* `data_collections` (for `END` events) - an array of pairs of `data_collection` and `event_count` that provides the number of events emitted by changes originating from given data collection\n+\n+.Examples\n+\n+[source,json,indent=0,subs=\"attributes\"]\n+----\n+{\n+  \"status\": \"BEGIN\",\n+  \"id\": \"571\",\n+  \"event_count\": null,\n+  \"data_collections\": null\n+}\n+\n+{\n+  \"status\": \"END\",\n+  \"id\": \"571\",\n+  \"event_count\": 2,\n+  \"data_collections\": [\n+    {\n+      \"data_collection\": \"s1.a\",\n+      \"event_count\": 1\n+    },\n+    {\n+      \"data_collection\": \"s2.a\",\n+      \"event_count\": 1\n+    }\n+  ]\n+}\n+----\n+\n+Transaction events are written to the topic named `_database.server.name_.transaction`.\n+\n+.Change data event enrichment\n+\n+When transaction metadata is enabled the data message `Envelope` is enriched with a new `transaction` field.\n+This field provides information about every event in the form of a composite of fields:\n+\n+* `id` - string representation of unique transaction identifier\n+* `total_order` - absolute position of the event among all events generated by the transaction\n+* `data_collection_order` - the per-data collection position of the event among all events that were emitted by the transaction\n \n-The `sourceOffset` portion of the message contains information about the location of the server where the event occurred:\n+Following is an example of a message:\n \n-* `lsn` represents the PostgreSQL https://www.postgresql.org/docs/current/static/datatype-pg-lsn.html[_log sequence number_] or `offset` in the transaction log\n-* `txId` represents the identifier of the server transaction which caused the event\n-* `ts_ms` represents the number of microseconds since Unix Epoch as the server time at which the transaction was committed\n+[source,json,indent=0,subs=\"attributes\"]\n+----\n+{\n+  \"before\": null,\n+  \"after\": {\n+    \"pk\": \"2\",\n+    \"aa\": \"1\"\n+  },\n+  \"source\": {\n+...\n+  },\n+  \"op\": \"c\",\n+  \"ts_ms\": \"1580390884335\",\n+  \"transaction\": {\n+    \"id\": \"571\",\n+    \"total_order\": \"1\",\n+    \"data_collection_order\": \"1\"\n+  }\n+}\n+----\n \n+// Type: assembly\n+// ModuleID: descriptions-of-debezium-postgresql-connector-data-change-events\n+// Title: Descriptions of {prodname} PostgreSQL connector data change events\n [[postgresql-events]]\n-==== Events\n+== Data change events\n \n-All data change events produced by the PostgreSQL connector have a key and a value, although the structure of the key and value depend on the table from which the change events originated (see {link-prefix}:{link-postgresql-connector}#postgresql-topic-names[Topic names]).\n+Each data change event produced by the PostgreSQL connector has a key and a value. The structure of the key and value depends on the table from which the change event originates. The default behavior is that the connector streams change event records to {link-prefix}:{link-postgresql-connector}#postgresql-topic-names[topic names] that are the same as the event's originating table.\n \n [NOTE]\n ====\n-Starting with Kafka 0.10, Kafka can optionally record with the message key and value the {link-kafka-docs}.html#upgrade_10_performance_impact[_timestamp_] at which the message was created (recorded by the producer) or written to the log by Kafka.\n+Starting with Kafka 0.10, Kafka can optionally record the event key and value with the {link-kafka-docs}.html#upgrade_10_performance_impact[_timestamp_] at which the message was created (recorded by the producer) or written to the log by Kafka.\n ====\n \n [WARNING]\n ====\n-The PostgreSQL connector ensures that all Kafka Connect _schema names_ are http://avro.apache.org/docs/current/spec.html#names[valid Avro schema names]. This means that the logical server name must start with Latin letters or an underscore (e.g., [a-z,A-Z,\\_]), and the remaining characters in the logical server name and all characters in the schema and table names must be Latin letters, digits, or an underscore (e.g., [a-z,A-Z,0-9,\\_]). If not, then all invalid characters will automatically be replaced with an underscore character.\n+The PostgreSQL connector ensures that all Kafka Connect _schema names_ are http://avro.apache.org/docs/current/spec.html#names[valid Avro schema names]. This means that the logical server name must start with a Latin letter or an underscore, that is, a-z, A-Z, or \\_. Each remaining character in the logical server name and each character in the schema and table names must be a Latin letter, a digit, or an underscore, that is, a-z, A-Z, 0-9, or \\_. If there is an invalid character it is replaced with an underscore character.\n \n-This can lead to unexpected conflicts when the logical server name, schema names, and table names contain other characters, and the only distinguishing characters between table full names are invalid and thus replaced with underscores.\n+This can lead to unexpected conflicts if the logical server name, a schema name, or a table name contains invalid characters, and the only characters that  distinguish names from one another are invalid and thus replaced with underscores.\n ====\n \n-{prodname} and Kafka Connect are designed around _continuous streams of event messages_, and the structure of these events may change over time. This could be difficult for consumers to deal with, so to make it easy Kafka Connect makes each event self-contained. Every message key and value has two parts: a _schema_ and _payload_. The schema describes the structure of the payload, while the payload contains the actual data.\n+{prodname} and Kafka Connect are designed around _continuous streams of event messages_. However, the structure of these events may change over time, which can be difficult for consumers to handle. To make it easier, Kafka Connect makes each event self-contained. Each message key and each message value has two parts: a _schema_ and a _payload_. The schema describes the structure of the payload, while the payload contains the actual data.\n+\n+ifdef::product[]\n+Details are in the following topics:\n+\n+* xref:about-keys-in-debezium-change-events[]\n+* xref:about-values-in-debezium-change-events[]\n+* xref:how-replica-identity-controls-data-that can-be-in-debezium-change-events[]\n+* xref:about-debezium-change-events-for-operations-that-create-content[]\n+* xref:about-debezium-change-events-for-operations-that-update-content[]\n+* xref:about-debezium-change-events-for-primary-key-updates[]\n+* xref:about-debezium-change-events-for-operations-that-delete-content[]\n+\n+endif::product[]\n \n+// Type: concept\n+// ModuleID: about-keys-in-debezium-change-events\n+// Title: About keys in {prodname} change events\n [[postgresql-change-events-key]]\n-===== Change Event's Key\n+=== Change event keys\n \n-For a given table, the change event's key will have a structure that contains a field for each column in the primary key (or unique key constraint with `REPLICA IDENTITY` set to `FULL` or `USING INDEX` on the table) of the table at the time the event was created.\n+For a given table, the change event's key has a structure that contains a field for each column in the primary key of the table at the time the event was created. Alternatively, if the table has `REPLICA IDENTITY` set to `FULL` or `USING INDEX` there is a field for each unique key constraint. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 745}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjU0NDE4OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDozMTo0M1rOG47VKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDozMTo0M1rOG47VKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM0NTUxNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This key describes output from the connector named `PostgreSQL_server`,  for the row in the `public.customers` table, whose primary key, the `id` column, had a value of `1`.\n          \n          \n            \n            This key describes output from the connector named `PostgreSQL_server`, for the row in the `public.customers` table, whose primary key, the `id` column, had a value of `1`.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462345514", "createdAt": "2020-07-29T14:31:43Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -592,55 +500,77 @@ If the `database.server.name` configuration property has the value `PostgreSQL_s\n   }\n ----\n \n-The `schema` portion of the key contains a Kafka Connect schema describing what is in the key portion. In this case, it means that the `payload` value is not optional, is a structure defined by a schema named `PostgreSQL_server.public.customers.Key`, and has one required field named `id` of type `int32`. If you look at the value of the key's `payload` field, you see that it is indeed a structure (which in JSON is just an object) with a single `id` field, whose value is `1`.\n+The `schema` portion of the key contains a Kafka Connect schema that describes what is in the key's `payload` portion. In this case, it means that the `payload` value is not optional, is a structure defined by a schema named `PostgreSQL_server.public.customers.Key`, and has one required field named `id` of type `INT32`. If you look at the value of the key's `payload` field, you see that it is indeed a structure, which in JSON is just an object, with a single `id` field, whose value is `1`.\n \n-Therefore, we interpret this key as describing the row in the `public.customers` table (output from the connector named `PostgreSQL_server`) whose `id` primary key column had a value of `1`.\n+This key describes output from the connector named `PostgreSQL_server`,  for the row in the `public.customers` table, whose primary key, the `id` column, had a value of `1`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 766}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjU4NDY2OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDozOTo0NVrOG47uLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDozOTo0NVrOG47uLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM1MTkxOA==", "bodyText": "In the NOTE that proceeds this section, we mention that a tombstone event is emitted after the DELETE event and prior to the CREATE event but we seem to omit that in this section.  I think that likely should be included here too for clarity for users who might navigate to this section and not read the prior section's note.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462351918", "createdAt": "2020-07-29T14:39:45Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -864,43 +801,45 @@ The value of an _update_ change event on this table will actually have the exact\n }\n ----\n \n-When we compare this to the value in the _insert_ event, we see a couple of differences in the `payload` section:\n+As compared with the payload section in the _create_ event, you can see the following differences in the payload section of the _update_ event:  \n \n-* The `op` field value is now `u`, signifying that this row changed because of an update\n-* The `before` field now has the state of the row with the values before the database commit, but only for the primary key column `id`. This is because the  {link-prefix}:{link-postgresql-connector}#postgresql-replica-identity[_REPLICA IDENTITY_] which is by default `DEFAULT`.\n+* The `op` field value is `u` to indicate that the row changed because of an `UPDATE`.\n+* The `before` field contains the values that were in the row before the database commit. In this example, only the primary key column, `id`, is present because the table's {link-prefix}:{link-postgresql-connector}#postgresql-replica-identity[`REPLICA IDENTITY`] setting is, by default, `DEFAULT`.\n++\n+For an _update_ event to contain the previous values of all columns in the row, you would have to change the `customers` table by running `ALTER TABLE customers REPLICA IDENTITY FULL`.\n \n-[NOTE]\n-====\n-Should we want to see the previous values of all the columns for the row, we would have to change the `customers` table first by running `ALTER TABLE customers REPLICA IDENTITY FULL`\n-====\n+* The `after` field has the updated state of the row. As you can see, the `first_name` value is now `Anne Marie`.\n+\n+* The `source` field structure has the same fields as the _create_ event, but the values are different since this event is from a different position in the WAL.\n \n-* The `after` field now has the updated state of the row, and here was can see that the `first_name` value is now `Anne Marie`.\n-* The `source` field structure has the same fields as before, but the values are different since this event is from a different position in the WAL.\n * The `ts_ms` shows the timestamp that {prodname} processed this event.\n \n-There are several things we can learn by just looking at this `payload` section. We can compare the `before` and `after` structures to determine what actually changed in this row because of the commit. The `source` structure tells us information about PostgreSQL's record of this change (providing traceability), but more importantly this has information we can compare to other events in this and other topics to know whether this event occurred before, after, or as part of the same PostgreSQL commit as other events.\n+There are several things to learn by looking at this `payload` section. You can compare the `before` and `after` structures to determine what changed in this row because of the commit. The `source` structure provides information about PostgreSQL's record of this change, which enables tracing. Perhaps more importantly, this payload has information that you can compare to other events. For example, a comparison typically indicates whether this event occurred before, after, or as part of the same PostgreSQL commit as another event.\n \n [NOTE]\n ====\n-When the columns for a row's primary/unique key are updated, the value of the row's key has changed so {prodname} will output _three_ events: a `DELETE` event and {link-prefix}:{link-postgresql-connector}#postgresql-tombstone-events[tombstone event] with the old key for the row, followed by an `INSERT` event with the new key for the row.\n+Updating the columns for a row's primary/unique key changes the value of the row's key. When a key changes, {prodname} outputs _three_ events: a `DELETE` event and a {link-prefix}:{link-postgresql-connector}#postgresql-tombstone-events[tombstone event] with the old key for the row, followed by an event with the new key for the row. Details are in the next section. \n ====\n \n-===== Primary Key Update Header\n+// Type: concept\n+// ModuleID: about-debezium-change-events-for-primary-key-updates\n+// Title: About {prodname} change events for primary key updates\n+=== Primary key updates", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 946}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjU5MjIwOnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDo0MTowOVrOG47ysg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDo0MTowOVrOG47ysg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM1MzA3NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            PostgreSQL connector events are designed to work with link:https://kafka.apache.org/documentation/#compaction[Kafka log compaction]. Log compaction enables removal of some older messages as long as at least the most recent message for every key is kept. This lets Kafka reclaim storage space while ensuring that the topic contains a complete data set an can be used for reloading key-based state.\n          \n          \n            \n            PostgreSQL connector events are designed to work with link:https://kafka.apache.org/documentation/#compaction[Kafka log compaction]. Log compaction enables removal of some older messages as long as at least the most recent message for every key is kept. This lets Kafka reclaim storage space while ensuring that the topic contains a complete data set and can be used for reloading key-based state.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462353074", "createdAt": "2020-07-29T14:41:09Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -930,114 +869,61 @@ So far we've seen samples of _create_ and _update_ events. Now, let's look at th\n }\n ----\n \n-If we look at the `payload` portion, we see a number of differences compared with the _create_ or _update_ event payloads:\n+Compared with the _create_ or _update_ event payloads, the differences in a _delete_ event are: \n \n-* The `op` field value is now `d`, signifying that this row was deleted\n-* The `before` field now has the state of the row that was deleted with the database commit. Again this only contains the primary key column due to the {link-prefix}:{link-postgresql-connector}#postgresql-replica-identity[_REPLICA IDENTITY_] setting\n-* The `after` field is null, signifying that the row no longer exists\n-* The `source` field structure has many of the same values as before, except the `ts_ms`, `lsn` and `txId` fields have changed\n+* The `before` field has the state of the row that was deleted by the database commit. In this example, the `before` field contains only the primary key column because the table's {link-prefix}:{link-postgresql-connector}#postgresql-replica-identity[`REPLICA IDENTITY`] setting is `DEFAULT`.\n+* The `after` field is null to indicate that the row no longer exists.\n+* The `source` field structure has many of the same values as in the _create_ and _update_ events, except the `ts_ms`, `lsn` and `txId` fields have changed.\n+* The `op` field value is `d` to indicate that this row was deleted.\n * The `ts_ms` shows the timestamp that {prodname} processed this event.\n \n This event gives a consumer all kinds of information that it can use to process the removal of this row.\n \n [WARNING]\n ====\n-Please pay attention to the tables without PK, any delete messages from such table with REPLICA IDENTITY DEFAULT will have no `before` part (because they have no PK which is the only field for the default identity level) and therefore will be skipped as totally empty.\n-To be able to process messages from tables without PK set REPLICA IDENTITY to FULL level.\n+For a consumer to be able to process a _delete_ event generated for a table that does not have a primary key, set the table's `REPLICA IDENTITY` to `FULL`. When a table does not have a primary key and the table's `REPLICA IDENTITY` is set to `DEFAULT` or `NOTHING`, a _delete_ event has no `before` field.\n ====\n \n-The PostgreSQL connector's events are designed to work with link:https://kafka.apache.org/documentation/#compaction[Kafka log compaction], which allows for the removal of some older messages as long as at least the most recent message for every key is kept. This allows Kafka to reclaim storage space while ensuring the topic contains a complete dataset and can be used for reloading key-based state.\n+PostgreSQL connector events are designed to work with link:https://kafka.apache.org/documentation/#compaction[Kafka log compaction]. Log compaction enables removal of some older messages as long as at least the most recent message for every key is kept. This lets Kafka reclaim storage space while ensuring that the topic contains a complete data set an can be used for reloading key-based state.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 1002}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjU5OTA4OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDo0MjozOVrOG473Gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDo0MjozOVrOG473Gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM1NDIwMg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            |The string representation of the interval value that follows the pattern `P<years>Y<months>M<days>DT<hours>H<minutes>M<seconds>S`, for example,  `P1Y2M3DT4H5M6.78S`.\n          \n          \n            \n            |The string representation of the interval value that follows the pattern `P<years>Y<months>M<days>DT<hours>H<minutes>M<seconds>S`, for example, `P1Y2M3DT4H5M6.78S`.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462354202", "createdAt": "2020-07-29T14:42:39Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -1107,29 +993,29 @@ The _semantic type_ describes how the Kafka Connect schema captures the _meaning\n |`TIMESTAMPTZ`, `TIMESTAMP WITH TIME ZONE`\n |`STRING`\n |`io.debezium.time.ZonedTimestamp`\n-| A string representation of a timestamp with timezone information, where the timezone is GMT\n+| A string representation of a timestamp with timezone information, where the timezone is GMT.\n \n |`TIMETZ`, `TIME WITH TIME ZONE`\n |`STRING`\n |`io.debezium.time.ZonedTime`\n-| A string representation of a time value with timezone information, where the timezone is GMT\n+| A string representation of a time value with timezone information, where the timezone is GMT.\n \n |`INTERVAL [P]`\n |`INT64`\n |`io.debezium.time.MicroDuration` +\n (default)\n-|The approximate number of microseconds for a time interval using the `365.25 / 12.0` formula for days per month average\n+|The approximate number of microseconds for a time interval using the `365.25 / 12.0` formula for days per month average.\n \n |`INTERVAL [P]`\n |`STRING`\n |`io.debezium.time.Interval` +\n (when `interval.handling.mode` is set to `string`)\n-|The string representation of the interval value that follows pattern `P<years>Y<months>M<days>DT<hours>H<minutes>M<seconds>S`, e.g. `P1Y2M3DT4H5M6.78S`\n+|The string representation of the interval value that follows the pattern `P<years>Y<months>M<days>DT<hours>H<minutes>M<seconds>S`, for example,  `P1Y2M3DT4H5M6.78S`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 1159}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjYwMTU5OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDo0MzoxMFrOG474rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNjozNjoxMlrOG5AxcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM1NDYwNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            |Contains the string representation of a timestamp range with the local system  time zone.\n          \n          \n            \n            |Contains the string representation of a timestamp range with the local system time zone.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462354606", "createdAt": "2020-07-29T14:43:10Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -1169,27 +1055,27 @@ The _semantic type_ describes how the Kafka Connect schema captures the _meaning\n |`INT4RANGE`\n |`STRING`\n |n/a\n-|Range of integer\n+|Range of integer.\n \n |`INT8RANGE`\n |`STRING`\n |n/a\n-|Range of bigint\n+|Range of `bigint`.\n \n |`NUMRANGE`\n |`STRING`\n |n/a\n-|Range of numeric\n+|Range of `numeric`.\n \n |`TSRANGE`\n |`STRING`\n |n/a\n-|Contains the string representation of timestamp range without time zone.\n+|Contains the string representation of a timestamp range without a time zone.\n \n |`TSTZRANGE`\n |`STRING`\n |n/a\n-|Contains the string representation of a timestamp range with (local system) time zone.\n+|Contains the string representation of a timestamp range with the local system  time zone.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 1225}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQzNDY3Mg==", "bodyText": "Fixed in the next commit.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462434672", "createdAt": "2020-07-29T16:36:12Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -1169,27 +1055,27 @@ The _semantic type_ describes how the Kafka Connect schema captures the _meaning\n |`INT4RANGE`\n |`STRING`\n |n/a\n-|Range of integer\n+|Range of integer.\n \n |`INT8RANGE`\n |`STRING`\n |n/a\n-|Range of bigint\n+|Range of `bigint`.\n \n |`NUMRANGE`\n |`STRING`\n |n/a\n-|Range of numeric\n+|Range of `numeric`.\n \n |`TSRANGE`\n |`STRING`\n |n/a\n-|Contains the string representation of timestamp range without time zone.\n+|Contains the string representation of a timestamp range without a time zone.\n \n |`TSTZRANGE`\n |`STRING`\n |n/a\n-|Contains the string representation of a timestamp range with (local system) time zone.\n+|Contains the string representation of a timestamp range with the local system  time zone.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM1NDYwNg=="}, "originalCommit": null, "originalPosition": 1225}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjYxNDk1OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDo0NTo1NlrOG48BIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNjozOToxMlrOG5A4NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM1Njc2OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            When a column is defined to contain a domain type that extends another domain type that defines a custom length or scale, the generated schema does *not* inherit the defined length oe scale because of the PostgreSQL driver's column metadata implementation.\n          \n          \n            \n            When a column is defined to contain a domain type that extends another domain type that defines a custom length or scale, the generated schema does *not* inherit the defined length or scale as that information isn't available in the PostgreSQL driver's column metadata.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462356768", "createdAt": "2020-07-29T14:45:56Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -1419,47 +1324,32 @@ When `hstore.handling.mode` configuration property is set to `json` (the default\n |`io.debezium.data.Json`\n | Example: output representation using the JSON converter is `{\\\"key\\\" : \\\"val\\\"}`\n \n-|===\n-\n-When `hstore.handling.mode` configuration property is set to `map`, then the connector will use the `MAP` schema type for all `HSTORE` columns.\n-\n-[cols=\"15%a,15%a,35%a,35%a\"]\n-|===\n-|PostgreSQL Data Type\n-|Literal type (schema type)\n-|Semantic type (schema name)\n-|Notes\n-\n |`HSTORE`\n |`MAP`\n |\n | Example: output representation  using the JSON converter is `{\"key\" : \"val\"}`\n \n |===\n \n-[[postgresql-domain-types]]\n-==== PostgreSQL Domain Types\n+[id=\"postgresql-domain-types\"]\n+=== Domain types\n \n-PostgreSQL also supports the notion of user-defined types that are based upon other underlying types.\n-When such column types are used, {prodname} exposes the column's representation based on the full type hierarchy.\n+PostgreSQL supports user-defined types that are based on other underlying types. When such column types are used, {prodname} exposes the column's representation based on the full type hierarchy.\n \n [IMPORTANT]\n ====\n-Special consideration should be taken when monitoring columns that use domain types.\n+Capturing changes in columns that use PostgreSQL domain types requires special consideration. When a column is defined to contain a domain type that extends one of the default database types and the domain type defines a custom length or scale, the generated schema inherits that defined length or scale.\n \n-When a column is defined using a domain type that extends one of the default database types and the domain type defines a custom length/scale, the generated schema will inherit that defined length/scale.\n-\n-When a column is defined using a domain type that extends another domain type that defines a custom length/scale, the generated schema will **not** inherit the defined length/scale because the PostgreSQL driver's column metadata implementation.\n+When a column is defined to contain a domain type that extends another domain type that defines a custom length or scale, the generated schema does *not* inherit the defined length oe scale because of the PostgreSQL driver's column metadata implementation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 1433}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQzNjQwNA==", "bodyText": "Fixed in the next commit.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462436404", "createdAt": "2020-07-29T16:39:12Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -1419,47 +1324,32 @@ When `hstore.handling.mode` configuration property is set to `json` (the default\n |`io.debezium.data.Json`\n | Example: output representation using the JSON converter is `{\\\"key\\\" : \\\"val\\\"}`\n \n-|===\n-\n-When `hstore.handling.mode` configuration property is set to `map`, then the connector will use the `MAP` schema type for all `HSTORE` columns.\n-\n-[cols=\"15%a,15%a,35%a,35%a\"]\n-|===\n-|PostgreSQL Data Type\n-|Literal type (schema type)\n-|Semantic type (schema name)\n-|Notes\n-\n |`HSTORE`\n |`MAP`\n |\n | Example: output representation  using the JSON converter is `{\"key\" : \"val\"}`\n \n |===\n \n-[[postgresql-domain-types]]\n-==== PostgreSQL Domain Types\n+[id=\"postgresql-domain-types\"]\n+=== Domain types\n \n-PostgreSQL also supports the notion of user-defined types that are based upon other underlying types.\n-When such column types are used, {prodname} exposes the column's representation based on the full type hierarchy.\n+PostgreSQL supports user-defined types that are based on other underlying types. When such column types are used, {prodname} exposes the column's representation based on the full type hierarchy.\n \n [IMPORTANT]\n ====\n-Special consideration should be taken when monitoring columns that use domain types.\n+Capturing changes in columns that use PostgreSQL domain types requires special consideration. When a column is defined to contain a domain type that extends one of the default database types and the domain type defines a custom length or scale, the generated schema inherits that defined length or scale.\n \n-When a column is defined using a domain type that extends one of the default database types and the domain type defines a custom length/scale, the generated schema will inherit that defined length/scale.\n-\n-When a column is defined using a domain type that extends another domain type that defines a custom length/scale, the generated schema will **not** inherit the defined length/scale because the PostgreSQL driver's column metadata implementation.\n+When a column is defined to contain a domain type that extends another domain type that defines a custom length or scale, the generated schema does *not* inherit the defined length oe scale because of the PostgreSQL driver's column metadata implementation.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM1Njc2OA=="}, "originalCommit": null, "originalPosition": 1433}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjYxNzE5OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDo0NjoyNlrOG48ChA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNjozOTo1MVrOG5A5uQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM1NzEyNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            PostgreSQL haddata types that can store IPv4, IPv6, and MAC addresses. It is better to use these types instead of plain text types to store network addresses. Network address types offer input error checking and specialized operators and functions.\n          \n          \n            \n            PostgreSQL has data types that can store IPv4, IPv6, and MAC addresses. It is better to use these types instead of plain text types to store network addresses. Network address types offer input error checking and specialized operators and functions.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462357124", "createdAt": "2020-07-29T14:46:26Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -1419,47 +1324,32 @@ When `hstore.handling.mode` configuration property is set to `json` (the default\n |`io.debezium.data.Json`\n | Example: output representation using the JSON converter is `{\\\"key\\\" : \\\"val\\\"}`\n \n-|===\n-\n-When `hstore.handling.mode` configuration property is set to `map`, then the connector will use the `MAP` schema type for all `HSTORE` columns.\n-\n-[cols=\"15%a,15%a,35%a,35%a\"]\n-|===\n-|PostgreSQL Data Type\n-|Literal type (schema type)\n-|Semantic type (schema name)\n-|Notes\n-\n |`HSTORE`\n |`MAP`\n |\n | Example: output representation  using the JSON converter is `{\"key\" : \"val\"}`\n \n |===\n \n-[[postgresql-domain-types]]\n-==== PostgreSQL Domain Types\n+[id=\"postgresql-domain-types\"]\n+=== Domain types\n \n-PostgreSQL also supports the notion of user-defined types that are based upon other underlying types.\n-When such column types are used, {prodname} exposes the column's representation based on the full type hierarchy.\n+PostgreSQL supports user-defined types that are based on other underlying types. When such column types are used, {prodname} exposes the column's representation based on the full type hierarchy.\n \n [IMPORTANT]\n ====\n-Special consideration should be taken when monitoring columns that use domain types.\n+Capturing changes in columns that use PostgreSQL domain types requires special consideration. When a column is defined to contain a domain type that extends one of the default database types and the domain type defines a custom length or scale, the generated schema inherits that defined length or scale.\n \n-When a column is defined using a domain type that extends one of the default database types and the domain type defines a custom length/scale, the generated schema will inherit that defined length/scale.\n-\n-When a column is defined using a domain type that extends another domain type that defines a custom length/scale, the generated schema will **not** inherit the defined length/scale because the PostgreSQL driver's column metadata implementation.\n+When a column is defined to contain a domain type that extends another domain type that defines a custom length or scale, the generated schema does *not* inherit the defined length oe scale because of the PostgreSQL driver's column metadata implementation.\n ====\n \n-[[postgresql-postgis-types]]\n-\n-[[postgresql-network-address-types]]\n-===== Network Address Types\n+[id=\"postgresql-network-address-types\"]\n+=== Network address types\n \n-PostgreSQL also have data types that can store IPv4, IPv6, and MAC addresses. It is better to use these instead of plain text types to store network addresses, because these types offer input error checking and specialized operators and functions.\n+PostgreSQL haddata types that can store IPv4, IPv6, and MAC addresses. It is better to use these types instead of plain text types to store network addresses. Network address types offer input error checking and specialized operators and functions.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 1444}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQzNjc5Mw==", "bodyText": "Fixed in the next commit.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462436793", "createdAt": "2020-07-29T16:39:51Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -1419,47 +1324,32 @@ When `hstore.handling.mode` configuration property is set to `json` (the default\n |`io.debezium.data.Json`\n | Example: output representation using the JSON converter is `{\\\"key\\\" : \\\"val\\\"}`\n \n-|===\n-\n-When `hstore.handling.mode` configuration property is set to `map`, then the connector will use the `MAP` schema type for all `HSTORE` columns.\n-\n-[cols=\"15%a,15%a,35%a,35%a\"]\n-|===\n-|PostgreSQL Data Type\n-|Literal type (schema type)\n-|Semantic type (schema name)\n-|Notes\n-\n |`HSTORE`\n |`MAP`\n |\n | Example: output representation  using the JSON converter is `{\"key\" : \"val\"}`\n \n |===\n \n-[[postgresql-domain-types]]\n-==== PostgreSQL Domain Types\n+[id=\"postgresql-domain-types\"]\n+=== Domain types\n \n-PostgreSQL also supports the notion of user-defined types that are based upon other underlying types.\n-When such column types are used, {prodname} exposes the column's representation based on the full type hierarchy.\n+PostgreSQL supports user-defined types that are based on other underlying types. When such column types are used, {prodname} exposes the column's representation based on the full type hierarchy.\n \n [IMPORTANT]\n ====\n-Special consideration should be taken when monitoring columns that use domain types.\n+Capturing changes in columns that use PostgreSQL domain types requires special consideration. When a column is defined to contain a domain type that extends one of the default database types and the domain type defines a custom length or scale, the generated schema inherits that defined length or scale.\n \n-When a column is defined using a domain type that extends one of the default database types and the domain type defines a custom length/scale, the generated schema will inherit that defined length/scale.\n-\n-When a column is defined using a domain type that extends another domain type that defines a custom length/scale, the generated schema will **not** inherit the defined length/scale because the PostgreSQL driver's column metadata implementation.\n+When a column is defined to contain a domain type that extends another domain type that defines a custom length or scale, the generated schema does *not* inherit the defined length oe scale because of the PostgreSQL driver's column metadata implementation.\n ====\n \n-[[postgresql-postgis-types]]\n-\n-[[postgresql-network-address-types]]\n-===== Network Address Types\n+[id=\"postgresql-network-address-types\"]\n+=== Network address types\n \n-PostgreSQL also have data types that can store IPv4, IPv6, and MAC addresses. It is better to use these instead of plain text types to store network addresses, because these types offer input error checking and specialized operators and functions.\n+PostgreSQL haddata types that can store IPv4, IPv6, and MAC addresses. It is better to use these types instead of plain text types to store network addresses. Network address types offer input error checking and specialized operators and functions.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM1NzEyNA=="}, "originalCommit": null, "originalPosition": 1444}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjY1NDAyOnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNDo1NDoyM1rOG48ZvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNjo0NToxNlrOG5BGUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM2MzA2OQ==", "bodyText": "Is it uncommon for downstream to have a default value that isn't within the expected value-set?  In other words, the plugin.name defaults to decoderbufs but the only supported option is pgoutput.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462363069", "createdAt": "2020-07-29T14:54:23Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -1728,36 +2021,40 @@ The following configuration properties are _required_ unless a default value is\n \n |[[postgresql-property-plugin-name]]<<postgresql-property-plugin-name, `plugin.name`>>\n |`decoderbufs`\n-|The name of the Postgres {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-in] installed on the server.\n+|The name of the PostgreSQL {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-in] installed on the PostgreSQL server.\n+\n+ifdef::community[]\n+Supported values are `decoderbufs`, `wal2json`, `wal2json_rds`, `wal2json_streaming`, `wal2json_rds_streaming` and `pgoutput`.\n+\n+If you are using a `wal2json` plug-in and transactions are very large, the JSON batch event that contains all transaction changes might not fit into the hard-coded memory buffer, which has a size of 1 GB. In such cases, switch to a streaming plug-in, by setting the `plugin-name` property to `wal2json_streaming` or  `wal2json_rds_streaming`. With a streaming plug-in, PostgreSQL sends the connector a separate message for each change in a transaction.\n+\n+endif::community[]\n ifdef::product[]\n The only supported value is `pgoutput`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 2134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjQ0MDAxOQ==", "bodyText": "No. Ouch. I should have caught this.\nDownstream, is the default for plugin.name - pgoutput?", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462440019", "createdAt": "2020-07-29T16:45:16Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -1728,36 +2021,40 @@ The following configuration properties are _required_ unless a default value is\n \n |[[postgresql-property-plugin-name]]<<postgresql-property-plugin-name, `plugin.name`>>\n |`decoderbufs`\n-|The name of the Postgres {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-in] installed on the server.\n+|The name of the PostgreSQL {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-in] installed on the PostgreSQL server.\n+\n+ifdef::community[]\n+Supported values are `decoderbufs`, `wal2json`, `wal2json_rds`, `wal2json_streaming`, `wal2json_rds_streaming` and `pgoutput`.\n+\n+If you are using a `wal2json` plug-in and transactions are very large, the JSON batch event that contains all transaction changes might not fit into the hard-coded memory buffer, which has a size of 1 GB. In such cases, switch to a streaming plug-in, by setting the `plugin-name` property to `wal2json_streaming` or  `wal2json_rds_streaming`. With a streaming plug-in, PostgreSQL sends the connector a separate message for each change in a transaction.\n+\n+endif::community[]\n ifdef::product[]\n The only supported value is `pgoutput`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM2MzA2OQ=="}, "originalCommit": null, "originalPosition": 2134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg4NjY5Mzg1OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOVQxNTowMjo0MFrOG48yeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQxNDoyNzozNlrOG6KkuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM2OTQwMA==", "bodyText": "We began the move away from using options=\"header\" for tables and I think we should continue that here for consistency across all of the documentation.  There are multiple occurrences of this in this file btw.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462369400", "createdAt": "2020-07-29T15:02:40Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -12,356 +14,156 @@ ifdef::community[]\n toc::[]\n endif::community[]\n \n-{prodname}'s PostgreSQL Connector can monitor and record row-level changes in the schemas of a PostgreSQL database.\n+{prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 10, 11, and 12 are supported. \n \n-The first time it connects to a PostgreSQL server/cluster, it reads a consistent snapshot of all of the schemas. When that snapshot is complete, the connector continuously streams the changes that were committed to PostgreSQL 9.6 or later and generates corresponding insert, update and delete events. All of the events for each table are recorded in a separate Kafka topic, where they can be easily consumed by applications and services.\n+The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic. \n \n+ifdef::product[]\n+Information and procedures for using a {prodname} PostgreSQL connector is organized as follows: \n+\n+* xref:overview-of-debezium-postgresql-connector[]\n+* xref:how-debezium-postgresql-connectors-work[]\n+* xref:descriptions-of-debezium-postgresql-connector-data-change-events[]\n+* xref:how-debezium-postgresql-connectors-map-data-types[]\n+* xref:setting-up-postgresql-to-run-a-debezium-connector[]\n+* xref:deploying-and-managing-debezium-postgresql-connectors[]\n+* xref:how-debezium-postgresql-connectors-handle-faults-and-problems[]\n+\n+endif::product[]\n \n+// Type: concept\n+// Title: Overview of {prodname} PostgreSQL connector \n+// ModuleID: overview-of-debezium-postgresql-connector\n [[postgresql-overview]]\n == Overview\n \n-PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was first introduced in version 9.4 and is a mechanism which allows the extraction of the changes which were committed to the transaction log and the processing of these changes in a user-friendly manner via the help of an https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. This output plug-in must be installed prior to running the PostgreSQL server and enabled together with a replication slot in order for clients to be able to consume the changes.\n+PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was introduced in version 9.4. It is a mechanism that allows the extraction of the changes that were committed to the transaction log and the processing of these changes in a user-friendly manner with the help of an link:https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. You must install this output plug-in and configure a replication slot that uses the output plug-in before running the PostgreSQL server. This enables clients to consume the changes. \n \n-PostgreSQL connector contains two different parts which work together in order to be able to read and process server changes:\n+The PostgreSQL connector contains two main parts that work together to read and process server changes:\n \n ifdef::product[]\n-* A logical decoding output plug-in, which has to be installed and configured in the PostgreSQL server.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server.\n+* Java code (the actual Kafka Connect connector), which reads the changes produced by the plug-in by using PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_] and the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_].\n endif::product[]\n \n ifdef::community[]\n-* a logical decoding output plug-in which has to be installed and configured in the PostgreSQL server, one of\n-** https://github.com/debezium/postgres-decoderbufs[decoderbufs] (maintained by the {prodname} community, based on ProtoBuf)\n-** https://github.com/eulerto/wal2json[wal2json] (maintained by the wal2json community, based on JSON)\n-** pgoutput, the standard logical decoding plug-in in PostgreSQL 10+ (maintained by the Postgres community, used by Postgres itself for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]);\n-this plug-in is always present, meaning that no additional libraries must be installed,\n-and the {prodname} connector will interpret the raw replication event stream into change events directly.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the chosen plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server. The plug-in is one of these: \n+** link:https://github.com/debezium/postgres-decoderbufs[`decoderbufs`] - maintained by the {prodname} community, based on ProtoBuf.\n+** link:https://github.com/eulerto/wal2json[`wal2json`] - maintained by the wal2json community, based on JSON.\n+** `pgoutput`, which is the standard logical decoding plug-in in PostgreSQL 10+. It is maintained by the PostgreSQL community, and used by PostgreSQL itself for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. This plug-in is always present so no additional libraries need to be installed. The {prodname} connector interprets the raw replication event stream directly into change events.\n+* Java code (the actual Kafka Connect connector) that reads the changes produced by the chosen plug-in. It uses PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], by means of the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n endif::community[]\n \n-The connector then produces a _change event_ for every row-level insert, update, and delete operation that was received, recording all the change events for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables they're interested in following, and react to every row-level event it sees in those topics.\n+The connector produces a _change event_ for every row-level insert, update, and delete operation that was captured and sends change event records for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables of interest, and can react to every row-level event they receive from those topics.\n \n-PostgreSQL normally purges WAL segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, we start with a consistent view of all of the data, yet continue reading without having lost any of the changes made while the snapshot was taking place.\n+PostgreSQL normally purges write-ahead log (WAL) segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, the connector starts with a consistent view of all of the data, and does not omit any changes that were made while the snapshot was being taken.\n \n-The connector is also tolerant of failures. As the connector reads changes and produces events, it records the position in the write-ahead log with each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart it simply continues reading the WAL where it last left off. This includes snapshots: if the snapshot was not completed when the connector is stopped, upon restart it will begin a new snapshot.\n+The connector is tolerant of failures. As the connector reads changes and produces events, it records the position in the WAL for each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart the connector continues reading the WAL where it last left off. This includes snapshots. If the connector stops during a snapshot, the connector begins a new snapshot when it restarts. \n \n ifdef::product[]\n [[postgresql-output-plugin]]\n-=== Logical decoding output plug-in\n-The `pgoutput` logical decoder is the only supported logical decoder in the Tecnhology Preview release of {prodname}.\n-\n-`pgoutput`, the standard logical decoding plug-in in PostgreSQL 10+, is maintained by the Postgres community, and is also used by Postgres for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication].\n-The `pgoutput` plug-in is always present, meaning that no additional libraries must be installed,\n-and the connector will interpret the raw replication event stream into change events directly.\n+.Logical decoding output plug-in\n+The `pgoutput` logical decoding plug-in, the only supported logical decoding plug-in in this {prodname} release, is the standard logical decoding plug-in in PostgreSQL 10+, it is maintained by the PostgreSQL community, and it is also used by PostgreSQL for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. The `pgoutput` plug-in is always present, meaning that no additional libraries must be installed. The connector directly interprets the raw replication event stream into change events records.\n endif::product[]\n \n [[postgresql-limitations]]\n [IMPORTANT]\n ====\n-The connector's functionality relies on PostgreSQL's logical decoding feature.\n-Please be aware of the following limitations which are also reflected by the connector:\n+The connector relies on and reflects the PostgreSQL logical decoding feature, which has the following limitations:\n+\n+* Logical decoding does not support DDL changes. This means that the connector is unable to report DDL change events back to consumers.\n+* Logical decoding replication slots are supported on only `primary` servers. When there is a cluster of PostgreSQL servers, the connector can run on only the active `primary` server. It cannot run on `hot` or `warm` standby replicas. If the `primary` server fails or is demoted, the connector stops. After the `primary` server has recovered, you can restart the connector. If a different PostgreSQL server has been promoted to `primary`, adjust the connector configuration before restarting the connector. \n \n-. Logical Decoding does not support DDL changes: this means that the connector is unable to report DDL change events back to consumers.\n-. Logical Decoding replication slots are only supported on `primary` servers: this means that when there is a cluster of PostgreSQL servers, the connector can only run on the active `primary` server. It cannot run on `hot` or `warm` standby replicas. If the `primary` server fails or is demoted, the connector will stop. Once the `primary` has recovered the connector can simply be restarted. If a different PostgreSQL server has been promoted to `primary`, the connector configuration must be adjusted before the connector is restarted. Make sure you read more about how the connector behaves {link-prefix}:{link-postgresql-connector}#postgresql-when-things-go-wrong[when things go wrong].\n+{link-prefix}:{link-postgresql-connector}#postgresql-when-things-go-wrong[ Behavior when things go wrong] describes what the connector does when there is a problem. \n ====\n \n [IMPORTANT]\n ====\n-{prodname} currently supports only databases with UTF-8 character encoding.\n-With a single byte character encoding it is not possible to correctly process strings containing extended ASCII code characters.\n+{prodname} currently supports databases with UTF-8 character encoding only.\n+With a single byte character encoding, it is not possible to correctly process strings that contain extended ASCII code characters.\n ====\n \n-[[setting-up-postgresql]]\n-== Setting up PostgreSQL\n-\n-ifdef::product[]\n-This release of {prodname} only supports the native pgoutput logical replication stream.\n-To set up PostgreSQL using pgoutput, you will need to enable a replication slot, and configure a user with sufficient privileges to perform the replication.\n-\n-=== Configuring the replication slot\n-\n-PostgreSQL's logical decoding uses replication slots.\n-\n-First, you configure the replication slot:\n-\n-.postgresql.conf\n+// Type: assembly\n+// ModuleID: how-debezium-postgresql-connectors-work\n+// Title: How {prodname} PostgreSQL connectors work\n+[[how-the-postgresql-connector-works]]\n+== How the connector works\n \n-[source]\n-----\n-wal_level=logical\n-max_wal_senders=1\n-max_replication_slots=1\n-----\n+To optimally configure and run a {prodname} PostgreSQL connector, it is helpful to understand how the connector performs snapshots, streams change events, determines Kafka topic names, and uses metadata. \n \n-* `wal_level` tells the server to use logical decoding with the write-ahead log\n-* `max_wal_senders` tells the server to use a maximum of 1 separate processes for processing WAL changes\n-* `max_replication_slots` tells the server to allow a maximum of 1 replication slots to be created for streaming WAL changes\n+ifdef::product[]\n+Details are in the following topics: \n \n-Replication slots are guaranteed to retain all WAL required for {prodname} even during {prodname} outages.\n-It is important for this reason to closely monitor replication slots to avoid too much disk consumption and other conditions that can happen such as catalog bloat if a replication slot stays unused for too long.\n-For more information, refer to the https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION-SLOTS[the Postgres documentation].\n+* xref:how-debezium-postgresql-connectors-perform-database-snapshots[]\n+* xref:how-debezium-postgresql-connectors-stream-change-event-records[]\n+* xref:postgresql-10-logical-decoding-support-pgoutput[]\n+* xref:default-names-of-kafka-topics-that-receive-debezium-change-event-records[]\n+* xref:metadata-in-debezium-change-event-records[]\n+* xref:debezium-connector-generated-events-that-represent-transaction-boundaries[]\n \n-[NOTE]\n-====\n-We recommend reading and understanding the https://www.postgresql.org/docs/current/static/wal-configuration.html[WAL configuration documentation] regarding the mechanics and configuration of the PostgreSQL write-ahead log.\n-====\n endif::product[]\n \n-ifdef::community[]\n-Before using the PostgreSQL connector to monitor the changes committed on a PostgreSQL server,\n-first decide which logical decoder method you intend to use.\n-If you plan *not* to use the native pgoutput logical replication stream support,\n-then you will need to install the logical decoding plug-in into the PostgreSQL server.\n-Afterward enable a replication slot, and configure a user with sufficient privileges to perform the replication.\n-\n-Note that if your database is hosted by a service such as https://www.heroku.com/postgres[Heroku Postgres] you may be unable to install the plug-in.\n-If so, and if you're using PostgreSQL 10+, you can use the pgoutput decoder support to monitor your database.\n-If that is not an option, you'll be unable to monitor your database with {prodname}.\n-\n-[[postgresql-on-amazon-rds]]\n-=== PostgreSQL on Amazon RDS\n-\n-It is possible to monitor PostgreSQL database running in https://aws.amazon.com/rds/[Amazon RDS]. To get it running you must fulfill the following conditions\n-\n-* The instance parameter `rds.logical_replication` is set to `1`.\n-* Verify that `wal_level` parameter is set to `logical` by running the query `SHOW wal_level` as DB master user; this might not be the case in multi-zone replication setups.\n-You cannot set this option manually, it is (https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html[automatically changed]) when the `rds.logical_replication` is set to `1`.\n-If the `wal_level` is not `logical` after the change above, it is probably because the instance has to be restarted due to the parameter group change. This happens accordingly to your maintenance window or can be done manually.\n-* Set `plugin.name` {prodname} parameter to `wal2json`.  You can skip this on PostgreSQL 10+ if you wish to use pgoutput logical replication stream support.\n-* Use database master account for replication as RDS currently does not support setting of `REPLICATION` privilege for another account.\n-\n-[IMPORTANT]\n-====\n-You should make sure to use the latest versions of Postgres 9.6, 10 or 11 on Amazon RDS.\n-Otherwise, older versions of the wal2json plug-in may be installed\n-(see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html[the official documentation] for the exact wal2json versions installed on Amazon RDS).\n-In that case, replication messages received from the database may not carry complete information about type constraints like length or scale or `NULL`/`NOT NULL`,\n-which in turn might cause creation of messages with an inconsistent schema for a short period of time in case of changes to a column's definition.\n-\n-As of January 2019, the following Postgres versions on RDS come with an up-to-date version of wal2json and thus should be used:\n-\n-* Postgres 9.6: 9.6.10 and newer\n-* Postgres 10: 10.5 and newer\n-* Postgres 11: any version\n-====\n-\n-\n-[[postgresql-output-plugin]]\n-=== Installing the Logical Decoding Output Plug-in\n-\n-[TIP]\n-====\n-Also see {link-prefix}:{link-postgresql-plugins}[Logical Decoding Output Plug-in Installation for PostgreSQL] for more detailed instructions of setting up and testing logical decoding plug-ins.\n-====\n-\n-[NOTE]\n-====\n-As of {prodname} 0.10, the connector supports PostgreSQL 10+ logical replication streaming using _pgoutput_.\n-This means that a logical decoding output plug-in is no longer necessary and changes can be emitted directly from the replication stream by the connector.\n-====\n+// Type: concept\n+// ModuleID: how-debezium-postgresql-connectors-perform-database-snapshots\n+// Title: How {prodname} PostgreSQL connectors perform database snapshots\n+[[postgresql-snapshots]]\n+=== Snapshots\n \n-As of PostgreSQL 9.4, the only way to read changes to the write-ahead-log is to first install a logical decoding output plug-in. Plugins are written in C, compiled, and installed on the machine which runs the PostgreSQL server. Plugins use  a number of PostgreSQL specific APIs, as described by the https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_PostgreSQL documentation_].\n+Most PostgreSQL servers are configured to not retain the complete history of the database in the WAL segments. This means that the PostgreSQL connector would be unable to see the entire history of the database by reading only the WAL. Consequently, the first time that the connector starts, it performs an initial _consistent snapshot_ of the database. The default behavior for performing a snapshot consists of the following steps. You can change this behavior by setting the {link-prefix}:{link-postgresql-connector}#postgresql-property-snapshot-mode[`snapshot.mode` connector configuration property] to a value other than `initial`. \n \n-The PostgreSQL connector works with one of {prodname}'s supported logical decoding plug-ins to encode the changes in either https://github.com/google/protobuf[_Protobuf format_] or http://www.json.org/[_JSON_] format.\n-See the documentation of your chosen plug-in (https://github.com/debezium/postgres-decoderbufs/blob/master/README.md[_protobuf_], https://github.com/eulerto/wal2json/blob/master/README.md[_wal2json_]) to learn more about the plug-in's requirements, limitations, and how to compile it.\n+. Start a transaction with a link:https://www.postgresql.org/docs/current/static/sql-set-transaction.html[SERIALIZABLE, READ ONLY, DEFERRABLE] isolation level to ensure that subsequent reads in this transaction are against a single consistent version of the data. Any changes to the data due to subsequent `INSERT`, `UPDATE`, and `DELETE` operations by other clients are not visible to this transaction.\n+. Obtain an `ACCESS SHARE MODE` lock on each of the tables being tracked to ensure that no structural changes can occur to any of the tables while the snapshot is taking place. These locks do not prevent table `INSERT`, `UPDATE` and `DELETE` operations from taking place during the snapshot. \n++\n+_This step is omitted when `snapshot.mode` is set to `exported`, which allows the connector to perform a lock-free snapshot_.\n+. Read the current position in the server's transaction log.\n+. Scan the database tables and schemas, generate a `READ` event for each row and write that event to the appropriate table-specific Kafka topic.\n+. Commit the transaction.\n+. Record the successful completion of the snapshot in the connector offsets.\n \n-For simplicity, {prodname} also provides a Docker image based on a vanilla PostgreSQL server image on top of which it compiles and installs the plug-ins. We recommend https://github.com/debezium/docker-images/tree/master/postgres/9.6[_using this image_] as an example of the detailed steps required for the installation.\n+If the connector fails, is rebalanced, or stops after Step 1 begins but before Step 6 completes, upon restart the connector begins a new snapshot. After the connector completes its initial snapshot, the PostgreSQL connector continues streaming from the position that it read in step 3. This ensures that the connector does not miss any updates. If the connector stops again for any reason, upon restart, the connector continues streaming changes from where it previously left off.\n \n [WARNING]\n ====\n-The {prodname} logical decoding plug-ins have only been installed and tested on _Linux_ machines.\n-For Windows and other operating systems it may require different installation steps.\n-====\n-\n-[[postgresql-differences-between-plugins]]\n-==== Differences Between Plug-ins\n-\n-The plug-ins' behavior is not completely same for all cases.\n-So far these differences have been identified:\n-\n-* wal2json plug-in is not able to process quoted identifiers (https://github.com/eulerto/wal2json/issues/35[issue])\n-* wal2json and decoderbufs plug-ins emit events for tables without primary keys\n-* wal2json plug-in does not support special values (`NaN` or `infinity`) for floating point types\n-* wal2json should be used with setting the `schema.refresh.mode` connector option to `columns_diff_exclude_unchanged_toast`;\n-otherwise, when receiving a change event for a row containing an unchanged TOAST column, no field for that column is contained in the emitted change event's `after` structure.\n-This is because wal2json's messages do not contain a field for such a column.\n-\n-The requirement for adding this is tracked under the wal2json https://github.com/eulerto/wal2json/issues/98[issue 98].\n-See the documentation of `columns_diff_exclude_unchanged_toast` further below for implications of using it.\n-\n-* pgoutput plug-in does not emit all the events for tables without primary keys, it only emits inserts\n-\n-All up-to-date differences are tracked in a test suite https://github.com/debezium/debezium/blob/master/debezium-connector-postgres/src/test/java/io/debezium/connector/postgresql/DecoderDifferences.java[Java class].\n-\n-\n-[[postgresql-server-configuration]]\n-=== Configuring the PostgreSQL Server\n-\n-If you are using one of the supported {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-ins] (i.e. not pgoutput) and it has been installed,\n-configure the server to load the plug-in at startup:\n-\n-.postgresql.conf\n-[source,properties]\n-----\n-# MODULES\n-shared_preload_libraries = 'decoderbufs,wal2json' // <1>\n-----\n-<1> tells the server that it should load at startup the `decoderbufs` and `wal2json` logical decoding plug-ins (the names of the plug-ins are set in https://github.com/debezium/postgres-decoderbufs/blob/v0.3.0/Makefile[_Protobuf_] and https://github.com/eulerto/wal2json/blob/master/Makefile[_wal2json_] Makefiles)\n-\n-Next is to configure the replication slot regardless of the decoder being used:\n-\n-.postgresql.conf\n-[source,properties]\n-----\n-# REPLICATION\n-wal_level = logical             // <1>\n-max_wal_senders = 1             // <2>\n-max_replication_slots = 1       // <3>\n-----\n-<1> tells the server that it should use logical decoding with the write-ahead log\n-<2> tells the server that it should use a maximum of `1` separate processes for processing WAL changes\n-<3> tells the server that it should allow a maximum of `1` replication slots to be created for streaming WAL changes\n-\n-{prodname} uses PostgreSQL's logical decoding, which uses replication slots.\n-Replication slots are guaranteed to retain all WAL required for {prodname} even during {prodname} outages.\n-It is important for this reason to closely monitor replication slots to avoid too much disk consumption and other conditions that can happen such as catalog bloat if a replication slot stays unused for too long.\n-For more information please see the official Postgres docs on https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION-SLOTS[this subject].\n-\n-If you are working with a `synchronous_commit` setting other than `on`,\n-the recommendation is to set `wal_writer_delay` to a value such as 10 ms to achieve a low latency of change events.\n-Otherwise, its default value is applied, which adds a latency of about 200 ms.\n-\n-[TIP]\n-====\n-We strongly recommend reading and understanding https://www.postgresql.org/docs/current/static/wal-configuration.html[the official documentation] regarding the mechanics and configuration of the PostgreSQL write-ahead log.\n-====\n-endif::community[]\n-\n-[[postgresql-permissions]]\n-=== Setting up Permissions\n-\n-Next, configure a database user who can perform replications.\n-\n-Replication can only be performed by a database user that has appropriate permissions and only for a configured number of hosts.\n-\n-In order to give a user replication permissions, define a PostgreSQL role that has _at least_ the `REPLICATION` and `LOGIN` permissions. For example:\n-\n-[source,sql]\n-----\n-CREATE ROLE name REPLICATION LOGIN;\n-----\n-\n-[NOTE]\n-====\n-Superusers have by default both of the above roles.\n+It is strongly recommended that you configure a PostgreSQL connector to set `snapshot.mode` to `exported`. The `initial`, `initial only` and `always` modes can lose a few events while a connector switches from performing the snapshot to streaming change event records when a database is under heavy load.\n+This is a known issue and the affected snapshot modes will be reworked to use `exported` mode internally (link:https://issues.redhat.com/browse/DBZ-2337[DBZ-2337]).\n ====\n \n-Finally, configure the PostgreSQL server to allow replication to take place between the server machine and the host on which the PostgreSQL connector is running:\n-\n-.pg_hba.conf\n-[source]\n-----\n-local   replication     <youruser>                          trust   // <1>\n-host    replication     <youruser>  127.0.0.1/32            trust   // <2>\n-host    replication     <youruser>  ::1/128                 trust   // <3>\n-----\n-<1> Tells the server to allow replication for `<youruser>` locally (i.e. on the server machine)\n-<2> Tells the server to allow `<youruser>` on `localhost` to receive replication changes using `IPV4`\n-<3> Tells the server to allow `<youruser>` on `localhost` to receive replication changes using `IPV6`\n+[id=\"snapshot-mode-settings\"]\n+.Settings for `snapshot.mode` connector configuration property\n+[cols=\"20%a,80%a\",options=\"header\"]", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 340}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM3ODc1MA==", "bodyText": "In a table, `options=\"header\" is required downstream because it improves accessibility for visually impaired folks who are hearing the text from a screen reader.\nWhy are you moving away from using it?\nAnd can you reverse that decision?", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462378750", "createdAt": "2020-07-29T15:15:10Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -12,356 +14,156 @@ ifdef::community[]\n toc::[]\n endif::community[]\n \n-{prodname}'s PostgreSQL Connector can monitor and record row-level changes in the schemas of a PostgreSQL database.\n+{prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 10, 11, and 12 are supported. \n \n-The first time it connects to a PostgreSQL server/cluster, it reads a consistent snapshot of all of the schemas. When that snapshot is complete, the connector continuously streams the changes that were committed to PostgreSQL 9.6 or later and generates corresponding insert, update and delete events. All of the events for each table are recorded in a separate Kafka topic, where they can be easily consumed by applications and services.\n+The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic. \n \n+ifdef::product[]\n+Information and procedures for using a {prodname} PostgreSQL connector is organized as follows: \n+\n+* xref:overview-of-debezium-postgresql-connector[]\n+* xref:how-debezium-postgresql-connectors-work[]\n+* xref:descriptions-of-debezium-postgresql-connector-data-change-events[]\n+* xref:how-debezium-postgresql-connectors-map-data-types[]\n+* xref:setting-up-postgresql-to-run-a-debezium-connector[]\n+* xref:deploying-and-managing-debezium-postgresql-connectors[]\n+* xref:how-debezium-postgresql-connectors-handle-faults-and-problems[]\n+\n+endif::product[]\n \n+// Type: concept\n+// Title: Overview of {prodname} PostgreSQL connector \n+// ModuleID: overview-of-debezium-postgresql-connector\n [[postgresql-overview]]\n == Overview\n \n-PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was first introduced in version 9.4 and is a mechanism which allows the extraction of the changes which were committed to the transaction log and the processing of these changes in a user-friendly manner via the help of an https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. This output plug-in must be installed prior to running the PostgreSQL server and enabled together with a replication slot in order for clients to be able to consume the changes.\n+PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was introduced in version 9.4. It is a mechanism that allows the extraction of the changes that were committed to the transaction log and the processing of these changes in a user-friendly manner with the help of an link:https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. You must install this output plug-in and configure a replication slot that uses the output plug-in before running the PostgreSQL server. This enables clients to consume the changes. \n \n-PostgreSQL connector contains two different parts which work together in order to be able to read and process server changes:\n+The PostgreSQL connector contains two main parts that work together to read and process server changes:\n \n ifdef::product[]\n-* A logical decoding output plug-in, which has to be installed and configured in the PostgreSQL server.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server.\n+* Java code (the actual Kafka Connect connector), which reads the changes produced by the plug-in by using PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_] and the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_].\n endif::product[]\n \n ifdef::community[]\n-* a logical decoding output plug-in which has to be installed and configured in the PostgreSQL server, one of\n-** https://github.com/debezium/postgres-decoderbufs[decoderbufs] (maintained by the {prodname} community, based on ProtoBuf)\n-** https://github.com/eulerto/wal2json[wal2json] (maintained by the wal2json community, based on JSON)\n-** pgoutput, the standard logical decoding plug-in in PostgreSQL 10+ (maintained by the Postgres community, used by Postgres itself for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]);\n-this plug-in is always present, meaning that no additional libraries must be installed,\n-and the {prodname} connector will interpret the raw replication event stream into change events directly.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the chosen plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server. The plug-in is one of these: \n+** link:https://github.com/debezium/postgres-decoderbufs[`decoderbufs`] - maintained by the {prodname} community, based on ProtoBuf.\n+** link:https://github.com/eulerto/wal2json[`wal2json`] - maintained by the wal2json community, based on JSON.\n+** `pgoutput`, which is the standard logical decoding plug-in in PostgreSQL 10+. It is maintained by the PostgreSQL community, and used by PostgreSQL itself for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. This plug-in is always present so no additional libraries need to be installed. The {prodname} connector interprets the raw replication event stream directly into change events.\n+* Java code (the actual Kafka Connect connector) that reads the changes produced by the chosen plug-in. It uses PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], by means of the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n endif::community[]\n \n-The connector then produces a _change event_ for every row-level insert, update, and delete operation that was received, recording all the change events for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables they're interested in following, and react to every row-level event it sees in those topics.\n+The connector produces a _change event_ for every row-level insert, update, and delete operation that was captured and sends change event records for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables of interest, and can react to every row-level event they receive from those topics.\n \n-PostgreSQL normally purges WAL segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, we start with a consistent view of all of the data, yet continue reading without having lost any of the changes made while the snapshot was taking place.\n+PostgreSQL normally purges write-ahead log (WAL) segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, the connector starts with a consistent view of all of the data, and does not omit any changes that were made while the snapshot was being taken.\n \n-The connector is also tolerant of failures. As the connector reads changes and produces events, it records the position in the write-ahead log with each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart it simply continues reading the WAL where it last left off. This includes snapshots: if the snapshot was not completed when the connector is stopped, upon restart it will begin a new snapshot.\n+The connector is tolerant of failures. As the connector reads changes and produces events, it records the position in the WAL for each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart the connector continues reading the WAL where it last left off. This includes snapshots. If the connector stops during a snapshot, the connector begins a new snapshot when it restarts. \n \n ifdef::product[]\n [[postgresql-output-plugin]]\n-=== Logical decoding output plug-in\n-The `pgoutput` logical decoder is the only supported logical decoder in the Tecnhology Preview release of {prodname}.\n-\n-`pgoutput`, the standard logical decoding plug-in in PostgreSQL 10+, is maintained by the Postgres community, and is also used by Postgres for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication].\n-The `pgoutput` plug-in is always present, meaning that no additional libraries must be installed,\n-and the connector will interpret the raw replication event stream into change events directly.\n+.Logical decoding output plug-in\n+The `pgoutput` logical decoding plug-in, the only supported logical decoding plug-in in this {prodname} release, is the standard logical decoding plug-in in PostgreSQL 10+, it is maintained by the PostgreSQL community, and it is also used by PostgreSQL for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. The `pgoutput` plug-in is always present, meaning that no additional libraries must be installed. The connector directly interprets the raw replication event stream into change events records.\n endif::product[]\n \n [[postgresql-limitations]]\n [IMPORTANT]\n ====\n-The connector's functionality relies on PostgreSQL's logical decoding feature.\n-Please be aware of the following limitations which are also reflected by the connector:\n+The connector relies on and reflects the PostgreSQL logical decoding feature, which has the following limitations:\n+\n+* Logical decoding does not support DDL changes. This means that the connector is unable to report DDL change events back to consumers.\n+* Logical decoding replication slots are supported on only `primary` servers. When there is a cluster of PostgreSQL servers, the connector can run on only the active `primary` server. It cannot run on `hot` or `warm` standby replicas. If the `primary` server fails or is demoted, the connector stops. After the `primary` server has recovered, you can restart the connector. If a different PostgreSQL server has been promoted to `primary`, adjust the connector configuration before restarting the connector. \n \n-. Logical Decoding does not support DDL changes: this means that the connector is unable to report DDL change events back to consumers.\n-. Logical Decoding replication slots are only supported on `primary` servers: this means that when there is a cluster of PostgreSQL servers, the connector can only run on the active `primary` server. It cannot run on `hot` or `warm` standby replicas. If the `primary` server fails or is demoted, the connector will stop. Once the `primary` has recovered the connector can simply be restarted. If a different PostgreSQL server has been promoted to `primary`, the connector configuration must be adjusted before the connector is restarted. Make sure you read more about how the connector behaves {link-prefix}:{link-postgresql-connector}#postgresql-when-things-go-wrong[when things go wrong].\n+{link-prefix}:{link-postgresql-connector}#postgresql-when-things-go-wrong[ Behavior when things go wrong] describes what the connector does when there is a problem. \n ====\n \n [IMPORTANT]\n ====\n-{prodname} currently supports only databases with UTF-8 character encoding.\n-With a single byte character encoding it is not possible to correctly process strings containing extended ASCII code characters.\n+{prodname} currently supports databases with UTF-8 character encoding only.\n+With a single byte character encoding, it is not possible to correctly process strings that contain extended ASCII code characters.\n ====\n \n-[[setting-up-postgresql]]\n-== Setting up PostgreSQL\n-\n-ifdef::product[]\n-This release of {prodname} only supports the native pgoutput logical replication stream.\n-To set up PostgreSQL using pgoutput, you will need to enable a replication slot, and configure a user with sufficient privileges to perform the replication.\n-\n-=== Configuring the replication slot\n-\n-PostgreSQL's logical decoding uses replication slots.\n-\n-First, you configure the replication slot:\n-\n-.postgresql.conf\n+// Type: assembly\n+// ModuleID: how-debezium-postgresql-connectors-work\n+// Title: How {prodname} PostgreSQL connectors work\n+[[how-the-postgresql-connector-works]]\n+== How the connector works\n \n-[source]\n-----\n-wal_level=logical\n-max_wal_senders=1\n-max_replication_slots=1\n-----\n+To optimally configure and run a {prodname} PostgreSQL connector, it is helpful to understand how the connector performs snapshots, streams change events, determines Kafka topic names, and uses metadata. \n \n-* `wal_level` tells the server to use logical decoding with the write-ahead log\n-* `max_wal_senders` tells the server to use a maximum of 1 separate processes for processing WAL changes\n-* `max_replication_slots` tells the server to allow a maximum of 1 replication slots to be created for streaming WAL changes\n+ifdef::product[]\n+Details are in the following topics: \n \n-Replication slots are guaranteed to retain all WAL required for {prodname} even during {prodname} outages.\n-It is important for this reason to closely monitor replication slots to avoid too much disk consumption and other conditions that can happen such as catalog bloat if a replication slot stays unused for too long.\n-For more information, refer to the https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION-SLOTS[the Postgres documentation].\n+* xref:how-debezium-postgresql-connectors-perform-database-snapshots[]\n+* xref:how-debezium-postgresql-connectors-stream-change-event-records[]\n+* xref:postgresql-10-logical-decoding-support-pgoutput[]\n+* xref:default-names-of-kafka-topics-that-receive-debezium-change-event-records[]\n+* xref:metadata-in-debezium-change-event-records[]\n+* xref:debezium-connector-generated-events-that-represent-transaction-boundaries[]\n \n-[NOTE]\n-====\n-We recommend reading and understanding the https://www.postgresql.org/docs/current/static/wal-configuration.html[WAL configuration documentation] regarding the mechanics and configuration of the PostgreSQL write-ahead log.\n-====\n endif::product[]\n \n-ifdef::community[]\n-Before using the PostgreSQL connector to monitor the changes committed on a PostgreSQL server,\n-first decide which logical decoder method you intend to use.\n-If you plan *not* to use the native pgoutput logical replication stream support,\n-then you will need to install the logical decoding plug-in into the PostgreSQL server.\n-Afterward enable a replication slot, and configure a user with sufficient privileges to perform the replication.\n-\n-Note that if your database is hosted by a service such as https://www.heroku.com/postgres[Heroku Postgres] you may be unable to install the plug-in.\n-If so, and if you're using PostgreSQL 10+, you can use the pgoutput decoder support to monitor your database.\n-If that is not an option, you'll be unable to monitor your database with {prodname}.\n-\n-[[postgresql-on-amazon-rds]]\n-=== PostgreSQL on Amazon RDS\n-\n-It is possible to monitor PostgreSQL database running in https://aws.amazon.com/rds/[Amazon RDS]. To get it running you must fulfill the following conditions\n-\n-* The instance parameter `rds.logical_replication` is set to `1`.\n-* Verify that `wal_level` parameter is set to `logical` by running the query `SHOW wal_level` as DB master user; this might not be the case in multi-zone replication setups.\n-You cannot set this option manually, it is (https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html[automatically changed]) when the `rds.logical_replication` is set to `1`.\n-If the `wal_level` is not `logical` after the change above, it is probably because the instance has to be restarted due to the parameter group change. This happens accordingly to your maintenance window or can be done manually.\n-* Set `plugin.name` {prodname} parameter to `wal2json`.  You can skip this on PostgreSQL 10+ if you wish to use pgoutput logical replication stream support.\n-* Use database master account for replication as RDS currently does not support setting of `REPLICATION` privilege for another account.\n-\n-[IMPORTANT]\n-====\n-You should make sure to use the latest versions of Postgres 9.6, 10 or 11 on Amazon RDS.\n-Otherwise, older versions of the wal2json plug-in may be installed\n-(see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html[the official documentation] for the exact wal2json versions installed on Amazon RDS).\n-In that case, replication messages received from the database may not carry complete information about type constraints like length or scale or `NULL`/`NOT NULL`,\n-which in turn might cause creation of messages with an inconsistent schema for a short period of time in case of changes to a column's definition.\n-\n-As of January 2019, the following Postgres versions on RDS come with an up-to-date version of wal2json and thus should be used:\n-\n-* Postgres 9.6: 9.6.10 and newer\n-* Postgres 10: 10.5 and newer\n-* Postgres 11: any version\n-====\n-\n-\n-[[postgresql-output-plugin]]\n-=== Installing the Logical Decoding Output Plug-in\n-\n-[TIP]\n-====\n-Also see {link-prefix}:{link-postgresql-plugins}[Logical Decoding Output Plug-in Installation for PostgreSQL] for more detailed instructions of setting up and testing logical decoding plug-ins.\n-====\n-\n-[NOTE]\n-====\n-As of {prodname} 0.10, the connector supports PostgreSQL 10+ logical replication streaming using _pgoutput_.\n-This means that a logical decoding output plug-in is no longer necessary and changes can be emitted directly from the replication stream by the connector.\n-====\n+// Type: concept\n+// ModuleID: how-debezium-postgresql-connectors-perform-database-snapshots\n+// Title: How {prodname} PostgreSQL connectors perform database snapshots\n+[[postgresql-snapshots]]\n+=== Snapshots\n \n-As of PostgreSQL 9.4, the only way to read changes to the write-ahead-log is to first install a logical decoding output plug-in. Plugins are written in C, compiled, and installed on the machine which runs the PostgreSQL server. Plugins use  a number of PostgreSQL specific APIs, as described by the https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_PostgreSQL documentation_].\n+Most PostgreSQL servers are configured to not retain the complete history of the database in the WAL segments. This means that the PostgreSQL connector would be unable to see the entire history of the database by reading only the WAL. Consequently, the first time that the connector starts, it performs an initial _consistent snapshot_ of the database. The default behavior for performing a snapshot consists of the following steps. You can change this behavior by setting the {link-prefix}:{link-postgresql-connector}#postgresql-property-snapshot-mode[`snapshot.mode` connector configuration property] to a value other than `initial`. \n \n-The PostgreSQL connector works with one of {prodname}'s supported logical decoding plug-ins to encode the changes in either https://github.com/google/protobuf[_Protobuf format_] or http://www.json.org/[_JSON_] format.\n-See the documentation of your chosen plug-in (https://github.com/debezium/postgres-decoderbufs/blob/master/README.md[_protobuf_], https://github.com/eulerto/wal2json/blob/master/README.md[_wal2json_]) to learn more about the plug-in's requirements, limitations, and how to compile it.\n+. Start a transaction with a link:https://www.postgresql.org/docs/current/static/sql-set-transaction.html[SERIALIZABLE, READ ONLY, DEFERRABLE] isolation level to ensure that subsequent reads in this transaction are against a single consistent version of the data. Any changes to the data due to subsequent `INSERT`, `UPDATE`, and `DELETE` operations by other clients are not visible to this transaction.\n+. Obtain an `ACCESS SHARE MODE` lock on each of the tables being tracked to ensure that no structural changes can occur to any of the tables while the snapshot is taking place. These locks do not prevent table `INSERT`, `UPDATE` and `DELETE` operations from taking place during the snapshot. \n++\n+_This step is omitted when `snapshot.mode` is set to `exported`, which allows the connector to perform a lock-free snapshot_.\n+. Read the current position in the server's transaction log.\n+. Scan the database tables and schemas, generate a `READ` event for each row and write that event to the appropriate table-specific Kafka topic.\n+. Commit the transaction.\n+. Record the successful completion of the snapshot in the connector offsets.\n \n-For simplicity, {prodname} also provides a Docker image based on a vanilla PostgreSQL server image on top of which it compiles and installs the plug-ins. We recommend https://github.com/debezium/docker-images/tree/master/postgres/9.6[_using this image_] as an example of the detailed steps required for the installation.\n+If the connector fails, is rebalanced, or stops after Step 1 begins but before Step 6 completes, upon restart the connector begins a new snapshot. After the connector completes its initial snapshot, the PostgreSQL connector continues streaming from the position that it read in step 3. This ensures that the connector does not miss any updates. If the connector stops again for any reason, upon restart, the connector continues streaming changes from where it previously left off.\n \n [WARNING]\n ====\n-The {prodname} logical decoding plug-ins have only been installed and tested on _Linux_ machines.\n-For Windows and other operating systems it may require different installation steps.\n-====\n-\n-[[postgresql-differences-between-plugins]]\n-==== Differences Between Plug-ins\n-\n-The plug-ins' behavior is not completely same for all cases.\n-So far these differences have been identified:\n-\n-* wal2json plug-in is not able to process quoted identifiers (https://github.com/eulerto/wal2json/issues/35[issue])\n-* wal2json and decoderbufs plug-ins emit events for tables without primary keys\n-* wal2json plug-in does not support special values (`NaN` or `infinity`) for floating point types\n-* wal2json should be used with setting the `schema.refresh.mode` connector option to `columns_diff_exclude_unchanged_toast`;\n-otherwise, when receiving a change event for a row containing an unchanged TOAST column, no field for that column is contained in the emitted change event's `after` structure.\n-This is because wal2json's messages do not contain a field for such a column.\n-\n-The requirement for adding this is tracked under the wal2json https://github.com/eulerto/wal2json/issues/98[issue 98].\n-See the documentation of `columns_diff_exclude_unchanged_toast` further below for implications of using it.\n-\n-* pgoutput plug-in does not emit all the events for tables without primary keys, it only emits inserts\n-\n-All up-to-date differences are tracked in a test suite https://github.com/debezium/debezium/blob/master/debezium-connector-postgres/src/test/java/io/debezium/connector/postgresql/DecoderDifferences.java[Java class].\n-\n-\n-[[postgresql-server-configuration]]\n-=== Configuring the PostgreSQL Server\n-\n-If you are using one of the supported {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-ins] (i.e. not pgoutput) and it has been installed,\n-configure the server to load the plug-in at startup:\n-\n-.postgresql.conf\n-[source,properties]\n-----\n-# MODULES\n-shared_preload_libraries = 'decoderbufs,wal2json' // <1>\n-----\n-<1> tells the server that it should load at startup the `decoderbufs` and `wal2json` logical decoding plug-ins (the names of the plug-ins are set in https://github.com/debezium/postgres-decoderbufs/blob/v0.3.0/Makefile[_Protobuf_] and https://github.com/eulerto/wal2json/blob/master/Makefile[_wal2json_] Makefiles)\n-\n-Next is to configure the replication slot regardless of the decoder being used:\n-\n-.postgresql.conf\n-[source,properties]\n-----\n-# REPLICATION\n-wal_level = logical             // <1>\n-max_wal_senders = 1             // <2>\n-max_replication_slots = 1       // <3>\n-----\n-<1> tells the server that it should use logical decoding with the write-ahead log\n-<2> tells the server that it should use a maximum of `1` separate processes for processing WAL changes\n-<3> tells the server that it should allow a maximum of `1` replication slots to be created for streaming WAL changes\n-\n-{prodname} uses PostgreSQL's logical decoding, which uses replication slots.\n-Replication slots are guaranteed to retain all WAL required for {prodname} even during {prodname} outages.\n-It is important for this reason to closely monitor replication slots to avoid too much disk consumption and other conditions that can happen such as catalog bloat if a replication slot stays unused for too long.\n-For more information please see the official Postgres docs on https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION-SLOTS[this subject].\n-\n-If you are working with a `synchronous_commit` setting other than `on`,\n-the recommendation is to set `wal_writer_delay` to a value such as 10 ms to achieve a low latency of change events.\n-Otherwise, its default value is applied, which adds a latency of about 200 ms.\n-\n-[TIP]\n-====\n-We strongly recommend reading and understanding https://www.postgresql.org/docs/current/static/wal-configuration.html[the official documentation] regarding the mechanics and configuration of the PostgreSQL write-ahead log.\n-====\n-endif::community[]\n-\n-[[postgresql-permissions]]\n-=== Setting up Permissions\n-\n-Next, configure a database user who can perform replications.\n-\n-Replication can only be performed by a database user that has appropriate permissions and only for a configured number of hosts.\n-\n-In order to give a user replication permissions, define a PostgreSQL role that has _at least_ the `REPLICATION` and `LOGIN` permissions. For example:\n-\n-[source,sql]\n-----\n-CREATE ROLE name REPLICATION LOGIN;\n-----\n-\n-[NOTE]\n-====\n-Superusers have by default both of the above roles.\n+It is strongly recommended that you configure a PostgreSQL connector to set `snapshot.mode` to `exported`. The `initial`, `initial only` and `always` modes can lose a few events while a connector switches from performing the snapshot to streaming change event records when a database is under heavy load.\n+This is a known issue and the affected snapshot modes will be reworked to use `exported` mode internally (link:https://issues.redhat.com/browse/DBZ-2337[DBZ-2337]).\n ====\n \n-Finally, configure the PostgreSQL server to allow replication to take place between the server machine and the host on which the PostgreSQL connector is running:\n-\n-.pg_hba.conf\n-[source]\n-----\n-local   replication     <youruser>                          trust   // <1>\n-host    replication     <youruser>  127.0.0.1/32            trust   // <2>\n-host    replication     <youruser>  ::1/128                 trust   // <3>\n-----\n-<1> Tells the server to allow replication for `<youruser>` locally (i.e. on the server machine)\n-<2> Tells the server to allow `<youruser>` on `localhost` to receive replication changes using `IPV4`\n-<3> Tells the server to allow `<youruser>` on `localhost` to receive replication changes using `IPV6`\n+[id=\"snapshot-mode-settings\"]\n+.Settings for `snapshot.mode` connector configuration property\n+[cols=\"20%a,80%a\",options=\"header\"]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM2OTQwMA=="}, "originalCommit": null, "originalPosition": 340}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUyODM2Ng==", "bodyText": "I believe this was a decision that came down when the MySQL documentation was first reworked.  We were told that the use of that was redundant and I've since stopped using it when adding new tables to the documentation.  If they should be retained, then we need to review existing documentation for all [cols= occurrences and make sure that the options block is restored.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462528366", "createdAt": "2020-07-29T19:14:47Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -12,356 +14,156 @@ ifdef::community[]\n toc::[]\n endif::community[]\n \n-{prodname}'s PostgreSQL Connector can monitor and record row-level changes in the schemas of a PostgreSQL database.\n+{prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 10, 11, and 12 are supported. \n \n-The first time it connects to a PostgreSQL server/cluster, it reads a consistent snapshot of all of the schemas. When that snapshot is complete, the connector continuously streams the changes that were committed to PostgreSQL 9.6 or later and generates corresponding insert, update and delete events. All of the events for each table are recorded in a separate Kafka topic, where they can be easily consumed by applications and services.\n+The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic. \n \n+ifdef::product[]\n+Information and procedures for using a {prodname} PostgreSQL connector is organized as follows: \n+\n+* xref:overview-of-debezium-postgresql-connector[]\n+* xref:how-debezium-postgresql-connectors-work[]\n+* xref:descriptions-of-debezium-postgresql-connector-data-change-events[]\n+* xref:how-debezium-postgresql-connectors-map-data-types[]\n+* xref:setting-up-postgresql-to-run-a-debezium-connector[]\n+* xref:deploying-and-managing-debezium-postgresql-connectors[]\n+* xref:how-debezium-postgresql-connectors-handle-faults-and-problems[]\n+\n+endif::product[]\n \n+// Type: concept\n+// Title: Overview of {prodname} PostgreSQL connector \n+// ModuleID: overview-of-debezium-postgresql-connector\n [[postgresql-overview]]\n == Overview\n \n-PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was first introduced in version 9.4 and is a mechanism which allows the extraction of the changes which were committed to the transaction log and the processing of these changes in a user-friendly manner via the help of an https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. This output plug-in must be installed prior to running the PostgreSQL server and enabled together with a replication slot in order for clients to be able to consume the changes.\n+PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was introduced in version 9.4. It is a mechanism that allows the extraction of the changes that were committed to the transaction log and the processing of these changes in a user-friendly manner with the help of an link:https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. You must install this output plug-in and configure a replication slot that uses the output plug-in before running the PostgreSQL server. This enables clients to consume the changes. \n \n-PostgreSQL connector contains two different parts which work together in order to be able to read and process server changes:\n+The PostgreSQL connector contains two main parts that work together to read and process server changes:\n \n ifdef::product[]\n-* A logical decoding output plug-in, which has to be installed and configured in the PostgreSQL server.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server.\n+* Java code (the actual Kafka Connect connector), which reads the changes produced by the plug-in by using PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_] and the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_].\n endif::product[]\n \n ifdef::community[]\n-* a logical decoding output plug-in which has to be installed and configured in the PostgreSQL server, one of\n-** https://github.com/debezium/postgres-decoderbufs[decoderbufs] (maintained by the {prodname} community, based on ProtoBuf)\n-** https://github.com/eulerto/wal2json[wal2json] (maintained by the wal2json community, based on JSON)\n-** pgoutput, the standard logical decoding plug-in in PostgreSQL 10+ (maintained by the Postgres community, used by Postgres itself for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]);\n-this plug-in is always present, meaning that no additional libraries must be installed,\n-and the {prodname} connector will interpret the raw replication event stream into change events directly.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the chosen plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server. The plug-in is one of these: \n+** link:https://github.com/debezium/postgres-decoderbufs[`decoderbufs`] - maintained by the {prodname} community, based on ProtoBuf.\n+** link:https://github.com/eulerto/wal2json[`wal2json`] - maintained by the wal2json community, based on JSON.\n+** `pgoutput`, which is the standard logical decoding plug-in in PostgreSQL 10+. It is maintained by the PostgreSQL community, and used by PostgreSQL itself for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. This plug-in is always present so no additional libraries need to be installed. The {prodname} connector interprets the raw replication event stream directly into change events.\n+* Java code (the actual Kafka Connect connector) that reads the changes produced by the chosen plug-in. It uses PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], by means of the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n endif::community[]\n \n-The connector then produces a _change event_ for every row-level insert, update, and delete operation that was received, recording all the change events for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables they're interested in following, and react to every row-level event it sees in those topics.\n+The connector produces a _change event_ for every row-level insert, update, and delete operation that was captured and sends change event records for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables of interest, and can react to every row-level event they receive from those topics.\n \n-PostgreSQL normally purges WAL segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, we start with a consistent view of all of the data, yet continue reading without having lost any of the changes made while the snapshot was taking place.\n+PostgreSQL normally purges write-ahead log (WAL) segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, the connector starts with a consistent view of all of the data, and does not omit any changes that were made while the snapshot was being taken.\n \n-The connector is also tolerant of failures. As the connector reads changes and produces events, it records the position in the write-ahead log with each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart it simply continues reading the WAL where it last left off. This includes snapshots: if the snapshot was not completed when the connector is stopped, upon restart it will begin a new snapshot.\n+The connector is tolerant of failures. As the connector reads changes and produces events, it records the position in the WAL for each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart the connector continues reading the WAL where it last left off. This includes snapshots. If the connector stops during a snapshot, the connector begins a new snapshot when it restarts. \n \n ifdef::product[]\n [[postgresql-output-plugin]]\n-=== Logical decoding output plug-in\n-The `pgoutput` logical decoder is the only supported logical decoder in the Tecnhology Preview release of {prodname}.\n-\n-`pgoutput`, the standard logical decoding plug-in in PostgreSQL 10+, is maintained by the Postgres community, and is also used by Postgres for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication].\n-The `pgoutput` plug-in is always present, meaning that no additional libraries must be installed,\n-and the connector will interpret the raw replication event stream into change events directly.\n+.Logical decoding output plug-in\n+The `pgoutput` logical decoding plug-in, the only supported logical decoding plug-in in this {prodname} release, is the standard logical decoding plug-in in PostgreSQL 10+, it is maintained by the PostgreSQL community, and it is also used by PostgreSQL for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. The `pgoutput` plug-in is always present, meaning that no additional libraries must be installed. The connector directly interprets the raw replication event stream into change events records.\n endif::product[]\n \n [[postgresql-limitations]]\n [IMPORTANT]\n ====\n-The connector's functionality relies on PostgreSQL's logical decoding feature.\n-Please be aware of the following limitations which are also reflected by the connector:\n+The connector relies on and reflects the PostgreSQL logical decoding feature, which has the following limitations:\n+\n+* Logical decoding does not support DDL changes. This means that the connector is unable to report DDL change events back to consumers.\n+* Logical decoding replication slots are supported on only `primary` servers. When there is a cluster of PostgreSQL servers, the connector can run on only the active `primary` server. It cannot run on `hot` or `warm` standby replicas. If the `primary` server fails or is demoted, the connector stops. After the `primary` server has recovered, you can restart the connector. If a different PostgreSQL server has been promoted to `primary`, adjust the connector configuration before restarting the connector. \n \n-. Logical Decoding does not support DDL changes: this means that the connector is unable to report DDL change events back to consumers.\n-. Logical Decoding replication slots are only supported on `primary` servers: this means that when there is a cluster of PostgreSQL servers, the connector can only run on the active `primary` server. It cannot run on `hot` or `warm` standby replicas. If the `primary` server fails or is demoted, the connector will stop. Once the `primary` has recovered the connector can simply be restarted. If a different PostgreSQL server has been promoted to `primary`, the connector configuration must be adjusted before the connector is restarted. Make sure you read more about how the connector behaves {link-prefix}:{link-postgresql-connector}#postgresql-when-things-go-wrong[when things go wrong].\n+{link-prefix}:{link-postgresql-connector}#postgresql-when-things-go-wrong[ Behavior when things go wrong] describes what the connector does when there is a problem. \n ====\n \n [IMPORTANT]\n ====\n-{prodname} currently supports only databases with UTF-8 character encoding.\n-With a single byte character encoding it is not possible to correctly process strings containing extended ASCII code characters.\n+{prodname} currently supports databases with UTF-8 character encoding only.\n+With a single byte character encoding, it is not possible to correctly process strings that contain extended ASCII code characters.\n ====\n \n-[[setting-up-postgresql]]\n-== Setting up PostgreSQL\n-\n-ifdef::product[]\n-This release of {prodname} only supports the native pgoutput logical replication stream.\n-To set up PostgreSQL using pgoutput, you will need to enable a replication slot, and configure a user with sufficient privileges to perform the replication.\n-\n-=== Configuring the replication slot\n-\n-PostgreSQL's logical decoding uses replication slots.\n-\n-First, you configure the replication slot:\n-\n-.postgresql.conf\n+// Type: assembly\n+// ModuleID: how-debezium-postgresql-connectors-work\n+// Title: How {prodname} PostgreSQL connectors work\n+[[how-the-postgresql-connector-works]]\n+== How the connector works\n \n-[source]\n-----\n-wal_level=logical\n-max_wal_senders=1\n-max_replication_slots=1\n-----\n+To optimally configure and run a {prodname} PostgreSQL connector, it is helpful to understand how the connector performs snapshots, streams change events, determines Kafka topic names, and uses metadata. \n \n-* `wal_level` tells the server to use logical decoding with the write-ahead log\n-* `max_wal_senders` tells the server to use a maximum of 1 separate processes for processing WAL changes\n-* `max_replication_slots` tells the server to allow a maximum of 1 replication slots to be created for streaming WAL changes\n+ifdef::product[]\n+Details are in the following topics: \n \n-Replication slots are guaranteed to retain all WAL required for {prodname} even during {prodname} outages.\n-It is important for this reason to closely monitor replication slots to avoid too much disk consumption and other conditions that can happen such as catalog bloat if a replication slot stays unused for too long.\n-For more information, refer to the https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION-SLOTS[the Postgres documentation].\n+* xref:how-debezium-postgresql-connectors-perform-database-snapshots[]\n+* xref:how-debezium-postgresql-connectors-stream-change-event-records[]\n+* xref:postgresql-10-logical-decoding-support-pgoutput[]\n+* xref:default-names-of-kafka-topics-that-receive-debezium-change-event-records[]\n+* xref:metadata-in-debezium-change-event-records[]\n+* xref:debezium-connector-generated-events-that-represent-transaction-boundaries[]\n \n-[NOTE]\n-====\n-We recommend reading and understanding the https://www.postgresql.org/docs/current/static/wal-configuration.html[WAL configuration documentation] regarding the mechanics and configuration of the PostgreSQL write-ahead log.\n-====\n endif::product[]\n \n-ifdef::community[]\n-Before using the PostgreSQL connector to monitor the changes committed on a PostgreSQL server,\n-first decide which logical decoder method you intend to use.\n-If you plan *not* to use the native pgoutput logical replication stream support,\n-then you will need to install the logical decoding plug-in into the PostgreSQL server.\n-Afterward enable a replication slot, and configure a user with sufficient privileges to perform the replication.\n-\n-Note that if your database is hosted by a service such as https://www.heroku.com/postgres[Heroku Postgres] you may be unable to install the plug-in.\n-If so, and if you're using PostgreSQL 10+, you can use the pgoutput decoder support to monitor your database.\n-If that is not an option, you'll be unable to monitor your database with {prodname}.\n-\n-[[postgresql-on-amazon-rds]]\n-=== PostgreSQL on Amazon RDS\n-\n-It is possible to monitor PostgreSQL database running in https://aws.amazon.com/rds/[Amazon RDS]. To get it running you must fulfill the following conditions\n-\n-* The instance parameter `rds.logical_replication` is set to `1`.\n-* Verify that `wal_level` parameter is set to `logical` by running the query `SHOW wal_level` as DB master user; this might not be the case in multi-zone replication setups.\n-You cannot set this option manually, it is (https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html[automatically changed]) when the `rds.logical_replication` is set to `1`.\n-If the `wal_level` is not `logical` after the change above, it is probably because the instance has to be restarted due to the parameter group change. This happens accordingly to your maintenance window or can be done manually.\n-* Set `plugin.name` {prodname} parameter to `wal2json`.  You can skip this on PostgreSQL 10+ if you wish to use pgoutput logical replication stream support.\n-* Use database master account for replication as RDS currently does not support setting of `REPLICATION` privilege for another account.\n-\n-[IMPORTANT]\n-====\n-You should make sure to use the latest versions of Postgres 9.6, 10 or 11 on Amazon RDS.\n-Otherwise, older versions of the wal2json plug-in may be installed\n-(see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html[the official documentation] for the exact wal2json versions installed on Amazon RDS).\n-In that case, replication messages received from the database may not carry complete information about type constraints like length or scale or `NULL`/`NOT NULL`,\n-which in turn might cause creation of messages with an inconsistent schema for a short period of time in case of changes to a column's definition.\n-\n-As of January 2019, the following Postgres versions on RDS come with an up-to-date version of wal2json and thus should be used:\n-\n-* Postgres 9.6: 9.6.10 and newer\n-* Postgres 10: 10.5 and newer\n-* Postgres 11: any version\n-====\n-\n-\n-[[postgresql-output-plugin]]\n-=== Installing the Logical Decoding Output Plug-in\n-\n-[TIP]\n-====\n-Also see {link-prefix}:{link-postgresql-plugins}[Logical Decoding Output Plug-in Installation for PostgreSQL] for more detailed instructions of setting up and testing logical decoding plug-ins.\n-====\n-\n-[NOTE]\n-====\n-As of {prodname} 0.10, the connector supports PostgreSQL 10+ logical replication streaming using _pgoutput_.\n-This means that a logical decoding output plug-in is no longer necessary and changes can be emitted directly from the replication stream by the connector.\n-====\n+// Type: concept\n+// ModuleID: how-debezium-postgresql-connectors-perform-database-snapshots\n+// Title: How {prodname} PostgreSQL connectors perform database snapshots\n+[[postgresql-snapshots]]\n+=== Snapshots\n \n-As of PostgreSQL 9.4, the only way to read changes to the write-ahead-log is to first install a logical decoding output plug-in. Plugins are written in C, compiled, and installed on the machine which runs the PostgreSQL server. Plugins use  a number of PostgreSQL specific APIs, as described by the https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_PostgreSQL documentation_].\n+Most PostgreSQL servers are configured to not retain the complete history of the database in the WAL segments. This means that the PostgreSQL connector would be unable to see the entire history of the database by reading only the WAL. Consequently, the first time that the connector starts, it performs an initial _consistent snapshot_ of the database. The default behavior for performing a snapshot consists of the following steps. You can change this behavior by setting the {link-prefix}:{link-postgresql-connector}#postgresql-property-snapshot-mode[`snapshot.mode` connector configuration property] to a value other than `initial`. \n \n-The PostgreSQL connector works with one of {prodname}'s supported logical decoding plug-ins to encode the changes in either https://github.com/google/protobuf[_Protobuf format_] or http://www.json.org/[_JSON_] format.\n-See the documentation of your chosen plug-in (https://github.com/debezium/postgres-decoderbufs/blob/master/README.md[_protobuf_], https://github.com/eulerto/wal2json/blob/master/README.md[_wal2json_]) to learn more about the plug-in's requirements, limitations, and how to compile it.\n+. Start a transaction with a link:https://www.postgresql.org/docs/current/static/sql-set-transaction.html[SERIALIZABLE, READ ONLY, DEFERRABLE] isolation level to ensure that subsequent reads in this transaction are against a single consistent version of the data. Any changes to the data due to subsequent `INSERT`, `UPDATE`, and `DELETE` operations by other clients are not visible to this transaction.\n+. Obtain an `ACCESS SHARE MODE` lock on each of the tables being tracked to ensure that no structural changes can occur to any of the tables while the snapshot is taking place. These locks do not prevent table `INSERT`, `UPDATE` and `DELETE` operations from taking place during the snapshot. \n++\n+_This step is omitted when `snapshot.mode` is set to `exported`, which allows the connector to perform a lock-free snapshot_.\n+. Read the current position in the server's transaction log.\n+. Scan the database tables and schemas, generate a `READ` event for each row and write that event to the appropriate table-specific Kafka topic.\n+. Commit the transaction.\n+. Record the successful completion of the snapshot in the connector offsets.\n \n-For simplicity, {prodname} also provides a Docker image based on a vanilla PostgreSQL server image on top of which it compiles and installs the plug-ins. We recommend https://github.com/debezium/docker-images/tree/master/postgres/9.6[_using this image_] as an example of the detailed steps required for the installation.\n+If the connector fails, is rebalanced, or stops after Step 1 begins but before Step 6 completes, upon restart the connector begins a new snapshot. After the connector completes its initial snapshot, the PostgreSQL connector continues streaming from the position that it read in step 3. This ensures that the connector does not miss any updates. If the connector stops again for any reason, upon restart, the connector continues streaming changes from where it previously left off.\n \n [WARNING]\n ====\n-The {prodname} logical decoding plug-ins have only been installed and tested on _Linux_ machines.\n-For Windows and other operating systems it may require different installation steps.\n-====\n-\n-[[postgresql-differences-between-plugins]]\n-==== Differences Between Plug-ins\n-\n-The plug-ins' behavior is not completely same for all cases.\n-So far these differences have been identified:\n-\n-* wal2json plug-in is not able to process quoted identifiers (https://github.com/eulerto/wal2json/issues/35[issue])\n-* wal2json and decoderbufs plug-ins emit events for tables without primary keys\n-* wal2json plug-in does not support special values (`NaN` or `infinity`) for floating point types\n-* wal2json should be used with setting the `schema.refresh.mode` connector option to `columns_diff_exclude_unchanged_toast`;\n-otherwise, when receiving a change event for a row containing an unchanged TOAST column, no field for that column is contained in the emitted change event's `after` structure.\n-This is because wal2json's messages do not contain a field for such a column.\n-\n-The requirement for adding this is tracked under the wal2json https://github.com/eulerto/wal2json/issues/98[issue 98].\n-See the documentation of `columns_diff_exclude_unchanged_toast` further below for implications of using it.\n-\n-* pgoutput plug-in does not emit all the events for tables without primary keys, it only emits inserts\n-\n-All up-to-date differences are tracked in a test suite https://github.com/debezium/debezium/blob/master/debezium-connector-postgres/src/test/java/io/debezium/connector/postgresql/DecoderDifferences.java[Java class].\n-\n-\n-[[postgresql-server-configuration]]\n-=== Configuring the PostgreSQL Server\n-\n-If you are using one of the supported {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-ins] (i.e. not pgoutput) and it has been installed,\n-configure the server to load the plug-in at startup:\n-\n-.postgresql.conf\n-[source,properties]\n-----\n-# MODULES\n-shared_preload_libraries = 'decoderbufs,wal2json' // <1>\n-----\n-<1> tells the server that it should load at startup the `decoderbufs` and `wal2json` logical decoding plug-ins (the names of the plug-ins are set in https://github.com/debezium/postgres-decoderbufs/blob/v0.3.0/Makefile[_Protobuf_] and https://github.com/eulerto/wal2json/blob/master/Makefile[_wal2json_] Makefiles)\n-\n-Next is to configure the replication slot regardless of the decoder being used:\n-\n-.postgresql.conf\n-[source,properties]\n-----\n-# REPLICATION\n-wal_level = logical             // <1>\n-max_wal_senders = 1             // <2>\n-max_replication_slots = 1       // <3>\n-----\n-<1> tells the server that it should use logical decoding with the write-ahead log\n-<2> tells the server that it should use a maximum of `1` separate processes for processing WAL changes\n-<3> tells the server that it should allow a maximum of `1` replication slots to be created for streaming WAL changes\n-\n-{prodname} uses PostgreSQL's logical decoding, which uses replication slots.\n-Replication slots are guaranteed to retain all WAL required for {prodname} even during {prodname} outages.\n-It is important for this reason to closely monitor replication slots to avoid too much disk consumption and other conditions that can happen such as catalog bloat if a replication slot stays unused for too long.\n-For more information please see the official Postgres docs on https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION-SLOTS[this subject].\n-\n-If you are working with a `synchronous_commit` setting other than `on`,\n-the recommendation is to set `wal_writer_delay` to a value such as 10 ms to achieve a low latency of change events.\n-Otherwise, its default value is applied, which adds a latency of about 200 ms.\n-\n-[TIP]\n-====\n-We strongly recommend reading and understanding https://www.postgresql.org/docs/current/static/wal-configuration.html[the official documentation] regarding the mechanics and configuration of the PostgreSQL write-ahead log.\n-====\n-endif::community[]\n-\n-[[postgresql-permissions]]\n-=== Setting up Permissions\n-\n-Next, configure a database user who can perform replications.\n-\n-Replication can only be performed by a database user that has appropriate permissions and only for a configured number of hosts.\n-\n-In order to give a user replication permissions, define a PostgreSQL role that has _at least_ the `REPLICATION` and `LOGIN` permissions. For example:\n-\n-[source,sql]\n-----\n-CREATE ROLE name REPLICATION LOGIN;\n-----\n-\n-[NOTE]\n-====\n-Superusers have by default both of the above roles.\n+It is strongly recommended that you configure a PostgreSQL connector to set `snapshot.mode` to `exported`. The `initial`, `initial only` and `always` modes can lose a few events while a connector switches from performing the snapshot to streaming change event records when a database is under heavy load.\n+This is a known issue and the affected snapshot modes will be reworked to use `exported` mode internally (link:https://issues.redhat.com/browse/DBZ-2337[DBZ-2337]).\n ====\n \n-Finally, configure the PostgreSQL server to allow replication to take place between the server machine and the host on which the PostgreSQL connector is running:\n-\n-.pg_hba.conf\n-[source]\n-----\n-local   replication     <youruser>                          trust   // <1>\n-host    replication     <youruser>  127.0.0.1/32            trust   // <2>\n-host    replication     <youruser>  ::1/128                 trust   // <3>\n-----\n-<1> Tells the server to allow replication for `<youruser>` locally (i.e. on the server machine)\n-<2> Tells the server to allow `<youruser>` on `localhost` to receive replication changes using `IPV4`\n-<3> Tells the server to allow `<youruser>` on `localhost` to receive replication changes using `IPV6`\n+[id=\"snapshot-mode-settings\"]\n+.Settings for `snapshot.mode` connector configuration property\n+[cols=\"20%a,80%a\",options=\"header\"]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM2OTQwMA=="}, "originalCommit": null, "originalPosition": 340}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjUyOTY4Ng==", "bodyText": "I can do that - Should I go ahead and create a DBZ Jira to update doc files so that all tables have the header option?", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r462529686", "createdAt": "2020-07-29T19:17:05Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -12,356 +14,156 @@ ifdef::community[]\n toc::[]\n endif::community[]\n \n-{prodname}'s PostgreSQL Connector can monitor and record row-level changes in the schemas of a PostgreSQL database.\n+{prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 10, 11, and 12 are supported. \n \n-The first time it connects to a PostgreSQL server/cluster, it reads a consistent snapshot of all of the schemas. When that snapshot is complete, the connector continuously streams the changes that were committed to PostgreSQL 9.6 or later and generates corresponding insert, update and delete events. All of the events for each table are recorded in a separate Kafka topic, where they can be easily consumed by applications and services.\n+The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic. \n \n+ifdef::product[]\n+Information and procedures for using a {prodname} PostgreSQL connector is organized as follows: \n+\n+* xref:overview-of-debezium-postgresql-connector[]\n+* xref:how-debezium-postgresql-connectors-work[]\n+* xref:descriptions-of-debezium-postgresql-connector-data-change-events[]\n+* xref:how-debezium-postgresql-connectors-map-data-types[]\n+* xref:setting-up-postgresql-to-run-a-debezium-connector[]\n+* xref:deploying-and-managing-debezium-postgresql-connectors[]\n+* xref:how-debezium-postgresql-connectors-handle-faults-and-problems[]\n+\n+endif::product[]\n \n+// Type: concept\n+// Title: Overview of {prodname} PostgreSQL connector \n+// ModuleID: overview-of-debezium-postgresql-connector\n [[postgresql-overview]]\n == Overview\n \n-PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was first introduced in version 9.4 and is a mechanism which allows the extraction of the changes which were committed to the transaction log and the processing of these changes in a user-friendly manner via the help of an https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. This output plug-in must be installed prior to running the PostgreSQL server and enabled together with a replication slot in order for clients to be able to consume the changes.\n+PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was introduced in version 9.4. It is a mechanism that allows the extraction of the changes that were committed to the transaction log and the processing of these changes in a user-friendly manner with the help of an link:https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. You must install this output plug-in and configure a replication slot that uses the output plug-in before running the PostgreSQL server. This enables clients to consume the changes. \n \n-PostgreSQL connector contains two different parts which work together in order to be able to read and process server changes:\n+The PostgreSQL connector contains two main parts that work together to read and process server changes:\n \n ifdef::product[]\n-* A logical decoding output plug-in, which has to be installed and configured in the PostgreSQL server.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server.\n+* Java code (the actual Kafka Connect connector), which reads the changes produced by the plug-in by using PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_] and the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_].\n endif::product[]\n \n ifdef::community[]\n-* a logical decoding output plug-in which has to be installed and configured in the PostgreSQL server, one of\n-** https://github.com/debezium/postgres-decoderbufs[decoderbufs] (maintained by the {prodname} community, based on ProtoBuf)\n-** https://github.com/eulerto/wal2json[wal2json] (maintained by the wal2json community, based on JSON)\n-** pgoutput, the standard logical decoding plug-in in PostgreSQL 10+ (maintained by the Postgres community, used by Postgres itself for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]);\n-this plug-in is always present, meaning that no additional libraries must be installed,\n-and the {prodname} connector will interpret the raw replication event stream into change events directly.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the chosen plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server. The plug-in is one of these: \n+** link:https://github.com/debezium/postgres-decoderbufs[`decoderbufs`] - maintained by the {prodname} community, based on ProtoBuf.\n+** link:https://github.com/eulerto/wal2json[`wal2json`] - maintained by the wal2json community, based on JSON.\n+** `pgoutput`, which is the standard logical decoding plug-in in PostgreSQL 10+. It is maintained by the PostgreSQL community, and used by PostgreSQL itself for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. This plug-in is always present so no additional libraries need to be installed. The {prodname} connector interprets the raw replication event stream directly into change events.\n+* Java code (the actual Kafka Connect connector) that reads the changes produced by the chosen plug-in. It uses PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], by means of the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n endif::community[]\n \n-The connector then produces a _change event_ for every row-level insert, update, and delete operation that was received, recording all the change events for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables they're interested in following, and react to every row-level event it sees in those topics.\n+The connector produces a _change event_ for every row-level insert, update, and delete operation that was captured and sends change event records for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables of interest, and can react to every row-level event they receive from those topics.\n \n-PostgreSQL normally purges WAL segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, we start with a consistent view of all of the data, yet continue reading without having lost any of the changes made while the snapshot was taking place.\n+PostgreSQL normally purges write-ahead log (WAL) segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, the connector starts with a consistent view of all of the data, and does not omit any changes that were made while the snapshot was being taken.\n \n-The connector is also tolerant of failures. As the connector reads changes and produces events, it records the position in the write-ahead log with each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart it simply continues reading the WAL where it last left off. This includes snapshots: if the snapshot was not completed when the connector is stopped, upon restart it will begin a new snapshot.\n+The connector is tolerant of failures. As the connector reads changes and produces events, it records the position in the WAL for each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart the connector continues reading the WAL where it last left off. This includes snapshots. If the connector stops during a snapshot, the connector begins a new snapshot when it restarts. \n \n ifdef::product[]\n [[postgresql-output-plugin]]\n-=== Logical decoding output plug-in\n-The `pgoutput` logical decoder is the only supported logical decoder in the Tecnhology Preview release of {prodname}.\n-\n-`pgoutput`, the standard logical decoding plug-in in PostgreSQL 10+, is maintained by the Postgres community, and is also used by Postgres for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication].\n-The `pgoutput` plug-in is always present, meaning that no additional libraries must be installed,\n-and the connector will interpret the raw replication event stream into change events directly.\n+.Logical decoding output plug-in\n+The `pgoutput` logical decoding plug-in, the only supported logical decoding plug-in in this {prodname} release, is the standard logical decoding plug-in in PostgreSQL 10+, it is maintained by the PostgreSQL community, and it is also used by PostgreSQL for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. The `pgoutput` plug-in is always present, meaning that no additional libraries must be installed. The connector directly interprets the raw replication event stream into change events records.\n endif::product[]\n \n [[postgresql-limitations]]\n [IMPORTANT]\n ====\n-The connector's functionality relies on PostgreSQL's logical decoding feature.\n-Please be aware of the following limitations which are also reflected by the connector:\n+The connector relies on and reflects the PostgreSQL logical decoding feature, which has the following limitations:\n+\n+* Logical decoding does not support DDL changes. This means that the connector is unable to report DDL change events back to consumers.\n+* Logical decoding replication slots are supported on only `primary` servers. When there is a cluster of PostgreSQL servers, the connector can run on only the active `primary` server. It cannot run on `hot` or `warm` standby replicas. If the `primary` server fails or is demoted, the connector stops. After the `primary` server has recovered, you can restart the connector. If a different PostgreSQL server has been promoted to `primary`, adjust the connector configuration before restarting the connector. \n \n-. Logical Decoding does not support DDL changes: this means that the connector is unable to report DDL change events back to consumers.\n-. Logical Decoding replication slots are only supported on `primary` servers: this means that when there is a cluster of PostgreSQL servers, the connector can only run on the active `primary` server. It cannot run on `hot` or `warm` standby replicas. If the `primary` server fails or is demoted, the connector will stop. Once the `primary` has recovered the connector can simply be restarted. If a different PostgreSQL server has been promoted to `primary`, the connector configuration must be adjusted before the connector is restarted. Make sure you read more about how the connector behaves {link-prefix}:{link-postgresql-connector}#postgresql-when-things-go-wrong[when things go wrong].\n+{link-prefix}:{link-postgresql-connector}#postgresql-when-things-go-wrong[ Behavior when things go wrong] describes what the connector does when there is a problem. \n ====\n \n [IMPORTANT]\n ====\n-{prodname} currently supports only databases with UTF-8 character encoding.\n-With a single byte character encoding it is not possible to correctly process strings containing extended ASCII code characters.\n+{prodname} currently supports databases with UTF-8 character encoding only.\n+With a single byte character encoding, it is not possible to correctly process strings that contain extended ASCII code characters.\n ====\n \n-[[setting-up-postgresql]]\n-== Setting up PostgreSQL\n-\n-ifdef::product[]\n-This release of {prodname} only supports the native pgoutput logical replication stream.\n-To set up PostgreSQL using pgoutput, you will need to enable a replication slot, and configure a user with sufficient privileges to perform the replication.\n-\n-=== Configuring the replication slot\n-\n-PostgreSQL's logical decoding uses replication slots.\n-\n-First, you configure the replication slot:\n-\n-.postgresql.conf\n+// Type: assembly\n+// ModuleID: how-debezium-postgresql-connectors-work\n+// Title: How {prodname} PostgreSQL connectors work\n+[[how-the-postgresql-connector-works]]\n+== How the connector works\n \n-[source]\n-----\n-wal_level=logical\n-max_wal_senders=1\n-max_replication_slots=1\n-----\n+To optimally configure and run a {prodname} PostgreSQL connector, it is helpful to understand how the connector performs snapshots, streams change events, determines Kafka topic names, and uses metadata. \n \n-* `wal_level` tells the server to use logical decoding with the write-ahead log\n-* `max_wal_senders` tells the server to use a maximum of 1 separate processes for processing WAL changes\n-* `max_replication_slots` tells the server to allow a maximum of 1 replication slots to be created for streaming WAL changes\n+ifdef::product[]\n+Details are in the following topics: \n \n-Replication slots are guaranteed to retain all WAL required for {prodname} even during {prodname} outages.\n-It is important for this reason to closely monitor replication slots to avoid too much disk consumption and other conditions that can happen such as catalog bloat if a replication slot stays unused for too long.\n-For more information, refer to the https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION-SLOTS[the Postgres documentation].\n+* xref:how-debezium-postgresql-connectors-perform-database-snapshots[]\n+* xref:how-debezium-postgresql-connectors-stream-change-event-records[]\n+* xref:postgresql-10-logical-decoding-support-pgoutput[]\n+* xref:default-names-of-kafka-topics-that-receive-debezium-change-event-records[]\n+* xref:metadata-in-debezium-change-event-records[]\n+* xref:debezium-connector-generated-events-that-represent-transaction-boundaries[]\n \n-[NOTE]\n-====\n-We recommend reading and understanding the https://www.postgresql.org/docs/current/static/wal-configuration.html[WAL configuration documentation] regarding the mechanics and configuration of the PostgreSQL write-ahead log.\n-====\n endif::product[]\n \n-ifdef::community[]\n-Before using the PostgreSQL connector to monitor the changes committed on a PostgreSQL server,\n-first decide which logical decoder method you intend to use.\n-If you plan *not* to use the native pgoutput logical replication stream support,\n-then you will need to install the logical decoding plug-in into the PostgreSQL server.\n-Afterward enable a replication slot, and configure a user with sufficient privileges to perform the replication.\n-\n-Note that if your database is hosted by a service such as https://www.heroku.com/postgres[Heroku Postgres] you may be unable to install the plug-in.\n-If so, and if you're using PostgreSQL 10+, you can use the pgoutput decoder support to monitor your database.\n-If that is not an option, you'll be unable to monitor your database with {prodname}.\n-\n-[[postgresql-on-amazon-rds]]\n-=== PostgreSQL on Amazon RDS\n-\n-It is possible to monitor PostgreSQL database running in https://aws.amazon.com/rds/[Amazon RDS]. To get it running you must fulfill the following conditions\n-\n-* The instance parameter `rds.logical_replication` is set to `1`.\n-* Verify that `wal_level` parameter is set to `logical` by running the query `SHOW wal_level` as DB master user; this might not be the case in multi-zone replication setups.\n-You cannot set this option manually, it is (https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html[automatically changed]) when the `rds.logical_replication` is set to `1`.\n-If the `wal_level` is not `logical` after the change above, it is probably because the instance has to be restarted due to the parameter group change. This happens accordingly to your maintenance window or can be done manually.\n-* Set `plugin.name` {prodname} parameter to `wal2json`.  You can skip this on PostgreSQL 10+ if you wish to use pgoutput logical replication stream support.\n-* Use database master account for replication as RDS currently does not support setting of `REPLICATION` privilege for another account.\n-\n-[IMPORTANT]\n-====\n-You should make sure to use the latest versions of Postgres 9.6, 10 or 11 on Amazon RDS.\n-Otherwise, older versions of the wal2json plug-in may be installed\n-(see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html[the official documentation] for the exact wal2json versions installed on Amazon RDS).\n-In that case, replication messages received from the database may not carry complete information about type constraints like length or scale or `NULL`/`NOT NULL`,\n-which in turn might cause creation of messages with an inconsistent schema for a short period of time in case of changes to a column's definition.\n-\n-As of January 2019, the following Postgres versions on RDS come with an up-to-date version of wal2json and thus should be used:\n-\n-* Postgres 9.6: 9.6.10 and newer\n-* Postgres 10: 10.5 and newer\n-* Postgres 11: any version\n-====\n-\n-\n-[[postgresql-output-plugin]]\n-=== Installing the Logical Decoding Output Plug-in\n-\n-[TIP]\n-====\n-Also see {link-prefix}:{link-postgresql-plugins}[Logical Decoding Output Plug-in Installation for PostgreSQL] for more detailed instructions of setting up and testing logical decoding plug-ins.\n-====\n-\n-[NOTE]\n-====\n-As of {prodname} 0.10, the connector supports PostgreSQL 10+ logical replication streaming using _pgoutput_.\n-This means that a logical decoding output plug-in is no longer necessary and changes can be emitted directly from the replication stream by the connector.\n-====\n+// Type: concept\n+// ModuleID: how-debezium-postgresql-connectors-perform-database-snapshots\n+// Title: How {prodname} PostgreSQL connectors perform database snapshots\n+[[postgresql-snapshots]]\n+=== Snapshots\n \n-As of PostgreSQL 9.4, the only way to read changes to the write-ahead-log is to first install a logical decoding output plug-in. Plugins are written in C, compiled, and installed on the machine which runs the PostgreSQL server. Plugins use  a number of PostgreSQL specific APIs, as described by the https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_PostgreSQL documentation_].\n+Most PostgreSQL servers are configured to not retain the complete history of the database in the WAL segments. This means that the PostgreSQL connector would be unable to see the entire history of the database by reading only the WAL. Consequently, the first time that the connector starts, it performs an initial _consistent snapshot_ of the database. The default behavior for performing a snapshot consists of the following steps. You can change this behavior by setting the {link-prefix}:{link-postgresql-connector}#postgresql-property-snapshot-mode[`snapshot.mode` connector configuration property] to a value other than `initial`. \n \n-The PostgreSQL connector works with one of {prodname}'s supported logical decoding plug-ins to encode the changes in either https://github.com/google/protobuf[_Protobuf format_] or http://www.json.org/[_JSON_] format.\n-See the documentation of your chosen plug-in (https://github.com/debezium/postgres-decoderbufs/blob/master/README.md[_protobuf_], https://github.com/eulerto/wal2json/blob/master/README.md[_wal2json_]) to learn more about the plug-in's requirements, limitations, and how to compile it.\n+. Start a transaction with a link:https://www.postgresql.org/docs/current/static/sql-set-transaction.html[SERIALIZABLE, READ ONLY, DEFERRABLE] isolation level to ensure that subsequent reads in this transaction are against a single consistent version of the data. Any changes to the data due to subsequent `INSERT`, `UPDATE`, and `DELETE` operations by other clients are not visible to this transaction.\n+. Obtain an `ACCESS SHARE MODE` lock on each of the tables being tracked to ensure that no structural changes can occur to any of the tables while the snapshot is taking place. These locks do not prevent table `INSERT`, `UPDATE` and `DELETE` operations from taking place during the snapshot. \n++\n+_This step is omitted when `snapshot.mode` is set to `exported`, which allows the connector to perform a lock-free snapshot_.\n+. Read the current position in the server's transaction log.\n+. Scan the database tables and schemas, generate a `READ` event for each row and write that event to the appropriate table-specific Kafka topic.\n+. Commit the transaction.\n+. Record the successful completion of the snapshot in the connector offsets.\n \n-For simplicity, {prodname} also provides a Docker image based on a vanilla PostgreSQL server image on top of which it compiles and installs the plug-ins. We recommend https://github.com/debezium/docker-images/tree/master/postgres/9.6[_using this image_] as an example of the detailed steps required for the installation.\n+If the connector fails, is rebalanced, or stops after Step 1 begins but before Step 6 completes, upon restart the connector begins a new snapshot. After the connector completes its initial snapshot, the PostgreSQL connector continues streaming from the position that it read in step 3. This ensures that the connector does not miss any updates. If the connector stops again for any reason, upon restart, the connector continues streaming changes from where it previously left off.\n \n [WARNING]\n ====\n-The {prodname} logical decoding plug-ins have only been installed and tested on _Linux_ machines.\n-For Windows and other operating systems it may require different installation steps.\n-====\n-\n-[[postgresql-differences-between-plugins]]\n-==== Differences Between Plug-ins\n-\n-The plug-ins' behavior is not completely same for all cases.\n-So far these differences have been identified:\n-\n-* wal2json plug-in is not able to process quoted identifiers (https://github.com/eulerto/wal2json/issues/35[issue])\n-* wal2json and decoderbufs plug-ins emit events for tables without primary keys\n-* wal2json plug-in does not support special values (`NaN` or `infinity`) for floating point types\n-* wal2json should be used with setting the `schema.refresh.mode` connector option to `columns_diff_exclude_unchanged_toast`;\n-otherwise, when receiving a change event for a row containing an unchanged TOAST column, no field for that column is contained in the emitted change event's `after` structure.\n-This is because wal2json's messages do not contain a field for such a column.\n-\n-The requirement for adding this is tracked under the wal2json https://github.com/eulerto/wal2json/issues/98[issue 98].\n-See the documentation of `columns_diff_exclude_unchanged_toast` further below for implications of using it.\n-\n-* pgoutput plug-in does not emit all the events for tables without primary keys, it only emits inserts\n-\n-All up-to-date differences are tracked in a test suite https://github.com/debezium/debezium/blob/master/debezium-connector-postgres/src/test/java/io/debezium/connector/postgresql/DecoderDifferences.java[Java class].\n-\n-\n-[[postgresql-server-configuration]]\n-=== Configuring the PostgreSQL Server\n-\n-If you are using one of the supported {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-ins] (i.e. not pgoutput) and it has been installed,\n-configure the server to load the plug-in at startup:\n-\n-.postgresql.conf\n-[source,properties]\n-----\n-# MODULES\n-shared_preload_libraries = 'decoderbufs,wal2json' // <1>\n-----\n-<1> tells the server that it should load at startup the `decoderbufs` and `wal2json` logical decoding plug-ins (the names of the plug-ins are set in https://github.com/debezium/postgres-decoderbufs/blob/v0.3.0/Makefile[_Protobuf_] and https://github.com/eulerto/wal2json/blob/master/Makefile[_wal2json_] Makefiles)\n-\n-Next is to configure the replication slot regardless of the decoder being used:\n-\n-.postgresql.conf\n-[source,properties]\n-----\n-# REPLICATION\n-wal_level = logical             // <1>\n-max_wal_senders = 1             // <2>\n-max_replication_slots = 1       // <3>\n-----\n-<1> tells the server that it should use logical decoding with the write-ahead log\n-<2> tells the server that it should use a maximum of `1` separate processes for processing WAL changes\n-<3> tells the server that it should allow a maximum of `1` replication slots to be created for streaming WAL changes\n-\n-{prodname} uses PostgreSQL's logical decoding, which uses replication slots.\n-Replication slots are guaranteed to retain all WAL required for {prodname} even during {prodname} outages.\n-It is important for this reason to closely monitor replication slots to avoid too much disk consumption and other conditions that can happen such as catalog bloat if a replication slot stays unused for too long.\n-For more information please see the official Postgres docs on https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION-SLOTS[this subject].\n-\n-If you are working with a `synchronous_commit` setting other than `on`,\n-the recommendation is to set `wal_writer_delay` to a value such as 10 ms to achieve a low latency of change events.\n-Otherwise, its default value is applied, which adds a latency of about 200 ms.\n-\n-[TIP]\n-====\n-We strongly recommend reading and understanding https://www.postgresql.org/docs/current/static/wal-configuration.html[the official documentation] regarding the mechanics and configuration of the PostgreSQL write-ahead log.\n-====\n-endif::community[]\n-\n-[[postgresql-permissions]]\n-=== Setting up Permissions\n-\n-Next, configure a database user who can perform replications.\n-\n-Replication can only be performed by a database user that has appropriate permissions and only for a configured number of hosts.\n-\n-In order to give a user replication permissions, define a PostgreSQL role that has _at least_ the `REPLICATION` and `LOGIN` permissions. For example:\n-\n-[source,sql]\n-----\n-CREATE ROLE name REPLICATION LOGIN;\n-----\n-\n-[NOTE]\n-====\n-Superusers have by default both of the above roles.\n+It is strongly recommended that you configure a PostgreSQL connector to set `snapshot.mode` to `exported`. The `initial`, `initial only` and `always` modes can lose a few events while a connector switches from performing the snapshot to streaming change event records when a database is under heavy load.\n+This is a known issue and the affected snapshot modes will be reworked to use `exported` mode internally (link:https://issues.redhat.com/browse/DBZ-2337[DBZ-2337]).\n ====\n \n-Finally, configure the PostgreSQL server to allow replication to take place between the server machine and the host on which the PostgreSQL connector is running:\n-\n-.pg_hba.conf\n-[source]\n-----\n-local   replication     <youruser>                          trust   // <1>\n-host    replication     <youruser>  127.0.0.1/32            trust   // <2>\n-host    replication     <youruser>  ::1/128                 trust   // <3>\n-----\n-<1> Tells the server to allow replication for `<youruser>` locally (i.e. on the server machine)\n-<2> Tells the server to allow `<youruser>` on `localhost` to receive replication changes using `IPV4`\n-<3> Tells the server to allow `<youruser>` on `localhost` to receive replication changes using `IPV6`\n+[id=\"snapshot-mode-settings\"]\n+.Settings for `snapshot.mode` connector configuration property\n+[cols=\"20%a,80%a\",options=\"header\"]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM2OTQwMA=="}, "originalCommit": null, "originalPosition": 340}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzY0MzgzMg==", "bodyText": "Sure; especially if its critical for downstream in someway.  It's something we can also work toward over time too if its not something that necessarily needs to all be done at once.", "url": "https://github.com/debezium/debezium/pull/1720#discussion_r463643832", "createdAt": "2020-07-31T14:27:36Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -12,356 +14,156 @@ ifdef::community[]\n toc::[]\n endif::community[]\n \n-{prodname}'s PostgreSQL Connector can monitor and record row-level changes in the schemas of a PostgreSQL database.\n+{prodname}'s PostgreSQL connector captures row-level changes in the schemas of a PostgreSQL database. PostgreSQL versions 10, 11, and 12 are supported. \n \n-The first time it connects to a PostgreSQL server/cluster, it reads a consistent snapshot of all of the schemas. When that snapshot is complete, the connector continuously streams the changes that were committed to PostgreSQL 9.6 or later and generates corresponding insert, update and delete events. All of the events for each table are recorded in a separate Kafka topic, where they can be easily consumed by applications and services.\n+The first time it connects to a PostgreSQL server or cluster, the connector takes a consistent snapshot of all schemas. After that snapshot is complete, the connector continuously captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database. The connector generates data change event records and streams them to Kafka topics. For each table, the default behavior is that the connector streams all generated events to a separate Kafka topic for that table. Applications and services consume data change event records from that topic. \n \n+ifdef::product[]\n+Information and procedures for using a {prodname} PostgreSQL connector is organized as follows: \n+\n+* xref:overview-of-debezium-postgresql-connector[]\n+* xref:how-debezium-postgresql-connectors-work[]\n+* xref:descriptions-of-debezium-postgresql-connector-data-change-events[]\n+* xref:how-debezium-postgresql-connectors-map-data-types[]\n+* xref:setting-up-postgresql-to-run-a-debezium-connector[]\n+* xref:deploying-and-managing-debezium-postgresql-connectors[]\n+* xref:how-debezium-postgresql-connectors-handle-faults-and-problems[]\n+\n+endif::product[]\n \n+// Type: concept\n+// Title: Overview of {prodname} PostgreSQL connector \n+// ModuleID: overview-of-debezium-postgresql-connector\n [[postgresql-overview]]\n == Overview\n \n-PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was first introduced in version 9.4 and is a mechanism which allows the extraction of the changes which were committed to the transaction log and the processing of these changes in a user-friendly manner via the help of an https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. This output plug-in must be installed prior to running the PostgreSQL server and enabled together with a replication slot in order for clients to be able to consume the changes.\n+PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-explanation.html[_logical decoding_] feature was introduced in version 9.4. It is a mechanism that allows the extraction of the changes that were committed to the transaction log and the processing of these changes in a user-friendly manner with the help of an link:https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_output plug-in_]. You must install this output plug-in and configure a replication slot that uses the output plug-in before running the PostgreSQL server. This enables clients to consume the changes. \n \n-PostgreSQL connector contains two different parts which work together in order to be able to read and process server changes:\n+The PostgreSQL connector contains two main parts that work together to read and process server changes:\n \n ifdef::product[]\n-* A logical decoding output plug-in, which has to be installed and configured in the PostgreSQL server.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server.\n+* Java code (the actual Kafka Connect connector), which reads the changes produced by the plug-in by using PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_] and the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_].\n endif::product[]\n \n ifdef::community[]\n-* a logical decoding output plug-in which has to be installed and configured in the PostgreSQL server, one of\n-** https://github.com/debezium/postgres-decoderbufs[decoderbufs] (maintained by the {prodname} community, based on ProtoBuf)\n-** https://github.com/eulerto/wal2json[wal2json] (maintained by the wal2json community, based on JSON)\n-** pgoutput, the standard logical decoding plug-in in PostgreSQL 10+ (maintained by the Postgres community, used by Postgres itself for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]);\n-this plug-in is always present, meaning that no additional libraries must be installed,\n-and the {prodname} connector will interpret the raw replication event stream into change events directly.\n-* Java code (the actual Kafka Connect connector) which reads the changes produced by the chosen plug-in, using PostgreSQL's https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], via the PostgreSQL https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n+* A logical decoding output plug-in, which must be installed and configured in the PostgreSQL server. The plug-in is one of these: \n+** link:https://github.com/debezium/postgres-decoderbufs[`decoderbufs`] - maintained by the {prodname} community, based on ProtoBuf.\n+** link:https://github.com/eulerto/wal2json[`wal2json`] - maintained by the wal2json community, based on JSON.\n+** `pgoutput`, which is the standard logical decoding plug-in in PostgreSQL 10+. It is maintained by the PostgreSQL community, and used by PostgreSQL itself for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. This plug-in is always present so no additional libraries need to be installed. The {prodname} connector interprets the raw replication event stream directly into change events.\n+* Java code (the actual Kafka Connect connector) that reads the changes produced by the chosen plug-in. It uses PostgreSQL's link:https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html[_streaming replication protocol_], by means of the PostgreSQL link:https://github.com/pgjdbc/pgjdbc[_JDBC driver_]\n endif::community[]\n \n-The connector then produces a _change event_ for every row-level insert, update, and delete operation that was received, recording all the change events for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables they're interested in following, and react to every row-level event it sees in those topics.\n+The connector produces a _change event_ for every row-level insert, update, and delete operation that was captured and sends change event records for each table in a separate Kafka topic. Your client applications read the Kafka topics that correspond to the database tables of interest, and can react to every row-level event they receive from those topics.\n \n-PostgreSQL normally purges WAL segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, we start with a consistent view of all of the data, yet continue reading without having lost any of the changes made while the snapshot was taking place.\n+PostgreSQL normally purges write-ahead log (WAL) segments after some period of time. This means that the connector does not have the complete history of all changes that have been made to the database. Therefore, when the PostgreSQL connector first connects to a particular PostgreSQL database, it starts by performing a _consistent snapshot_ of each of the database schemas. After the connector completes the snapshot, it continues streaming changes from the exact point at which the snapshot was made. This way, the connector starts with a consistent view of all of the data, and does not omit any changes that were made while the snapshot was being taken.\n \n-The connector is also tolerant of failures. As the connector reads changes and produces events, it records the position in the write-ahead log with each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart it simply continues reading the WAL where it last left off. This includes snapshots: if the snapshot was not completed when the connector is stopped, upon restart it will begin a new snapshot.\n+The connector is tolerant of failures. As the connector reads changes and produces events, it records the position in the WAL for each event. If the connector stops for any reason (including communication failures, network problems, or crashes), upon restart the connector continues reading the WAL where it last left off. This includes snapshots. If the connector stops during a snapshot, the connector begins a new snapshot when it restarts. \n \n ifdef::product[]\n [[postgresql-output-plugin]]\n-=== Logical decoding output plug-in\n-The `pgoutput` logical decoder is the only supported logical decoder in the Tecnhology Preview release of {prodname}.\n-\n-`pgoutput`, the standard logical decoding plug-in in PostgreSQL 10+, is maintained by the Postgres community, and is also used by Postgres for https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication].\n-The `pgoutput` plug-in is always present, meaning that no additional libraries must be installed,\n-and the connector will interpret the raw replication event stream into change events directly.\n+.Logical decoding output plug-in\n+The `pgoutput` logical decoding plug-in, the only supported logical decoding plug-in in this {prodname} release, is the standard logical decoding plug-in in PostgreSQL 10+, it is maintained by the PostgreSQL community, and it is also used by PostgreSQL for link:https://www.postgresql.org/docs/current/logical-replication-architecture.html[logical replication]. The `pgoutput` plug-in is always present, meaning that no additional libraries must be installed. The connector directly interprets the raw replication event stream into change events records.\n endif::product[]\n \n [[postgresql-limitations]]\n [IMPORTANT]\n ====\n-The connector's functionality relies on PostgreSQL's logical decoding feature.\n-Please be aware of the following limitations which are also reflected by the connector:\n+The connector relies on and reflects the PostgreSQL logical decoding feature, which has the following limitations:\n+\n+* Logical decoding does not support DDL changes. This means that the connector is unable to report DDL change events back to consumers.\n+* Logical decoding replication slots are supported on only `primary` servers. When there is a cluster of PostgreSQL servers, the connector can run on only the active `primary` server. It cannot run on `hot` or `warm` standby replicas. If the `primary` server fails or is demoted, the connector stops. After the `primary` server has recovered, you can restart the connector. If a different PostgreSQL server has been promoted to `primary`, adjust the connector configuration before restarting the connector. \n \n-. Logical Decoding does not support DDL changes: this means that the connector is unable to report DDL change events back to consumers.\n-. Logical Decoding replication slots are only supported on `primary` servers: this means that when there is a cluster of PostgreSQL servers, the connector can only run on the active `primary` server. It cannot run on `hot` or `warm` standby replicas. If the `primary` server fails or is demoted, the connector will stop. Once the `primary` has recovered the connector can simply be restarted. If a different PostgreSQL server has been promoted to `primary`, the connector configuration must be adjusted before the connector is restarted. Make sure you read more about how the connector behaves {link-prefix}:{link-postgresql-connector}#postgresql-when-things-go-wrong[when things go wrong].\n+{link-prefix}:{link-postgresql-connector}#postgresql-when-things-go-wrong[ Behavior when things go wrong] describes what the connector does when there is a problem. \n ====\n \n [IMPORTANT]\n ====\n-{prodname} currently supports only databases with UTF-8 character encoding.\n-With a single byte character encoding it is not possible to correctly process strings containing extended ASCII code characters.\n+{prodname} currently supports databases with UTF-8 character encoding only.\n+With a single byte character encoding, it is not possible to correctly process strings that contain extended ASCII code characters.\n ====\n \n-[[setting-up-postgresql]]\n-== Setting up PostgreSQL\n-\n-ifdef::product[]\n-This release of {prodname} only supports the native pgoutput logical replication stream.\n-To set up PostgreSQL using pgoutput, you will need to enable a replication slot, and configure a user with sufficient privileges to perform the replication.\n-\n-=== Configuring the replication slot\n-\n-PostgreSQL's logical decoding uses replication slots.\n-\n-First, you configure the replication slot:\n-\n-.postgresql.conf\n+// Type: assembly\n+// ModuleID: how-debezium-postgresql-connectors-work\n+// Title: How {prodname} PostgreSQL connectors work\n+[[how-the-postgresql-connector-works]]\n+== How the connector works\n \n-[source]\n-----\n-wal_level=logical\n-max_wal_senders=1\n-max_replication_slots=1\n-----\n+To optimally configure and run a {prodname} PostgreSQL connector, it is helpful to understand how the connector performs snapshots, streams change events, determines Kafka topic names, and uses metadata. \n \n-* `wal_level` tells the server to use logical decoding with the write-ahead log\n-* `max_wal_senders` tells the server to use a maximum of 1 separate processes for processing WAL changes\n-* `max_replication_slots` tells the server to allow a maximum of 1 replication slots to be created for streaming WAL changes\n+ifdef::product[]\n+Details are in the following topics: \n \n-Replication slots are guaranteed to retain all WAL required for {prodname} even during {prodname} outages.\n-It is important for this reason to closely monitor replication slots to avoid too much disk consumption and other conditions that can happen such as catalog bloat if a replication slot stays unused for too long.\n-For more information, refer to the https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION-SLOTS[the Postgres documentation].\n+* xref:how-debezium-postgresql-connectors-perform-database-snapshots[]\n+* xref:how-debezium-postgresql-connectors-stream-change-event-records[]\n+* xref:postgresql-10-logical-decoding-support-pgoutput[]\n+* xref:default-names-of-kafka-topics-that-receive-debezium-change-event-records[]\n+* xref:metadata-in-debezium-change-event-records[]\n+* xref:debezium-connector-generated-events-that-represent-transaction-boundaries[]\n \n-[NOTE]\n-====\n-We recommend reading and understanding the https://www.postgresql.org/docs/current/static/wal-configuration.html[WAL configuration documentation] regarding the mechanics and configuration of the PostgreSQL write-ahead log.\n-====\n endif::product[]\n \n-ifdef::community[]\n-Before using the PostgreSQL connector to monitor the changes committed on a PostgreSQL server,\n-first decide which logical decoder method you intend to use.\n-If you plan *not* to use the native pgoutput logical replication stream support,\n-then you will need to install the logical decoding plug-in into the PostgreSQL server.\n-Afterward enable a replication slot, and configure a user with sufficient privileges to perform the replication.\n-\n-Note that if your database is hosted by a service such as https://www.heroku.com/postgres[Heroku Postgres] you may be unable to install the plug-in.\n-If so, and if you're using PostgreSQL 10+, you can use the pgoutput decoder support to monitor your database.\n-If that is not an option, you'll be unable to monitor your database with {prodname}.\n-\n-[[postgresql-on-amazon-rds]]\n-=== PostgreSQL on Amazon RDS\n-\n-It is possible to monitor PostgreSQL database running in https://aws.amazon.com/rds/[Amazon RDS]. To get it running you must fulfill the following conditions\n-\n-* The instance parameter `rds.logical_replication` is set to `1`.\n-* Verify that `wal_level` parameter is set to `logical` by running the query `SHOW wal_level` as DB master user; this might not be the case in multi-zone replication setups.\n-You cannot set this option manually, it is (https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html[automatically changed]) when the `rds.logical_replication` is set to `1`.\n-If the `wal_level` is not `logical` after the change above, it is probably because the instance has to be restarted due to the parameter group change. This happens accordingly to your maintenance window or can be done manually.\n-* Set `plugin.name` {prodname} parameter to `wal2json`.  You can skip this on PostgreSQL 10+ if you wish to use pgoutput logical replication stream support.\n-* Use database master account for replication as RDS currently does not support setting of `REPLICATION` privilege for another account.\n-\n-[IMPORTANT]\n-====\n-You should make sure to use the latest versions of Postgres 9.6, 10 or 11 on Amazon RDS.\n-Otherwise, older versions of the wal2json plug-in may be installed\n-(see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html[the official documentation] for the exact wal2json versions installed on Amazon RDS).\n-In that case, replication messages received from the database may not carry complete information about type constraints like length or scale or `NULL`/`NOT NULL`,\n-which in turn might cause creation of messages with an inconsistent schema for a short period of time in case of changes to a column's definition.\n-\n-As of January 2019, the following Postgres versions on RDS come with an up-to-date version of wal2json and thus should be used:\n-\n-* Postgres 9.6: 9.6.10 and newer\n-* Postgres 10: 10.5 and newer\n-* Postgres 11: any version\n-====\n-\n-\n-[[postgresql-output-plugin]]\n-=== Installing the Logical Decoding Output Plug-in\n-\n-[TIP]\n-====\n-Also see {link-prefix}:{link-postgresql-plugins}[Logical Decoding Output Plug-in Installation for PostgreSQL] for more detailed instructions of setting up and testing logical decoding plug-ins.\n-====\n-\n-[NOTE]\n-====\n-As of {prodname} 0.10, the connector supports PostgreSQL 10+ logical replication streaming using _pgoutput_.\n-This means that a logical decoding output plug-in is no longer necessary and changes can be emitted directly from the replication stream by the connector.\n-====\n+// Type: concept\n+// ModuleID: how-debezium-postgresql-connectors-perform-database-snapshots\n+// Title: How {prodname} PostgreSQL connectors perform database snapshots\n+[[postgresql-snapshots]]\n+=== Snapshots\n \n-As of PostgreSQL 9.4, the only way to read changes to the write-ahead-log is to first install a logical decoding output plug-in. Plugins are written in C, compiled, and installed on the machine which runs the PostgreSQL server. Plugins use  a number of PostgreSQL specific APIs, as described by the https://www.postgresql.org/docs/current/static/logicaldecoding-output-plugin.html[_PostgreSQL documentation_].\n+Most PostgreSQL servers are configured to not retain the complete history of the database in the WAL segments. This means that the PostgreSQL connector would be unable to see the entire history of the database by reading only the WAL. Consequently, the first time that the connector starts, it performs an initial _consistent snapshot_ of the database. The default behavior for performing a snapshot consists of the following steps. You can change this behavior by setting the {link-prefix}:{link-postgresql-connector}#postgresql-property-snapshot-mode[`snapshot.mode` connector configuration property] to a value other than `initial`. \n \n-The PostgreSQL connector works with one of {prodname}'s supported logical decoding plug-ins to encode the changes in either https://github.com/google/protobuf[_Protobuf format_] or http://www.json.org/[_JSON_] format.\n-See the documentation of your chosen plug-in (https://github.com/debezium/postgres-decoderbufs/blob/master/README.md[_protobuf_], https://github.com/eulerto/wal2json/blob/master/README.md[_wal2json_]) to learn more about the plug-in's requirements, limitations, and how to compile it.\n+. Start a transaction with a link:https://www.postgresql.org/docs/current/static/sql-set-transaction.html[SERIALIZABLE, READ ONLY, DEFERRABLE] isolation level to ensure that subsequent reads in this transaction are against a single consistent version of the data. Any changes to the data due to subsequent `INSERT`, `UPDATE`, and `DELETE` operations by other clients are not visible to this transaction.\n+. Obtain an `ACCESS SHARE MODE` lock on each of the tables being tracked to ensure that no structural changes can occur to any of the tables while the snapshot is taking place. These locks do not prevent table `INSERT`, `UPDATE` and `DELETE` operations from taking place during the snapshot. \n++\n+_This step is omitted when `snapshot.mode` is set to `exported`, which allows the connector to perform a lock-free snapshot_.\n+. Read the current position in the server's transaction log.\n+. Scan the database tables and schemas, generate a `READ` event for each row and write that event to the appropriate table-specific Kafka topic.\n+. Commit the transaction.\n+. Record the successful completion of the snapshot in the connector offsets.\n \n-For simplicity, {prodname} also provides a Docker image based on a vanilla PostgreSQL server image on top of which it compiles and installs the plug-ins. We recommend https://github.com/debezium/docker-images/tree/master/postgres/9.6[_using this image_] as an example of the detailed steps required for the installation.\n+If the connector fails, is rebalanced, or stops after Step 1 begins but before Step 6 completes, upon restart the connector begins a new snapshot. After the connector completes its initial snapshot, the PostgreSQL connector continues streaming from the position that it read in step 3. This ensures that the connector does not miss any updates. If the connector stops again for any reason, upon restart, the connector continues streaming changes from where it previously left off.\n \n [WARNING]\n ====\n-The {prodname} logical decoding plug-ins have only been installed and tested on _Linux_ machines.\n-For Windows and other operating systems it may require different installation steps.\n-====\n-\n-[[postgresql-differences-between-plugins]]\n-==== Differences Between Plug-ins\n-\n-The plug-ins' behavior is not completely same for all cases.\n-So far these differences have been identified:\n-\n-* wal2json plug-in is not able to process quoted identifiers (https://github.com/eulerto/wal2json/issues/35[issue])\n-* wal2json and decoderbufs plug-ins emit events for tables without primary keys\n-* wal2json plug-in does not support special values (`NaN` or `infinity`) for floating point types\n-* wal2json should be used with setting the `schema.refresh.mode` connector option to `columns_diff_exclude_unchanged_toast`;\n-otherwise, when receiving a change event for a row containing an unchanged TOAST column, no field for that column is contained in the emitted change event's `after` structure.\n-This is because wal2json's messages do not contain a field for such a column.\n-\n-The requirement for adding this is tracked under the wal2json https://github.com/eulerto/wal2json/issues/98[issue 98].\n-See the documentation of `columns_diff_exclude_unchanged_toast` further below for implications of using it.\n-\n-* pgoutput plug-in does not emit all the events for tables without primary keys, it only emits inserts\n-\n-All up-to-date differences are tracked in a test suite https://github.com/debezium/debezium/blob/master/debezium-connector-postgres/src/test/java/io/debezium/connector/postgresql/DecoderDifferences.java[Java class].\n-\n-\n-[[postgresql-server-configuration]]\n-=== Configuring the PostgreSQL Server\n-\n-If you are using one of the supported {link-prefix}:{link-postgresql-connector}#postgresql-output-plugin[logical decoding plug-ins] (i.e. not pgoutput) and it has been installed,\n-configure the server to load the plug-in at startup:\n-\n-.postgresql.conf\n-[source,properties]\n-----\n-# MODULES\n-shared_preload_libraries = 'decoderbufs,wal2json' // <1>\n-----\n-<1> tells the server that it should load at startup the `decoderbufs` and `wal2json` logical decoding plug-ins (the names of the plug-ins are set in https://github.com/debezium/postgres-decoderbufs/blob/v0.3.0/Makefile[_Protobuf_] and https://github.com/eulerto/wal2json/blob/master/Makefile[_wal2json_] Makefiles)\n-\n-Next is to configure the replication slot regardless of the decoder being used:\n-\n-.postgresql.conf\n-[source,properties]\n-----\n-# REPLICATION\n-wal_level = logical             // <1>\n-max_wal_senders = 1             // <2>\n-max_replication_slots = 1       // <3>\n-----\n-<1> tells the server that it should use logical decoding with the write-ahead log\n-<2> tells the server that it should use a maximum of `1` separate processes for processing WAL changes\n-<3> tells the server that it should allow a maximum of `1` replication slots to be created for streaming WAL changes\n-\n-{prodname} uses PostgreSQL's logical decoding, which uses replication slots.\n-Replication slots are guaranteed to retain all WAL required for {prodname} even during {prodname} outages.\n-It is important for this reason to closely monitor replication slots to avoid too much disk consumption and other conditions that can happen such as catalog bloat if a replication slot stays unused for too long.\n-For more information please see the official Postgres docs on https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION-SLOTS[this subject].\n-\n-If you are working with a `synchronous_commit` setting other than `on`,\n-the recommendation is to set `wal_writer_delay` to a value such as 10 ms to achieve a low latency of change events.\n-Otherwise, its default value is applied, which adds a latency of about 200 ms.\n-\n-[TIP]\n-====\n-We strongly recommend reading and understanding https://www.postgresql.org/docs/current/static/wal-configuration.html[the official documentation] regarding the mechanics and configuration of the PostgreSQL write-ahead log.\n-====\n-endif::community[]\n-\n-[[postgresql-permissions]]\n-=== Setting up Permissions\n-\n-Next, configure a database user who can perform replications.\n-\n-Replication can only be performed by a database user that has appropriate permissions and only for a configured number of hosts.\n-\n-In order to give a user replication permissions, define a PostgreSQL role that has _at least_ the `REPLICATION` and `LOGIN` permissions. For example:\n-\n-[source,sql]\n-----\n-CREATE ROLE name REPLICATION LOGIN;\n-----\n-\n-[NOTE]\n-====\n-Superusers have by default both of the above roles.\n+It is strongly recommended that you configure a PostgreSQL connector to set `snapshot.mode` to `exported`. The `initial`, `initial only` and `always` modes can lose a few events while a connector switches from performing the snapshot to streaming change event records when a database is under heavy load.\n+This is a known issue and the affected snapshot modes will be reworked to use `exported` mode internally (link:https://issues.redhat.com/browse/DBZ-2337[DBZ-2337]).\n ====\n \n-Finally, configure the PostgreSQL server to allow replication to take place between the server machine and the host on which the PostgreSQL connector is running:\n-\n-.pg_hba.conf\n-[source]\n-----\n-local   replication     <youruser>                          trust   // <1>\n-host    replication     <youruser>  127.0.0.1/32            trust   // <2>\n-host    replication     <youruser>  ::1/128                 trust   // <3>\n-----\n-<1> Tells the server to allow replication for `<youruser>` locally (i.e. on the server machine)\n-<2> Tells the server to allow `<youruser>` on `localhost` to receive replication changes using `IPV4`\n-<3> Tells the server to allow `<youruser>` on `localhost` to receive replication changes using `IPV6`\n+[id=\"snapshot-mode-settings\"]\n+.Settings for `snapshot.mode` connector configuration property\n+[cols=\"20%a,80%a\",options=\"header\"]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MjM2OTQwMA=="}, "originalCommit": null, "originalPosition": 340}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4253, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}