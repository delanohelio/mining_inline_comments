{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgyMjk2MjI2", "number": 1801, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwODozMTowNlrOEhkJrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxNjozMTowNlrOEinIKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNjMwNzY3OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/mongodb.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwODozMTowNlrOHO8g_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQxNjo0MzowNFrOHPQrJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQzMzU5OA==", "bodyText": "Ah, seeing this existed before, but this blog post is super-old and obsolete. Rather link to the OpenShift docs page you recently reworked (here and wherever else it's referenced).", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r485433598", "createdAt": "2020-09-09T08:31:06Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/connectors/mongodb.adoc", "diffHunk": "@@ -594,60 +594,58 @@ Following is an example of what a message looks like:\n == Deploying the MongoDB connector\n \n ifdef::community[]\n-If you have already installed https://zookeeper.apache.org[Zookeeper], http://kafka.apache.org/[Kafka], and {link-kafka-docs}.html#connect[Kafka Connect], then using {prodname}'s MongoDB connector is easy.\n-Simply download the\n+With https://zookeeper.apache.org[Zookeeper], http://kafka.apache.org/[Kafka], and {link-kafka-docs}.html#connect[Kafka Connect] installed, the remaining tasks to deploy a {prodname} MongoDB connector are to \n+download the\n ifeval::['{page-version}' == 'master']\n {link-mongodb-plugin-snapshot}[connector's plug-in archive],\n endif::[]\n ifeval::['{page-version}' != 'master']\n https://repo1.maven.org/maven2/io/debezium/debezium-connector-mongodb/{debezium-version}/debezium-connector-mongodb-{debezium-version}-plugin.tar.gz[connector's plug-in archive],\n endif::[]\n-extract the JARs into your Kafka Connect environment, and add the directory with the JARs to Kafka Connect's `plugin.path` by using the {link-kafka-docs}/#connectconfigs[plugin.path] configuration property.\n-Restart your Kafka Connect process to pick up the new JARs.\n+extract the JAR files into your Kafka Connect environment, and add the directory with the JAR files to  {link-kafka-docs}/#connectconfigs[Kafka Connect's `plugin.path`]. You then need to \n+restart your Kafka Connect process to pick up the new JAR files.\n+\n+If you need immutable containers, see link:https://hub.docker.com/r/debezium/[{prodname}'s Container images] for Zookeeper, Kafka, and Kafka Connect with the MongoDB connector already installed and ready to run. You can also link:https://debezium.io/blog/2016/05/31/Debezium-on-Kubernetes/[run {prodname} on Kubernetes and OpenShift].", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTc2Mzg3OQ==", "bodyText": "I updated this in each connector file, each in a separate commit. Just pushed those commits to this PR.", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r485763879", "createdAt": "2020-09-09T16:43:04Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/mongodb.adoc", "diffHunk": "@@ -594,60 +594,58 @@ Following is an example of what a message looks like:\n == Deploying the MongoDB connector\n \n ifdef::community[]\n-If you have already installed https://zookeeper.apache.org[Zookeeper], http://kafka.apache.org/[Kafka], and {link-kafka-docs}.html#connect[Kafka Connect], then using {prodname}'s MongoDB connector is easy.\n-Simply download the\n+With https://zookeeper.apache.org[Zookeeper], http://kafka.apache.org/[Kafka], and {link-kafka-docs}.html#connect[Kafka Connect] installed, the remaining tasks to deploy a {prodname} MongoDB connector are to \n+download the\n ifeval::['{page-version}' == 'master']\n {link-mongodb-plugin-snapshot}[connector's plug-in archive],\n endif::[]\n ifeval::['{page-version}' != 'master']\n https://repo1.maven.org/maven2/io/debezium/debezium-connector-mongodb/{debezium-version}/debezium-connector-mongodb-{debezium-version}-plugin.tar.gz[connector's plug-in archive],\n endif::[]\n-extract the JARs into your Kafka Connect environment, and add the directory with the JARs to Kafka Connect's `plugin.path` by using the {link-kafka-docs}/#connectconfigs[plugin.path] configuration property.\n-Restart your Kafka Connect process to pick up the new JARs.\n+extract the JAR files into your Kafka Connect environment, and add the directory with the JAR files to  {link-kafka-docs}/#connectconfigs[Kafka Connect's `plugin.path`]. You then need to \n+restart your Kafka Connect process to pick up the new JAR files.\n+\n+If you need immutable containers, see link:https://hub.docker.com/r/debezium/[{prodname}'s Container images] for Zookeeper, Kafka, and Kafka Connect with the MongoDB connector already installed and ready to run. You can also link:https://debezium.io/blog/2016/05/31/Debezium-on-Kubernetes/[run {prodname} on Kubernetes and OpenShift].", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQzMzU5OA=="}, "originalCommit": null, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNjMxNjI4OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/mongodb.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwODozMzoxOVrOHO8mTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQxNjo0NDoxM1rOHPQurA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQzNDk1OA==", "bodyText": "It's not really a file, rather a call of the REST API, as you describe further below.", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r485434958", "createdAt": "2020-09-09T08:33:19Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/connectors/mongodb.adoc", "diffHunk": "@@ -705,6 +703,116 @@ See the {link-prefix}:{link-mongodb-connector}#mongodb-connector-properties[comp\n \n This configuration can be sent via POST to a running Kafka Connect service, which will then record the configuration and start up the one connector task that will connect to the MongoDB replica set or sharded cluster, assign tasks for each replica set, perform a snapshot if necessary, read the oplog, and record events to Kafka topics.\n \n+[[mongodb-adding-connector-configuration]]\n+=== Adding connector configuration\n+\n+ifdef::community[]\n+To run a {prodname} MongoDB connector, create a connector configuration file and add it to your Kafka Connect cluster. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTc2NDc4MA==", "bodyText": "For each connector, I updated this to:\n... create a connector configuration and add the configuration to your .....\nIf that's not right, let me know.\nI committed and pushed these updates.", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r485764780", "createdAt": "2020-09-09T16:44:13Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/mongodb.adoc", "diffHunk": "@@ -705,6 +703,116 @@ See the {link-prefix}:{link-mongodb-connector}#mongodb-connector-properties[comp\n \n This configuration can be sent via POST to a running Kafka Connect service, which will then record the configuration and start up the one connector task that will connect to the MongoDB replica set or sharded cluster, assign tasks for each replica set, perform a snapshot if necessary, read the oplog, and record events to Kafka topics.\n \n+[[mongodb-adding-connector-configuration]]\n+=== Adding connector configuration\n+\n+ifdef::community[]\n+To run a {prodname} MongoDB connector, create a connector configuration file and add it to your Kafka Connect cluster. ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQzNDk1OA=="}, "originalCommit": null, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNjMxODU4OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/mongodb.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwODozMzo1NVrOHO8nsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQxNjo0NDozM1rOHPQvtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQzNTMxMw==", "bodyText": "Podman or Docker perhaps?", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r485435313", "createdAt": "2020-09-09T08:33:55Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/connectors/mongodb.adoc", "diffHunk": "@@ -705,6 +703,116 @@ See the {link-prefix}:{link-mongodb-connector}#mongodb-connector-properties[comp\n \n This configuration can be sent via POST to a running Kafka Connect service, which will then record the configuration and start up the one connector task that will connect to the MongoDB replica set or sharded cluster, assign tasks for each replica set, perform a snapshot if necessary, read the oplog, and record events to Kafka topics.\n \n+[[mongodb-adding-connector-configuration]]\n+=== Adding connector configuration\n+\n+ifdef::community[]\n+To run a {prodname} MongoDB connector, create a connector configuration file and add it to your Kafka Connect cluster. \n+\n+.Prerequisites\n+\n+* MongoDB is set up to run a {prodname} connector.\n+\n+* A {prodname} MongoDB connector is installed. \n+\n+.Procedure\n+\n+. Create a configuration file for the MongoDB connector.\n+\n+. Use the link:{link-kafka-docs}/#connect_rest[Kafka Connect REST API] to add that connector configuration to your Kafka Connect cluster. \n+\n+endif::community[]\n+\n+ifdef::product[]\n+You can use a provided {prodname} container to deploy a {prodname} MongoDB connector. In this procedure, you build a custom Kafka Connect container image for {prodname}, configure the {prodname} connector as needed, and then add your connector configuration to your Kafka Connect environment. \n+\n+.Prerequisites\n+\n+* You have Podman installed and sufficient rights to create and manage containers.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTc2NTA0NQ==", "bodyText": "Sure. I did this update for each connector.", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r485765045", "createdAt": "2020-09-09T16:44:33Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/mongodb.adoc", "diffHunk": "@@ -705,6 +703,116 @@ See the {link-prefix}:{link-mongodb-connector}#mongodb-connector-properties[comp\n \n This configuration can be sent via POST to a running Kafka Connect service, which will then record the configuration and start up the one connector task that will connect to the MongoDB replica set or sharded cluster, assign tasks for each replica set, perform a snapshot if necessary, read the oplog, and record events to Kafka topics.\n \n+[[mongodb-adding-connector-configuration]]\n+=== Adding connector configuration\n+\n+ifdef::community[]\n+To run a {prodname} MongoDB connector, create a connector configuration file and add it to your Kafka Connect cluster. \n+\n+.Prerequisites\n+\n+* MongoDB is set up to run a {prodname} connector.\n+\n+* A {prodname} MongoDB connector is installed. \n+\n+.Procedure\n+\n+. Create a configuration file for the MongoDB connector.\n+\n+. Use the link:{link-kafka-docs}/#connect_rest[Kafka Connect REST API] to add that connector configuration to your Kafka Connect cluster. \n+\n+endif::community[]\n+\n+ifdef::product[]\n+You can use a provided {prodname} container to deploy a {prodname} MongoDB connector. In this procedure, you build a custom Kafka Connect container image for {prodname}, configure the {prodname} connector as needed, and then add your connector configuration to your Kafka Connect environment. \n+\n+.Prerequisites\n+\n+* You have Podman installed and sufficient rights to create and manage containers.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQzNTMxMw=="}, "originalCommit": null, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNjMyMTA0OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/mongodb.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwODozNDoyOVrOHO8pFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQxNTowNzo1MlrOHPMCIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQzNTY3MA==", "bodyText": "Don't we have variable/attribute for the Strimzi (and its Kafka) version?", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r485435670", "createdAt": "2020-09-09T08:34:29Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/connectors/mongodb.adoc", "diffHunk": "@@ -705,6 +703,116 @@ See the {link-prefix}:{link-mongodb-connector}#mongodb-connector-properties[comp\n \n This configuration can be sent via POST to a running Kafka Connect service, which will then record the configuration and start up the one connector task that will connect to the MongoDB replica set or sharded cluster, assign tasks for each replica set, perform a snapshot if necessary, read the oplog, and record events to Kafka topics.\n \n+[[mongodb-adding-connector-configuration]]\n+=== Adding connector configuration\n+\n+ifdef::community[]\n+To run a {prodname} MongoDB connector, create a connector configuration file and add it to your Kafka Connect cluster. \n+\n+.Prerequisites\n+\n+* MongoDB is set up to run a {prodname} connector.\n+\n+* A {prodname} MongoDB connector is installed. \n+\n+.Procedure\n+\n+. Create a configuration file for the MongoDB connector.\n+\n+. Use the link:{link-kafka-docs}/#connect_rest[Kafka Connect REST API] to add that connector configuration to your Kafka Connect cluster. \n+\n+endif::community[]\n+\n+ifdef::product[]\n+You can use a provided {prodname} container to deploy a {prodname} MongoDB connector. In this procedure, you build a custom Kafka Connect container image for {prodname}, configure the {prodname} connector as needed, and then add your connector configuration to your Kafka Connect environment. \n+\n+.Prerequisites\n+\n+* You have Podman installed and sufficient rights to create and manage containers.\n+* You installed the {prodname} MongoDB connector archive. \n+\n+.Procedure\n+\n+. Extract the {prodname} MongoDB connector archive to create a directory structure for the connector plug-in, for example: \n++\n+[subs=+macros]\n+----\n+pass:quotes[*tree ./my-plugins/*]\n+./my-plugins/\n+\u251c\u2500\u2500 debezium-connector-mongodb\n+\u2502   \u251c\u2500\u2500 ...\n+----\n+\n+. Create and publish a custom image for running your {prodname} connector:\n+\n+.. Create a new `Dockerfile` by using `{DockerKafkaConnect}` as the base image. In the following example, you would replace _my-plugins_ with the name of your plug-ins directory:\n++\n+[subs=+macros]\n+----\n+FROM registry.redhat.io/amq7/amq-streams-kafka-25:1.5.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 169}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTY4Nzg0Mw==", "bodyText": "I don't see a variable in antora.yml.  We do have one downstream:\nDockerKafkaConnect: registry.redhat.io/amq7/amq-streams-kafka-25:1.5.0\nI am adding that to antora.yml with the next commit. I hope the different attribute name case is okay.", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r485687843", "createdAt": "2020-09-09T15:07:52Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/mongodb.adoc", "diffHunk": "@@ -705,6 +703,116 @@ See the {link-prefix}:{link-mongodb-connector}#mongodb-connector-properties[comp\n \n This configuration can be sent via POST to a running Kafka Connect service, which will then record the configuration and start up the one connector task that will connect to the MongoDB replica set or sharded cluster, assign tasks for each replica set, perform a snapshot if necessary, read the oplog, and record events to Kafka topics.\n \n+[[mongodb-adding-connector-configuration]]\n+=== Adding connector configuration\n+\n+ifdef::community[]\n+To run a {prodname} MongoDB connector, create a connector configuration file and add it to your Kafka Connect cluster. \n+\n+.Prerequisites\n+\n+* MongoDB is set up to run a {prodname} connector.\n+\n+* A {prodname} MongoDB connector is installed. \n+\n+.Procedure\n+\n+. Create a configuration file for the MongoDB connector.\n+\n+. Use the link:{link-kafka-docs}/#connect_rest[Kafka Connect REST API] to add that connector configuration to your Kafka Connect cluster. \n+\n+endif::community[]\n+\n+ifdef::product[]\n+You can use a provided {prodname} container to deploy a {prodname} MongoDB connector. In this procedure, you build a custom Kafka Connect container image for {prodname}, configure the {prodname} connector as needed, and then add your connector configuration to your Kafka Connect environment. \n+\n+.Prerequisites\n+\n+* You have Podman installed and sufficient rights to create and manage containers.\n+* You installed the {prodname} MongoDB connector archive. \n+\n+.Procedure\n+\n+. Extract the {prodname} MongoDB connector archive to create a directory structure for the connector plug-in, for example: \n++\n+[subs=+macros]\n+----\n+pass:quotes[*tree ./my-plugins/*]\n+./my-plugins/\n+\u251c\u2500\u2500 debezium-connector-mongodb\n+\u2502   \u251c\u2500\u2500 ...\n+----\n+\n+. Create and publish a custom image for running your {prodname} connector:\n+\n+.. Create a new `Dockerfile` by using `{DockerKafkaConnect}` as the base image. In the following example, you would replace _my-plugins_ with the name of your plug-ins directory:\n++\n+[subs=+macros]\n+----\n+FROM registry.redhat.io/amq7/amq-streams-kafka-25:1.5.0", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQzNTY3MA=="}, "originalCommit": null, "originalPosition": 169}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNjMyNjk4OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/mongodb.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwODozNTo0OFrOHO8sqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQxNjo0NTowMlrOHPQxZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQzNjU4NA==", "bodyText": "docker container image -> container image.\ndocker file -> Dockerfile (that's the actual file name)?", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r485436584", "createdAt": "2020-09-09T08:35:48Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/connectors/mongodb.adoc", "diffHunk": "@@ -705,6 +703,116 @@ See the {link-prefix}:{link-mongodb-connector}#mongodb-connector-properties[comp\n \n This configuration can be sent via POST to a running Kafka Connect service, which will then record the configuration and start up the one connector task that will connect to the MongoDB replica set or sharded cluster, assign tasks for each replica set, perform a snapshot if necessary, read the oplog, and record events to Kafka topics.\n \n+[[mongodb-adding-connector-configuration]]\n+=== Adding connector configuration\n+\n+ifdef::community[]\n+To run a {prodname} MongoDB connector, create a connector configuration file and add it to your Kafka Connect cluster. \n+\n+.Prerequisites\n+\n+* MongoDB is set up to run a {prodname} connector.\n+\n+* A {prodname} MongoDB connector is installed. \n+\n+.Procedure\n+\n+. Create a configuration file for the MongoDB connector.\n+\n+. Use the link:{link-kafka-docs}/#connect_rest[Kafka Connect REST API] to add that connector configuration to your Kafka Connect cluster. \n+\n+endif::community[]\n+\n+ifdef::product[]\n+You can use a provided {prodname} container to deploy a {prodname} MongoDB connector. In this procedure, you build a custom Kafka Connect container image for {prodname}, configure the {prodname} connector as needed, and then add your connector configuration to your Kafka Connect environment. \n+\n+.Prerequisites\n+\n+* You have Podman installed and sufficient rights to create and manage containers.\n+* You installed the {prodname} MongoDB connector archive. \n+\n+.Procedure\n+\n+. Extract the {prodname} MongoDB connector archive to create a directory structure for the connector plug-in, for example: \n++\n+[subs=+macros]\n+----\n+pass:quotes[*tree ./my-plugins/*]\n+./my-plugins/\n+\u251c\u2500\u2500 debezium-connector-mongodb\n+\u2502   \u251c\u2500\u2500 ...\n+----\n+\n+. Create and publish a custom image for running your {prodname} connector:\n+\n+.. Create a new `Dockerfile` by using `{DockerKafkaConnect}` as the base image. In the following example, you would replace _my-plugins_ with the name of your plug-ins directory:\n++\n+[subs=+macros]\n+----\n+FROM registry.redhat.io/amq7/amq-streams-kafka-25:1.5.0\n+USER root:root\n+pass:quotes[COPY _./my-plugins/_ /opt/kafka/plugins/]\n+USER 1001\n+----\n++\n+Before Kafka Connect starts running the connector, Kafka Connect loads any third-party plug-ins that are in the `/opt/kafka/plugins` directory.\n+\n+.. Build the docker container image. For example, if you saved the docker file that you created in the previous step as `debezium-container-for-mongodb`, then you would run the following command:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTc2NTQ3OQ==", "bodyText": "I made these updates in the doc for each connector. Committed and pushed.", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r485765479", "createdAt": "2020-09-09T16:45:02Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/mongodb.adoc", "diffHunk": "@@ -705,6 +703,116 @@ See the {link-prefix}:{link-mongodb-connector}#mongodb-connector-properties[comp\n \n This configuration can be sent via POST to a running Kafka Connect service, which will then record the configuration and start up the one connector task that will connect to the MongoDB replica set or sharded cluster, assign tasks for each replica set, perform a snapshot if necessary, read the oplog, and record events to Kafka topics.\n \n+[[mongodb-adding-connector-configuration]]\n+=== Adding connector configuration\n+\n+ifdef::community[]\n+To run a {prodname} MongoDB connector, create a connector configuration file and add it to your Kafka Connect cluster. \n+\n+.Prerequisites\n+\n+* MongoDB is set up to run a {prodname} connector.\n+\n+* A {prodname} MongoDB connector is installed. \n+\n+.Procedure\n+\n+. Create a configuration file for the MongoDB connector.\n+\n+. Use the link:{link-kafka-docs}/#connect_rest[Kafka Connect REST API] to add that connector configuration to your Kafka Connect cluster. \n+\n+endif::community[]\n+\n+ifdef::product[]\n+You can use a provided {prodname} container to deploy a {prodname} MongoDB connector. In this procedure, you build a custom Kafka Connect container image for {prodname}, configure the {prodname} connector as needed, and then add your connector configuration to your Kafka Connect environment. \n+\n+.Prerequisites\n+\n+* You have Podman installed and sufficient rights to create and manage containers.\n+* You installed the {prodname} MongoDB connector archive. \n+\n+.Procedure\n+\n+. Extract the {prodname} MongoDB connector archive to create a directory structure for the connector plug-in, for example: \n++\n+[subs=+macros]\n+----\n+pass:quotes[*tree ./my-plugins/*]\n+./my-plugins/\n+\u251c\u2500\u2500 debezium-connector-mongodb\n+\u2502   \u251c\u2500\u2500 ...\n+----\n+\n+. Create and publish a custom image for running your {prodname} connector:\n+\n+.. Create a new `Dockerfile` by using `{DockerKafkaConnect}` as the base image. In the following example, you would replace _my-plugins_ with the name of your plug-ins directory:\n++\n+[subs=+macros]\n+----\n+FROM registry.redhat.io/amq7/amq-streams-kafka-25:1.5.0\n+USER root:root\n+pass:quotes[COPY _./my-plugins/_ /opt/kafka/plugins/]\n+USER 1001\n+----\n++\n+Before Kafka Connect starts running the connector, Kafka Connect loads any third-party plug-ins that are in the `/opt/kafka/plugins` directory.\n+\n+.. Build the docker container image. For example, if you saved the docker file that you created in the previous step as `debezium-container-for-mongodb`, then you would run the following command:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQzNjU4NA=="}, "originalCommit": null, "originalPosition": 177}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNjMzMDg1OnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwODozNjozOFrOHO8u2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQxNjo0NToyMlrOHPQyZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQzNzE0Ng==", "bodyText": "See above.", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r485437146", "createdAt": "2020-09-09T08:36:38Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -1741,7 +1741,7 @@ endif::community[]\n ifdef::community[]\n With link:https://zookeeper.apache.org[Zookeeper], link:http://kafka.apache.org/[Kafka], and {link-kafka-docs}.html#connect[Kafka Connect] installed, the remaining tasks to deploy a {prodname} PostgreSQL connector are to download the link:https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/{debezium-version}/debezium-connector-postgres-{debezium-version}-plugin.tar.gz[connector's plug-in archive], extract the JAR files into your Kafka Connect environment, and add the directory with the JAR files to {link-kafka-docs}/#connectconfigs[Kafka Connect's `plugin.path`]. You then need to restart your Kafka Connect process to pick up the new JAR files.\n \n-If you need immutable containers, see link:https://hub.docker.com/r/debezium/[{prodname}'s Container images] for Zookeeper, Kafka, PostgreSQL and Kafka Connect with the PostgreSQL connector already installed and ready to run. You can also link:/blog/2016/05/31/Debezium-on-Kubernetes/[run {prodname} on Kubernetes and OpenShift].\n+If you need immutable containers, see link:https://hub.docker.com/r/debezium/[{prodname}'s Container images] for Zookeeper, Kafka, PostgreSQL and Kafka Connect with the PostgreSQL connector already installed and ready to run. You can also link:https://debezium.io/blog/2016/05/31/Debezium-on-Kubernetes/[run {prodname} on Kubernetes and OpenShift].", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTc2NTczNQ==", "bodyText": "Updated for each connector. Committed and pushed.", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r485765735", "createdAt": "2020-09-09T16:45:22Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/postgresql.adoc", "diffHunk": "@@ -1741,7 +1741,7 @@ endif::community[]\n ifdef::community[]\n With link:https://zookeeper.apache.org[Zookeeper], link:http://kafka.apache.org/[Kafka], and {link-kafka-docs}.html#connect[Kafka Connect] installed, the remaining tasks to deploy a {prodname} PostgreSQL connector are to download the link:https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/{debezium-version}/debezium-connector-postgres-{debezium-version}-plugin.tar.gz[connector's plug-in archive], extract the JAR files into your Kafka Connect environment, and add the directory with the JAR files to {link-kafka-docs}/#connectconfigs[Kafka Connect's `plugin.path`]. You then need to restart your Kafka Connect process to pick up the new JAR files.\n \n-If you need immutable containers, see link:https://hub.docker.com/r/debezium/[{prodname}'s Container images] for Zookeeper, Kafka, PostgreSQL and Kafka Connect with the PostgreSQL connector already installed and ready to run. You can also link:/blog/2016/05/31/Debezium-on-Kubernetes/[run {prodname} on Kubernetes and OpenShift].\n+If you need immutable containers, see link:https://hub.docker.com/r/debezium/[{prodname}'s Container images] for Zookeeper, Kafka, PostgreSQL and Kafka Connect with the PostgreSQL connector already installed and ready to run. You can also link:https://debezium.io/blog/2016/05/31/Debezium-on-Kubernetes/[run {prodname} on Kubernetes and OpenShift].", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQzNzE0Ng=="}, "originalCommit": null, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNjMzMzUxOnYy", "diffSide": "RIGHT", "path": "documentation/modules/ROOT/pages/connectors/sqlserver.adoc", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwODozNzoxOVrOHO8wmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQxNjo0MjoxOFrOHPQotw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQzNzU5Mg==", "bodyText": "So I find the \"If you need\" sounds a bit odd. Perhaps \"If you're working with...\"? What's really needed in life, after all :)", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r485437592", "createdAt": "2020-09-09T08:37:19Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/connectors/sqlserver.adoc", "diffHunk": "@@ -1208,51 +1208,46 @@ The `connect.decimal.precision` schema parameter contains an integer representin\n |===\n \n [[sqlserver-deploying-a-connector]]\n-== Deploying the SQL Server connector\n+== Deployment\n \n ifdef::community[]\n-If you have already installed https://zookeeper.apache.org[Zookeeper], http://kafka.apache.org/[Kafka], and {link-kafka-docs}.html#connect[Kafka Connect], then using {prodname}'s SQL Server` connector is easy.\n-Simply download the https://repo1.maven.org/maven2/io/debezium/debezium-connector-sqlserver/{debezium-version}/debezium-connector-sqlserver-{debezium-version}-plugin.tar.gz[connector's plug-in archive], extract the JARs into your Kafka Connect environment, and add the directory with the JARs to {link-kafka-docs}/#connectconfigs[Kafka Connect's `plugin.path`].\n-Restart your Kafka Connect process to pick up the new JARs.\n+With link:https://zookeeper.apache.org[Zookeeper], http://kafka.apache.org/[Kafka], and {link-kafka-docs}.html#connect[Kafka Connect] installed, the remaining tasks to deploy a {prodname} SQL Server connector are to download the https://repo1.maven.org/maven2/io/debezium/debezium-connector-sqlserver/{debezium-version}/debezium-connector-sqlserver-{debezium-version}-plugin.tar.gz[connector's plug-in archive], extract the JAR files into your Kafka Connect environment, and add the directory with the JAR files to {link-kafka-docs}/#connectconfigs[Kafka Connect's `plugin.path`].\n+Restart your Kafka Connect process to pick up the new JAR files.\n+\n+If you need immutable containers, see link:https://hub.docker.com/r/debezium/[{prodname}'s container images] for Zookeeper, Kafka, SQL Server and Kafka Connect with the SQL Server connector already installed and ready to run. You can also link:https://debezium.io/blog/2016/05/31/Debezium-on-Kubernetes/[run {prodname} on Kubernetes and OpenShift].", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTc2MzI1NQ==", "bodyText": "Sure!  Updated in the recent commits, which reflect your comments, and which I just pushed to this PR.", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r485763255", "createdAt": "2020-09-09T16:42:18Z", "author": {"login": "TovaCohen"}, "path": "documentation/modules/ROOT/pages/connectors/sqlserver.adoc", "diffHunk": "@@ -1208,51 +1208,46 @@ The `connect.decimal.precision` schema parameter contains an integer representin\n |===\n \n [[sqlserver-deploying-a-connector]]\n-== Deploying the SQL Server connector\n+== Deployment\n \n ifdef::community[]\n-If you have already installed https://zookeeper.apache.org[Zookeeper], http://kafka.apache.org/[Kafka], and {link-kafka-docs}.html#connect[Kafka Connect], then using {prodname}'s SQL Server` connector is easy.\n-Simply download the https://repo1.maven.org/maven2/io/debezium/debezium-connector-sqlserver/{debezium-version}/debezium-connector-sqlserver-{debezium-version}-plugin.tar.gz[connector's plug-in archive], extract the JARs into your Kafka Connect environment, and add the directory with the JARs to {link-kafka-docs}/#connectconfigs[Kafka Connect's `plugin.path`].\n-Restart your Kafka Connect process to pick up the new JARs.\n+With link:https://zookeeper.apache.org[Zookeeper], http://kafka.apache.org/[Kafka], and {link-kafka-docs}.html#connect[Kafka Connect] installed, the remaining tasks to deploy a {prodname} SQL Server connector are to download the https://repo1.maven.org/maven2/io/debezium/debezium-connector-sqlserver/{debezium-version}/debezium-connector-sqlserver-{debezium-version}-plugin.tar.gz[connector's plug-in archive], extract the JAR files into your Kafka Connect environment, and add the directory with the JAR files to {link-kafka-docs}/#connectconfigs[Kafka Connect's `plugin.path`].\n+Restart your Kafka Connect process to pick up the new JAR files.\n+\n+If you need immutable containers, see link:https://hub.docker.com/r/debezium/[{prodname}'s container images] for Zookeeper, Kafka, SQL Server and Kafka Connect with the SQL Server connector already installed and ready to run. You can also link:https://debezium.io/blog/2016/05/31/Debezium-on-Kubernetes/[run {prodname} on Kubernetes and OpenShift].", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTQzNzU5Mg=="}, "originalCommit": null, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0NzI4MTA3OnYy", "diffSide": "RIGHT", "path": "documentation/antora.yml", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxNjozMTowNlrOHQlvPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQxNzoxMDozM1rOHQnAUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzE1NzU2NQ==", "bodyText": "Isn't this variable only used in ifdef blocks related to downstream?\nIf so, we probably don't need it in the upstream antora.yml since we would never reference it in our rendered output.\nIt's fine of course to leave it if you'd rather but that implies if this changes downstream, upstream would require the same change and that's my concern wrt maintenance of the link.", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r487157565", "createdAt": "2020-09-11T16:31:06Z", "author": {"login": "Naros"}, "path": "documentation/antora.yml", "diffHunk": "@@ -10,6 +10,7 @@ asciidoc:\n     debezium-dev-version: '1.3'\n     debezium-kafka-version: '2.6.0'\n     debezium-docker-label: '1.2'\n+    DockerKafkaConnect: registry.redhat.io/amq7/amq-streams-kafka-25:1.5.0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eec5719c5c252f65c7cf28271beda3802ad13724"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzE1OTA4Nw==", "bodyText": "Yes, I think that is correct.\nIs there a problem with having it here?\nIf yes, we don't need it in the antora file.", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r487159087", "createdAt": "2020-09-11T16:34:01Z", "author": {"login": "TovaCohen"}, "path": "documentation/antora.yml", "diffHunk": "@@ -10,6 +10,7 @@ asciidoc:\n     debezium-dev-version: '1.3'\n     debezium-kafka-version: '2.6.0'\n     debezium-docker-label: '1.2'\n+    DockerKafkaConnect: registry.redhat.io/amq7/amq-streams-kafka-25:1.5.0", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzE1NzU2NQ=="}, "originalCommit": {"oid": "eec5719c5c252f65c7cf28271beda3802ad13724"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzE2Nzg2Nw==", "bodyText": "There isn't a problem having it other than the maintenance aspect.", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r487167867", "createdAt": "2020-09-11T16:50:39Z", "author": {"login": "Naros"}, "path": "documentation/antora.yml", "diffHunk": "@@ -10,6 +10,7 @@ asciidoc:\n     debezium-dev-version: '1.3'\n     debezium-kafka-version: '2.6.0'\n     debezium-docker-label: '1.2'\n+    DockerKafkaConnect: registry.redhat.io/amq7/amq-streams-kafka-25:1.5.0", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzE1NzU2NQ=="}, "originalCommit": {"oid": "eec5719c5c252f65c7cf28271beda3802ad13724"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzE3ODMyMA==", "bodyText": "Please do whatever you think best.\nIf you want me to do something, please let me know.\nAs soon as this PR is merged and backported to 1.2, I will fetch the updates and rebuild the downstream UG.", "url": "https://github.com/debezium/debezium/pull/1801#discussion_r487178320", "createdAt": "2020-09-11T17:10:33Z", "author": {"login": "TovaCohen"}, "path": "documentation/antora.yml", "diffHunk": "@@ -10,6 +10,7 @@ asciidoc:\n     debezium-dev-version: '1.3'\n     debezium-kafka-version: '2.6.0'\n     debezium-docker-label: '1.2'\n+    DockerKafkaConnect: registry.redhat.io/amq7/amq-streams-kafka-25:1.5.0", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzE1NzU2NQ=="}, "originalCommit": {"oid": "eec5719c5c252f65c7cf28271beda3802ad13724"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4192, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}