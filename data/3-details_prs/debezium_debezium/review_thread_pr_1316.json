{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg1MzA5ODYy", "number": 1316, "reviewThreads": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQwNjoxNTozOFrODmK7sQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxMjoxNzowM1rODmp3yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMzUxNjAxOnYy", "diffSide": "RIGHT", "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbOffsetContext.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQwNjoxNTozOFrOFzb24g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxODo1Mzo0N1rOF0cpIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ3ODExNA==", "bodyText": "I wonder, does it make sense to have an interface hierarchy\nSingleOfsetContext extends OffsetConext\nPartitionedOffsetContext extends OffsetContext", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r389478114", "createdAt": "2020-03-09T06:15:38Z", "author": {"login": "jpechane"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbOffsetContext.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.Struct;\n+import org.bson.Document;\n+\n+import io.debezium.connector.SnapshotRecord;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.schema.DataCollectionId;\n+\n+/**\n+ * A context that facilitates the management of the current offsets across a set of mongodb replica sets.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbOffsetContext implements OffsetContext {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56d4a1cf6a8ce61d877e8881e682fab584ac53e1"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY3OTgwMQ==", "bodyText": "If I understand correctly, then mongo would use this PartitionedOffsetContext since it relies on multiple partitions per task instance where-as our other connectors would use the other implementation since they only rely on a single offset with a hard-coded partition?", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r389679801", "createdAt": "2020-03-09T13:42:48Z", "author": {"login": "Naros"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbOffsetContext.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.Struct;\n+import org.bson.Document;\n+\n+import io.debezium.connector.SnapshotRecord;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.schema.DataCollectionId;\n+\n+/**\n+ * A context that facilitates the management of the current offsets across a set of mongodb replica sets.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbOffsetContext implements OffsetContext {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ3ODExNA=="}, "originalCommit": {"oid": "56d4a1cf6a8ce61d877e8881e682fab584ac53e1"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUzOTU1Mg==", "bodyText": "Added this as a follow-up in DBZ-1860.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390539552", "createdAt": "2020-03-10T18:53:47Z", "author": {"login": "Naros"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbOffsetContext.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.Struct;\n+import org.bson.Document;\n+\n+import io.debezium.connector.SnapshotRecord;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.schema.DataCollectionId;\n+\n+/**\n+ * A context that facilitates the management of the current offsets across a set of mongodb replica sets.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbOffsetContext implements OffsetContext {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTQ3ODExNA=="}, "originalCommit": {"oid": "56d4a1cf6a8ce61d877e8881e682fab584ac53e1"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMzg3OTM2OnYy", "diffSide": "RIGHT", "path": "debezium-core/src/main/java/io/debezium/pipeline/metrics/SnapshotChangeEventSourceMetrics.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQwOTowODo1NVrOFzfRNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxNTowOToyM1rOFzsqiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTUzNDAwNw==", "bodyText": "Could this be DataCollectionId instead of String?", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r389534007", "createdAt": "2020-03-09T09:08:55Z", "author": {"login": "gunnarmorling"}, "path": "debezium-core/src/main/java/io/debezium/pipeline/metrics/SnapshotChangeEventSourceMetrics.java", "diffHunk": "@@ -100,9 +100,9 @@ public void monitoredTablesDetermined(Iterable<TableId> tableIds) {\n     }\n \n     @Override\n-    public void tableSnapshotCompleted(TableId tableId, long numRows) {\n-        rowsScanned.put(tableId.toString(), numRows);\n-        remainingTables.remove(tableId.toString());\n+    public void dataCollectionSnapshotCompleted(String dataCollectionId, long numRows) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56d4a1cf6a8ce61d877e8881e682fab584ac53e1"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY3NzIwMQ==", "bodyText": "It could though I would suggest adding a method to DataCollectionId rather than relying on toString().  Wdyt?", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r389677201", "createdAt": "2020-03-09T13:40:38Z", "author": {"login": "Naros"}, "path": "debezium-core/src/main/java/io/debezium/pipeline/metrics/SnapshotChangeEventSourceMetrics.java", "diffHunk": "@@ -100,9 +100,9 @@ public void monitoredTablesDetermined(Iterable<TableId> tableIds) {\n     }\n \n     @Override\n-    public void tableSnapshotCompleted(TableId tableId, long numRows) {\n-        rowsScanned.put(tableId.toString(), numRows);\n-        remainingTables.remove(tableId.toString());\n+    public void dataCollectionSnapshotCompleted(String dataCollectionId, long numRows) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTUzNDAwNw=="}, "originalCommit": {"oid": "56d4a1cf6a8ce61d877e8881e682fab584ac53e1"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTY5OTYzMg==", "bodyText": "Ok, works for me \ud83d\udc4d", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r389699632", "createdAt": "2020-03-09T14:00:05Z", "author": {"login": "gunnarmorling"}, "path": "debezium-core/src/main/java/io/debezium/pipeline/metrics/SnapshotChangeEventSourceMetrics.java", "diffHunk": "@@ -100,9 +100,9 @@ public void monitoredTablesDetermined(Iterable<TableId> tableIds) {\n     }\n \n     @Override\n-    public void tableSnapshotCompleted(TableId tableId, long numRows) {\n-        rowsScanned.put(tableId.toString(), numRows);\n-        remainingTables.remove(tableId.toString());\n+    public void dataCollectionSnapshotCompleted(String dataCollectionId, long numRows) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTUzNDAwNw=="}, "originalCommit": {"oid": "56d4a1cf6a8ce61d877e8881e682fab584ac53e1"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTc1MzQ4MQ==", "bodyText": "Added this as a part of commit 62123b6.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r389753481", "createdAt": "2020-03-09T15:09:23Z", "author": {"login": "Naros"}, "path": "debezium-core/src/main/java/io/debezium/pipeline/metrics/SnapshotChangeEventSourceMetrics.java", "diffHunk": "@@ -100,9 +100,9 @@ public void monitoredTablesDetermined(Iterable<TableId> tableIds) {\n     }\n \n     @Override\n-    public void tableSnapshotCompleted(TableId tableId, long numRows) {\n-        rowsScanned.put(tableId.toString(), numRows);\n-        remainingTables.remove(tableId.toString());\n+    public void dataCollectionSnapshotCompleted(String dataCollectionId, long numRows) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTUzNDAwNw=="}, "originalCommit": {"oid": "56d4a1cf6a8ce61d877e8881e682fab584ac53e1"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMzg4MDYwOnYy", "diffSide": "RIGHT", "path": "debezium-core/src/main/java/io/debezium/pipeline/source/spi/SnapshotProgressListener.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQwOTowOToyNFrOFzfSBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQwNjozNTo0NVrOF0C4Yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTUzNDIxMw==", "bodyText": "Same question as above.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r389534213", "createdAt": "2020-03-09T09:09:24Z", "author": {"login": "gunnarmorling"}, "path": "debezium-core/src/main/java/io/debezium/pipeline/source/spi/SnapshotProgressListener.java", "diffHunk": "@@ -41,7 +49,7 @@ public void monitoredTablesDetermined(Iterable<TableId> tableIds) {\n         }\n \n         @Override\n-        public void tableSnapshotCompleted(TableId tableId, long numRows) {\n+        public void dataCollectionSnapshotCompleted(String dataCollectionId, long numRows) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "56d4a1cf6a8ce61d877e8881e682fab584ac53e1"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTc1MzU3Mw==", "bodyText": "See above.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r389753573", "createdAt": "2020-03-09T15:09:30Z", "author": {"login": "Naros"}, "path": "debezium-core/src/main/java/io/debezium/pipeline/source/spi/SnapshotProgressListener.java", "diffHunk": "@@ -41,7 +49,7 @@ public void monitoredTablesDetermined(Iterable<TableId> tableIds) {\n         }\n \n         @Override\n-        public void tableSnapshotCompleted(TableId tableId, long numRows) {\n+        public void dataCollectionSnapshotCompleted(String dataCollectionId, long numRows) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTUzNDIxMw=="}, "originalCommit": {"oid": "56d4a1cf6a8ce61d877e8881e682fab584ac53e1"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDExNzQ3NQ==", "bodyText": "Thx!", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390117475", "createdAt": "2020-03-10T06:35:45Z", "author": {"login": "gunnarmorling"}, "path": "debezium-core/src/main/java/io/debezium/pipeline/source/spi/SnapshotProgressListener.java", "diffHunk": "@@ -41,7 +49,7 @@ public void monitoredTablesDetermined(Iterable<TableId> tableIds) {\n         }\n \n         @Override\n-        public void tableSnapshotCompleted(TableId tableId, long numRows) {\n+        public void dataCollectionSnapshotCompleted(String dataCollectionId, long numRows) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTUzNDIxMw=="}, "originalCommit": {"oid": "56d4a1cf6a8ce61d877e8881e682fab584ac53e1"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxNzc2OTc4OnYy", "diffSide": "RIGHT", "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbOffsetContext.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQwODowMzowOVrOF0EgmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQwODowMzowOVrOF0EgmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NDE1Mw==", "bodyText": "connectorConfig is unused.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390144153", "createdAt": "2020-03-10T08:03:09Z", "author": {"login": "gunnarmorling"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbOffsetContext.java", "diffHunk": "@@ -0,0 +1,158 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.Struct;\n+import org.bson.Document;\n+\n+import io.debezium.connector.SnapshotRecord;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.schema.DataCollectionId;\n+\n+/**\n+ * A context that facilitates the management of the current offsets across a set of mongodb replica sets.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbOffsetContext implements OffsetContext {\n+\n+    private final MongoDbConnectorConfig connectorConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62123b66a26d38590acc3a123295bb3c2d3d18a9"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxNzc3MjE4OnYy", "diffSide": "RIGHT", "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbStreamingChangeEventSource.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQwODowNDoxMlrOF0EiHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQwODowNDoxMlrOF0EiHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDE0NDU0MA==", "bodyText": "That's unused.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390144540", "createdAt": "2020-03-10T08:04:12Z", "author": {"login": "gunnarmorling"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbStreamingChangeEventSource.java", "diffHunk": "@@ -0,0 +1,408 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.OptionalLong;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.bson.BsonTimestamp;\n+import org.bson.Document;\n+import org.bson.conversions.Bson;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.mongodb.CursorType;\n+import com.mongodb.MongoClient;\n+import com.mongodb.ServerAddress;\n+import com.mongodb.client.FindIterable;\n+import com.mongodb.client.MongoCollection;\n+import com.mongodb.client.MongoCursor;\n+import com.mongodb.client.model.Filters;\n+\n+import io.debezium.connector.mongodb.ConnectionContext.MongoPrimary;\n+import io.debezium.pipeline.ErrorHandler;\n+import io.debezium.pipeline.EventDispatcher;\n+import io.debezium.pipeline.source.spi.StreamingChangeEventSource;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.util.Clock;\n+import io.debezium.util.Metronome;\n+import io.debezium.util.Threads;\n+\n+/**\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbStreamingChangeEventSource implements StreamingChangeEventSource {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(MongoDbStreamingChangeEventSource.class);\n+\n+    private static final String AUTHORIZATION_FAILURE_MESSAGE = \"Command failed with error 13\";\n+\n+    private static final String OPERATION_FIELD = \"op\";\n+    private static final String OBJECT_FIELD = \"o\";\n+    private static final String OPERATION_CONTROL = \"c\";\n+    private static final String TX_OPS = \"applyOps\";\n+\n+    private final EventDispatcher<CollectionId> dispatcher;\n+    private final ErrorHandler errorHandler;\n+    private final Clock clock;\n+    private final MongoDbOffsetContext offsetContext;\n+    private final MongoDbConnectorConfig connectorConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62123b66a26d38590acc3a123295bb3c2d3d18a9"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxODQ5MzM3OnYy", "diffSide": "RIGHT", "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbSnapshotChangeEventSource.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxMTo0NjozNFrOF0LfQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxNTo0NzozMFrOF0VFkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI1ODQ5Nw==", "bodyText": "There should be a common parent for RelationalSnapshotChangeEventSource and MongoDbSnapshotChangeEventSource that would contain a shared code like this.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390258497", "createdAt": "2020-03-10T11:46:34Z", "author": {"login": "jpechane"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbSnapshotChangeEventSource.java", "diffHunk": "@@ -0,0 +1,599 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.bson.BsonTimestamp;\n+import org.bson.Document;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.mongodb.client.MongoCollection;\n+import com.mongodb.client.MongoCursor;\n+import com.mongodb.client.MongoDatabase;\n+\n+import io.debezium.config.ConfigurationDefaults;\n+import io.debezium.connector.mongodb.ConnectionContext.MongoPrimary;\n+import io.debezium.pipeline.ErrorHandler;\n+import io.debezium.pipeline.EventDispatcher;\n+import io.debezium.pipeline.EventDispatcher.SnapshotReceiver;\n+import io.debezium.pipeline.source.spi.SnapshotChangeEventSource;\n+import io.debezium.pipeline.source.spi.SnapshotProgressListener;\n+import io.debezium.pipeline.spi.ChangeRecordEmitter;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.pipeline.spi.SnapshotResult;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.util.Clock;\n+import io.debezium.util.Metronome;\n+import io.debezium.util.Strings;\n+import io.debezium.util.Threads;\n+import io.debezium.util.Threads.Timer;\n+\n+/**\n+ * A {@link SnapshotChangeEventSource} that performs multi-threaded snapshots of replica sets.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbSnapshotChangeEventSource implements SnapshotChangeEventSource {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(MongoDbSnapshotChangeEventSource.class);\n+\n+    private static final String AUTHORIZATION_FAILURE_MESSAGE = \"Command failed with error 13\";\n+\n+    private final MongoDbConnectorConfig connectorConfig;\n+    private final MongoDbTaskContext taskContext;\n+    private final MongoDbOffsetContext previousOffset;\n+    private final ConnectionContext connectionContext;\n+    private final ReplicaSets replicaSets;\n+    private final EventDispatcher<CollectionId> dispatcher;\n+    protected final Clock clock;\n+    private final SnapshotProgressListener snapshotProgressListener;\n+    private final ErrorHandler errorHandler;\n+    private AtomicBoolean aborted = new AtomicBoolean(false);\n+\n+    public MongoDbSnapshotChangeEventSource(MongoDbConnectorConfig connectorConfig, MongoDbTaskContext taskContext,\n+                                            ReplicaSets replicaSets, MongoDbOffsetContext previousOffset,\n+                                            EventDispatcher<CollectionId> dispatcher, Clock clock,\n+                                            SnapshotProgressListener snapshotProgressListener, ErrorHandler errorHandler) {\n+        this.connectorConfig = connectorConfig;\n+        this.taskContext = taskContext;\n+        this.connectionContext = taskContext.getConnectionContext();\n+        this.previousOffset = previousOffset;\n+        this.replicaSets = replicaSets;\n+        this.dispatcher = dispatcher;\n+        this.clock = clock;\n+        this.snapshotProgressListener = snapshotProgressListener;\n+        this.errorHandler = errorHandler;\n+    }\n+\n+    @Override\n+    public SnapshotResult execute(ChangeEventSourceContext context) throws InterruptedException {\n+        SnapshottingTask snapshottingTask = getSnapshottingTask(previousOffset, replicaSets);\n+        if (!snapshottingTask.snapshotData()) {\n+            LOGGER.debug(\"Skipping snapshotting\");\n+            return SnapshotResult.skipped(previousOffset);\n+        }\n+\n+        delaySnapshotIfNeeded(context);\n+\n+        final SnapshotContext ctx;\n+        try {\n+            ctx = prepare(context);\n+        }\n+        catch (Exception e) {\n+            LOGGER.error(\"Failed to initialize snapshot context.\", e);\n+            throw new RuntimeException(e);\n+        }\n+\n+        try {\n+            LOGGER.info(\"Snapshot step 1 - Preparing\");\n+            snapshotProgressListener.snapshotStarted();\n+\n+            if (previousOffset != null && previousOffset.isSnapshotRunning()) {\n+                LOGGER.info(\"Previous snapshot was cancelled before completion; a new snapshot will be taken.\");\n+            }\n+\n+            LOGGER.info(\"Snapshot step 2 - Determining snapshot offsets\");\n+            determineSnapshotOffsets(ctx, replicaSets);\n+\n+            List<ReplicaSet> replicaSetsToSnapshot = snapshottingTask.getReplicaSetsToSnapshot();\n+\n+            final int threads = replicaSetsToSnapshot.size();\n+            final ExecutorService executor = Threads.newFixedThreadPool(MongoDbConnector.class, taskContext.serverName(), \"replicator-snapshot\", threads);\n+            final CountDownLatch latch = new CountDownLatch(threads);\n+\n+            LOGGER.info(\"Ignoring unnamed replica sets: {}\", replicaSets.unnamedReplicaSets());\n+            LOGGER.info(\"Starting {} thread(s) to snapshot replica sets: {}\", threads, replicaSetsToSnapshot);\n+\n+            LOGGER.info(\"Snapshot step 3 - Snapshotting data\");\n+            replicaSetsToSnapshot.forEach(replicaSet -> {\n+                executor.submit(() -> {\n+                    try {\n+                        taskContext.configureLoggingContext(replicaSet.replicaSetName());\n+                        try {\n+                            snapshotReplicaSet(context, ctx, replicaSet);\n+                        }\n+                        finally {\n+                            final MongoDbOffsetContext offset = (MongoDbOffsetContext) ctx.offset;\n+                            // todo: DBZ-1726 - this causes MongoDbConnectorIT#shouldEmitHeartbeatMessages to fail\n+                            // omitted for now since it does not appear we did this in previous connector code.\n+                            // dispatcher.alwaysDispatchHeartbeatEvent(offset.getReplicaSetOffsetContext(replicaSet));\n+                        }\n+                    }\n+                    catch (Throwable t) {\n+                        LOGGER.error(\"Snapshot for replica set {} failed\", replicaSet.replicaSetName(), t);\n+                        errorHandler.setProducerThrowable(t);\n+                    }\n+                    finally {\n+                        latch.countDown();\n+                    }\n+                });\n+            });\n+\n+            // Wait for the executor service threads to end.\n+            try {\n+                latch.await();\n+            }\n+            catch (InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                aborted.set(true);\n+            }\n+\n+            // Shutdown executor and close connections\n+            try {\n+                executor.shutdown();\n+            }\n+            finally {\n+                LOGGER.info(\"Stopping mongodb connections\");\n+                taskContext.getConnectionContext().shutdown();\n+            }\n+\n+            if (aborted.get()) {\n+                return SnapshotResult.aborted();\n+            }\n+\n+            snapshotProgressListener.snapshotCompleted();\n+\n+            return SnapshotResult.completed(ctx.offset);\n+        }\n+        catch (InterruptedException e) {\n+            LOGGER.warn(\"Snapshot was interrupted before completion\");\n+            snapshotProgressListener.snapshotAborted();\n+            throw e;\n+        }\n+        catch (RuntimeException e) {\n+            snapshotProgressListener.snapshotAborted();\n+            throw e;\n+        }\n+        catch (Throwable t) {\n+            snapshotProgressListener.snapshotAborted();\n+            throw new RuntimeException(t);\n+        }\n+        finally {\n+            LOGGER.info(\"Snapshot step 4 - Finalizing\");\n+            complete(ctx);\n+        }\n+    }\n+\n+    protected SnapshottingTask getSnapshottingTask(OffsetContext previousOffset, ReplicaSets replicaSets) {\n+        if (previousOffset == null) {\n+            LOGGER.info(\"No previous offset has been found\");\n+            if (connectorConfig.getSnapshotMode().equals(MongoDbConnectorConfig.SnapshotMode.NEVER)) {\n+                LOGGER.info(\"According to the connector configuration, no snapshot will occur.\");\n+                return new SnapshottingTask(Collections.emptyList());\n+            }\n+            return new SnapshottingTask(replicaSets.all());\n+        }\n+\n+        // Even if there are previous offsets, if no snapshot should occur, return task with no replica sets\n+        if (connectorConfig.getSnapshotMode().equals(MongoDbConnectorConfig.SnapshotMode.NEVER)) {\n+            LOGGER.info(\"According to the connector configuration, no snapshot will occur.\");\n+            return new SnapshottingTask(Collections.emptyList());\n+        }\n+\n+        // Collect which replica-sets require being snapshotted\n+        final List<ReplicaSet> replicaSetSnapshots = new ArrayList<>();\n+        final MongoDbOffsetContext offsetContext = (MongoDbOffsetContext) previousOffset;\n+        try {\n+            replicaSets.onEachReplicaSet(replicaSet -> {\n+                MongoPrimary primary = null;\n+                try {\n+                    primary = establishConnectionToPrimary(replicaSet);\n+                    final ReplicaSetOffsetContext rsOffsetContext = offsetContext.getReplicaSetOffsetContext(replicaSet);\n+                    if (primary != null && isInitialSyncExpected(primary, rsOffsetContext)) {\n+                        replicaSetSnapshots.add(replicaSet);\n+                    }\n+                }\n+                finally {\n+                    if (primary != null) {\n+                        primary.stop();\n+                    }\n+                }\n+            });\n+        }\n+        finally {\n+            taskContext.getConnectionContext().shutdown();\n+        }\n+\n+        return new SnapshottingTask(replicaSetSnapshots);\n+    }\n+\n+    private void delaySnapshotIfNeeded(ChangeEventSourceContext context) throws InterruptedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 238}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQxNTc2MQ==", "bodyText": "See commit 0158b56.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390415761", "createdAt": "2020-03-10T15:47:30Z", "author": {"login": "Naros"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbSnapshotChangeEventSource.java", "diffHunk": "@@ -0,0 +1,599 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.bson.BsonTimestamp;\n+import org.bson.Document;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.mongodb.client.MongoCollection;\n+import com.mongodb.client.MongoCursor;\n+import com.mongodb.client.MongoDatabase;\n+\n+import io.debezium.config.ConfigurationDefaults;\n+import io.debezium.connector.mongodb.ConnectionContext.MongoPrimary;\n+import io.debezium.pipeline.ErrorHandler;\n+import io.debezium.pipeline.EventDispatcher;\n+import io.debezium.pipeline.EventDispatcher.SnapshotReceiver;\n+import io.debezium.pipeline.source.spi.SnapshotChangeEventSource;\n+import io.debezium.pipeline.source.spi.SnapshotProgressListener;\n+import io.debezium.pipeline.spi.ChangeRecordEmitter;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.pipeline.spi.SnapshotResult;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.util.Clock;\n+import io.debezium.util.Metronome;\n+import io.debezium.util.Strings;\n+import io.debezium.util.Threads;\n+import io.debezium.util.Threads.Timer;\n+\n+/**\n+ * A {@link SnapshotChangeEventSource} that performs multi-threaded snapshots of replica sets.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbSnapshotChangeEventSource implements SnapshotChangeEventSource {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(MongoDbSnapshotChangeEventSource.class);\n+\n+    private static final String AUTHORIZATION_FAILURE_MESSAGE = \"Command failed with error 13\";\n+\n+    private final MongoDbConnectorConfig connectorConfig;\n+    private final MongoDbTaskContext taskContext;\n+    private final MongoDbOffsetContext previousOffset;\n+    private final ConnectionContext connectionContext;\n+    private final ReplicaSets replicaSets;\n+    private final EventDispatcher<CollectionId> dispatcher;\n+    protected final Clock clock;\n+    private final SnapshotProgressListener snapshotProgressListener;\n+    private final ErrorHandler errorHandler;\n+    private AtomicBoolean aborted = new AtomicBoolean(false);\n+\n+    public MongoDbSnapshotChangeEventSource(MongoDbConnectorConfig connectorConfig, MongoDbTaskContext taskContext,\n+                                            ReplicaSets replicaSets, MongoDbOffsetContext previousOffset,\n+                                            EventDispatcher<CollectionId> dispatcher, Clock clock,\n+                                            SnapshotProgressListener snapshotProgressListener, ErrorHandler errorHandler) {\n+        this.connectorConfig = connectorConfig;\n+        this.taskContext = taskContext;\n+        this.connectionContext = taskContext.getConnectionContext();\n+        this.previousOffset = previousOffset;\n+        this.replicaSets = replicaSets;\n+        this.dispatcher = dispatcher;\n+        this.clock = clock;\n+        this.snapshotProgressListener = snapshotProgressListener;\n+        this.errorHandler = errorHandler;\n+    }\n+\n+    @Override\n+    public SnapshotResult execute(ChangeEventSourceContext context) throws InterruptedException {\n+        SnapshottingTask snapshottingTask = getSnapshottingTask(previousOffset, replicaSets);\n+        if (!snapshottingTask.snapshotData()) {\n+            LOGGER.debug(\"Skipping snapshotting\");\n+            return SnapshotResult.skipped(previousOffset);\n+        }\n+\n+        delaySnapshotIfNeeded(context);\n+\n+        final SnapshotContext ctx;\n+        try {\n+            ctx = prepare(context);\n+        }\n+        catch (Exception e) {\n+            LOGGER.error(\"Failed to initialize snapshot context.\", e);\n+            throw new RuntimeException(e);\n+        }\n+\n+        try {\n+            LOGGER.info(\"Snapshot step 1 - Preparing\");\n+            snapshotProgressListener.snapshotStarted();\n+\n+            if (previousOffset != null && previousOffset.isSnapshotRunning()) {\n+                LOGGER.info(\"Previous snapshot was cancelled before completion; a new snapshot will be taken.\");\n+            }\n+\n+            LOGGER.info(\"Snapshot step 2 - Determining snapshot offsets\");\n+            determineSnapshotOffsets(ctx, replicaSets);\n+\n+            List<ReplicaSet> replicaSetsToSnapshot = snapshottingTask.getReplicaSetsToSnapshot();\n+\n+            final int threads = replicaSetsToSnapshot.size();\n+            final ExecutorService executor = Threads.newFixedThreadPool(MongoDbConnector.class, taskContext.serverName(), \"replicator-snapshot\", threads);\n+            final CountDownLatch latch = new CountDownLatch(threads);\n+\n+            LOGGER.info(\"Ignoring unnamed replica sets: {}\", replicaSets.unnamedReplicaSets());\n+            LOGGER.info(\"Starting {} thread(s) to snapshot replica sets: {}\", threads, replicaSetsToSnapshot);\n+\n+            LOGGER.info(\"Snapshot step 3 - Snapshotting data\");\n+            replicaSetsToSnapshot.forEach(replicaSet -> {\n+                executor.submit(() -> {\n+                    try {\n+                        taskContext.configureLoggingContext(replicaSet.replicaSetName());\n+                        try {\n+                            snapshotReplicaSet(context, ctx, replicaSet);\n+                        }\n+                        finally {\n+                            final MongoDbOffsetContext offset = (MongoDbOffsetContext) ctx.offset;\n+                            // todo: DBZ-1726 - this causes MongoDbConnectorIT#shouldEmitHeartbeatMessages to fail\n+                            // omitted for now since it does not appear we did this in previous connector code.\n+                            // dispatcher.alwaysDispatchHeartbeatEvent(offset.getReplicaSetOffsetContext(replicaSet));\n+                        }\n+                    }\n+                    catch (Throwable t) {\n+                        LOGGER.error(\"Snapshot for replica set {} failed\", replicaSet.replicaSetName(), t);\n+                        errorHandler.setProducerThrowable(t);\n+                    }\n+                    finally {\n+                        latch.countDown();\n+                    }\n+                });\n+            });\n+\n+            // Wait for the executor service threads to end.\n+            try {\n+                latch.await();\n+            }\n+            catch (InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                aborted.set(true);\n+            }\n+\n+            // Shutdown executor and close connections\n+            try {\n+                executor.shutdown();\n+            }\n+            finally {\n+                LOGGER.info(\"Stopping mongodb connections\");\n+                taskContext.getConnectionContext().shutdown();\n+            }\n+\n+            if (aborted.get()) {\n+                return SnapshotResult.aborted();\n+            }\n+\n+            snapshotProgressListener.snapshotCompleted();\n+\n+            return SnapshotResult.completed(ctx.offset);\n+        }\n+        catch (InterruptedException e) {\n+            LOGGER.warn(\"Snapshot was interrupted before completion\");\n+            snapshotProgressListener.snapshotAborted();\n+            throw e;\n+        }\n+        catch (RuntimeException e) {\n+            snapshotProgressListener.snapshotAborted();\n+            throw e;\n+        }\n+        catch (Throwable t) {\n+            snapshotProgressListener.snapshotAborted();\n+            throw new RuntimeException(t);\n+        }\n+        finally {\n+            LOGGER.info(\"Snapshot step 4 - Finalizing\");\n+            complete(ctx);\n+        }\n+    }\n+\n+    protected SnapshottingTask getSnapshottingTask(OffsetContext previousOffset, ReplicaSets replicaSets) {\n+        if (previousOffset == null) {\n+            LOGGER.info(\"No previous offset has been found\");\n+            if (connectorConfig.getSnapshotMode().equals(MongoDbConnectorConfig.SnapshotMode.NEVER)) {\n+                LOGGER.info(\"According to the connector configuration, no snapshot will occur.\");\n+                return new SnapshottingTask(Collections.emptyList());\n+            }\n+            return new SnapshottingTask(replicaSets.all());\n+        }\n+\n+        // Even if there are previous offsets, if no snapshot should occur, return task with no replica sets\n+        if (connectorConfig.getSnapshotMode().equals(MongoDbConnectorConfig.SnapshotMode.NEVER)) {\n+            LOGGER.info(\"According to the connector configuration, no snapshot will occur.\");\n+            return new SnapshottingTask(Collections.emptyList());\n+        }\n+\n+        // Collect which replica-sets require being snapshotted\n+        final List<ReplicaSet> replicaSetSnapshots = new ArrayList<>();\n+        final MongoDbOffsetContext offsetContext = (MongoDbOffsetContext) previousOffset;\n+        try {\n+            replicaSets.onEachReplicaSet(replicaSet -> {\n+                MongoPrimary primary = null;\n+                try {\n+                    primary = establishConnectionToPrimary(replicaSet);\n+                    final ReplicaSetOffsetContext rsOffsetContext = offsetContext.getReplicaSetOffsetContext(replicaSet);\n+                    if (primary != null && isInitialSyncExpected(primary, rsOffsetContext)) {\n+                        replicaSetSnapshots.add(replicaSet);\n+                    }\n+                }\n+                finally {\n+                    if (primary != null) {\n+                        primary.stop();\n+                    }\n+                }\n+            });\n+        }\n+        finally {\n+            taskContext.getConnectionContext().shutdown();\n+        }\n+\n+        return new SnapshottingTask(replicaSetSnapshots);\n+    }\n+\n+    private void delaySnapshotIfNeeded(ChangeEventSourceContext context) throws InterruptedException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI1ODQ5Nw=="}, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 238}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxODUwNzA0OnYy", "diffSide": "RIGHT", "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbSnapshotChangeEventSource.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxMTo1MToyMlrOF0LntA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxNTo0NzoyMVrOF0VFGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI2MDY2MA==", "bodyText": "Hardcoded return false value - etiher a bug or code not needed", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390260660", "createdAt": "2020-03-10T11:51:22Z", "author": {"login": "jpechane"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbSnapshotChangeEventSource.java", "diffHunk": "@@ -0,0 +1,599 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.bson.BsonTimestamp;\n+import org.bson.Document;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.mongodb.client.MongoCollection;\n+import com.mongodb.client.MongoCursor;\n+import com.mongodb.client.MongoDatabase;\n+\n+import io.debezium.config.ConfigurationDefaults;\n+import io.debezium.connector.mongodb.ConnectionContext.MongoPrimary;\n+import io.debezium.pipeline.ErrorHandler;\n+import io.debezium.pipeline.EventDispatcher;\n+import io.debezium.pipeline.EventDispatcher.SnapshotReceiver;\n+import io.debezium.pipeline.source.spi.SnapshotChangeEventSource;\n+import io.debezium.pipeline.source.spi.SnapshotProgressListener;\n+import io.debezium.pipeline.spi.ChangeRecordEmitter;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.pipeline.spi.SnapshotResult;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.util.Clock;\n+import io.debezium.util.Metronome;\n+import io.debezium.util.Strings;\n+import io.debezium.util.Threads;\n+import io.debezium.util.Threads.Timer;\n+\n+/**\n+ * A {@link SnapshotChangeEventSource} that performs multi-threaded snapshots of replica sets.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbSnapshotChangeEventSource implements SnapshotChangeEventSource {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(MongoDbSnapshotChangeEventSource.class);\n+\n+    private static final String AUTHORIZATION_FAILURE_MESSAGE = \"Command failed with error 13\";\n+\n+    private final MongoDbConnectorConfig connectorConfig;\n+    private final MongoDbTaskContext taskContext;\n+    private final MongoDbOffsetContext previousOffset;\n+    private final ConnectionContext connectionContext;\n+    private final ReplicaSets replicaSets;\n+    private final EventDispatcher<CollectionId> dispatcher;\n+    protected final Clock clock;\n+    private final SnapshotProgressListener snapshotProgressListener;\n+    private final ErrorHandler errorHandler;\n+    private AtomicBoolean aborted = new AtomicBoolean(false);\n+\n+    public MongoDbSnapshotChangeEventSource(MongoDbConnectorConfig connectorConfig, MongoDbTaskContext taskContext,\n+                                            ReplicaSets replicaSets, MongoDbOffsetContext previousOffset,\n+                                            EventDispatcher<CollectionId> dispatcher, Clock clock,\n+                                            SnapshotProgressListener snapshotProgressListener, ErrorHandler errorHandler) {\n+        this.connectorConfig = connectorConfig;\n+        this.taskContext = taskContext;\n+        this.connectionContext = taskContext.getConnectionContext();\n+        this.previousOffset = previousOffset;\n+        this.replicaSets = replicaSets;\n+        this.dispatcher = dispatcher;\n+        this.clock = clock;\n+        this.snapshotProgressListener = snapshotProgressListener;\n+        this.errorHandler = errorHandler;\n+    }\n+\n+    @Override\n+    public SnapshotResult execute(ChangeEventSourceContext context) throws InterruptedException {\n+        SnapshottingTask snapshottingTask = getSnapshottingTask(previousOffset, replicaSets);\n+        if (!snapshottingTask.snapshotData()) {\n+            LOGGER.debug(\"Skipping snapshotting\");\n+            return SnapshotResult.skipped(previousOffset);\n+        }\n+\n+        delaySnapshotIfNeeded(context);\n+\n+        final SnapshotContext ctx;\n+        try {\n+            ctx = prepare(context);\n+        }\n+        catch (Exception e) {\n+            LOGGER.error(\"Failed to initialize snapshot context.\", e);\n+            throw new RuntimeException(e);\n+        }\n+\n+        try {\n+            LOGGER.info(\"Snapshot step 1 - Preparing\");\n+            snapshotProgressListener.snapshotStarted();\n+\n+            if (previousOffset != null && previousOffset.isSnapshotRunning()) {\n+                LOGGER.info(\"Previous snapshot was cancelled before completion; a new snapshot will be taken.\");\n+            }\n+\n+            LOGGER.info(\"Snapshot step 2 - Determining snapshot offsets\");\n+            determineSnapshotOffsets(ctx, replicaSets);\n+\n+            List<ReplicaSet> replicaSetsToSnapshot = snapshottingTask.getReplicaSetsToSnapshot();\n+\n+            final int threads = replicaSetsToSnapshot.size();\n+            final ExecutorService executor = Threads.newFixedThreadPool(MongoDbConnector.class, taskContext.serverName(), \"replicator-snapshot\", threads);\n+            final CountDownLatch latch = new CountDownLatch(threads);\n+\n+            LOGGER.info(\"Ignoring unnamed replica sets: {}\", replicaSets.unnamedReplicaSets());\n+            LOGGER.info(\"Starting {} thread(s) to snapshot replica sets: {}\", threads, replicaSetsToSnapshot);\n+\n+            LOGGER.info(\"Snapshot step 3 - Snapshotting data\");\n+            replicaSetsToSnapshot.forEach(replicaSet -> {\n+                executor.submit(() -> {\n+                    try {\n+                        taskContext.configureLoggingContext(replicaSet.replicaSetName());\n+                        try {\n+                            snapshotReplicaSet(context, ctx, replicaSet);\n+                        }\n+                        finally {\n+                            final MongoDbOffsetContext offset = (MongoDbOffsetContext) ctx.offset;\n+                            // todo: DBZ-1726 - this causes MongoDbConnectorIT#shouldEmitHeartbeatMessages to fail\n+                            // omitted for now since it does not appear we did this in previous connector code.\n+                            // dispatcher.alwaysDispatchHeartbeatEvent(offset.getReplicaSetOffsetContext(replicaSet));\n+                        }\n+                    }\n+                    catch (Throwable t) {\n+                        LOGGER.error(\"Snapshot for replica set {} failed\", replicaSet.replicaSetName(), t);\n+                        errorHandler.setProducerThrowable(t);\n+                    }\n+                    finally {\n+                        latch.countDown();\n+                    }\n+                });\n+            });\n+\n+            // Wait for the executor service threads to end.\n+            try {\n+                latch.await();\n+            }\n+            catch (InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                aborted.set(true);\n+            }\n+\n+            // Shutdown executor and close connections\n+            try {\n+                executor.shutdown();\n+            }\n+            finally {\n+                LOGGER.info(\"Stopping mongodb connections\");\n+                taskContext.getConnectionContext().shutdown();\n+            }\n+\n+            if (aborted.get()) {\n+                return SnapshotResult.aborted();\n+            }\n+\n+            snapshotProgressListener.snapshotCompleted();\n+\n+            return SnapshotResult.completed(ctx.offset);\n+        }\n+        catch (InterruptedException e) {\n+            LOGGER.warn(\"Snapshot was interrupted before completion\");\n+            snapshotProgressListener.snapshotAborted();\n+            throw e;\n+        }\n+        catch (RuntimeException e) {\n+            snapshotProgressListener.snapshotAborted();\n+            throw e;\n+        }\n+        catch (Throwable t) {\n+            snapshotProgressListener.snapshotAborted();\n+            throw new RuntimeException(t);\n+        }\n+        finally {\n+            LOGGER.info(\"Snapshot step 4 - Finalizing\");\n+            complete(ctx);\n+        }\n+    }\n+\n+    protected SnapshottingTask getSnapshottingTask(OffsetContext previousOffset, ReplicaSets replicaSets) {\n+        if (previousOffset == null) {\n+            LOGGER.info(\"No previous offset has been found\");\n+            if (connectorConfig.getSnapshotMode().equals(MongoDbConnectorConfig.SnapshotMode.NEVER)) {\n+                LOGGER.info(\"According to the connector configuration, no snapshot will occur.\");\n+                return new SnapshottingTask(Collections.emptyList());\n+            }\n+            return new SnapshottingTask(replicaSets.all());\n+        }\n+\n+        // Even if there are previous offsets, if no snapshot should occur, return task with no replica sets\n+        if (connectorConfig.getSnapshotMode().equals(MongoDbConnectorConfig.SnapshotMode.NEVER)) {\n+            LOGGER.info(\"According to the connector configuration, no snapshot will occur.\");\n+            return new SnapshottingTask(Collections.emptyList());\n+        }\n+\n+        // Collect which replica-sets require being snapshotted\n+        final List<ReplicaSet> replicaSetSnapshots = new ArrayList<>();\n+        final MongoDbOffsetContext offsetContext = (MongoDbOffsetContext) previousOffset;\n+        try {\n+            replicaSets.onEachReplicaSet(replicaSet -> {\n+                MongoPrimary primary = null;\n+                try {\n+                    primary = establishConnectionToPrimary(replicaSet);\n+                    final ReplicaSetOffsetContext rsOffsetContext = offsetContext.getReplicaSetOffsetContext(replicaSet);\n+                    if (primary != null && isInitialSyncExpected(primary, rsOffsetContext)) {\n+                        replicaSetSnapshots.add(replicaSet);\n+                    }\n+                }\n+                finally {\n+                    if (primary != null) {\n+                        primary.stop();\n+                    }\n+                }\n+            });\n+        }\n+        finally {\n+            taskContext.getConnectionContext().shutdown();\n+        }\n+\n+        return new SnapshottingTask(replicaSetSnapshots);\n+    }\n+\n+    private void delaySnapshotIfNeeded(ChangeEventSourceContext context) throws InterruptedException {\n+        Duration snapshotDelay = connectorConfig.getSnapshotDelay();\n+\n+        if (snapshotDelay.isZero() || snapshotDelay.isNegative()) {\n+            return;\n+        }\n+\n+        Timer timer = Threads.timer(Clock.SYSTEM, snapshotDelay);\n+        Metronome metronome = Metronome.parker(ConfigurationDefaults.RETURN_CONTROL_INTERVAL, Clock.SYSTEM);\n+\n+        while (!timer.expired()) {\n+            if (!context.isRunning()) {\n+                throw new InterruptedException(\"Interrupted while awaiting initial snapshot delay\");\n+            }\n+\n+            LOGGER.info(\"The connector will wait for {}s before proceeding\", timer.remaining().getSeconds());\n+            metronome.pause();\n+        }\n+    }\n+\n+    protected SnapshotContext prepare(ChangeEventSourceContext sourceContext) throws Exception {\n+        return new MongoDbSnapshotContext();\n+    }\n+\n+    protected void complete(SnapshotContext snapshotContext) {\n+    }\n+\n+    private void snapshotReplicaSet(ChangeEventSourceContext sourceContext, SnapshotContext ctx, ReplicaSet replicaSet) throws InterruptedException {\n+        MongoPrimary primaryClient = null;\n+        try {\n+            primaryClient = establishConnectionToPrimary(replicaSet);\n+            if (primaryClient != null) {\n+                createDataEvents(sourceContext, ctx, replicaSet, primaryClient);\n+            }\n+        }\n+        finally {\n+            if (primaryClient != null) {\n+                primaryClient.stop();\n+            }\n+        }\n+    }\n+\n+    private MongoPrimary establishConnectionToPrimary(ReplicaSet replicaSet) {\n+        return connectionContext.primaryFor(replicaSet, taskContext.filters(), (desc, error) -> {\n+            // propagate authorization failures\n+            if (error.getMessage() != null && error.getMessage().startsWith(AUTHORIZATION_FAILURE_MESSAGE)) {\n+                throw new ConnectException(\"Error while attempting to \" + desc, error);\n+            }\n+            else {\n+                LOGGER.error(\"Error while attempting to {}: \", desc, error.getMessage(), error);\n+                throw new ConnectException(\"Error while attempting to \" + desc, error);\n+            }\n+        });\n+    }\n+\n+    private boolean isInitialSyncExpected(MongoPrimary primaryClient, ReplicaSetOffsetContext offsetContext) {\n+        boolean performSnapshot = true;\n+        if (offsetContext.hasOffset()) {\n+            if (LOGGER.isInfoEnabled()) {\n+                LOGGER.info(\"Found existing offset for replica set '{}' at {}\", offsetContext.getReplicaSetName(), offsetContext.getOffset());\n+            }\n+            performSnapshot = false;\n+            if (connectionContext.performSnapshotEvenIfNotNeeded()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 300}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDM5MDI2MA==", "bodyText": "I guess that depends on how you look at it :P.\nMongoDB does not have a concept of SnapshotMode.ALWAYS like other connectors, it only supports never or initial, so I'd say this code that isn't needed.  wdyt?", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390390260", "createdAt": "2020-03-10T15:14:13Z", "author": {"login": "Naros"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbSnapshotChangeEventSource.java", "diffHunk": "@@ -0,0 +1,599 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.bson.BsonTimestamp;\n+import org.bson.Document;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.mongodb.client.MongoCollection;\n+import com.mongodb.client.MongoCursor;\n+import com.mongodb.client.MongoDatabase;\n+\n+import io.debezium.config.ConfigurationDefaults;\n+import io.debezium.connector.mongodb.ConnectionContext.MongoPrimary;\n+import io.debezium.pipeline.ErrorHandler;\n+import io.debezium.pipeline.EventDispatcher;\n+import io.debezium.pipeline.EventDispatcher.SnapshotReceiver;\n+import io.debezium.pipeline.source.spi.SnapshotChangeEventSource;\n+import io.debezium.pipeline.source.spi.SnapshotProgressListener;\n+import io.debezium.pipeline.spi.ChangeRecordEmitter;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.pipeline.spi.SnapshotResult;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.util.Clock;\n+import io.debezium.util.Metronome;\n+import io.debezium.util.Strings;\n+import io.debezium.util.Threads;\n+import io.debezium.util.Threads.Timer;\n+\n+/**\n+ * A {@link SnapshotChangeEventSource} that performs multi-threaded snapshots of replica sets.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbSnapshotChangeEventSource implements SnapshotChangeEventSource {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(MongoDbSnapshotChangeEventSource.class);\n+\n+    private static final String AUTHORIZATION_FAILURE_MESSAGE = \"Command failed with error 13\";\n+\n+    private final MongoDbConnectorConfig connectorConfig;\n+    private final MongoDbTaskContext taskContext;\n+    private final MongoDbOffsetContext previousOffset;\n+    private final ConnectionContext connectionContext;\n+    private final ReplicaSets replicaSets;\n+    private final EventDispatcher<CollectionId> dispatcher;\n+    protected final Clock clock;\n+    private final SnapshotProgressListener snapshotProgressListener;\n+    private final ErrorHandler errorHandler;\n+    private AtomicBoolean aborted = new AtomicBoolean(false);\n+\n+    public MongoDbSnapshotChangeEventSource(MongoDbConnectorConfig connectorConfig, MongoDbTaskContext taskContext,\n+                                            ReplicaSets replicaSets, MongoDbOffsetContext previousOffset,\n+                                            EventDispatcher<CollectionId> dispatcher, Clock clock,\n+                                            SnapshotProgressListener snapshotProgressListener, ErrorHandler errorHandler) {\n+        this.connectorConfig = connectorConfig;\n+        this.taskContext = taskContext;\n+        this.connectionContext = taskContext.getConnectionContext();\n+        this.previousOffset = previousOffset;\n+        this.replicaSets = replicaSets;\n+        this.dispatcher = dispatcher;\n+        this.clock = clock;\n+        this.snapshotProgressListener = snapshotProgressListener;\n+        this.errorHandler = errorHandler;\n+    }\n+\n+    @Override\n+    public SnapshotResult execute(ChangeEventSourceContext context) throws InterruptedException {\n+        SnapshottingTask snapshottingTask = getSnapshottingTask(previousOffset, replicaSets);\n+        if (!snapshottingTask.snapshotData()) {\n+            LOGGER.debug(\"Skipping snapshotting\");\n+            return SnapshotResult.skipped(previousOffset);\n+        }\n+\n+        delaySnapshotIfNeeded(context);\n+\n+        final SnapshotContext ctx;\n+        try {\n+            ctx = prepare(context);\n+        }\n+        catch (Exception e) {\n+            LOGGER.error(\"Failed to initialize snapshot context.\", e);\n+            throw new RuntimeException(e);\n+        }\n+\n+        try {\n+            LOGGER.info(\"Snapshot step 1 - Preparing\");\n+            snapshotProgressListener.snapshotStarted();\n+\n+            if (previousOffset != null && previousOffset.isSnapshotRunning()) {\n+                LOGGER.info(\"Previous snapshot was cancelled before completion; a new snapshot will be taken.\");\n+            }\n+\n+            LOGGER.info(\"Snapshot step 2 - Determining snapshot offsets\");\n+            determineSnapshotOffsets(ctx, replicaSets);\n+\n+            List<ReplicaSet> replicaSetsToSnapshot = snapshottingTask.getReplicaSetsToSnapshot();\n+\n+            final int threads = replicaSetsToSnapshot.size();\n+            final ExecutorService executor = Threads.newFixedThreadPool(MongoDbConnector.class, taskContext.serverName(), \"replicator-snapshot\", threads);\n+            final CountDownLatch latch = new CountDownLatch(threads);\n+\n+            LOGGER.info(\"Ignoring unnamed replica sets: {}\", replicaSets.unnamedReplicaSets());\n+            LOGGER.info(\"Starting {} thread(s) to snapshot replica sets: {}\", threads, replicaSetsToSnapshot);\n+\n+            LOGGER.info(\"Snapshot step 3 - Snapshotting data\");\n+            replicaSetsToSnapshot.forEach(replicaSet -> {\n+                executor.submit(() -> {\n+                    try {\n+                        taskContext.configureLoggingContext(replicaSet.replicaSetName());\n+                        try {\n+                            snapshotReplicaSet(context, ctx, replicaSet);\n+                        }\n+                        finally {\n+                            final MongoDbOffsetContext offset = (MongoDbOffsetContext) ctx.offset;\n+                            // todo: DBZ-1726 - this causes MongoDbConnectorIT#shouldEmitHeartbeatMessages to fail\n+                            // omitted for now since it does not appear we did this in previous connector code.\n+                            // dispatcher.alwaysDispatchHeartbeatEvent(offset.getReplicaSetOffsetContext(replicaSet));\n+                        }\n+                    }\n+                    catch (Throwable t) {\n+                        LOGGER.error(\"Snapshot for replica set {} failed\", replicaSet.replicaSetName(), t);\n+                        errorHandler.setProducerThrowable(t);\n+                    }\n+                    finally {\n+                        latch.countDown();\n+                    }\n+                });\n+            });\n+\n+            // Wait for the executor service threads to end.\n+            try {\n+                latch.await();\n+            }\n+            catch (InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                aborted.set(true);\n+            }\n+\n+            // Shutdown executor and close connections\n+            try {\n+                executor.shutdown();\n+            }\n+            finally {\n+                LOGGER.info(\"Stopping mongodb connections\");\n+                taskContext.getConnectionContext().shutdown();\n+            }\n+\n+            if (aborted.get()) {\n+                return SnapshotResult.aborted();\n+            }\n+\n+            snapshotProgressListener.snapshotCompleted();\n+\n+            return SnapshotResult.completed(ctx.offset);\n+        }\n+        catch (InterruptedException e) {\n+            LOGGER.warn(\"Snapshot was interrupted before completion\");\n+            snapshotProgressListener.snapshotAborted();\n+            throw e;\n+        }\n+        catch (RuntimeException e) {\n+            snapshotProgressListener.snapshotAborted();\n+            throw e;\n+        }\n+        catch (Throwable t) {\n+            snapshotProgressListener.snapshotAborted();\n+            throw new RuntimeException(t);\n+        }\n+        finally {\n+            LOGGER.info(\"Snapshot step 4 - Finalizing\");\n+            complete(ctx);\n+        }\n+    }\n+\n+    protected SnapshottingTask getSnapshottingTask(OffsetContext previousOffset, ReplicaSets replicaSets) {\n+        if (previousOffset == null) {\n+            LOGGER.info(\"No previous offset has been found\");\n+            if (connectorConfig.getSnapshotMode().equals(MongoDbConnectorConfig.SnapshotMode.NEVER)) {\n+                LOGGER.info(\"According to the connector configuration, no snapshot will occur.\");\n+                return new SnapshottingTask(Collections.emptyList());\n+            }\n+            return new SnapshottingTask(replicaSets.all());\n+        }\n+\n+        // Even if there are previous offsets, if no snapshot should occur, return task with no replica sets\n+        if (connectorConfig.getSnapshotMode().equals(MongoDbConnectorConfig.SnapshotMode.NEVER)) {\n+            LOGGER.info(\"According to the connector configuration, no snapshot will occur.\");\n+            return new SnapshottingTask(Collections.emptyList());\n+        }\n+\n+        // Collect which replica-sets require being snapshotted\n+        final List<ReplicaSet> replicaSetSnapshots = new ArrayList<>();\n+        final MongoDbOffsetContext offsetContext = (MongoDbOffsetContext) previousOffset;\n+        try {\n+            replicaSets.onEachReplicaSet(replicaSet -> {\n+                MongoPrimary primary = null;\n+                try {\n+                    primary = establishConnectionToPrimary(replicaSet);\n+                    final ReplicaSetOffsetContext rsOffsetContext = offsetContext.getReplicaSetOffsetContext(replicaSet);\n+                    if (primary != null && isInitialSyncExpected(primary, rsOffsetContext)) {\n+                        replicaSetSnapshots.add(replicaSet);\n+                    }\n+                }\n+                finally {\n+                    if (primary != null) {\n+                        primary.stop();\n+                    }\n+                }\n+            });\n+        }\n+        finally {\n+            taskContext.getConnectionContext().shutdown();\n+        }\n+\n+        return new SnapshottingTask(replicaSetSnapshots);\n+    }\n+\n+    private void delaySnapshotIfNeeded(ChangeEventSourceContext context) throws InterruptedException {\n+        Duration snapshotDelay = connectorConfig.getSnapshotDelay();\n+\n+        if (snapshotDelay.isZero() || snapshotDelay.isNegative()) {\n+            return;\n+        }\n+\n+        Timer timer = Threads.timer(Clock.SYSTEM, snapshotDelay);\n+        Metronome metronome = Metronome.parker(ConfigurationDefaults.RETURN_CONTROL_INTERVAL, Clock.SYSTEM);\n+\n+        while (!timer.expired()) {\n+            if (!context.isRunning()) {\n+                throw new InterruptedException(\"Interrupted while awaiting initial snapshot delay\");\n+            }\n+\n+            LOGGER.info(\"The connector will wait for {}s before proceeding\", timer.remaining().getSeconds());\n+            metronome.pause();\n+        }\n+    }\n+\n+    protected SnapshotContext prepare(ChangeEventSourceContext sourceContext) throws Exception {\n+        return new MongoDbSnapshotContext();\n+    }\n+\n+    protected void complete(SnapshotContext snapshotContext) {\n+    }\n+\n+    private void snapshotReplicaSet(ChangeEventSourceContext sourceContext, SnapshotContext ctx, ReplicaSet replicaSet) throws InterruptedException {\n+        MongoPrimary primaryClient = null;\n+        try {\n+            primaryClient = establishConnectionToPrimary(replicaSet);\n+            if (primaryClient != null) {\n+                createDataEvents(sourceContext, ctx, replicaSet, primaryClient);\n+            }\n+        }\n+        finally {\n+            if (primaryClient != null) {\n+                primaryClient.stop();\n+            }\n+        }\n+    }\n+\n+    private MongoPrimary establishConnectionToPrimary(ReplicaSet replicaSet) {\n+        return connectionContext.primaryFor(replicaSet, taskContext.filters(), (desc, error) -> {\n+            // propagate authorization failures\n+            if (error.getMessage() != null && error.getMessage().startsWith(AUTHORIZATION_FAILURE_MESSAGE)) {\n+                throw new ConnectException(\"Error while attempting to \" + desc, error);\n+            }\n+            else {\n+                LOGGER.error(\"Error while attempting to {}: \", desc, error.getMessage(), error);\n+                throw new ConnectException(\"Error while attempting to \" + desc, error);\n+            }\n+        });\n+    }\n+\n+    private boolean isInitialSyncExpected(MongoPrimary primaryClient, ReplicaSetOffsetContext offsetContext) {\n+        boolean performSnapshot = true;\n+        if (offsetContext.hasOffset()) {\n+            if (LOGGER.isInfoEnabled()) {\n+                LOGGER.info(\"Found existing offset for replica set '{}' at {}\", offsetContext.getReplicaSetName(), offsetContext.getOffset());\n+            }\n+            performSnapshot = false;\n+            if (connectionContext.performSnapshotEvenIfNotNeeded()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI2MDY2MA=="}, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 300}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQxNTY0Mg==", "bodyText": "For now I removed this dead code in commit 0158b56.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390415642", "createdAt": "2020-03-10T15:47:21Z", "author": {"login": "Naros"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbSnapshotChangeEventSource.java", "diffHunk": "@@ -0,0 +1,599 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.bson.BsonTimestamp;\n+import org.bson.Document;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.mongodb.client.MongoCollection;\n+import com.mongodb.client.MongoCursor;\n+import com.mongodb.client.MongoDatabase;\n+\n+import io.debezium.config.ConfigurationDefaults;\n+import io.debezium.connector.mongodb.ConnectionContext.MongoPrimary;\n+import io.debezium.pipeline.ErrorHandler;\n+import io.debezium.pipeline.EventDispatcher;\n+import io.debezium.pipeline.EventDispatcher.SnapshotReceiver;\n+import io.debezium.pipeline.source.spi.SnapshotChangeEventSource;\n+import io.debezium.pipeline.source.spi.SnapshotProgressListener;\n+import io.debezium.pipeline.spi.ChangeRecordEmitter;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.pipeline.spi.SnapshotResult;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.util.Clock;\n+import io.debezium.util.Metronome;\n+import io.debezium.util.Strings;\n+import io.debezium.util.Threads;\n+import io.debezium.util.Threads.Timer;\n+\n+/**\n+ * A {@link SnapshotChangeEventSource} that performs multi-threaded snapshots of replica sets.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbSnapshotChangeEventSource implements SnapshotChangeEventSource {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(MongoDbSnapshotChangeEventSource.class);\n+\n+    private static final String AUTHORIZATION_FAILURE_MESSAGE = \"Command failed with error 13\";\n+\n+    private final MongoDbConnectorConfig connectorConfig;\n+    private final MongoDbTaskContext taskContext;\n+    private final MongoDbOffsetContext previousOffset;\n+    private final ConnectionContext connectionContext;\n+    private final ReplicaSets replicaSets;\n+    private final EventDispatcher<CollectionId> dispatcher;\n+    protected final Clock clock;\n+    private final SnapshotProgressListener snapshotProgressListener;\n+    private final ErrorHandler errorHandler;\n+    private AtomicBoolean aborted = new AtomicBoolean(false);\n+\n+    public MongoDbSnapshotChangeEventSource(MongoDbConnectorConfig connectorConfig, MongoDbTaskContext taskContext,\n+                                            ReplicaSets replicaSets, MongoDbOffsetContext previousOffset,\n+                                            EventDispatcher<CollectionId> dispatcher, Clock clock,\n+                                            SnapshotProgressListener snapshotProgressListener, ErrorHandler errorHandler) {\n+        this.connectorConfig = connectorConfig;\n+        this.taskContext = taskContext;\n+        this.connectionContext = taskContext.getConnectionContext();\n+        this.previousOffset = previousOffset;\n+        this.replicaSets = replicaSets;\n+        this.dispatcher = dispatcher;\n+        this.clock = clock;\n+        this.snapshotProgressListener = snapshotProgressListener;\n+        this.errorHandler = errorHandler;\n+    }\n+\n+    @Override\n+    public SnapshotResult execute(ChangeEventSourceContext context) throws InterruptedException {\n+        SnapshottingTask snapshottingTask = getSnapshottingTask(previousOffset, replicaSets);\n+        if (!snapshottingTask.snapshotData()) {\n+            LOGGER.debug(\"Skipping snapshotting\");\n+            return SnapshotResult.skipped(previousOffset);\n+        }\n+\n+        delaySnapshotIfNeeded(context);\n+\n+        final SnapshotContext ctx;\n+        try {\n+            ctx = prepare(context);\n+        }\n+        catch (Exception e) {\n+            LOGGER.error(\"Failed to initialize snapshot context.\", e);\n+            throw new RuntimeException(e);\n+        }\n+\n+        try {\n+            LOGGER.info(\"Snapshot step 1 - Preparing\");\n+            snapshotProgressListener.snapshotStarted();\n+\n+            if (previousOffset != null && previousOffset.isSnapshotRunning()) {\n+                LOGGER.info(\"Previous snapshot was cancelled before completion; a new snapshot will be taken.\");\n+            }\n+\n+            LOGGER.info(\"Snapshot step 2 - Determining snapshot offsets\");\n+            determineSnapshotOffsets(ctx, replicaSets);\n+\n+            List<ReplicaSet> replicaSetsToSnapshot = snapshottingTask.getReplicaSetsToSnapshot();\n+\n+            final int threads = replicaSetsToSnapshot.size();\n+            final ExecutorService executor = Threads.newFixedThreadPool(MongoDbConnector.class, taskContext.serverName(), \"replicator-snapshot\", threads);\n+            final CountDownLatch latch = new CountDownLatch(threads);\n+\n+            LOGGER.info(\"Ignoring unnamed replica sets: {}\", replicaSets.unnamedReplicaSets());\n+            LOGGER.info(\"Starting {} thread(s) to snapshot replica sets: {}\", threads, replicaSetsToSnapshot);\n+\n+            LOGGER.info(\"Snapshot step 3 - Snapshotting data\");\n+            replicaSetsToSnapshot.forEach(replicaSet -> {\n+                executor.submit(() -> {\n+                    try {\n+                        taskContext.configureLoggingContext(replicaSet.replicaSetName());\n+                        try {\n+                            snapshotReplicaSet(context, ctx, replicaSet);\n+                        }\n+                        finally {\n+                            final MongoDbOffsetContext offset = (MongoDbOffsetContext) ctx.offset;\n+                            // todo: DBZ-1726 - this causes MongoDbConnectorIT#shouldEmitHeartbeatMessages to fail\n+                            // omitted for now since it does not appear we did this in previous connector code.\n+                            // dispatcher.alwaysDispatchHeartbeatEvent(offset.getReplicaSetOffsetContext(replicaSet));\n+                        }\n+                    }\n+                    catch (Throwable t) {\n+                        LOGGER.error(\"Snapshot for replica set {} failed\", replicaSet.replicaSetName(), t);\n+                        errorHandler.setProducerThrowable(t);\n+                    }\n+                    finally {\n+                        latch.countDown();\n+                    }\n+                });\n+            });\n+\n+            // Wait for the executor service threads to end.\n+            try {\n+                latch.await();\n+            }\n+            catch (InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                aborted.set(true);\n+            }\n+\n+            // Shutdown executor and close connections\n+            try {\n+                executor.shutdown();\n+            }\n+            finally {\n+                LOGGER.info(\"Stopping mongodb connections\");\n+                taskContext.getConnectionContext().shutdown();\n+            }\n+\n+            if (aborted.get()) {\n+                return SnapshotResult.aborted();\n+            }\n+\n+            snapshotProgressListener.snapshotCompleted();\n+\n+            return SnapshotResult.completed(ctx.offset);\n+        }\n+        catch (InterruptedException e) {\n+            LOGGER.warn(\"Snapshot was interrupted before completion\");\n+            snapshotProgressListener.snapshotAborted();\n+            throw e;\n+        }\n+        catch (RuntimeException e) {\n+            snapshotProgressListener.snapshotAborted();\n+            throw e;\n+        }\n+        catch (Throwable t) {\n+            snapshotProgressListener.snapshotAborted();\n+            throw new RuntimeException(t);\n+        }\n+        finally {\n+            LOGGER.info(\"Snapshot step 4 - Finalizing\");\n+            complete(ctx);\n+        }\n+    }\n+\n+    protected SnapshottingTask getSnapshottingTask(OffsetContext previousOffset, ReplicaSets replicaSets) {\n+        if (previousOffset == null) {\n+            LOGGER.info(\"No previous offset has been found\");\n+            if (connectorConfig.getSnapshotMode().equals(MongoDbConnectorConfig.SnapshotMode.NEVER)) {\n+                LOGGER.info(\"According to the connector configuration, no snapshot will occur.\");\n+                return new SnapshottingTask(Collections.emptyList());\n+            }\n+            return new SnapshottingTask(replicaSets.all());\n+        }\n+\n+        // Even if there are previous offsets, if no snapshot should occur, return task with no replica sets\n+        if (connectorConfig.getSnapshotMode().equals(MongoDbConnectorConfig.SnapshotMode.NEVER)) {\n+            LOGGER.info(\"According to the connector configuration, no snapshot will occur.\");\n+            return new SnapshottingTask(Collections.emptyList());\n+        }\n+\n+        // Collect which replica-sets require being snapshotted\n+        final List<ReplicaSet> replicaSetSnapshots = new ArrayList<>();\n+        final MongoDbOffsetContext offsetContext = (MongoDbOffsetContext) previousOffset;\n+        try {\n+            replicaSets.onEachReplicaSet(replicaSet -> {\n+                MongoPrimary primary = null;\n+                try {\n+                    primary = establishConnectionToPrimary(replicaSet);\n+                    final ReplicaSetOffsetContext rsOffsetContext = offsetContext.getReplicaSetOffsetContext(replicaSet);\n+                    if (primary != null && isInitialSyncExpected(primary, rsOffsetContext)) {\n+                        replicaSetSnapshots.add(replicaSet);\n+                    }\n+                }\n+                finally {\n+                    if (primary != null) {\n+                        primary.stop();\n+                    }\n+                }\n+            });\n+        }\n+        finally {\n+            taskContext.getConnectionContext().shutdown();\n+        }\n+\n+        return new SnapshottingTask(replicaSetSnapshots);\n+    }\n+\n+    private void delaySnapshotIfNeeded(ChangeEventSourceContext context) throws InterruptedException {\n+        Duration snapshotDelay = connectorConfig.getSnapshotDelay();\n+\n+        if (snapshotDelay.isZero() || snapshotDelay.isNegative()) {\n+            return;\n+        }\n+\n+        Timer timer = Threads.timer(Clock.SYSTEM, snapshotDelay);\n+        Metronome metronome = Metronome.parker(ConfigurationDefaults.RETURN_CONTROL_INTERVAL, Clock.SYSTEM);\n+\n+        while (!timer.expired()) {\n+            if (!context.isRunning()) {\n+                throw new InterruptedException(\"Interrupted while awaiting initial snapshot delay\");\n+            }\n+\n+            LOGGER.info(\"The connector will wait for {}s before proceeding\", timer.remaining().getSeconds());\n+            metronome.pause();\n+        }\n+    }\n+\n+    protected SnapshotContext prepare(ChangeEventSourceContext sourceContext) throws Exception {\n+        return new MongoDbSnapshotContext();\n+    }\n+\n+    protected void complete(SnapshotContext snapshotContext) {\n+    }\n+\n+    private void snapshotReplicaSet(ChangeEventSourceContext sourceContext, SnapshotContext ctx, ReplicaSet replicaSet) throws InterruptedException {\n+        MongoPrimary primaryClient = null;\n+        try {\n+            primaryClient = establishConnectionToPrimary(replicaSet);\n+            if (primaryClient != null) {\n+                createDataEvents(sourceContext, ctx, replicaSet, primaryClient);\n+            }\n+        }\n+        finally {\n+            if (primaryClient != null) {\n+                primaryClient.stop();\n+            }\n+        }\n+    }\n+\n+    private MongoPrimary establishConnectionToPrimary(ReplicaSet replicaSet) {\n+        return connectionContext.primaryFor(replicaSet, taskContext.filters(), (desc, error) -> {\n+            // propagate authorization failures\n+            if (error.getMessage() != null && error.getMessage().startsWith(AUTHORIZATION_FAILURE_MESSAGE)) {\n+                throw new ConnectException(\"Error while attempting to \" + desc, error);\n+            }\n+            else {\n+                LOGGER.error(\"Error while attempting to {}: \", desc, error.getMessage(), error);\n+                throw new ConnectException(\"Error while attempting to \" + desc, error);\n+            }\n+        });\n+    }\n+\n+    private boolean isInitialSyncExpected(MongoPrimary primaryClient, ReplicaSetOffsetContext offsetContext) {\n+        boolean performSnapshot = true;\n+        if (offsetContext.hasOffset()) {\n+            if (LOGGER.isInfoEnabled()) {\n+                LOGGER.info(\"Found existing offset for replica set '{}' at {}\", offsetContext.getReplicaSetName(), offsetContext.getOffset());\n+            }\n+            performSnapshot = false;\n+            if (connectionContext.performSnapshotEvenIfNotNeeded()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI2MDY2MA=="}, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 300}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxODUxNjkwOnYy", "diffSide": "RIGHT", "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbOffsetContext.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxMTo1NDozOVrOF0Ltqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxNTo0Njo1N1rOF0VD8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI2MjE4Ng==", "bodyText": "Could we use snapshot instead of initialSync to be consistent with other connectors? Same for other instances", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390262186", "createdAt": "2020-03-10T11:54:39Z", "author": {"login": "jpechane"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbOffsetContext.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.Struct;\n+import org.bson.Document;\n+\n+import io.debezium.connector.SnapshotRecord;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.schema.DataCollectionId;\n+\n+/**\n+ * A context that facilitates the management of the current offsets across a set of mongodb replica sets.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbOffsetContext implements OffsetContext {\n+\n+    private final SourceInfo sourceInfo;\n+    private final TransactionContext transactionContext;\n+    private final Map<ReplicaSet, ReplicaSetOffsetContext> replicaSetOffsetContexts = new ConcurrentHashMap<>();\n+\n+    public MongoDbOffsetContext(SourceInfo sourceInfo, TransactionContext transactionContext) {\n+        this.sourceInfo = sourceInfo;\n+        this.transactionContext = transactionContext;\n+    }\n+\n+    public MongoDbOffsetContext(SourceInfo sourceInfo, TransactionContext transactionContext, Map<ReplicaSet, Document> offsets) {\n+        this(sourceInfo, transactionContext);\n+        offsets.forEach((replicaSet, document) -> sourceInfo.opLogEvent(replicaSet.replicaSetName(), document, document, 0));\n+    }\n+\n+    void startInitialSync(String replicaSetName) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDM5MDU3OA==", "bodyText": "Absolutely.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390390578", "createdAt": "2020-03-10T15:14:37Z", "author": {"login": "Naros"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbOffsetContext.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.Struct;\n+import org.bson.Document;\n+\n+import io.debezium.connector.SnapshotRecord;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.schema.DataCollectionId;\n+\n+/**\n+ * A context that facilitates the management of the current offsets across a set of mongodb replica sets.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbOffsetContext implements OffsetContext {\n+\n+    private final SourceInfo sourceInfo;\n+    private final TransactionContext transactionContext;\n+    private final Map<ReplicaSet, ReplicaSetOffsetContext> replicaSetOffsetContexts = new ConcurrentHashMap<>();\n+\n+    public MongoDbOffsetContext(SourceInfo sourceInfo, TransactionContext transactionContext) {\n+        this.sourceInfo = sourceInfo;\n+        this.transactionContext = transactionContext;\n+    }\n+\n+    public MongoDbOffsetContext(SourceInfo sourceInfo, TransactionContext transactionContext, Map<ReplicaSet, Document> offsets) {\n+        this(sourceInfo, transactionContext);\n+        offsets.forEach((replicaSet, document) -> sourceInfo.opLogEvent(replicaSet.replicaSetName(), document, document, 0));\n+    }\n+\n+    void startInitialSync(String replicaSetName) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI2MjE4Ng=="}, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQxNTM0Ng==", "bodyText": "See commit 0158b56.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390415346", "createdAt": "2020-03-10T15:46:57Z", "author": {"login": "Naros"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbOffsetContext.java", "diffHunk": "@@ -0,0 +1,154 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Instant;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.kafka.connect.data.Schema;\n+import org.apache.kafka.connect.data.Struct;\n+import org.bson.Document;\n+\n+import io.debezium.connector.SnapshotRecord;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.schema.DataCollectionId;\n+\n+/**\n+ * A context that facilitates the management of the current offsets across a set of mongodb replica sets.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbOffsetContext implements OffsetContext {\n+\n+    private final SourceInfo sourceInfo;\n+    private final TransactionContext transactionContext;\n+    private final Map<ReplicaSet, ReplicaSetOffsetContext> replicaSetOffsetContexts = new ConcurrentHashMap<>();\n+\n+    public MongoDbOffsetContext(SourceInfo sourceInfo, TransactionContext transactionContext) {\n+        this.sourceInfo = sourceInfo;\n+        this.transactionContext = transactionContext;\n+    }\n+\n+    public MongoDbOffsetContext(SourceInfo sourceInfo, TransactionContext transactionContext, Map<ReplicaSet, Document> offsets) {\n+        this(sourceInfo, transactionContext);\n+        offsets.forEach((replicaSet, document) -> sourceInfo.opLogEvent(replicaSet.replicaSetName(), document, document, 0));\n+    }\n+\n+    void startInitialSync(String replicaSetName) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI2MjE4Ng=="}, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxODUyMzkzOnYy", "diffSide": "RIGHT", "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbSnapshotChangeEventSource.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxMTo1NzowMlrOF0Lx5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxNTo0Njo1M1rOF0VDwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI2MzI3MQ==", "bodyText": "I believe this deserves a TODO note - right now we implement when needed snapshot by default. In future we should provide the same otpions as other connectors and this should server as a reminder where wheen_needed functionlaity lies.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390263271", "createdAt": "2020-03-10T11:57:02Z", "author": {"login": "jpechane"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbSnapshotChangeEventSource.java", "diffHunk": "@@ -0,0 +1,599 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.bson.BsonTimestamp;\n+import org.bson.Document;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.mongodb.client.MongoCollection;\n+import com.mongodb.client.MongoCursor;\n+import com.mongodb.client.MongoDatabase;\n+\n+import io.debezium.config.ConfigurationDefaults;\n+import io.debezium.connector.mongodb.ConnectionContext.MongoPrimary;\n+import io.debezium.pipeline.ErrorHandler;\n+import io.debezium.pipeline.EventDispatcher;\n+import io.debezium.pipeline.EventDispatcher.SnapshotReceiver;\n+import io.debezium.pipeline.source.spi.SnapshotChangeEventSource;\n+import io.debezium.pipeline.source.spi.SnapshotProgressListener;\n+import io.debezium.pipeline.spi.ChangeRecordEmitter;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.pipeline.spi.SnapshotResult;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.util.Clock;\n+import io.debezium.util.Metronome;\n+import io.debezium.util.Strings;\n+import io.debezium.util.Threads;\n+import io.debezium.util.Threads.Timer;\n+\n+/**\n+ * A {@link SnapshotChangeEventSource} that performs multi-threaded snapshots of replica sets.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbSnapshotChangeEventSource implements SnapshotChangeEventSource {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(MongoDbSnapshotChangeEventSource.class);\n+\n+    private static final String AUTHORIZATION_FAILURE_MESSAGE = \"Command failed with error 13\";\n+\n+    private final MongoDbConnectorConfig connectorConfig;\n+    private final MongoDbTaskContext taskContext;\n+    private final MongoDbOffsetContext previousOffset;\n+    private final ConnectionContext connectionContext;\n+    private final ReplicaSets replicaSets;\n+    private final EventDispatcher<CollectionId> dispatcher;\n+    protected final Clock clock;\n+    private final SnapshotProgressListener snapshotProgressListener;\n+    private final ErrorHandler errorHandler;\n+    private AtomicBoolean aborted = new AtomicBoolean(false);\n+\n+    public MongoDbSnapshotChangeEventSource(MongoDbConnectorConfig connectorConfig, MongoDbTaskContext taskContext,\n+                                            ReplicaSets replicaSets, MongoDbOffsetContext previousOffset,\n+                                            EventDispatcher<CollectionId> dispatcher, Clock clock,\n+                                            SnapshotProgressListener snapshotProgressListener, ErrorHandler errorHandler) {\n+        this.connectorConfig = connectorConfig;\n+        this.taskContext = taskContext;\n+        this.connectionContext = taskContext.getConnectionContext();\n+        this.previousOffset = previousOffset;\n+        this.replicaSets = replicaSets;\n+        this.dispatcher = dispatcher;\n+        this.clock = clock;\n+        this.snapshotProgressListener = snapshotProgressListener;\n+        this.errorHandler = errorHandler;\n+    }\n+\n+    @Override\n+    public SnapshotResult execute(ChangeEventSourceContext context) throws InterruptedException {\n+        SnapshottingTask snapshottingTask = getSnapshottingTask(previousOffset, replicaSets);\n+        if (!snapshottingTask.snapshotData()) {\n+            LOGGER.debug(\"Skipping snapshotting\");\n+            return SnapshotResult.skipped(previousOffset);\n+        }\n+\n+        delaySnapshotIfNeeded(context);\n+\n+        final SnapshotContext ctx;\n+        try {\n+            ctx = prepare(context);\n+        }\n+        catch (Exception e) {\n+            LOGGER.error(\"Failed to initialize snapshot context.\", e);\n+            throw new RuntimeException(e);\n+        }\n+\n+        try {\n+            LOGGER.info(\"Snapshot step 1 - Preparing\");\n+            snapshotProgressListener.snapshotStarted();\n+\n+            if (previousOffset != null && previousOffset.isSnapshotRunning()) {\n+                LOGGER.info(\"Previous snapshot was cancelled before completion; a new snapshot will be taken.\");\n+            }\n+\n+            LOGGER.info(\"Snapshot step 2 - Determining snapshot offsets\");\n+            determineSnapshotOffsets(ctx, replicaSets);\n+\n+            List<ReplicaSet> replicaSetsToSnapshot = snapshottingTask.getReplicaSetsToSnapshot();\n+\n+            final int threads = replicaSetsToSnapshot.size();\n+            final ExecutorService executor = Threads.newFixedThreadPool(MongoDbConnector.class, taskContext.serverName(), \"replicator-snapshot\", threads);\n+            final CountDownLatch latch = new CountDownLatch(threads);\n+\n+            LOGGER.info(\"Ignoring unnamed replica sets: {}\", replicaSets.unnamedReplicaSets());\n+            LOGGER.info(\"Starting {} thread(s) to snapshot replica sets: {}\", threads, replicaSetsToSnapshot);\n+\n+            LOGGER.info(\"Snapshot step 3 - Snapshotting data\");\n+            replicaSetsToSnapshot.forEach(replicaSet -> {\n+                executor.submit(() -> {\n+                    try {\n+                        taskContext.configureLoggingContext(replicaSet.replicaSetName());\n+                        try {\n+                            snapshotReplicaSet(context, ctx, replicaSet);\n+                        }\n+                        finally {\n+                            final MongoDbOffsetContext offset = (MongoDbOffsetContext) ctx.offset;\n+                            // todo: DBZ-1726 - this causes MongoDbConnectorIT#shouldEmitHeartbeatMessages to fail\n+                            // omitted for now since it does not appear we did this in previous connector code.\n+                            // dispatcher.alwaysDispatchHeartbeatEvent(offset.getReplicaSetOffsetContext(replicaSet));\n+                        }\n+                    }\n+                    catch (Throwable t) {\n+                        LOGGER.error(\"Snapshot for replica set {} failed\", replicaSet.replicaSetName(), t);\n+                        errorHandler.setProducerThrowable(t);\n+                    }\n+                    finally {\n+                        latch.countDown();\n+                    }\n+                });\n+            });\n+\n+            // Wait for the executor service threads to end.\n+            try {\n+                latch.await();\n+            }\n+            catch (InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                aborted.set(true);\n+            }\n+\n+            // Shutdown executor and close connections\n+            try {\n+                executor.shutdown();\n+            }\n+            finally {\n+                LOGGER.info(\"Stopping mongodb connections\");\n+                taskContext.getConnectionContext().shutdown();\n+            }\n+\n+            if (aborted.get()) {\n+                return SnapshotResult.aborted();\n+            }\n+\n+            snapshotProgressListener.snapshotCompleted();\n+\n+            return SnapshotResult.completed(ctx.offset);\n+        }\n+        catch (InterruptedException e) {\n+            LOGGER.warn(\"Snapshot was interrupted before completion\");\n+            snapshotProgressListener.snapshotAborted();\n+            throw e;\n+        }\n+        catch (RuntimeException e) {\n+            snapshotProgressListener.snapshotAborted();\n+            throw e;\n+        }\n+        catch (Throwable t) {\n+            snapshotProgressListener.snapshotAborted();\n+            throw new RuntimeException(t);\n+        }\n+        finally {\n+            LOGGER.info(\"Snapshot step 4 - Finalizing\");\n+            complete(ctx);\n+        }\n+    }\n+\n+    protected SnapshottingTask getSnapshottingTask(OffsetContext previousOffset, ReplicaSets replicaSets) {\n+        if (previousOffset == null) {\n+            LOGGER.info(\"No previous offset has been found\");\n+            if (connectorConfig.getSnapshotMode().equals(MongoDbConnectorConfig.SnapshotMode.NEVER)) {\n+                LOGGER.info(\"According to the connector configuration, no snapshot will occur.\");\n+                return new SnapshottingTask(Collections.emptyList());\n+            }\n+            return new SnapshottingTask(replicaSets.all());\n+        }\n+\n+        // Even if there are previous offsets, if no snapshot should occur, return task with no replica sets\n+        if (connectorConfig.getSnapshotMode().equals(MongoDbConnectorConfig.SnapshotMode.NEVER)) {\n+            LOGGER.info(\"According to the connector configuration, no snapshot will occur.\");\n+            return new SnapshottingTask(Collections.emptyList());\n+        }\n+\n+        // Collect which replica-sets require being snapshotted\n+        final List<ReplicaSet> replicaSetSnapshots = new ArrayList<>();\n+        final MongoDbOffsetContext offsetContext = (MongoDbOffsetContext) previousOffset;\n+        try {\n+            replicaSets.onEachReplicaSet(replicaSet -> {\n+                MongoPrimary primary = null;\n+                try {\n+                    primary = establishConnectionToPrimary(replicaSet);\n+                    final ReplicaSetOffsetContext rsOffsetContext = offsetContext.getReplicaSetOffsetContext(replicaSet);\n+                    if (primary != null && isInitialSyncExpected(primary, rsOffsetContext)) {\n+                        replicaSetSnapshots.add(replicaSet);\n+                    }\n+                }\n+                finally {\n+                    if (primary != null) {\n+                        primary.stop();\n+                    }\n+                }\n+            });\n+        }\n+        finally {\n+            taskContext.getConnectionContext().shutdown();\n+        }\n+\n+        return new SnapshottingTask(replicaSetSnapshots);\n+    }\n+\n+    private void delaySnapshotIfNeeded(ChangeEventSourceContext context) throws InterruptedException {\n+        Duration snapshotDelay = connectorConfig.getSnapshotDelay();\n+\n+        if (snapshotDelay.isZero() || snapshotDelay.isNegative()) {\n+            return;\n+        }\n+\n+        Timer timer = Threads.timer(Clock.SYSTEM, snapshotDelay);\n+        Metronome metronome = Metronome.parker(ConfigurationDefaults.RETURN_CONTROL_INTERVAL, Clock.SYSTEM);\n+\n+        while (!timer.expired()) {\n+            if (!context.isRunning()) {\n+                throw new InterruptedException(\"Interrupted while awaiting initial snapshot delay\");\n+            }\n+\n+            LOGGER.info(\"The connector will wait for {}s before proceeding\", timer.remaining().getSeconds());\n+            metronome.pause();\n+        }\n+    }\n+\n+    protected SnapshotContext prepare(ChangeEventSourceContext sourceContext) throws Exception {\n+        return new MongoDbSnapshotContext();\n+    }\n+\n+    protected void complete(SnapshotContext snapshotContext) {\n+    }\n+\n+    private void snapshotReplicaSet(ChangeEventSourceContext sourceContext, SnapshotContext ctx, ReplicaSet replicaSet) throws InterruptedException {\n+        MongoPrimary primaryClient = null;\n+        try {\n+            primaryClient = establishConnectionToPrimary(replicaSet);\n+            if (primaryClient != null) {\n+                createDataEvents(sourceContext, ctx, replicaSet, primaryClient);\n+            }\n+        }\n+        finally {\n+            if (primaryClient != null) {\n+                primaryClient.stop();\n+            }\n+        }\n+    }\n+\n+    private MongoPrimary establishConnectionToPrimary(ReplicaSet replicaSet) {\n+        return connectionContext.primaryFor(replicaSet, taskContext.filters(), (desc, error) -> {\n+            // propagate authorization failures\n+            if (error.getMessage() != null && error.getMessage().startsWith(AUTHORIZATION_FAILURE_MESSAGE)) {\n+                throw new ConnectException(\"Error while attempting to \" + desc, error);\n+            }\n+            else {\n+                LOGGER.error(\"Error while attempting to {}: \", desc, error.getMessage(), error);\n+                throw new ConnectException(\"Error while attempting to \" + desc, error);\n+            }\n+        });\n+    }\n+\n+    private boolean isInitialSyncExpected(MongoPrimary primaryClient, ReplicaSetOffsetContext offsetContext) {\n+        boolean performSnapshot = true;\n+        if (offsetContext.hasOffset()) {\n+            if (LOGGER.isInfoEnabled()) {\n+                LOGGER.info(\"Found existing offset for replica set '{}' at {}\", offsetContext.getReplicaSetName(), offsetContext.getOffset());\n+            }\n+            performSnapshot = false;\n+            if (connectionContext.performSnapshotEvenIfNotNeeded()) {\n+                LOGGER.info(\"Configured to perform initial sync of replica set '{}'\", offsetContext.getReplicaSetName());\n+                performSnapshot = true;\n+            }\n+            else {\n+                if (offsetContext.isInitialSyncOngoing()) {\n+                    // The latest snapshot was not completed, so restart it\n+                    LOGGER.info(\"The previous initial sync was incomplete for '{}', so initiating another initial sync\", offsetContext.getReplicaSetName());\n+                    performSnapshot = true;\n+                }\n+                else {\n+                    // There is no ongoing initial sync, so look to see if our last recorded offset still exists in the oplog.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 311}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQxNTI5Nw==", "bodyText": "See commit 0158b56.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390415297", "createdAt": "2020-03-10T15:46:53Z", "author": {"login": "Naros"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbSnapshotChangeEventSource.java", "diffHunk": "@@ -0,0 +1,599 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.bson.BsonTimestamp;\n+import org.bson.Document;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.mongodb.client.MongoCollection;\n+import com.mongodb.client.MongoCursor;\n+import com.mongodb.client.MongoDatabase;\n+\n+import io.debezium.config.ConfigurationDefaults;\n+import io.debezium.connector.mongodb.ConnectionContext.MongoPrimary;\n+import io.debezium.pipeline.ErrorHandler;\n+import io.debezium.pipeline.EventDispatcher;\n+import io.debezium.pipeline.EventDispatcher.SnapshotReceiver;\n+import io.debezium.pipeline.source.spi.SnapshotChangeEventSource;\n+import io.debezium.pipeline.source.spi.SnapshotProgressListener;\n+import io.debezium.pipeline.spi.ChangeRecordEmitter;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.pipeline.spi.SnapshotResult;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.util.Clock;\n+import io.debezium.util.Metronome;\n+import io.debezium.util.Strings;\n+import io.debezium.util.Threads;\n+import io.debezium.util.Threads.Timer;\n+\n+/**\n+ * A {@link SnapshotChangeEventSource} that performs multi-threaded snapshots of replica sets.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbSnapshotChangeEventSource implements SnapshotChangeEventSource {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(MongoDbSnapshotChangeEventSource.class);\n+\n+    private static final String AUTHORIZATION_FAILURE_MESSAGE = \"Command failed with error 13\";\n+\n+    private final MongoDbConnectorConfig connectorConfig;\n+    private final MongoDbTaskContext taskContext;\n+    private final MongoDbOffsetContext previousOffset;\n+    private final ConnectionContext connectionContext;\n+    private final ReplicaSets replicaSets;\n+    private final EventDispatcher<CollectionId> dispatcher;\n+    protected final Clock clock;\n+    private final SnapshotProgressListener snapshotProgressListener;\n+    private final ErrorHandler errorHandler;\n+    private AtomicBoolean aborted = new AtomicBoolean(false);\n+\n+    public MongoDbSnapshotChangeEventSource(MongoDbConnectorConfig connectorConfig, MongoDbTaskContext taskContext,\n+                                            ReplicaSets replicaSets, MongoDbOffsetContext previousOffset,\n+                                            EventDispatcher<CollectionId> dispatcher, Clock clock,\n+                                            SnapshotProgressListener snapshotProgressListener, ErrorHandler errorHandler) {\n+        this.connectorConfig = connectorConfig;\n+        this.taskContext = taskContext;\n+        this.connectionContext = taskContext.getConnectionContext();\n+        this.previousOffset = previousOffset;\n+        this.replicaSets = replicaSets;\n+        this.dispatcher = dispatcher;\n+        this.clock = clock;\n+        this.snapshotProgressListener = snapshotProgressListener;\n+        this.errorHandler = errorHandler;\n+    }\n+\n+    @Override\n+    public SnapshotResult execute(ChangeEventSourceContext context) throws InterruptedException {\n+        SnapshottingTask snapshottingTask = getSnapshottingTask(previousOffset, replicaSets);\n+        if (!snapshottingTask.snapshotData()) {\n+            LOGGER.debug(\"Skipping snapshotting\");\n+            return SnapshotResult.skipped(previousOffset);\n+        }\n+\n+        delaySnapshotIfNeeded(context);\n+\n+        final SnapshotContext ctx;\n+        try {\n+            ctx = prepare(context);\n+        }\n+        catch (Exception e) {\n+            LOGGER.error(\"Failed to initialize snapshot context.\", e);\n+            throw new RuntimeException(e);\n+        }\n+\n+        try {\n+            LOGGER.info(\"Snapshot step 1 - Preparing\");\n+            snapshotProgressListener.snapshotStarted();\n+\n+            if (previousOffset != null && previousOffset.isSnapshotRunning()) {\n+                LOGGER.info(\"Previous snapshot was cancelled before completion; a new snapshot will be taken.\");\n+            }\n+\n+            LOGGER.info(\"Snapshot step 2 - Determining snapshot offsets\");\n+            determineSnapshotOffsets(ctx, replicaSets);\n+\n+            List<ReplicaSet> replicaSetsToSnapshot = snapshottingTask.getReplicaSetsToSnapshot();\n+\n+            final int threads = replicaSetsToSnapshot.size();\n+            final ExecutorService executor = Threads.newFixedThreadPool(MongoDbConnector.class, taskContext.serverName(), \"replicator-snapshot\", threads);\n+            final CountDownLatch latch = new CountDownLatch(threads);\n+\n+            LOGGER.info(\"Ignoring unnamed replica sets: {}\", replicaSets.unnamedReplicaSets());\n+            LOGGER.info(\"Starting {} thread(s) to snapshot replica sets: {}\", threads, replicaSetsToSnapshot);\n+\n+            LOGGER.info(\"Snapshot step 3 - Snapshotting data\");\n+            replicaSetsToSnapshot.forEach(replicaSet -> {\n+                executor.submit(() -> {\n+                    try {\n+                        taskContext.configureLoggingContext(replicaSet.replicaSetName());\n+                        try {\n+                            snapshotReplicaSet(context, ctx, replicaSet);\n+                        }\n+                        finally {\n+                            final MongoDbOffsetContext offset = (MongoDbOffsetContext) ctx.offset;\n+                            // todo: DBZ-1726 - this causes MongoDbConnectorIT#shouldEmitHeartbeatMessages to fail\n+                            // omitted for now since it does not appear we did this in previous connector code.\n+                            // dispatcher.alwaysDispatchHeartbeatEvent(offset.getReplicaSetOffsetContext(replicaSet));\n+                        }\n+                    }\n+                    catch (Throwable t) {\n+                        LOGGER.error(\"Snapshot for replica set {} failed\", replicaSet.replicaSetName(), t);\n+                        errorHandler.setProducerThrowable(t);\n+                    }\n+                    finally {\n+                        latch.countDown();\n+                    }\n+                });\n+            });\n+\n+            // Wait for the executor service threads to end.\n+            try {\n+                latch.await();\n+            }\n+            catch (InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                aborted.set(true);\n+            }\n+\n+            // Shutdown executor and close connections\n+            try {\n+                executor.shutdown();\n+            }\n+            finally {\n+                LOGGER.info(\"Stopping mongodb connections\");\n+                taskContext.getConnectionContext().shutdown();\n+            }\n+\n+            if (aborted.get()) {\n+                return SnapshotResult.aborted();\n+            }\n+\n+            snapshotProgressListener.snapshotCompleted();\n+\n+            return SnapshotResult.completed(ctx.offset);\n+        }\n+        catch (InterruptedException e) {\n+            LOGGER.warn(\"Snapshot was interrupted before completion\");\n+            snapshotProgressListener.snapshotAborted();\n+            throw e;\n+        }\n+        catch (RuntimeException e) {\n+            snapshotProgressListener.snapshotAborted();\n+            throw e;\n+        }\n+        catch (Throwable t) {\n+            snapshotProgressListener.snapshotAborted();\n+            throw new RuntimeException(t);\n+        }\n+        finally {\n+            LOGGER.info(\"Snapshot step 4 - Finalizing\");\n+            complete(ctx);\n+        }\n+    }\n+\n+    protected SnapshottingTask getSnapshottingTask(OffsetContext previousOffset, ReplicaSets replicaSets) {\n+        if (previousOffset == null) {\n+            LOGGER.info(\"No previous offset has been found\");\n+            if (connectorConfig.getSnapshotMode().equals(MongoDbConnectorConfig.SnapshotMode.NEVER)) {\n+                LOGGER.info(\"According to the connector configuration, no snapshot will occur.\");\n+                return new SnapshottingTask(Collections.emptyList());\n+            }\n+            return new SnapshottingTask(replicaSets.all());\n+        }\n+\n+        // Even if there are previous offsets, if no snapshot should occur, return task with no replica sets\n+        if (connectorConfig.getSnapshotMode().equals(MongoDbConnectorConfig.SnapshotMode.NEVER)) {\n+            LOGGER.info(\"According to the connector configuration, no snapshot will occur.\");\n+            return new SnapshottingTask(Collections.emptyList());\n+        }\n+\n+        // Collect which replica-sets require being snapshotted\n+        final List<ReplicaSet> replicaSetSnapshots = new ArrayList<>();\n+        final MongoDbOffsetContext offsetContext = (MongoDbOffsetContext) previousOffset;\n+        try {\n+            replicaSets.onEachReplicaSet(replicaSet -> {\n+                MongoPrimary primary = null;\n+                try {\n+                    primary = establishConnectionToPrimary(replicaSet);\n+                    final ReplicaSetOffsetContext rsOffsetContext = offsetContext.getReplicaSetOffsetContext(replicaSet);\n+                    if (primary != null && isInitialSyncExpected(primary, rsOffsetContext)) {\n+                        replicaSetSnapshots.add(replicaSet);\n+                    }\n+                }\n+                finally {\n+                    if (primary != null) {\n+                        primary.stop();\n+                    }\n+                }\n+            });\n+        }\n+        finally {\n+            taskContext.getConnectionContext().shutdown();\n+        }\n+\n+        return new SnapshottingTask(replicaSetSnapshots);\n+    }\n+\n+    private void delaySnapshotIfNeeded(ChangeEventSourceContext context) throws InterruptedException {\n+        Duration snapshotDelay = connectorConfig.getSnapshotDelay();\n+\n+        if (snapshotDelay.isZero() || snapshotDelay.isNegative()) {\n+            return;\n+        }\n+\n+        Timer timer = Threads.timer(Clock.SYSTEM, snapshotDelay);\n+        Metronome metronome = Metronome.parker(ConfigurationDefaults.RETURN_CONTROL_INTERVAL, Clock.SYSTEM);\n+\n+        while (!timer.expired()) {\n+            if (!context.isRunning()) {\n+                throw new InterruptedException(\"Interrupted while awaiting initial snapshot delay\");\n+            }\n+\n+            LOGGER.info(\"The connector will wait for {}s before proceeding\", timer.remaining().getSeconds());\n+            metronome.pause();\n+        }\n+    }\n+\n+    protected SnapshotContext prepare(ChangeEventSourceContext sourceContext) throws Exception {\n+        return new MongoDbSnapshotContext();\n+    }\n+\n+    protected void complete(SnapshotContext snapshotContext) {\n+    }\n+\n+    private void snapshotReplicaSet(ChangeEventSourceContext sourceContext, SnapshotContext ctx, ReplicaSet replicaSet) throws InterruptedException {\n+        MongoPrimary primaryClient = null;\n+        try {\n+            primaryClient = establishConnectionToPrimary(replicaSet);\n+            if (primaryClient != null) {\n+                createDataEvents(sourceContext, ctx, replicaSet, primaryClient);\n+            }\n+        }\n+        finally {\n+            if (primaryClient != null) {\n+                primaryClient.stop();\n+            }\n+        }\n+    }\n+\n+    private MongoPrimary establishConnectionToPrimary(ReplicaSet replicaSet) {\n+        return connectionContext.primaryFor(replicaSet, taskContext.filters(), (desc, error) -> {\n+            // propagate authorization failures\n+            if (error.getMessage() != null && error.getMessage().startsWith(AUTHORIZATION_FAILURE_MESSAGE)) {\n+                throw new ConnectException(\"Error while attempting to \" + desc, error);\n+            }\n+            else {\n+                LOGGER.error(\"Error while attempting to {}: \", desc, error.getMessage(), error);\n+                throw new ConnectException(\"Error while attempting to \" + desc, error);\n+            }\n+        });\n+    }\n+\n+    private boolean isInitialSyncExpected(MongoPrimary primaryClient, ReplicaSetOffsetContext offsetContext) {\n+        boolean performSnapshot = true;\n+        if (offsetContext.hasOffset()) {\n+            if (LOGGER.isInfoEnabled()) {\n+                LOGGER.info(\"Found existing offset for replica set '{}' at {}\", offsetContext.getReplicaSetName(), offsetContext.getOffset());\n+            }\n+            performSnapshot = false;\n+            if (connectionContext.performSnapshotEvenIfNotNeeded()) {\n+                LOGGER.info(\"Configured to perform initial sync of replica set '{}'\", offsetContext.getReplicaSetName());\n+                performSnapshot = true;\n+            }\n+            else {\n+                if (offsetContext.isInitialSyncOngoing()) {\n+                    // The latest snapshot was not completed, so restart it\n+                    LOGGER.info(\"The previous initial sync was incomplete for '{}', so initiating another initial sync\", offsetContext.getReplicaSetName());\n+                    performSnapshot = true;\n+                }\n+                else {\n+                    // There is no ongoing initial sync, so look to see if our last recorded offset still exists in the oplog.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI2MzI3MQ=="}, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 311}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxODUzNzg0OnYy", "diffSide": "RIGHT", "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbSnapshotChangeEventSource.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxMjowMTozMVrOF0L6OQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxNTo0Njo0OVrOF0VDkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI2NTQwMQ==", "bodyText": "Again a common ancestor to this and RelationalSnapshotChangeEventSource would be good together with intorducing doExectue() method.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390265401", "createdAt": "2020-03-10T12:01:31Z", "author": {"login": "jpechane"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbSnapshotChangeEventSource.java", "diffHunk": "@@ -0,0 +1,599 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.bson.BsonTimestamp;\n+import org.bson.Document;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.mongodb.client.MongoCollection;\n+import com.mongodb.client.MongoCursor;\n+import com.mongodb.client.MongoDatabase;\n+\n+import io.debezium.config.ConfigurationDefaults;\n+import io.debezium.connector.mongodb.ConnectionContext.MongoPrimary;\n+import io.debezium.pipeline.ErrorHandler;\n+import io.debezium.pipeline.EventDispatcher;\n+import io.debezium.pipeline.EventDispatcher.SnapshotReceiver;\n+import io.debezium.pipeline.source.spi.SnapshotChangeEventSource;\n+import io.debezium.pipeline.source.spi.SnapshotProgressListener;\n+import io.debezium.pipeline.spi.ChangeRecordEmitter;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.pipeline.spi.SnapshotResult;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.util.Clock;\n+import io.debezium.util.Metronome;\n+import io.debezium.util.Strings;\n+import io.debezium.util.Threads;\n+import io.debezium.util.Threads.Timer;\n+\n+/**\n+ * A {@link SnapshotChangeEventSource} that performs multi-threaded snapshots of replica sets.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbSnapshotChangeEventSource implements SnapshotChangeEventSource {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(MongoDbSnapshotChangeEventSource.class);\n+\n+    private static final String AUTHORIZATION_FAILURE_MESSAGE = \"Command failed with error 13\";\n+\n+    private final MongoDbConnectorConfig connectorConfig;\n+    private final MongoDbTaskContext taskContext;\n+    private final MongoDbOffsetContext previousOffset;\n+    private final ConnectionContext connectionContext;\n+    private final ReplicaSets replicaSets;\n+    private final EventDispatcher<CollectionId> dispatcher;\n+    protected final Clock clock;\n+    private final SnapshotProgressListener snapshotProgressListener;\n+    private final ErrorHandler errorHandler;\n+    private AtomicBoolean aborted = new AtomicBoolean(false);\n+\n+    public MongoDbSnapshotChangeEventSource(MongoDbConnectorConfig connectorConfig, MongoDbTaskContext taskContext,\n+                                            ReplicaSets replicaSets, MongoDbOffsetContext previousOffset,\n+                                            EventDispatcher<CollectionId> dispatcher, Clock clock,\n+                                            SnapshotProgressListener snapshotProgressListener, ErrorHandler errorHandler) {\n+        this.connectorConfig = connectorConfig;\n+        this.taskContext = taskContext;\n+        this.connectionContext = taskContext.getConnectionContext();\n+        this.previousOffset = previousOffset;\n+        this.replicaSets = replicaSets;\n+        this.dispatcher = dispatcher;\n+        this.clock = clock;\n+        this.snapshotProgressListener = snapshotProgressListener;\n+        this.errorHandler = errorHandler;\n+    }\n+\n+    @Override\n+    public SnapshotResult execute(ChangeEventSourceContext context) throws InterruptedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQxNTI0OQ==", "bodyText": "See commit 0158b56.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390415249", "createdAt": "2020-03-10T15:46:49Z", "author": {"login": "Naros"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbSnapshotChangeEventSource.java", "diffHunk": "@@ -0,0 +1,599 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.concurrent.ConcurrentLinkedQueue;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.bson.BsonTimestamp;\n+import org.bson.Document;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.mongodb.client.MongoCollection;\n+import com.mongodb.client.MongoCursor;\n+import com.mongodb.client.MongoDatabase;\n+\n+import io.debezium.config.ConfigurationDefaults;\n+import io.debezium.connector.mongodb.ConnectionContext.MongoPrimary;\n+import io.debezium.pipeline.ErrorHandler;\n+import io.debezium.pipeline.EventDispatcher;\n+import io.debezium.pipeline.EventDispatcher.SnapshotReceiver;\n+import io.debezium.pipeline.source.spi.SnapshotChangeEventSource;\n+import io.debezium.pipeline.source.spi.SnapshotProgressListener;\n+import io.debezium.pipeline.spi.ChangeRecordEmitter;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.pipeline.spi.SnapshotResult;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.util.Clock;\n+import io.debezium.util.Metronome;\n+import io.debezium.util.Strings;\n+import io.debezium.util.Threads;\n+import io.debezium.util.Threads.Timer;\n+\n+/**\n+ * A {@link SnapshotChangeEventSource} that performs multi-threaded snapshots of replica sets.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbSnapshotChangeEventSource implements SnapshotChangeEventSource {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(MongoDbSnapshotChangeEventSource.class);\n+\n+    private static final String AUTHORIZATION_FAILURE_MESSAGE = \"Command failed with error 13\";\n+\n+    private final MongoDbConnectorConfig connectorConfig;\n+    private final MongoDbTaskContext taskContext;\n+    private final MongoDbOffsetContext previousOffset;\n+    private final ConnectionContext connectionContext;\n+    private final ReplicaSets replicaSets;\n+    private final EventDispatcher<CollectionId> dispatcher;\n+    protected final Clock clock;\n+    private final SnapshotProgressListener snapshotProgressListener;\n+    private final ErrorHandler errorHandler;\n+    private AtomicBoolean aborted = new AtomicBoolean(false);\n+\n+    public MongoDbSnapshotChangeEventSource(MongoDbConnectorConfig connectorConfig, MongoDbTaskContext taskContext,\n+                                            ReplicaSets replicaSets, MongoDbOffsetContext previousOffset,\n+                                            EventDispatcher<CollectionId> dispatcher, Clock clock,\n+                                            SnapshotProgressListener snapshotProgressListener, ErrorHandler errorHandler) {\n+        this.connectorConfig = connectorConfig;\n+        this.taskContext = taskContext;\n+        this.connectionContext = taskContext.getConnectionContext();\n+        this.previousOffset = previousOffset;\n+        this.replicaSets = replicaSets;\n+        this.dispatcher = dispatcher;\n+        this.clock = clock;\n+        this.snapshotProgressListener = snapshotProgressListener;\n+        this.errorHandler = errorHandler;\n+    }\n+\n+    @Override\n+    public SnapshotResult execute(ChangeEventSourceContext context) throws InterruptedException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI2NTQwMQ=="}, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxODU1MjIwOnYy", "diffSide": "RIGHT", "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbStreamingChangeEventSource.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxMjowNjozMlrOF0MC-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQwOTowNzoyM1rOF0uSzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI2NzY0Mg==", "bodyText": "Do we need a separate thread here? I mean is there any blocking op or can we unwind ans simplify it to a single thread?", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390267642", "createdAt": "2020-03-10T12:06:32Z", "author": {"login": "jpechane"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbStreamingChangeEventSource.java", "diffHunk": "@@ -0,0 +1,406 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.OptionalLong;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.bson.BsonTimestamp;\n+import org.bson.Document;\n+import org.bson.conversions.Bson;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.mongodb.CursorType;\n+import com.mongodb.MongoClient;\n+import com.mongodb.ServerAddress;\n+import com.mongodb.client.FindIterable;\n+import com.mongodb.client.MongoCollection;\n+import com.mongodb.client.MongoCursor;\n+import com.mongodb.client.model.Filters;\n+\n+import io.debezium.connector.mongodb.ConnectionContext.MongoPrimary;\n+import io.debezium.pipeline.ErrorHandler;\n+import io.debezium.pipeline.EventDispatcher;\n+import io.debezium.pipeline.source.spi.StreamingChangeEventSource;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.util.Clock;\n+import io.debezium.util.Metronome;\n+import io.debezium.util.Threads;\n+\n+/**\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbStreamingChangeEventSource implements StreamingChangeEventSource {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(MongoDbStreamingChangeEventSource.class);\n+\n+    private static final String AUTHORIZATION_FAILURE_MESSAGE = \"Command failed with error 13\";\n+\n+    private static final String OPERATION_FIELD = \"op\";\n+    private static final String OBJECT_FIELD = \"o\";\n+    private static final String OPERATION_CONTROL = \"c\";\n+    private static final String TX_OPS = \"applyOps\";\n+\n+    private final EventDispatcher<CollectionId> dispatcher;\n+    private final ErrorHandler errorHandler;\n+    private final Clock clock;\n+    private final MongoDbOffsetContext offsetContext;\n+    private final ConnectionContext connectionContext;\n+    private final ReplicaSets replicaSets;\n+    private final MongoDbTaskContext taskContext;\n+\n+    public MongoDbStreamingChangeEventSource(MongoDbConnectorConfig connectorConfig, MongoDbTaskContext taskContext,\n+                                             ReplicaSets replicaSets, MongoDbOffsetContext offsetContext,\n+                                             EventDispatcher<CollectionId> dispatcher, ErrorHandler errorHandler, Clock clock) {\n+        this.connectionContext = taskContext.getConnectionContext();\n+        this.dispatcher = dispatcher;\n+        this.errorHandler = errorHandler;\n+        this.clock = clock;\n+        this.replicaSets = replicaSets;\n+        this.taskContext = taskContext;\n+        this.offsetContext = (offsetContext != null) ? offsetContext : initializeOffsets(connectorConfig, replicaSets);\n+    }\n+\n+    @Override\n+    public void execute(ChangeEventSourceContext context) throws InterruptedException {\n+        // Starts a thread for each replica-set and executes the streaming process\n+        final int threads = replicaSets.replicaSetCount();\n+        final ExecutorService executor = Threads.newFixedThreadPool(MongoDbConnector.class, taskContext.serverName(), \"replicator-streaming\", threads);\n+        final CountDownLatch latch = new CountDownLatch(threads);\n+\n+        LOGGER.info(\"Starting {} thread(s) to stream changes for replica sets: {}\", threads, replicaSets);\n+        replicaSets.validReplicaSets().forEach(replicaSet -> {\n+            executor.submit(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQwNTI3Mg==", "bodyText": "The task submitted to the executor does block when executing the read from oplog call using the primaryClient.execute call.  Since a connector may be configured with multiple replica-sets, I believe this has to remain for now. We might be able to work around this a bit when migrating to change streams, but that's a tbd.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390405272", "createdAt": "2020-03-10T15:33:36Z", "author": {"login": "Naros"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbStreamingChangeEventSource.java", "diffHunk": "@@ -0,0 +1,406 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.OptionalLong;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.bson.BsonTimestamp;\n+import org.bson.Document;\n+import org.bson.conversions.Bson;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.mongodb.CursorType;\n+import com.mongodb.MongoClient;\n+import com.mongodb.ServerAddress;\n+import com.mongodb.client.FindIterable;\n+import com.mongodb.client.MongoCollection;\n+import com.mongodb.client.MongoCursor;\n+import com.mongodb.client.model.Filters;\n+\n+import io.debezium.connector.mongodb.ConnectionContext.MongoPrimary;\n+import io.debezium.pipeline.ErrorHandler;\n+import io.debezium.pipeline.EventDispatcher;\n+import io.debezium.pipeline.source.spi.StreamingChangeEventSource;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.util.Clock;\n+import io.debezium.util.Metronome;\n+import io.debezium.util.Threads;\n+\n+/**\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbStreamingChangeEventSource implements StreamingChangeEventSource {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(MongoDbStreamingChangeEventSource.class);\n+\n+    private static final String AUTHORIZATION_FAILURE_MESSAGE = \"Command failed with error 13\";\n+\n+    private static final String OPERATION_FIELD = \"op\";\n+    private static final String OBJECT_FIELD = \"o\";\n+    private static final String OPERATION_CONTROL = \"c\";\n+    private static final String TX_OPS = \"applyOps\";\n+\n+    private final EventDispatcher<CollectionId> dispatcher;\n+    private final ErrorHandler errorHandler;\n+    private final Clock clock;\n+    private final MongoDbOffsetContext offsetContext;\n+    private final ConnectionContext connectionContext;\n+    private final ReplicaSets replicaSets;\n+    private final MongoDbTaskContext taskContext;\n+\n+    public MongoDbStreamingChangeEventSource(MongoDbConnectorConfig connectorConfig, MongoDbTaskContext taskContext,\n+                                             ReplicaSets replicaSets, MongoDbOffsetContext offsetContext,\n+                                             EventDispatcher<CollectionId> dispatcher, ErrorHandler errorHandler, Clock clock) {\n+        this.connectionContext = taskContext.getConnectionContext();\n+        this.dispatcher = dispatcher;\n+        this.errorHandler = errorHandler;\n+        this.clock = clock;\n+        this.replicaSets = replicaSets;\n+        this.taskContext = taskContext;\n+        this.offsetContext = (offsetContext != null) ? offsetContext : initializeOffsets(connectorConfig, replicaSets);\n+    }\n+\n+    @Override\n+    public void execute(ChangeEventSourceContext context) throws InterruptedException {\n+        // Starts a thread for each replica-set and executes the streaming process\n+        final int threads = replicaSets.replicaSetCount();\n+        final ExecutorService executor = Threads.newFixedThreadPool(MongoDbConnector.class, taskContext.serverName(), \"replicator-streaming\", threads);\n+        final CountDownLatch latch = new CountDownLatch(threads);\n+\n+        LOGGER.info(\"Starting {} thread(s) to stream changes for replica sets: {}\", threads, replicaSets);\n+        replicaSets.validReplicaSets().forEach(replicaSet -> {\n+            executor.submit(() -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI2NzY0Mg=="}, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDc1MjQyMA==", "bodyText": "@Naros I belive we could do it after the release. It seems to me the only blocking operation is com.mongodb.client.MongoCursor.next() which can be replaced with com.mongodb.client.MongoCursor.tryNext(). But taking multiple rs into considertion this is definitely larger task and requires further discussion.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390752420", "createdAt": "2020-03-11T05:27:01Z", "author": {"login": "jpechane"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbStreamingChangeEventSource.java", "diffHunk": "@@ -0,0 +1,406 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.OptionalLong;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.bson.BsonTimestamp;\n+import org.bson.Document;\n+import org.bson.conversions.Bson;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.mongodb.CursorType;\n+import com.mongodb.MongoClient;\n+import com.mongodb.ServerAddress;\n+import com.mongodb.client.FindIterable;\n+import com.mongodb.client.MongoCollection;\n+import com.mongodb.client.MongoCursor;\n+import com.mongodb.client.model.Filters;\n+\n+import io.debezium.connector.mongodb.ConnectionContext.MongoPrimary;\n+import io.debezium.pipeline.ErrorHandler;\n+import io.debezium.pipeline.EventDispatcher;\n+import io.debezium.pipeline.source.spi.StreamingChangeEventSource;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.util.Clock;\n+import io.debezium.util.Metronome;\n+import io.debezium.util.Threads;\n+\n+/**\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbStreamingChangeEventSource implements StreamingChangeEventSource {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(MongoDbStreamingChangeEventSource.class);\n+\n+    private static final String AUTHORIZATION_FAILURE_MESSAGE = \"Command failed with error 13\";\n+\n+    private static final String OPERATION_FIELD = \"op\";\n+    private static final String OBJECT_FIELD = \"o\";\n+    private static final String OPERATION_CONTROL = \"c\";\n+    private static final String TX_OPS = \"applyOps\";\n+\n+    private final EventDispatcher<CollectionId> dispatcher;\n+    private final ErrorHandler errorHandler;\n+    private final Clock clock;\n+    private final MongoDbOffsetContext offsetContext;\n+    private final ConnectionContext connectionContext;\n+    private final ReplicaSets replicaSets;\n+    private final MongoDbTaskContext taskContext;\n+\n+    public MongoDbStreamingChangeEventSource(MongoDbConnectorConfig connectorConfig, MongoDbTaskContext taskContext,\n+                                             ReplicaSets replicaSets, MongoDbOffsetContext offsetContext,\n+                                             EventDispatcher<CollectionId> dispatcher, ErrorHandler errorHandler, Clock clock) {\n+        this.connectionContext = taskContext.getConnectionContext();\n+        this.dispatcher = dispatcher;\n+        this.errorHandler = errorHandler;\n+        this.clock = clock;\n+        this.replicaSets = replicaSets;\n+        this.taskContext = taskContext;\n+        this.offsetContext = (offsetContext != null) ? offsetContext : initializeOffsets(connectorConfig, replicaSets);\n+    }\n+\n+    @Override\n+    public void execute(ChangeEventSourceContext context) throws InterruptedException {\n+        // Starts a thread for each replica-set and executes the streaming process\n+        final int threads = replicaSets.replicaSetCount();\n+        final ExecutorService executor = Threads.newFixedThreadPool(MongoDbConnector.class, taskContext.serverName(), \"replicator-streaming\", threads);\n+        final CountDownLatch latch = new CountDownLatch(threads);\n+\n+        LOGGER.info(\"Starting {} thread(s) to stream changes for replica sets: {}\", threads, replicaSets);\n+        replicaSets.validReplicaSets().forEach(replicaSet -> {\n+            executor.submit(() -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI2NzY0Mg=="}, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDgyODc1MA==", "bodyText": "I've added this as one todo item to DBZ-1860.", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390828750", "createdAt": "2020-03-11T09:07:23Z", "author": {"login": "gunnarmorling"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbStreamingChangeEventSource.java", "diffHunk": "@@ -0,0 +1,406 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.OptionalLong;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.bson.BsonTimestamp;\n+import org.bson.Document;\n+import org.bson.conversions.Bson;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.mongodb.CursorType;\n+import com.mongodb.MongoClient;\n+import com.mongodb.ServerAddress;\n+import com.mongodb.client.FindIterable;\n+import com.mongodb.client.MongoCollection;\n+import com.mongodb.client.MongoCursor;\n+import com.mongodb.client.model.Filters;\n+\n+import io.debezium.connector.mongodb.ConnectionContext.MongoPrimary;\n+import io.debezium.pipeline.ErrorHandler;\n+import io.debezium.pipeline.EventDispatcher;\n+import io.debezium.pipeline.source.spi.StreamingChangeEventSource;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.util.Clock;\n+import io.debezium.util.Metronome;\n+import io.debezium.util.Threads;\n+\n+/**\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbStreamingChangeEventSource implements StreamingChangeEventSource {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(MongoDbStreamingChangeEventSource.class);\n+\n+    private static final String AUTHORIZATION_FAILURE_MESSAGE = \"Command failed with error 13\";\n+\n+    private static final String OPERATION_FIELD = \"op\";\n+    private static final String OBJECT_FIELD = \"o\";\n+    private static final String OPERATION_CONTROL = \"c\";\n+    private static final String TX_OPS = \"applyOps\";\n+\n+    private final EventDispatcher<CollectionId> dispatcher;\n+    private final ErrorHandler errorHandler;\n+    private final Clock clock;\n+    private final MongoDbOffsetContext offsetContext;\n+    private final ConnectionContext connectionContext;\n+    private final ReplicaSets replicaSets;\n+    private final MongoDbTaskContext taskContext;\n+\n+    public MongoDbStreamingChangeEventSource(MongoDbConnectorConfig connectorConfig, MongoDbTaskContext taskContext,\n+                                             ReplicaSets replicaSets, MongoDbOffsetContext offsetContext,\n+                                             EventDispatcher<CollectionId> dispatcher, ErrorHandler errorHandler, Clock clock) {\n+        this.connectionContext = taskContext.getConnectionContext();\n+        this.dispatcher = dispatcher;\n+        this.errorHandler = errorHandler;\n+        this.clock = clock;\n+        this.replicaSets = replicaSets;\n+        this.taskContext = taskContext;\n+        this.offsetContext = (offsetContext != null) ? offsetContext : initializeOffsets(connectorConfig, replicaSets);\n+    }\n+\n+    @Override\n+    public void execute(ChangeEventSourceContext context) throws InterruptedException {\n+        // Starts a thread for each replica-set and executes the streaming process\n+        final int threads = replicaSets.replicaSetCount();\n+        final ExecutorService executor = Threads.newFixedThreadPool(MongoDbConnector.class, taskContext.serverName(), \"replicator-streaming\", threads);\n+        final CountDownLatch latch = new CountDownLatch(threads);\n+\n+        LOGGER.info(\"Starting {} thread(s) to stream changes for replica sets: {}\", threads, replicaSets);\n+        replicaSets.validReplicaSets().forEach(replicaSet -> {\n+            executor.submit(() -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI2NzY0Mg=="}, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxODU4MjM2OnYy", "diffSide": "RIGHT", "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbEventMetadataProvider.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxMjoxNjoxNVrOF0MVZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxNTo0NjoyOFrOF0VCjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI3MjM1OA==", "bodyText": "MongoDb supports transactions - so it should be added here, see io.debezium.connector.mongodb.MongoDbStreamingChangeEventSource.handleOplogEvent(ServerAddress, Document, Document, long, ReplicaSetOplogContext)", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390272358", "createdAt": "2020-03-10T12:16:15Z", "author": {"login": "jpechane"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbEventMetadataProvider.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Instant;\n+import java.util.Map;\n+\n+import org.apache.kafka.connect.data.Struct;\n+\n+import io.debezium.data.Envelope;\n+import io.debezium.pipeline.source.spi.EventMetadataProvider;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.schema.DataCollectionId;\n+import io.debezium.util.Collect;\n+\n+/**\n+ * An {@link EventMetadataProvider} implementation for Mongodb to extract metrics data from events.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbEventMetadataProvider implements EventMetadataProvider {\n+\n+    @Override\n+    public Instant getEventTimestamp(DataCollectionId source, OffsetContext offset, Object key, Struct value) {\n+        if (value == null) {\n+            return null;\n+        }\n+        final Struct sourceInfo = value.getStruct(Envelope.FieldName.SOURCE);\n+        if (source == null) {\n+            return null;\n+        }\n+        final Long timestamp = sourceInfo.getInt64(SourceInfo.TIMESTAMP_KEY);\n+        return timestamp == null ? null : Instant.ofEpochMilli(timestamp);\n+    }\n+\n+    @Override\n+    public Map<String, String> getEventSourcePosition(DataCollectionId source, OffsetContext offset, Object key, Struct value) {\n+        if (value == null) {\n+            return null;\n+        }\n+        final Struct sourceInfo = value.getStruct(Envelope.FieldName.SOURCE);\n+        if (source == null) {\n+            return null;\n+        }\n+\n+        Integer ord = sourceInfo.getInt32(SourceInfo.ORDER);\n+        return Collect.hashMapOf(SourceInfo.ORDER, Integer.toString(ord));\n+    }\n+\n+    @Override\n+    public String getTransactionId(DataCollectionId source, OffsetContext offset, Object key, Struct value) {\n+        // todo: DBZ-1726 for now this returns null; is there an implementation alternative?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDQxNDk4OQ==", "bodyText": "See commit 7ece206", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390414989", "createdAt": "2020-03-10T15:46:28Z", "author": {"login": "Naros"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbEventMetadataProvider.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Instant;\n+import java.util.Map;\n+\n+import org.apache.kafka.connect.data.Struct;\n+\n+import io.debezium.data.Envelope;\n+import io.debezium.pipeline.source.spi.EventMetadataProvider;\n+import io.debezium.pipeline.spi.OffsetContext;\n+import io.debezium.schema.DataCollectionId;\n+import io.debezium.util.Collect;\n+\n+/**\n+ * An {@link EventMetadataProvider} implementation for Mongodb to extract metrics data from events.\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbEventMetadataProvider implements EventMetadataProvider {\n+\n+    @Override\n+    public Instant getEventTimestamp(DataCollectionId source, OffsetContext offset, Object key, Struct value) {\n+        if (value == null) {\n+            return null;\n+        }\n+        final Struct sourceInfo = value.getStruct(Envelope.FieldName.SOURCE);\n+        if (source == null) {\n+            return null;\n+        }\n+        final Long timestamp = sourceInfo.getInt64(SourceInfo.TIMESTAMP_KEY);\n+        return timestamp == null ? null : Instant.ofEpochMilli(timestamp);\n+    }\n+\n+    @Override\n+    public Map<String, String> getEventSourcePosition(DataCollectionId source, OffsetContext offset, Object key, Struct value) {\n+        if (value == null) {\n+            return null;\n+        }\n+        final Struct sourceInfo = value.getStruct(Envelope.FieldName.SOURCE);\n+        if (source == null) {\n+            return null;\n+        }\n+\n+        Integer ord = sourceInfo.getInt32(SourceInfo.ORDER);\n+        return Collect.hashMapOf(SourceInfo.ORDER, Integer.toString(ord));\n+    }\n+\n+    @Override\n+    public String getTransactionId(DataCollectionId source, OffsetContext offset, Object key, Struct value) {\n+        // todo: DBZ-1726 for now this returns null; is there an implementation alternative?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI3MjM1OA=="}, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxODU4NTA2OnYy", "diffSide": "RIGHT", "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbStreamingChangeEventSource.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxMjoxNzowM1rOF0MW-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQxODo1Mjo1N1rOF0cneA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI3Mjc2Mw==", "bodyText": "Should integrate TransactionMonitor", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390272763", "createdAt": "2020-03-10T12:17:03Z", "author": {"login": "jpechane"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbStreamingChangeEventSource.java", "diffHunk": "@@ -0,0 +1,406 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.OptionalLong;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.bson.BsonTimestamp;\n+import org.bson.Document;\n+import org.bson.conversions.Bson;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.mongodb.CursorType;\n+import com.mongodb.MongoClient;\n+import com.mongodb.ServerAddress;\n+import com.mongodb.client.FindIterable;\n+import com.mongodb.client.MongoCollection;\n+import com.mongodb.client.MongoCursor;\n+import com.mongodb.client.model.Filters;\n+\n+import io.debezium.connector.mongodb.ConnectionContext.MongoPrimary;\n+import io.debezium.pipeline.ErrorHandler;\n+import io.debezium.pipeline.EventDispatcher;\n+import io.debezium.pipeline.source.spi.StreamingChangeEventSource;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.util.Clock;\n+import io.debezium.util.Metronome;\n+import io.debezium.util.Threads;\n+\n+/**\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbStreamingChangeEventSource implements StreamingChangeEventSource {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(MongoDbStreamingChangeEventSource.class);\n+\n+    private static final String AUTHORIZATION_FAILURE_MESSAGE = \"Command failed with error 13\";\n+\n+    private static final String OPERATION_FIELD = \"op\";\n+    private static final String OBJECT_FIELD = \"o\";\n+    private static final String OPERATION_CONTROL = \"c\";\n+    private static final String TX_OPS = \"applyOps\";\n+\n+    private final EventDispatcher<CollectionId> dispatcher;\n+    private final ErrorHandler errorHandler;\n+    private final Clock clock;\n+    private final MongoDbOffsetContext offsetContext;\n+    private final ConnectionContext connectionContext;\n+    private final ReplicaSets replicaSets;\n+    private final MongoDbTaskContext taskContext;\n+\n+    public MongoDbStreamingChangeEventSource(MongoDbConnectorConfig connectorConfig, MongoDbTaskContext taskContext,\n+                                             ReplicaSets replicaSets, MongoDbOffsetContext offsetContext,\n+                                             EventDispatcher<CollectionId> dispatcher, ErrorHandler errorHandler, Clock clock) {\n+        this.connectionContext = taskContext.getConnectionContext();\n+        this.dispatcher = dispatcher;\n+        this.errorHandler = errorHandler;\n+        this.clock = clock;\n+        this.replicaSets = replicaSets;\n+        this.taskContext = taskContext;\n+        this.offsetContext = (offsetContext != null) ? offsetContext : initializeOffsets(connectorConfig, replicaSets);\n+    }\n+\n+    @Override\n+    public void execute(ChangeEventSourceContext context) throws InterruptedException {\n+        // Starts a thread for each replica-set and executes the streaming process\n+        final int threads = replicaSets.replicaSetCount();\n+        final ExecutorService executor = Threads.newFixedThreadPool(MongoDbConnector.class, taskContext.serverName(), \"replicator-streaming\", threads);\n+        final CountDownLatch latch = new CountDownLatch(threads);\n+\n+        LOGGER.info(\"Starting {} thread(s) to stream changes for replica sets: {}\", threads, replicaSets);\n+        replicaSets.validReplicaSets().forEach(replicaSet -> {\n+            executor.submit(() -> {\n+                MongoPrimary primaryClient = null;\n+                try {\n+                    primaryClient = establishConnectionToPrimary(replicaSet);\n+                    if (primaryClient != null) {\n+                        final AtomicReference<MongoPrimary> primaryReference = new AtomicReference<>(primaryClient);\n+                        primaryClient.execute(\"read from oplog on '\" + replicaSet + \"'\", primary -> {\n+                            readOplog(primary, primaryReference.get(), replicaSet, context);\n+                        });\n+                    }\n+                }\n+                catch (Throwable t) {\n+                    LOGGER.error(\"Streaming for replica set {} failed\", replicaSet.replicaSetName(), t);\n+                    errorHandler.setProducerThrowable(t);\n+                }\n+                finally {\n+                    if (primaryClient != null) {\n+                        primaryClient.stop();\n+                    }\n+\n+                    latch.countDown();\n+                }\n+            });\n+        });\n+\n+        // Wait for the executor service to terminate.\n+        try {\n+            latch.await();\n+        }\n+        catch (InterruptedException e) {\n+            Thread.currentThread().interrupt();\n+        }\n+\n+        // Shutdown the executor and cleanup connections\n+        try {\n+            executor.shutdown();\n+        }\n+        finally {\n+            taskContext.getConnectionContext().shutdown();\n+        }\n+    }\n+\n+    private MongoPrimary establishConnectionToPrimary(ReplicaSet replicaSet) {\n+        return connectionContext.primaryFor(replicaSet, taskContext.filters(), (desc, error) -> {\n+            // propagate authorization failures\n+            if (error.getMessage() != null && error.getMessage().startsWith(AUTHORIZATION_FAILURE_MESSAGE)) {\n+                throw new ConnectException(\"Error while attempting to \" + desc, error);\n+            }\n+            else {\n+                LOGGER.error(\"Error while attempting to {}: {}\", desc, error.getMessage(), error);\n+                throw new ConnectException(\"Error while attempting to \" + desc, error);\n+            }\n+        });\n+    }\n+\n+    private void readOplog(MongoClient primary, MongoPrimary primaryClient, ReplicaSet replicaSet, ChangeEventSourceContext context) {\n+        final ReplicaSetOffsetContext rsOffsetContext = offsetContext.getReplicaSetOffsetContext(replicaSet);\n+\n+        final BsonTimestamp oplogStart = rsOffsetContext.lastOffsetTimestamp();\n+        final OptionalLong txOrder = rsOffsetContext.lastOffsetTxOrder();\n+\n+        final ServerAddress primaryAddress = primary.getAddress();\n+        LOGGER.info(\"Reading oplog for '{}' primary {} starting at {}\", replicaSet, primaryAddress, oplogStart);\n+\n+        // Include none of the cluster-internal operations and only those events since the previous timestamp\n+        MongoCollection<Document> oplog = primary.getDatabase(\"local\").getCollection(\"oplog.rs\");\n+\n+        ReplicaSetOplogContext oplogContext = new ReplicaSetOplogContext(rsOffsetContext, primaryClient, replicaSet);\n+\n+        Bson filter = null;\n+        if (!txOrder.isPresent()) {\n+            LOGGER.info(\"The last event processed was not transactional, resuming at the oplog event after '{}'\", oplogStart);\n+            filter = Filters.and(Filters.gt(\"ts\", oplogStart), // start just after our last position\n+                    Filters.exists(\"fromMigrate\", false)); // skip internal movements across shards\n+        }\n+        else {\n+            LOGGER.info(\"The last event processed was transactional, resuming at the oplog event '{}', expecting to skip '{}' events\",\n+                    oplogStart, txOrder.getAsLong());\n+            filter = Filters.and(Filters.gte(\"ts\", oplogStart), Filters.exists(\"fromMigrate\", false));\n+            oplogContext.setIncompleteEventTimestamp(oplogStart);\n+            oplogContext.setIncompleteTxOrder(txOrder.getAsLong());\n+        }\n+\n+        final FindIterable<Document> results = oplog.find(filter)\n+                .sort(new Document(\"$natural\", 1))\n+                .oplogReplay(true)\n+                .cursorType(CursorType.TailableAwait);\n+\n+        try (MongoCursor<Document> cursor = results.iterator()) {\n+            // In Replicator, this used cursor.hasNext() but this is a blocking call and I observed that this can\n+            // delay the shutdown of the connector by up to 15 seconds or longer. By introducing a Metronome, we\n+            // can respond to the stop request much faster and without much overhead.\n+            Metronome pause = Metronome.sleeper(Duration.ofMillis(500), clock);\n+            while (context.isRunning()) {\n+                // Use tryNext which will return null if no document is yet available from the cursor.\n+                // In this situation if not document is available, we'll pause.\n+                final Document event = cursor.tryNext();\n+                if (event != null) {\n+                    if (!handleOplogEvent(primaryAddress, event, event, 0, oplogContext)) {\n+                        // Something happened and we are supposed to stop reading\n+                        return;\n+                    }\n+\n+                    try {\n+                        dispatcher.dispatchHeartbeatEvent(oplogContext.getOffset());\n+                    }\n+                    catch (InterruptedException e) {\n+                        LOGGER.info(\"Replicator thread is interrupted\");\n+                        Thread.currentThread().interrupt();\n+                        return;\n+                    }\n+                }\n+                else {\n+                    try {\n+                        pause.pause();\n+                    }\n+                    catch (InterruptedException e) {\n+                        break;\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private boolean handleOplogEvent(ServerAddress primaryAddress, Document event, Document masterEvent, long txOrder, ReplicaSetOplogContext oplogContext) {\n+        String ns = event.getString(\"ns\");\n+        Document object = event.get(OBJECT_FIELD, Document.class);\n+        if (Objects.isNull(object)) {\n+            if (LOGGER.isWarnEnabled()) {\n+                LOGGER.warn(\"Missing 'o' field in event, so skipping {}\", event.toJson());\n+            }\n+            return true;\n+        }\n+\n+        if (Objects.isNull(ns) || ns.isEmpty()) {\n+            // These are considered replica set events\n+            String msg = object.getString(\"msg\");\n+            if (\"new primary\".equals(msg)) {\n+                AtomicReference<ServerAddress> address = new AtomicReference<>();\n+                try {\n+                    oplogContext.getPrimary().executeBlocking(\"conn\", mongoClient -> {\n+                        ServerAddress currentPrimary = mongoClient.getAddress();\n+                        address.set(currentPrimary);\n+                    });\n+                }\n+                catch (InterruptedException e) {\n+                    LOGGER.error(\"Get current primary executeBlocking\", e);\n+                }\n+\n+                ServerAddress serverAddress = address.get();\n+                if (Objects.nonNull(serverAddress) && !serverAddress.equals(primaryAddress)) {\n+                    LOGGER.info(\"Found new primary event in oplog, so stopping use of {} to continue with new primary {}\",\n+                            primaryAddress, serverAddress);\n+                }\n+                else {\n+                    LOGGER.info(\"Found new primary event in oplog, current {} is new primary. \" +\n+                            \"Continue to process oplog event.\", primaryAddress);\n+                }\n+            }\n+            // Otherwise ignore\n+            if (LOGGER.isDebugEnabled()) {\n+                LOGGER.debug(\"Skipping event with no namespace: {}\", event.toJson());\n+            }\n+            return true;\n+        }\n+\n+        final List<Document> txChanges = transactionChanges(event);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 253}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDUzOTEyOA==", "bodyText": "See commit 53a38fd", "url": "https://github.com/debezium/debezium/pull/1316#discussion_r390539128", "createdAt": "2020-03-10T18:52:57Z", "author": {"login": "Naros"}, "path": "debezium-connector-mongodb/src/main/java/io/debezium/connector/mongodb/MongoDbStreamingChangeEventSource.java", "diffHunk": "@@ -0,0 +1,406 @@\n+/*\n+ * Copyright Debezium Authors.\n+ *\n+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.debezium.connector.mongodb;\n+\n+import java.time.Duration;\n+import java.util.Collections;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.OptionalLong;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import org.apache.kafka.connect.errors.ConnectException;\n+import org.bson.BsonTimestamp;\n+import org.bson.Document;\n+import org.bson.conversions.Bson;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import com.mongodb.CursorType;\n+import com.mongodb.MongoClient;\n+import com.mongodb.ServerAddress;\n+import com.mongodb.client.FindIterable;\n+import com.mongodb.client.MongoCollection;\n+import com.mongodb.client.MongoCursor;\n+import com.mongodb.client.model.Filters;\n+\n+import io.debezium.connector.mongodb.ConnectionContext.MongoPrimary;\n+import io.debezium.pipeline.ErrorHandler;\n+import io.debezium.pipeline.EventDispatcher;\n+import io.debezium.pipeline.source.spi.StreamingChangeEventSource;\n+import io.debezium.pipeline.txmetadata.TransactionContext;\n+import io.debezium.util.Clock;\n+import io.debezium.util.Metronome;\n+import io.debezium.util.Threads;\n+\n+/**\n+ *\n+ * @author Chris Cranford\n+ */\n+public class MongoDbStreamingChangeEventSource implements StreamingChangeEventSource {\n+\n+    private static final Logger LOGGER = LoggerFactory.getLogger(MongoDbStreamingChangeEventSource.class);\n+\n+    private static final String AUTHORIZATION_FAILURE_MESSAGE = \"Command failed with error 13\";\n+\n+    private static final String OPERATION_FIELD = \"op\";\n+    private static final String OBJECT_FIELD = \"o\";\n+    private static final String OPERATION_CONTROL = \"c\";\n+    private static final String TX_OPS = \"applyOps\";\n+\n+    private final EventDispatcher<CollectionId> dispatcher;\n+    private final ErrorHandler errorHandler;\n+    private final Clock clock;\n+    private final MongoDbOffsetContext offsetContext;\n+    private final ConnectionContext connectionContext;\n+    private final ReplicaSets replicaSets;\n+    private final MongoDbTaskContext taskContext;\n+\n+    public MongoDbStreamingChangeEventSource(MongoDbConnectorConfig connectorConfig, MongoDbTaskContext taskContext,\n+                                             ReplicaSets replicaSets, MongoDbOffsetContext offsetContext,\n+                                             EventDispatcher<CollectionId> dispatcher, ErrorHandler errorHandler, Clock clock) {\n+        this.connectionContext = taskContext.getConnectionContext();\n+        this.dispatcher = dispatcher;\n+        this.errorHandler = errorHandler;\n+        this.clock = clock;\n+        this.replicaSets = replicaSets;\n+        this.taskContext = taskContext;\n+        this.offsetContext = (offsetContext != null) ? offsetContext : initializeOffsets(connectorConfig, replicaSets);\n+    }\n+\n+    @Override\n+    public void execute(ChangeEventSourceContext context) throws InterruptedException {\n+        // Starts a thread for each replica-set and executes the streaming process\n+        final int threads = replicaSets.replicaSetCount();\n+        final ExecutorService executor = Threads.newFixedThreadPool(MongoDbConnector.class, taskContext.serverName(), \"replicator-streaming\", threads);\n+        final CountDownLatch latch = new CountDownLatch(threads);\n+\n+        LOGGER.info(\"Starting {} thread(s) to stream changes for replica sets: {}\", threads, replicaSets);\n+        replicaSets.validReplicaSets().forEach(replicaSet -> {\n+            executor.submit(() -> {\n+                MongoPrimary primaryClient = null;\n+                try {\n+                    primaryClient = establishConnectionToPrimary(replicaSet);\n+                    if (primaryClient != null) {\n+                        final AtomicReference<MongoPrimary> primaryReference = new AtomicReference<>(primaryClient);\n+                        primaryClient.execute(\"read from oplog on '\" + replicaSet + \"'\", primary -> {\n+                            readOplog(primary, primaryReference.get(), replicaSet, context);\n+                        });\n+                    }\n+                }\n+                catch (Throwable t) {\n+                    LOGGER.error(\"Streaming for replica set {} failed\", replicaSet.replicaSetName(), t);\n+                    errorHandler.setProducerThrowable(t);\n+                }\n+                finally {\n+                    if (primaryClient != null) {\n+                        primaryClient.stop();\n+                    }\n+\n+                    latch.countDown();\n+                }\n+            });\n+        });\n+\n+        // Wait for the executor service to terminate.\n+        try {\n+            latch.await();\n+        }\n+        catch (InterruptedException e) {\n+            Thread.currentThread().interrupt();\n+        }\n+\n+        // Shutdown the executor and cleanup connections\n+        try {\n+            executor.shutdown();\n+        }\n+        finally {\n+            taskContext.getConnectionContext().shutdown();\n+        }\n+    }\n+\n+    private MongoPrimary establishConnectionToPrimary(ReplicaSet replicaSet) {\n+        return connectionContext.primaryFor(replicaSet, taskContext.filters(), (desc, error) -> {\n+            // propagate authorization failures\n+            if (error.getMessage() != null && error.getMessage().startsWith(AUTHORIZATION_FAILURE_MESSAGE)) {\n+                throw new ConnectException(\"Error while attempting to \" + desc, error);\n+            }\n+            else {\n+                LOGGER.error(\"Error while attempting to {}: {}\", desc, error.getMessage(), error);\n+                throw new ConnectException(\"Error while attempting to \" + desc, error);\n+            }\n+        });\n+    }\n+\n+    private void readOplog(MongoClient primary, MongoPrimary primaryClient, ReplicaSet replicaSet, ChangeEventSourceContext context) {\n+        final ReplicaSetOffsetContext rsOffsetContext = offsetContext.getReplicaSetOffsetContext(replicaSet);\n+\n+        final BsonTimestamp oplogStart = rsOffsetContext.lastOffsetTimestamp();\n+        final OptionalLong txOrder = rsOffsetContext.lastOffsetTxOrder();\n+\n+        final ServerAddress primaryAddress = primary.getAddress();\n+        LOGGER.info(\"Reading oplog for '{}' primary {} starting at {}\", replicaSet, primaryAddress, oplogStart);\n+\n+        // Include none of the cluster-internal operations and only those events since the previous timestamp\n+        MongoCollection<Document> oplog = primary.getDatabase(\"local\").getCollection(\"oplog.rs\");\n+\n+        ReplicaSetOplogContext oplogContext = new ReplicaSetOplogContext(rsOffsetContext, primaryClient, replicaSet);\n+\n+        Bson filter = null;\n+        if (!txOrder.isPresent()) {\n+            LOGGER.info(\"The last event processed was not transactional, resuming at the oplog event after '{}'\", oplogStart);\n+            filter = Filters.and(Filters.gt(\"ts\", oplogStart), // start just after our last position\n+                    Filters.exists(\"fromMigrate\", false)); // skip internal movements across shards\n+        }\n+        else {\n+            LOGGER.info(\"The last event processed was transactional, resuming at the oplog event '{}', expecting to skip '{}' events\",\n+                    oplogStart, txOrder.getAsLong());\n+            filter = Filters.and(Filters.gte(\"ts\", oplogStart), Filters.exists(\"fromMigrate\", false));\n+            oplogContext.setIncompleteEventTimestamp(oplogStart);\n+            oplogContext.setIncompleteTxOrder(txOrder.getAsLong());\n+        }\n+\n+        final FindIterable<Document> results = oplog.find(filter)\n+                .sort(new Document(\"$natural\", 1))\n+                .oplogReplay(true)\n+                .cursorType(CursorType.TailableAwait);\n+\n+        try (MongoCursor<Document> cursor = results.iterator()) {\n+            // In Replicator, this used cursor.hasNext() but this is a blocking call and I observed that this can\n+            // delay the shutdown of the connector by up to 15 seconds or longer. By introducing a Metronome, we\n+            // can respond to the stop request much faster and without much overhead.\n+            Metronome pause = Metronome.sleeper(Duration.ofMillis(500), clock);\n+            while (context.isRunning()) {\n+                // Use tryNext which will return null if no document is yet available from the cursor.\n+                // In this situation if not document is available, we'll pause.\n+                final Document event = cursor.tryNext();\n+                if (event != null) {\n+                    if (!handleOplogEvent(primaryAddress, event, event, 0, oplogContext)) {\n+                        // Something happened and we are supposed to stop reading\n+                        return;\n+                    }\n+\n+                    try {\n+                        dispatcher.dispatchHeartbeatEvent(oplogContext.getOffset());\n+                    }\n+                    catch (InterruptedException e) {\n+                        LOGGER.info(\"Replicator thread is interrupted\");\n+                        Thread.currentThread().interrupt();\n+                        return;\n+                    }\n+                }\n+                else {\n+                    try {\n+                        pause.pause();\n+                    }\n+                    catch (InterruptedException e) {\n+                        break;\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private boolean handleOplogEvent(ServerAddress primaryAddress, Document event, Document masterEvent, long txOrder, ReplicaSetOplogContext oplogContext) {\n+        String ns = event.getString(\"ns\");\n+        Document object = event.get(OBJECT_FIELD, Document.class);\n+        if (Objects.isNull(object)) {\n+            if (LOGGER.isWarnEnabled()) {\n+                LOGGER.warn(\"Missing 'o' field in event, so skipping {}\", event.toJson());\n+            }\n+            return true;\n+        }\n+\n+        if (Objects.isNull(ns) || ns.isEmpty()) {\n+            // These are considered replica set events\n+            String msg = object.getString(\"msg\");\n+            if (\"new primary\".equals(msg)) {\n+                AtomicReference<ServerAddress> address = new AtomicReference<>();\n+                try {\n+                    oplogContext.getPrimary().executeBlocking(\"conn\", mongoClient -> {\n+                        ServerAddress currentPrimary = mongoClient.getAddress();\n+                        address.set(currentPrimary);\n+                    });\n+                }\n+                catch (InterruptedException e) {\n+                    LOGGER.error(\"Get current primary executeBlocking\", e);\n+                }\n+\n+                ServerAddress serverAddress = address.get();\n+                if (Objects.nonNull(serverAddress) && !serverAddress.equals(primaryAddress)) {\n+                    LOGGER.info(\"Found new primary event in oplog, so stopping use of {} to continue with new primary {}\",\n+                            primaryAddress, serverAddress);\n+                }\n+                else {\n+                    LOGGER.info(\"Found new primary event in oplog, current {} is new primary. \" +\n+                            \"Continue to process oplog event.\", primaryAddress);\n+                }\n+            }\n+            // Otherwise ignore\n+            if (LOGGER.isDebugEnabled()) {\n+                LOGGER.debug(\"Skipping event with no namespace: {}\", event.toJson());\n+            }\n+            return true;\n+        }\n+\n+        final List<Document> txChanges = transactionChanges(event);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDI3Mjc2Mw=="}, "originalCommit": {"oid": "7d5eb1b8561e565f093f9c73b3faddd6d5ea7381"}, "originalPosition": 253}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4440, "cost": 1, "resetAt": "2021-11-12T13:16:51Z"}}}