{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM2NjYwNzAw", "number": 1622, "title": "DBZ-1928 updated doc for Avro serialization", "bodyText": "Here's the updated doc for Avro serialization. There is lots of conditionalization. Please review this carefully.", "createdAt": "2020-06-18T17:30:29Z", "url": "https://github.com/debezium/debezium/pull/1622", "merged": true, "mergeCommit": {"oid": "f37bbdc7368b6de751e809255c082ee0533b2c90"}, "closed": true, "closedAt": "2020-06-30T14:23:27Z", "author": {"login": "TovaCohen"}, "timelineItems": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcslCOWAFqTQzMzY0Mzc0Nw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcwWaRJgFqTQ0MDA2NTkwMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzNjQzNzQ3", "url": "https://github.com/debezium/debezium/pull/1622#pullrequestreview-433643747", "createdAt": "2020-06-18T21:00:35Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTowMDozNVrOGmAC3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTowNzozM1rOGmAP0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ5OTgwNw==", "bodyText": "I feel like this and the following two paragraphs apply to both Kafka worker or individual connector setups and probably shouldn't be indented under the second option.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442499807", "createdAt": "2020-06-18T21:00:35Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n++\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwMzEyMg==", "bodyText": "Looks like this is a typo, should be endif::community[]?", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442503122", "createdAt": "2020-06-18T21:07:33Z", "author": {"login": "Naros"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -199,3 +358,5 @@ Note that some details around Kafka Connect converters have slightly changed sin\n \n For a complete example of using Avro as the message format for {prodname} data change events,\n please see the https://github.com/debezium/debezium-examples/tree/master/tutorial#using-mysql-and-the-avro-message-format[MySQL and the Avro message format] tutorial example.\n+\n+endif::[community]", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 363}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMzOTUwMTcy", "url": "https://github.com/debezium/debezium/pull/1622#pullrequestreview-433950172", "createdAt": "2020-06-19T09:55:44Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwOTo1NTo0NVrOGmPFnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxMDozODo0NlrOGmQTPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc0NjI2OQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            [id=\"overview-of-deploying-a-debezium-connector-that-uses-avro-serializatioin\"]\n          \n          \n            \n            [id=\"overview-of-deploying-a-debezium-connector-that-uses-avro-serialization\"]", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442746269", "createdAt": "2020-06-19T09:55:45Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. \n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Including schemas causes the messages to  be very verbose. If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the message keys and values by using link:https://avro.apache.org/[Apache Avro].\n+The Avro binary format is compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure. Avro's schema evolution mechanism makes it possible to evolve the schemas over time, which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables. Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema. Avro serialization makes it easier for consumers to adapt to the changing schema.\n \n-[NOTE]\n+ifdef::product[]\n+[IMPORTANT]\n ====\n-The Apicurio project also provides a JSON converter that can be used with the Apicurio registry.\n-This combines the advantage of less verbose messages (as messages do not contain the schema information themselves, but only a schema id)\n-with human-readable JSON.\n+Using Avro to serialize message keys and values is a Technology Preview feature. Technology Preview features are not supported with Red Hat production service-level agreements (SLAs) and might not be functionally complete; therefore, Red Hat does not recommend implementing any Technology Preview features in production environments. This Technology Preview feature provides early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process. For more information about support scope, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].\n ====\n+endif::product[]\n \n-Another option is using the Confluent schema registry, which is described further below.\n+// Type: concept\n+// Title: About the {registry}\n+[id=\"about-the-registry\"]\n+== About the {registry-name-full}\n \n-== Technical Information\n+ifdef::community[]\n+The link:https://github.com/Apicurio/apicurio-registry[{registry}] open-source project provides several components that work with Avro:\n+endif::community[]\n \n-A system that wants to use Avro serialization needs to complete two steps:\n+ifdef::product[]\n+{LinkServiceRegistryGetStart}[{registry-name-full}] provides several components that work with Avro:\n+endif::product[]\n \n-* Deploy an https://github.com/Apicurio/apicurio-registry[Apicurio API/Schema Registry] instance\n-* Install the Avro converter from https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka's _libs_ directory or directly into a plug-in directory\n-* Use thes the following properties to configure Apache Connect instance\n+* An Avro converter that you can configure in Kafka Connect workers. This converter maps Kafka Connect schemas to Avro schemas. The converter then uses the Avro schemas to serialize the message keys and values into Avro's compact binary form.\n \n-[source]\n+* An API/Schema registry that tracks:\n++\n+** Avro schemas that are used in Kafka topics\n+** Where the Avro converter sends the generated Avro schemas\n+\n++\n+Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n+This makes each message even smaller. For an I/O bound system like Kafka, this means more total throughput of the producers and consumers.\n+\n+* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n+Any Kafka consumer applications that you write to consume change event records can use Avro Serdes to deserialize the change event records.\n+\n+To use the {registry} with {prodname}, you must add {registry} converters and their dependencies to the Kafka Connect container image that you are using for running {prodname}.\n+\n+[NOTE]\n+====\n+The {registry} project also provides a JSON converter. This converter combines the advantage of less verbose messages with human-readable JSON. Messages do not contain the schema information themselves, but only a schema ID.\n+====\n+\n+ifdef::community[]\n+Another option is using the Confluent schema registry, which is described later.\n+endif::community[]\n+\n+// Type: concept\n+// Title: Overview of deploying a {prodname} connector that uses Avro serialization\n+[id=\"overview-of-deploying-a-debezium-connector-that-uses-avro-serializatioin\"]", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1NDg5Mw==", "bodyText": "Without the section before (which now is part of the Apicurio section), there's some context lacking why a registry is needed. How about adding something like this here:\nWhen using Apache Avro as a message format, a schema registry must be deployed which manages Avro message schemas and their versions. Options include the Apicurio API and Schema Registry and the Confluent Schema registry.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442754893", "createdAt": "2020-06-19T10:13:28Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. \n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Including schemas causes the messages to  be very verbose. If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the message keys and values by using link:https://avro.apache.org/[Apache Avro].\n+The Avro binary format is compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure. Avro's schema evolution mechanism makes it possible to evolve the schemas over time, which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables. Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema. Avro serialization makes it easier for consumers to adapt to the changing schema.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1NTIyNg==", "bodyText": "A reference would be nice.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442755226", "createdAt": "2020-06-19T10:14:05Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. \n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Including schemas causes the messages to  be very verbose. If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the message keys and values by using link:https://avro.apache.org/[Apache Avro].\n+The Avro binary format is compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure. Avro's schema evolution mechanism makes it possible to evolve the schemas over time, which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables. Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema. Avro serialization makes it easier for consumers to adapt to the changing schema.\n \n-[NOTE]\n+ifdef::product[]\n+[IMPORTANT]\n ====\n-The Apicurio project also provides a JSON converter that can be used with the Apicurio registry.\n-This combines the advantage of less verbose messages (as messages do not contain the schema information themselves, but only a schema id)\n-with human-readable JSON.\n+Using Avro to serialize message keys and values is a Technology Preview feature. Technology Preview features are not supported with Red Hat production service-level agreements (SLAs) and might not be functionally complete; therefore, Red Hat does not recommend implementing any Technology Preview features in production environments. This Technology Preview feature provides early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process. For more information about support scope, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].\n ====\n+endif::product[]\n \n-Another option is using the Confluent schema registry, which is described further below.\n+// Type: concept\n+// Title: About the {registry}\n+[id=\"about-the-registry\"]\n+== About the {registry-name-full}\n \n-== Technical Information\n+ifdef::community[]\n+The link:https://github.com/Apicurio/apicurio-registry[{registry}] open-source project provides several components that work with Avro:\n+endif::community[]\n \n-A system that wants to use Avro serialization needs to complete two steps:\n+ifdef::product[]\n+{LinkServiceRegistryGetStart}[{registry-name-full}] provides several components that work with Avro:\n+endif::product[]\n \n-* Deploy an https://github.com/Apicurio/apicurio-registry[Apicurio API/Schema Registry] instance\n-* Install the Avro converter from https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka's _libs_ directory or directly into a plug-in directory\n-* Use thes the following properties to configure Apache Connect instance\n+* An Avro converter that you can configure in Kafka Connect workers. This converter maps Kafka Connect schemas to Avro schemas. The converter then uses the Avro schemas to serialize the message keys and values into Avro's compact binary form.\n \n-[source]\n+* An API/Schema registry that tracks:\n++\n+** Avro schemas that are used in Kafka topics\n+** Where the Avro converter sends the generated Avro schemas\n+\n++\n+Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n+This makes each message even smaller. For an I/O bound system like Kafka, this means more total throughput of the producers and consumers.\n+\n+* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n+Any Kafka consumer applications that you write to consume change event records can use Avro Serdes to deserialize the change event records.\n+\n+To use the {registry} with {prodname}, you must add {registry} converters and their dependencies to the Kafka Connect container image that you are using for running {prodname}.\n+\n+[NOTE]\n+====\n+The {registry} project also provides a JSON converter. This converter combines the advantage of less verbose messages with human-readable JSON. Messages do not contain the schema information themselves, but only a schema ID.\n+====\n+\n+ifdef::community[]\n+Another option is using the Confluent schema registry, which is described later.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1NjM3NA==", "bodyText": "Can you add a sentence after the listing like this: Alternatively, you can also configure specific connectors to use the Avro converter.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442756374", "createdAt": "2020-06-19T10:16:36Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,92 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. \n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Including schemas causes the messages to  be very verbose. If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the message keys and values by using link:https://avro.apache.org/[Apache Avro].\n+The Avro binary format is compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure. Avro's schema evolution mechanism makes it possible to evolve the schemas over time, which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables. Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema. Avro serialization makes it easier for consumers to adapt to the changing schema.\n \n-[NOTE]\n+ifdef::product[]\n+[IMPORTANT]\n ====\n-The Apicurio project also provides a JSON converter that can be used with the Apicurio registry.\n-This combines the advantage of less verbose messages (as messages do not contain the schema information themselves, but only a schema id)\n-with human-readable JSON.\n+Using Avro to serialize message keys and values is a Technology Preview feature. Technology Preview features are not supported with Red Hat production service-level agreements (SLAs) and might not be functionally complete; therefore, Red Hat does not recommend implementing any Technology Preview features in production environments. This Technology Preview feature provides early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process. For more information about support scope, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].\n ====\n+endif::product[]\n \n-Another option is using the Confluent schema registry, which is described further below.\n+// Type: concept\n+// Title: About the {registry}\n+[id=\"about-the-registry\"]\n+== About the {registry-name-full}\n \n-== Technical Information\n+ifdef::community[]\n+The link:https://github.com/Apicurio/apicurio-registry[{registry}] open-source project provides several components that work with Avro:\n+endif::community[]\n \n-A system that wants to use Avro serialization needs to complete two steps:\n+ifdef::product[]\n+{LinkServiceRegistryGetStart}[{registry-name-full}] provides several components that work with Avro:\n+endif::product[]\n \n-* Deploy an https://github.com/Apicurio/apicurio-registry[Apicurio API/Schema Registry] instance\n-* Install the Avro converter from https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka's _libs_ directory or directly into a plug-in directory\n-* Use thes the following properties to configure Apache Connect instance\n+* An Avro converter that you can configure in Kafka Connect workers. This converter maps Kafka Connect schemas to Avro schemas. The converter then uses the Avro schemas to serialize the message keys and values into Avro's compact binary form.\n \n-[source]\n+* An API/Schema registry that tracks:\n++\n+** Avro schemas that are used in Kafka topics\n+** Where the Avro converter sends the generated Avro schemas\n+\n++\n+Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n+This makes each message even smaller. For an I/O bound system like Kafka, this means more total throughput of the producers and consumers.\n+\n+* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n+Any Kafka consumer applications that you write to consume change event records can use Avro Serdes to deserialize the change event records.\n+\n+To use the {registry} with {prodname}, you must add {registry} converters and their dependencies to the Kafka Connect container image that you are using for running {prodname}.\n+\n+[NOTE]\n+====\n+The {registry} project also provides a JSON converter. This converter combines the advantage of less verbose messages with human-readable JSON. Messages do not contain the schema information themselves, but only a schema ID.\n+====\n+\n+ifdef::community[]\n+Another option is using the Confluent schema registry, which is described later.\n+endif::community[]\n+\n+// Type: concept\n+// Title: Overview of deploying a {prodname} connector that uses Avro serialization\n+[id=\"overview-of-deploying-a-debezium-connector-that-uses-avro-serializatioin\"]\n+== Overview\n+\n+To deploy a {prodname} connector that uses Avro serialization, there are three main tasks: \n+\n+ifdef::community[]\n+. Deploy an link:https://github.com/Apicurio/apicurio-registry[{registry-name-full}] instance.\n+endif::community[]\n+ifdef::product[]\n+. Deploy a link:{LinkServiceRegistryGetStart}[{registry-name-full}] instance by following the instructions in {NameServiceRegistryGetStart}.\n+endif::product[]\n+\n+ifdef::community[]\n+. Install the Avro converter from link:https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka Connect's _libs_ directory or directly into a plug-in directory.\n+endif::community[]\n+ifdef::product[]\n+. Install the Avro converter by downloading the {prodname} link:https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?product=red.hat.integration&downloadType=distributions[Service Registry Kafka Connect] zip file and extracting it into the {prodname} connector's directory.\n+endif::product[]\n+\n+. Configure a Kafka Connect instance with the following property settings: ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1ODA0OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n          \n          \n            \n            In your environment, you might want to use a {prodname} pre-built container to deploy {prodname} connectors that use Avro serialization. Follow the procedure here to do that. In this procedure, you build a custom Kafka Connect container image for {prodname} which contains the Avro converter.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442758048", "createdAt": "2020-06-19T10:19:57Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1ODQwNw==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            . Build a {prodname} container image that implements the Avro converter:\n          \n          \n            \n            . Build a {prodname} container image that contains the Avro converter:", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442758407", "createdAt": "2020-06-19T10:20:48Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.\n++\n+.. Create a new {registry} deployment by specifying the in-memory persistence option in one of the example custom resources, for example:\n++\n+`oc create -f https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/example-cr/in-memory.yaml`\n++\n+The in-memory deployment is not suitable for production. Use the Kafka Streams persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].\n+endif::product[]\n+\n+. Build a {prodname} container image that implements the Avro converter:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 188}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1OTA1Mg==", "bodyText": "There's a variable defined for the version (in antora.yml in case of upstream docs): {apicurio-version}. Enable substitution for the listing in order to reference it.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442759052", "createdAt": "2020-06-19T10:22:00Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.\n++\n+.. Create a new {registry} deployment by specifying the in-memory persistence option in one of the example custom resources, for example:\n++\n+`oc create -f https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/example-cr/in-memory.yaml`\n++\n+The in-memory deployment is not suitable for production. Use the Kafka Streams persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].\n+endif::product[]\n+\n+. Build a {prodname} container image that implements the Avro converter:\n++\n+ifdef::community[]\n+.. Copy link:https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[`Dockerfile`] to a convenient location. This file has the following content: \n++\n+[listing,options=\"nowrap\"]\n+----\n+ARG DEBEZIUM_VERSION\n+FROM debezium/connect:$DEBEZIUM_VERSION\n+ENV KAFKA_CONNECT_DEBEZIUM_DIR=$KAFKA_CONNECT_PLUGINS_DIR/debezium-connector-mysql\n+ENV APICURIO_VERSION=1.1.2.Final", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1OTY5Ng==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            . Run a Kafka Connect image that is configured to use Avro:\n          \n          \n            \n            . Run the newly built Kafka Connect image, configuring it so it uses the Avro converter (or configure specific converters to do so):", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442759696", "createdAt": "2020-06-19T10:23:26Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.\n++\n+.. Create a new {registry} deployment by specifying the in-memory persistence option in one of the example custom resources, for example:\n++\n+`oc create -f https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/example-cr/in-memory.yaml`\n++\n+The in-memory deployment is not suitable for production. Use the Kafka Streams persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].\n+endif::product[]\n+\n+. Build a {prodname} container image that implements the Avro converter:\n++\n+ifdef::community[]\n+.. Copy link:https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[`Dockerfile`] to a convenient location. This file has the following content: \n++\n+[listing,options=\"nowrap\"]\n+----\n+ARG DEBEZIUM_VERSION\n+FROM debezium/connect:$DEBEZIUM_VERSION\n+ENV KAFKA_CONNECT_DEBEZIUM_DIR=$KAFKA_CONNECT_PLUGINS_DIR/debezium-connector-mysql\n+ENV APICURIO_VERSION=1.1.2.Final\n \n-Build a {prodname} image with Avro converter from https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[Dockerfile]:\n+RUN cd $KAFKA_CONNECT_DEBEZIUM_DIR &&\\\n+    curl https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/$APICURIO_VERSION/apicurio-registry-distro-connect-converter-$APICURIO_VERSION-converter.tar.gz | tar xzv\n+----\n \n-[source]\n-[subs=\"attributes\"]\n+.. Run the following command: \n++\n+[source,subs=\"attributes\"]\n ----\n docker build --build-arg DEBEZIUM_VERSION={debezium-docker-label} -t debezium/connect-apicurio:{debezium-docker-label} .\n ----\n-\n-Run a Kafka Connect image configured to use Avro:\n-\n-[source]\n-[subs=\"attributes\"]\n+endif::community[]\n+ifdef::product[]\n+.. Download the link:https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?product=red.hat.integration&downloadType=distributions[{registry} Kafka Connect] zip file. \n+.. Extract the content into the directory that contains the {prodname} connector that you are configuring to use Avro serialization. \n+.. Create a custom image for Kafka Connect. See link:{LinkCDCInstallOpenShift}[NameCDCInstallOpenShift] for an example of how to do this. Start with the `Dockerfile` in that example. Then add the {registry} converters to the connector directories. \n+endif::product[]\n+\n+. Run a Kafka Connect image that is configured to use Avro:", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 225}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2MzQ5Mg==", "bodyText": "Is this the right resource for downstream?", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442763492", "createdAt": "2020-06-19T10:32:19Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2MzYxOA==", "bodyText": "This surely should be a stable released version, not master?", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442763618", "createdAt": "2020-06-19T10:32:38Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2MzgzOQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            The in-memory deployment is not suitable for production. Use the Kafka Streams persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].\n          \n          \n            \n            The in-memory deployment is not suitable for production. Use the Apache Kafka persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442763839", "createdAt": "2020-06-19T10:33:04Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +108,81 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n+\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n+\n+In your environment, you might want to use a {prodname} pre-built container to deploy a {prodname} connector that uses Avro serializaion. Follow the procedure here to do that. In this procedure, you build a {prodname} image in which {prodname} uses the Avro converter.  \n+\n+.Prerequisites\n \n-== {prodname} Container Images\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+.Procedure\n \n-[source,subs=\"+attributes\"]\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Choose the OpenShift project in which you want to deploy the {prodname} connector. In the following command, `$NAMESPACE` represents your project.  \n+.. Deploy the latest published {registry} operator by running the following command:\n++\n+`curl -sSL https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/install.yaml | sed \"s/{NAMESPACE}/$NAMESPACE/g\" | oc apply -f -`\n++\n+This deploys the latest development version of the {registry} operator from the `master` branch. To deploy other versions, specify a different branch or tag, or edit the operator image reference in the file.\n++\n+.. Create a new {registry} deployment by specifying the in-memory persistence option in one of the example custom resources, for example:\n++\n+`oc create -f https://raw.githubusercontent.com/apicurio/apicurio-registry-operator/master/docs/resources/example-cr/in-memory.yaml`\n++\n+The in-memory deployment is not suitable for production. Use the Kafka Streams persistence option for production. For more information, see {LinkServiceRegistryGetStart}[NameServiceRegistryGetStart].", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc2NjE0MA==", "bodyText": "Ok, so that's good to show this connector-specific configuration here. Taking a step back, there's still some inconsistency overall. The situation is this:\n\nAvro can be enabled on the Kafka Connect worker level (as shown above via \"Configure a Kafka Connect instance with the following property settings\", and also then via the environment variables in the community variant)\nAvro can be enabled for a specific connector (as shown here for product, it's currently not shown for the community variant)\n\nMy general recommendation would be to only show the second approach, enabling it for a specific connector, and not mention the worker-level approach, as it's less flexible.", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r442766140", "createdAt": "2020-06-19T10:38:46Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -112,19 +202,90 @@ docker run -it --rm --name connect \\\n     -e CONNECT_VALUE_CONVERTER_APICURIO_REGISTRY_GLOBAL-ID=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy \\\n     -p 8083:8083 debezium/connect-apicurio:{debezium-docker-label}\n ----\n+endif::community[]\n+ifdef::product[]\n+.. Follow the steps in the link:{LinkCDCGettingStarted}#deploying-kafka-connect[example of deploying Kafka Connect] in {NameCDCGettingStarted}.\n+.. Open the `examples/kafka-connect/kafka-connect-s2i-single-node-kafka.yaml` file that you used to deploy Kafka Connect.\n++\n+Before you can create the connector instance,\n+you must first enable connector resources in the `KafkaConnectS2I` Custom Resource (CR).\n+\n+.. In the `metadata.annotations` section, enable Kafka Connect to use connector resources.\n++\n+.kafka-connect-s2i-single-node-kafka.yaml\n+[source,yaml,options=\"nowrap\"]\n+----\n+apiVersion: kafka.strimzi.io/v1beta1\n+kind: KafkaConnectS2I\n+metadata:\n+  name: my-connect-cluster\n+  annotations:\n+    strimzi.io/use-connector-resources: \"true\"\n+spec:\n+   ...\n+----\n \n+.. Apply the updated `kafka-connect-s2i-single-node-kafka.yaml` file to update the `KafkaConnectS2I` CR:\n++\n+`oc apply -f kafka-connect-s2i-single-node-kafka.yaml`\n+\n+.. In the Kafka Connect CR that defines the connector, add the properties that are required by the Avro converter. The CR looks like this:\n++\n+.inventory-connector.yaml\n+[source,yaml,options=\"nowrap\"]\n+----\n+  apiVersion: kafka.strimzi.io/v1beta1\n+  kind: KafkaConnector\n+  metadata:\n+    name: inventory-connector  \n+    labels:\n+      strimzi.io/cluster: my-connect-cluster\n+  spec:\n+    class: io.debezium.connector.mysql.MySqlConnector\n+    tasksMax: 1  \n+    config:  \n+      database.hostname: mysql  \n+      database.port: 3306\n+      database.user: debezium\n+      database.password: dbz\n+      database.server.id: 184054  \n+      database.server.name: dbserver1  \n+      database.whitelist: inventory  \n+      database.history.kafka.bootstrap.servers: my-cluster-kafka-bootstrap:9092  \n+      database.history.kafka.topic: schema-changes.inventory  \n+      key.converter: io.apicurio.registry.utils.converter.AvroConverter\n+      key.converter.apicurio.registry.url: http://apicurio:8080/api\n+      key.converter.apicurio.registry.global-id: io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n+      value.converter: io.apicurio.registry.utils.converter.AvroConverter\n+      value.converter.apicurio.registry.url: http://apicurio:8080/api\n+      value.converter.apicurio.registry.global-id: io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 303}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM1NjM2NTc4", "url": "https://github.com/debezium/debezium/pull/1622#pullrequestreview-435636578", "createdAt": "2020-06-23T10:09:12Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDowOToxM1rOGnigTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxMDowOToxM1rOGnigTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDExMjk3Mg==", "bodyText": "I don't think the tutorial link will work downstream, or would it? Rather link to the \"Getting Started\" documented for that?", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r444112972", "createdAt": "2020-06-23T10:09:13Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,94 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+{prodname} connectors work with the Kafka Connect framework to capture changes in databases and generate change event records. The Kafka Connect workers then apply any configured transformations to each of the messages generated by the connector, serialize each message key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[_converters_], and write each message into the correct Kafka topic.\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+You can specify converters in several ways: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+* In the Kafka Connect worker configuration. \n++\n+In this case, the same converters are used for all connectors that are deployed to that worker's cluster.\n \n-== The Apicurio API and Schema Registry\n+* For an individual connector. \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+Kafka Connect comes with a _JSON converter_ that serializes message keys and values into JSON documents. You can configure the JSON converter to include or exclude the message schema by specifying the `key.converter.schemas.enable` and `value.converter.schemas.enable` properties. The {prodname} {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 41}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM3NTQzMjgx", "url": "https://github.com/debezium/debezium/pull/1622#pullrequestreview-437543281", "createdAt": "2020-06-25T14:20:50Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNDoyMDo1MFrOGo8_pQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNDoyMDo1MFrOGo8_pQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTU5NTU1Nw==", "bodyText": "@TovaCohen, I somehow had missed this before (thanks to @jcechace for pointing it out), here you're adding new S2I docs. While we agreed to only fully transition to the custom containers approach for the next release docs, why adding more S2I-flavoured this time, which we only will have to modify next time?", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r445595557", "createdAt": "2020-06-25T14:20:50Z", "author": {"login": "gunnarmorling"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -112,26 +194,98 @@ docker run -it --rm --name connect \\\n     -e CONNECT_VALUE_CONVERTER_APICURIO_REGISTRY_GLOBAL-ID=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy \\\n     -p 8083:8083 debezium/connect-apicurio:{debezium-docker-label}\n ----\n+endif::community[]\n+ifdef::product[]\n+\n+.. Follow the steps in the link:{LinkCDCGettingStarted}#deploying-kafka-connect[example of deploying Kafka Connect] in {NameCDCGettingStarted}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 244}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM4MzMwODQ0", "url": "https://github.com/debezium/debezium/pull/1622#pullrequestreview-438330844", "createdAt": "2020-06-26T14:13:41Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNDoxMzo0MVrOGpif2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxNDoxMzo0MVrOGpif2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIxMDAxMQ==", "bodyText": "This doesn't render (yet?) into working link", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r446210011", "createdAt": "2020-06-26T14:13:41Z", "author": {"login": "jcechace"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -9,57 +13,94 @@\n \n toc::[]\n \n-{prodname} connectors are used with the Kafka Connect framework to capture changes in databases and generate change events.\n-The Kafka Connect workers then apply to each of the messages generated by the connector the transformations configured for the connector,\n-serialize each message key and value into a binary form using the configured https://kafka.apache.org/documentation/#connect_running[_converters_],\n-and finally write each messages into the correct Kafka topic.\n+ifdef::product[]\n+[IMPORTANT]\n+====\n+Using Avro to serialize record keys and values is a Technology Preview feature. Technology Preview features are not supported with Red Hat production service-level agreements (SLAs) and might not be functionally complete; therefore, Red Hat does not recommend implementing any Technology Preview features in production environments. This Technology Preview feature provides early access to upcoming product innovations, enabling you to test functionality and provide feedback during the development process. For more information about support scope, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].\n+====\n+endif::product[]\n \n-The converters can either be specified in the Kafka Connect worker configuration,\n-in which case the same converters are used for all connectors deployed to that worker's cluster.\n-Alternatively, they can be specified for an individual connector.\n-Kafka Connect comes with a _JSON converter_ that serializes the message keys and values into JSON documents.\n-The JSON converter can be configured to include or exclude the message schema using the (`key.converter.schemas.enable` and `value.converter.schemas.enable`) properties.\n-Our {link-prefix}:{link-tutorial}[tutorial] shows what the messages look like when both payload and schemas are included, but the schemas make the messages very verbose.\n-If you want your messages serialized with JSON, consider setting these properties to `false` to exclude the verbose schema information.\n+A {prodname} connector works in the Kafka Connect framework to capture each row-level change in a database by generating a change event record. For each change event record, the {prodname} connector does the following: \n \n-Alternatively, you can serialize the message keys and values using https://avro.apache.org/[Apache Avro].\n-The Avro binary format is extremely compact and efficient, and Avro schemas make it possible to ensure that the messages have the correct structure.\n-Avro's schema evolution mechanism makes it possible to evolve the schemas over time,\n-which is essential for {prodname} connectors that dynamically generate the message schemas to match the structure of the database tables.\n-Over time, the change events captured by {prodname} connectors and written by Kafka Connect into a topic may have different versions of the same schema,\n-and Avro serialization makes it far easier for consumers to adapt to the changing schema.\n+. Applies configured transformations\n+. Serializes the record key and value into a binary form by using the configured link:https://kafka.apache.org/documentation/#connect_running[Kafka Connect converters]\n+. Writes the record to the correct Kafka topic\n \n-== The Apicurio API and Schema Registry\n+You can specify converters for each individual {prodname} connector instance. Kafka Connect provides a JSON converter that serializes the record keys and values into JSON documents. The default behavior is that the JSON converter includes the record's message schema, which makes each record very verbose. The {link-prefix}:{link-tutorial}[{name-tutorial}] shows what the records look like when both payload and schemas are included. If you want records to be serialized with JSON, consider setting the following connector configuration properties to `false`: \n \n-The open-source project https://github.com/Apicurio/apicurio-registry[Apicurio Registry] provides several components that work with Avro:\n+* `key.converter.schemas.enable`\n+* `value.converter.schemas.enable`\n \n-* An Avro converter that can be used in Kafka Connect workers to map the Kafka Connect schemas into Avro schemas and to then use those Avro schemas to serialize the message keys and values into the very compact Avro binary form.\n-* An API/Schema registry that tracks all of the Avro schemas used in Kafka topics, and where the Avro Converter sends the generated Avro schemas.\n-Since the Avro schemas are stored in this registry, each message need only include a tiny _schema identifier_.\n-This makes each message even smaller, and for an I/O bound system like Kafka this means more total throughput of the producers and consumers.\n-* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers.\n-Any Kafka consumer applications you write to consume change events can use the Avro Serdes to deserialize the changes events.\n+Setting these properties to `false` excludes the verbose schema information from each record. \n \n-You can install them into any Kafka distribution and use them with Kafka Connect.\n+Alternatively, you can serialize the record keys and values by using https://avro.apache.org/[Apache Avro]. The Avro binary format is compact and efficient. Avro schemas make it possible to ensure that each record has the correct structure. Avro's schema evolution mechanism enables schemas to evolve. This is essential for {prodname} connectors, which dynamically generate each record's schema to match the structure of the database table that was changed. Over time, change event records written to the same Kafka topic might have different versions of the same schema. Avro serialization makes it easier for change event record consumers to adapt to a changing record schema.\n \n-[NOTE]\n-====\n-The Apicurio project also provides a JSON converter that can be used with the Apicurio registry.\n-This combines the advantage of less verbose messages (as messages do not contain the schema information themselves, but only a schema id)\n-with human-readable JSON.\n-====\n+ifdef::community[]\n+To use Apache Avro serialization, you must deploy a schema registry that manages Avro message schemas and their versions. \n+Available options include the {registry-name-full} as well as the Confluent Schema Registry. Both are described here.\n+endif::community[]\n \n-Another option is using the Confluent schema registry, which is described further below.\n+ifdef::product[]\n+To use Apache Avro serialization, you must deploy a schema registry that manages Avro message schemas and their versions. For information about setting up this registry, see the documentation for  {LinkServiceRegistryGetStart}[{registry-name-full}].\n+endif::product[]\n \n-== Technical Information\n+// Type: concept\n+// Title: About the {registry}\n+[id=\"about-the-registry\"]\n+== About the {registry-name-full}\n \n-A system that wants to use Avro serialization needs to complete two steps:\n+ifdef::community[]\n+The link:https://github.com/Apicurio/apicurio-registry[{registry}] open-source project provides several components that work with Avro:\n+endif::community[]\n \n-* Deploy an https://github.com/Apicurio/apicurio-registry[Apicurio API/Schema Registry] instance\n-* Install the Avro converter from https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/{apicurio-version}/apicurio-registry-distro-connect-converter-{apicurio-version}-converter.tar.gz[the installation package] into Kafka's _libs_ directory or directly into a plug-in directory\n-* Use thes the following properties to configure Apache Connect instance\n+ifdef::product[]\n+{LinkServiceRegistryGetStart}[{registry-name-full}] provides several components that work with Avro:\n+endif::product[]\n \n-[source]\n+* An Avro converter that you can specify in {prodname} connector configurations. This converter maps Kafka Connect schemas to Avro schemas. The converter then uses the Avro schemas to serialize the record keys and values into Avro's compact binary form.\n+\n+* An API and schema registry that tracks:\n++\n+** Avro schemas that are used in Kafka topics\n+** Where the Avro converter sends the generated Avro schemas\n+\n++\n+Since the Avro schemas are stored in this registry, each record needs to contain only a tiny _schema identifier_.\n+This makes each record even smaller. For an I/O bound system like Kafka, this means more total throughput for producers and consumers.\n+\n+* Avro _Serdes_ (serializers and deserializers) for Kafka producers and consumers. Kafka consumer applications that you write to consume change event records can use Avro Serdes to deserialize the change event records.\n+\n+To use the {registry} with {prodname}, add {registry} converters and their dependencies to the Kafka Connect container image that you are using for running a {prodname} connector.\n+\n+[NOTE]\n+====\n+The {registry} project also provides a JSON converter. This converter combines the advantage of less verbose messages with human-readable JSON. Messages do not contain the schema information themselves, but only a schema ID.\n+====\n+\n+// Type: concept\n+// Title: Overview of deploying a {prodname} connector that uses Avro serialization\n+[id=\"overview-of-deploying-a-debezium-connector-that-uses-avro-serialization\"]\n+== Deployment overview\n+\n+To deploy a {prodname} connector that uses Avro serialization, there are three main tasks: \n+\n+ifdef::community[]\n+. Deploy an link:https://github.com/Apicurio/apicurio-registry[{registry-name-full}] instance.\n+endif::community[]\n+ifdef::product[]\n+. Deploy a link:{LinkServiceRegistryGetStart}[{registry-name-full} instance by following the instructions in {NameServiceRegistryGetStart}].", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 126}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM5MDM2Mzg4", "url": "https://github.com/debezium/debezium/pull/1622#pullrequestreview-439036388", "createdAt": "2020-06-29T11:16:59Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMToxNjo1OVrOGqMH5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxMToxNjo1OVrOGqMH5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njg5MjAwNg==", "bodyText": "This section talks about Docker file, which means building a custom Kafka Connect image (which is correct), the issue comes in my next comment", "url": "https://github.com/debezium/debezium/pull/1622#discussion_r446892006", "createdAt": "2020-06-29T11:16:59Z", "author": {"login": "jcechace"}, "path": "documentation/modules/ROOT/pages/configuration/avro.adoc", "diffHunk": "@@ -69,30 +110,71 @@ value.converter.apicurio.registry.url=http://apicurio:8080/api\n value.converter.apicurio.registry.global-id=io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy\n ----\n \n-Note that Kafka Connect internally always uses the JSON key/value converters for storing configuration and offsets.\n+Internally, Kafka Connect always uses JSON key/value converters for storing configuration and offsets.\n \n-== {prodname} Container Images\n+// Type: procedure\n+// Title: Deploying connectors that use Avro in {prodname} containers\n+// ModuleID: deploying-connectors-that-use-avro-in-debezium-containers\n+== Deploying with {prodname} containers\n \n-Deploy an Apicurio Registry instance (this example uses a non-production in-memory instance):\n+In your environment, you might want to use a provided {prodname} container to deploy {prodname} connectors that use Avro serializaion. Follow the procedure here to do that. In this procedure, you build a custom Kafka Connect container image for {prodname}, which uses the Avro converter. \n \n-[source,subs=\"+attributes\"]\n+.Prerequisites\n+\n+* You have the required permissions on a Kafka cluster.\n+* You downloaded the {prodname} connector plug-in that you want to deploy with Avro serialization.\n+\n+.Procedure\n+\n+. Deploy an instance of {registry}. \n++\n+ifdef::community[]\n+The following example uses a non-production, in-memory, {registry}  instance:\n++\n+[source,subs=\"attributes+\"]\n ----\n docker run -it --rm --name apicurio \\\n     -p 8080:8080 apicurio/apicurio-registry-mem:{apicurio-version}\n ----\n+endif::community[]\n+ifdef::product[]\n+Follow the instructions in \n+{LinkServiceRegistryGetStart}#installing-registry-operatorhub[NameServiceRegistryGetStart, Installing Service Registry from the OpenShift OperatorHub].\n+endif::product[]\n+\n+. Build a {prodname} container image that contains the Avro converter:\n++\n+ifdef::community[]\n+.. Copy link:https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[`Dockerfile`] to a convenient location. This file has the following content: \n++\n+[listing,subs=\"attributes\",options=\"nowrap\"]\n+----\n+ARG DEBEZIUM_VERSION\n+FROM debezium/connect:$DEBEZIUM_VERSION\n+ENV KAFKA_CONNECT_DEBEZIUM_DIR=$KAFKA_CONNECT_PLUGINS_DIR/debezium-connector-mysql\n+ENV APICURIO_VERSION={apicurio-version}\n \n-Build a {prodname} image with Avro converter from https://github.com/debezium/debezium-examples/blob/master/tutorial/debezium-with-apicurio/Dockerfile[Dockerfile]:\n+RUN cd $KAFKA_CONNECT_DEBEZIUM_DIR &&\\\n+    curl https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/$APICURIO_VERSION/apicurio-registry-distro-connect-converter-$APICURIO_VERSION-converter.tar.gz | tar xzv\n+----\n \n-[source]\n-[subs=\"attributes\"]\n+.. Run the following command: \n++\n+[source,subs=\"attributes+\"]\n ----\n docker build --build-arg DEBEZIUM_VERSION={debezium-docker-label} -t debezium/connect-apicurio:{debezium-docker-label} .\n ----\n-\n-Run a Kafka Connect image configured to use Avro:\n-\n-[source]\n-[subs=\"attributes\"]\n+endif::community[]\n+ifdef::product[]\n+.. Download the link:https://access.redhat.com/jbossnetwork/restricted/listSoftware.html?product=red.hat.integration&downloadType=distributions[{registry} Kafka Connect] zip file. \n+.. Extract the content into the directory that contains the {prodname} connector that you are configuring to use Avro serialization. \n+.. Create a custom image for Kafka Connect. See link:{LinkCDCInstallOpenShift}[{NameCDCInstallOpenShift}, Creating a container image from the Kafka Connect base image] for an example of how to do this. Start with the `Dockerfile` in that example. Then add the {registry} converters to the connector directories. \n+endif::product[]", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 217}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d194e09a26f930a864521dc4a2487df771f026e0", "author": {"user": {"login": "TovaCohen", "name": null}}, "url": "https://github.com/debezium/debezium/commit/d194e09a26f930a864521dc4a2487df771f026e0", "committedDate": "2020-06-29T15:02:11Z", "message": "DBZ-1928 Initial edits of avro content"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9ff2ee51c56a19c4de8b6611b9d1b7e7dc6ac7ff", "author": {"user": {"login": "TovaCohen", "name": null}}, "url": "https://github.com/debezium/debezium/commit/9ff2ee51c56a19c4de8b6611b9d1b7e7dc6ac7ff", "committedDate": "2020-06-29T15:02:11Z", "message": "DBZ-1928 Updates based on SME comments and answers"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3ad905e48617a4998e24f902a9d8c927249b166c", "author": {"user": {"login": "TovaCohen", "name": null}}, "url": "https://github.com/debezium/debezium/commit/3ad905e48617a4998e24f902a9d8c927249b166c", "committedDate": "2020-06-29T15:02:11Z", "message": "DBZ-1928 Updated doc for Avro serialization"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "adaa956ea3f06d3d3bc6483b3b3a228e76ed97f1", "author": {"user": {"login": "TovaCohen", "name": null}}, "url": "https://github.com/debezium/debezium/commit/adaa956ea3f06d3d3bc6483b3b3a228e76ed97f1", "committedDate": "2020-06-29T15:02:11Z", "message": "DBZ-1928 Unindented 3 paragraphs after bullet. Corrected endif statement."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bd08bf6406f976f44e8087d2e20378e1d62d2d08", "author": {"user": {"login": "TovaCohen", "name": null}}, "url": "https://github.com/debezium/debezium/commit/bd08bf6406f976f44e8087d2e20378e1d62d2d08", "committedDate": "2020-06-29T15:02:11Z", "message": "DBZ-1928 Remove temporary \"community\" statement needed to check rendering"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ba9c15f80f379cc4c2f8b3ca658b8a5a799db592", "author": {"user": {"login": "TovaCohen", "name": null}}, "url": "https://github.com/debezium/debezium/commit/ba9c15f80f379cc4c2f8b3ca658b8a5a799db592", "committedDate": "2020-06-29T15:02:11Z", "message": "DBZ-1928 Updates based on Gunnar's review comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a36284f4ce75179e585f988d4703918beae827d", "author": {"user": {"login": "TovaCohen", "name": null}}, "url": "https://github.com/debezium/debezium/commit/1a36284f4ce75179e585f988d4703918beae827d", "committedDate": "2020-06-29T15:02:11Z", "message": "DBZ-1928 Missed this update from Gunnar's review"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c09798884b4b3075a43278e48f4c3b391edac32b", "author": {"user": {"login": "TovaCohen", "name": null}}, "url": "https://github.com/debezium/debezium/commit/c09798884b4b3075a43278e48f4c3b391edac32b", "committedDate": "2020-06-29T15:02:11Z", "message": "DBZ-1928 Replaced incorrect \"converter\" with \"connector\""}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6389c58e045dbef898a91284b6931921e86c641f", "author": {"user": {"login": "TovaCohen", "name": null}}, "url": "https://github.com/debezium/debezium/commit/6389c58e045dbef898a91284b6931921e86c641f", "committedDate": "2020-06-29T15:02:11Z", "message": "DBZ-1928 Removed \"also\"."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2b94dc533a800d30e0f0b24afe03833a185897a1", "author": {"user": {"login": "TovaCohen", "name": null}}, "url": "https://github.com/debezium/debezium/commit/2b94dc533a800d30e0f0b24afe03833a185897a1", "committedDate": "2020-06-29T15:02:11Z", "message": "DBZ-1928 use attribute for apicurio-version"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ba8ba7b3a8215b5f15ce9dc20426bf7eedfc8785", "author": {"user": {"login": "TovaCohen", "name": null}}, "url": "https://github.com/debezium/debezium/commit/ba8ba7b3a8215b5f15ce9dc20426bf7eedfc8785", "committedDate": "2020-06-29T15:02:11Z", "message": "DBZ-1928 Remove refs to workers. Add xref to downstream steps for installing Service Registry from OperatorHub."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9d010854d291638eba1d752f12a53e1dd1f94987", "author": {"user": {"login": "TovaCohen", "name": null}}, "url": "https://github.com/debezium/debezium/commit/9d010854d291638eba1d752f12a53e1dd1f94987", "committedDate": "2020-06-29T15:02:11Z", "message": "DBZ-1928 typo fix: Enclosed a downstream attribute in curly braces"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1200953de9c4fc7d4d1c8a7f0f36cfad72648855", "author": {"user": {"login": "TovaCohen", "name": null}}, "url": "https://github.com/debezium/debezium/commit/1200953de9c4fc7d4d1c8a7f0f36cfad72648855", "committedDate": "2020-06-29T17:23:07Z", "message": "DBZ-1928 Update downstream procedure for deploying connectors with Avro configuration"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "1200953de9c4fc7d4d1c8a7f0f36cfad72648855", "author": {"user": {"login": "TovaCohen", "name": null}}, "url": "https://github.com/debezium/debezium/commit/1200953de9c4fc7d4d1c8a7f0f36cfad72648855", "committedDate": "2020-06-29T17:23:07Z", "message": "DBZ-1928 Update downstream procedure for deploying connectors with Avro configuration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7ed7242afaff53447b0006bbb4778b7e1c3de4d6", "author": {"user": {"login": "TovaCohen", "name": null}}, "url": "https://github.com/debezium/debezium/commit/7ed7242afaff53447b0006bbb4778b7e1c3de4d6", "committedDate": "2020-06-30T13:37:02Z", "message": "DBZ-1928 Some tweaks to fine tune the content"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQwMDY1OTAy", "url": "https://github.com/debezium/debezium/pull/1622#pullrequestreview-440065902", "createdAt": "2020-06-30T14:21:03Z", "commit": {"oid": "7ed7242afaff53447b0006bbb4778b7e1c3de4d6"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2616, "cost": 1, "resetAt": "2021-11-01T13:51:04Z"}}}