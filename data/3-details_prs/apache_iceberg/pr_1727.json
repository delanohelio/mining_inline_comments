{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE2MjczNTcy", "number": 1727, "title": "[python] Adding parquet package and the classes for reading parquet", "bodyText": "This adds a parquet reader class that leverages pyarrow.datasets to read parquet files and apply row-level filters.  Additionally, there are some helper classes to convert between arrow schemas and iceberg schemas, so that we can apply any necessary schema evolution.\ncc @danielcweeks @rymurr", "createdAt": "2020-11-05T19:01:17Z", "url": "https://github.com/apache/iceberg/pull/1727", "merged": true, "mergeCommit": {"oid": "e8ec3a3f42b36fa3fdb64094cf5b007614441e28"}, "closed": true, "closedAt": "2020-12-05T00:19:04Z", "author": {"login": "TGooch44"}, "timelineItems": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdZnLf-ABqjM5NjM5NDc5Mzg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdjAKMRgBqjQwNzQ4MzYxMzM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c6d9a269229ebed5e52060a4ba09d56393617a4a", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/c6d9a269229ebed5e52060a4ba09d56393617a4a", "committedDate": "2020-11-05T18:56:33Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}, "afterCommit": {"oid": "5976811c354be139b875a51ca7c8bad1ab5225f8", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/5976811c354be139b875a51ca7c8bad1ab5225f8", "committedDate": "2020-11-05T19:04:05Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5976811c354be139b875a51ca7c8bad1ab5225f8", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/5976811c354be139b875a51ca7c8bad1ab5225f8", "committedDate": "2020-11-05T19:04:05Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}, "afterCommit": {"oid": "7c6864f20018e7f513ddc4942fe3de43845cb40e", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/7c6864f20018e7f513ddc4942fe3de43845cb40e", "committedDate": "2020-11-05T19:06:13Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7c6864f20018e7f513ddc4942fe3de43845cb40e", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/7c6864f20018e7f513ddc4942fe3de43845cb40e", "committedDate": "2020-11-05T19:06:13Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}, "afterCommit": {"oid": "b88e778a3ad70c53474bac4e8771e2402e88002f", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/b88e778a3ad70c53474bac4e8771e2402e88002f", "committedDate": "2020-11-05T19:21:53Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b88e778a3ad70c53474bac4e8771e2402e88002f", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/b88e778a3ad70c53474bac4e8771e2402e88002f", "committedDate": "2020-11-05T19:21:53Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}, "afterCommit": {"oid": "1eda220360d954ee9c67c72a9d17b276d7588191", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/1eda220360d954ee9c67c72a9d17b276d7588191", "committedDate": "2020-11-06T00:28:07Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1eda220360d954ee9c67c72a9d17b276d7588191", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/1eda220360d954ee9c67c72a9d17b276d7588191", "committedDate": "2020-11-06T00:28:07Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}, "afterCommit": {"oid": "00cdc50c58a68a5b2467ff99437cdb126f768fbb", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/00cdc50c58a68a5b2467ff99437cdb126f768fbb", "committedDate": "2020-11-09T19:04:25Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2MjgxODM2", "url": "https://github.com/apache/iceberg/pull/1727#pullrequestreview-526281836", "createdAt": "2020-11-09T13:58:54Z", "commit": {"oid": "1eda220360d954ee9c67c72a9d17b276d7588191"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxMzo1ODo1NFrOHvwHXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxMjozMTo0MlrOHwaZmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTgzMzQzNg==", "bodyText": "just curious why we have get_dataset_filter and get_expr. Maybe it will become clear as I review the rest but from here it seems redundnat", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r519833436", "createdAt": "2020-11-09T13:58:54Z", "author": {"login": "rymurr"}, "path": "python/iceberg/parquet/dataset_utils.py", "diffHunk": "@@ -0,0 +1,177 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\n+from iceberg.api.expressions import Expression, Operation, Predicate\n+import pyarrow.dataset as ds\n+\n+\n+def get_dataset_filter(expr: Expression, expected_to_file_map: dict) -> ds.Expression:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1eda220360d954ee9c67c72a9d17b276d7588191"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTg0MTQyMQ==", "bodyText": "probably want this to be _logger.debug", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r519841421", "createdAt": "2020-11-09T14:10:57Z", "author": {"login": "rymurr"}, "path": "python/iceberg/core/util/profile.py", "diffHunk": "@@ -0,0 +1,36 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\n+from contextlib import contextmanager\n+import logging\n+import time\n+\n+_logger = logging.getLogger(__name__)\n+\n+\n+@contextmanager\n+def profile(label, stats_dict=None):\n+    if stats_dict is None:\n+        print('PROFILE: %s starting' % label)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1eda220360d954ee9c67c72a9d17b276d7588191"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyMTA3MQ==", "bodyText": "Is there any way to do this w/o first going to pandas? Pandas is the most expensive part of this operation and if we could avoid it (say get tuples direct from pyarrow) performance would significantly improve", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r520521071", "createdAt": "2020-11-10T12:22:47Z", "author": {"login": "rymurr"}, "path": "python/iceberg/parquet/parquet_reader.py", "diffHunk": "@@ -0,0 +1,240 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\n+from datetime import datetime\n+import decimal\n+import logging\n+from typing import Any, Callable, Dict, List, Tuple, Union\n+\n+from iceberg.api import Schema\n+from iceberg.api.expressions import Expression\n+from iceberg.api.io import InputFile\n+from iceberg.api.types import NestedField, Type, TypeID\n+from iceberg.core.util.profile import profile\n+from iceberg.exceptions import InvalidCastException\n+import numpy as np\n+import pandas as pd\n+import pyarrow as pa\n+from pyarrow import fs\n+import pyarrow.dataset as ds\n+import pyarrow.parquet as pq\n+\n+from .dataset_utils import get_dataset_filter\n+from .parquet_schema_utils import prune_columns\n+from .parquet_to_iceberg import convert_parquet_to_iceberg\n+\n+_logger = logging.getLogger(__name__)\n+\n+DTYPE_MAP: Dict[TypeID,\n+                Callable[[NestedField], Tuple[pa.Field, Any]]] = \\\n+    {TypeID.BINARY: lambda field: pa.binary(),\n+     TypeID.BOOLEAN: lambda field: (pa.bool_(), False),\n+     TypeID.DATE: lambda field: (pa.date32(), datetime.now()),\n+     TypeID.DECIMAL: lambda field: (pa.decimal128(field.type.precision, field.type.scale),\n+                                    decimal.Decimal()),\n+     TypeID.DOUBLE: lambda field: (pa.float64(), np.nan),\n+     TypeID.FIXED: lambda field: pa.binary(field.length),\n+     TypeID.FLOAT: lambda field: (pa.float32(), np.nan),\n+     TypeID.INTEGER: lambda field: (pa.int32(), np.nan),\n+     TypeID.LIST: lambda field: (pa.list_(pa.field(\"element\",\n+                                                   DTYPE_MAP[field.type.element_type.type_id](field.type)[0])),\n+                                 None),\n+     TypeID.LONG: lambda field: (pa.int64(), np.nan),\n+     # To-Do: update to support reading map fields\n+     # TypeID.MAP: lambda field: (,),\n+     TypeID.STRING: lambda field: (pa.string(), \"\"),\n+     TypeID.STRUCT: lambda field: (pa.struct([(nested_field.name,\n+                                               DTYPE_MAP[nested_field.type.type_id](nested_field.type)[0])\n+                                              for nested_field in field.type.fields]), {}),\n+     TypeID.TIMESTAMP: lambda field: (pa.timestamp(\"us\"), datetime.now()),\n+     # not used in SPARK, so not implementing for now\n+     # TypeID.TIME: pa.time64(None)\n+     }\n+\n+\n+class ParquetReader(object):\n+\n+    def __init__(self, input: InputFile, expected_schema: Schema, options, filter_expr: Expression,\n+                 case_sensitive: bool, start: int = None, end: int = None):\n+        self._stats: Dict[str, int] = dict()\n+\n+        self._input = input\n+        self._input_fo = input.new_fo()\n+\n+        self._arrow_file = pq.ParquetFile(self._input_fo)\n+        self._file_schema = convert_parquet_to_iceberg(self._arrow_file)\n+        self._expected_schema = expected_schema\n+        self._file_to_expected_name_map = ParquetReader.get_field_map(self._file_schema,\n+                                                                      self._expected_schema)\n+        self._options = options\n+        self._filter = get_dataset_filter(filter_expr, ParquetReader.get_reverse_field_map(self._file_schema,\n+                                                                                           self._expected_schema))\n+\n+        self._case_sensitive = case_sensitive\n+        if start is not None or end is not None:\n+            raise NotImplementedError(\"Partial file reads are not yet supported\")\n+            # self.start = start\n+            # self.end = end\n+\n+        self.materialized_table = False\n+        self.curr_iterator = None\n+        self._table = None\n+        self._df = None\n+\n+        _logger.debug(\"Reader initialized for %s\" % self._input.path)\n+\n+    @property\n+    def stats(self) -> dict:\n+        return dict(self._stats)\n+\n+    def to_pandas(self) -> Union[pd.Series, pd.DataFrame]:\n+        if not self.materialized_table:\n+            self._read_data()\n+            with profile(\"to_pandas\", self._stats):\n+                if self._table is not None:\n+                    self._df = self._table.to_pandas(use_threads=True)\n+                else:\n+                    self._df = None\n+\n+        return self._df\n+\n+    def to_arrow_table(self) -> pa.Table:\n+        if not self.materialized_table:\n+            self._read_data()\n+\n+        return self._table\n+\n+    def _read_data(self) -> None:\n+        _logger.debug(\"Starting data read\")\n+\n+        # only scan the columns projected and in our file\n+        cols_to_read = prune_columns(self._file_schema, self._expected_schema)\n+\n+        with profile(\"read data\", self._stats):\n+            arrow_dataset = ds.FileSystemDataset.from_paths([self._input.location()],\n+                                                            schema=self._arrow_file.schema_arrow,\n+                                                            format=ds.ParquetFileFormat(),\n+                                                            filesystem=fs.LocalFileSystem())\n+\n+            arrow_table = arrow_dataset.to_table(columns=cols_to_read, filter=self._filter)\n+\n+        # process schema evolution if needed\n+        with profile(\"schema_evol_proc\", self._stats):\n+            processed_tbl = self.migrate_schema(arrow_table)\n+            for i, field in self.get_missing_fields():\n+                dtype_func = DTYPE_MAP.get(field.type.type_id)\n+                if dtype_func is None:\n+                    raise RuntimeError(\"Unable to create null column for type %s\" % field.type.type_id)\n+\n+                dtype = dtype_func(field)\n+                processed_tbl = (processed_tbl.add_column(i,\n+                                                          pa.field(field.name, dtype[0], True, None),\n+                                                          ParquetReader.create_null_column(processed_tbl[0],\n+                                                                                           dtype)))\n+        self._table = processed_tbl\n+        self.materialized_table = True\n+\n+    def __iter__(self):\n+        return self\n+\n+    def __next__(self):\n+        if self.curr_iterator is None:\n+            if not self.materialized_table:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00cdc50c58a68a5b2467ff99437cdb126f768fbb"}, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNTIyNA==", "bodyText": "Its a shame that we can't do this for a batch of files. Seems like a common pattern that pyarrow should be able to handle. Have you asked on user@arrow.apache.org if there is a plan to support this?\nDo you have a sense of the performance hit of reading files individually as opposed to in one batch? I am worried it could be significant.\nFinally, what is the path for reading/iterating multi-file iceberg tables? Do we read 1 parrquet file at a time or all at once or all at once as a pandas df? One of the key features of Datasets (for me) was being able to control memory when doing large reads.", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r520525224", "createdAt": "2020-11-10T12:30:01Z", "author": {"login": "rymurr"}, "path": "python/iceberg/parquet/parquet_reader.py", "diffHunk": "@@ -0,0 +1,240 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\n+from datetime import datetime\n+import decimal\n+import logging\n+from typing import Any, Callable, Dict, List, Tuple, Union\n+\n+from iceberg.api import Schema\n+from iceberg.api.expressions import Expression\n+from iceberg.api.io import InputFile\n+from iceberg.api.types import NestedField, Type, TypeID\n+from iceberg.core.util.profile import profile\n+from iceberg.exceptions import InvalidCastException\n+import numpy as np\n+import pandas as pd\n+import pyarrow as pa\n+from pyarrow import fs\n+import pyarrow.dataset as ds\n+import pyarrow.parquet as pq\n+\n+from .dataset_utils import get_dataset_filter\n+from .parquet_schema_utils import prune_columns\n+from .parquet_to_iceberg import convert_parquet_to_iceberg\n+\n+_logger = logging.getLogger(__name__)\n+\n+DTYPE_MAP: Dict[TypeID,\n+                Callable[[NestedField], Tuple[pa.Field, Any]]] = \\\n+    {TypeID.BINARY: lambda field: pa.binary(),\n+     TypeID.BOOLEAN: lambda field: (pa.bool_(), False),\n+     TypeID.DATE: lambda field: (pa.date32(), datetime.now()),\n+     TypeID.DECIMAL: lambda field: (pa.decimal128(field.type.precision, field.type.scale),\n+                                    decimal.Decimal()),\n+     TypeID.DOUBLE: lambda field: (pa.float64(), np.nan),\n+     TypeID.FIXED: lambda field: pa.binary(field.length),\n+     TypeID.FLOAT: lambda field: (pa.float32(), np.nan),\n+     TypeID.INTEGER: lambda field: (pa.int32(), np.nan),\n+     TypeID.LIST: lambda field: (pa.list_(pa.field(\"element\",\n+                                                   DTYPE_MAP[field.type.element_type.type_id](field.type)[0])),\n+                                 None),\n+     TypeID.LONG: lambda field: (pa.int64(), np.nan),\n+     # To-Do: update to support reading map fields\n+     # TypeID.MAP: lambda field: (,),\n+     TypeID.STRING: lambda field: (pa.string(), \"\"),\n+     TypeID.STRUCT: lambda field: (pa.struct([(nested_field.name,\n+                                               DTYPE_MAP[nested_field.type.type_id](nested_field.type)[0])\n+                                              for nested_field in field.type.fields]), {}),\n+     TypeID.TIMESTAMP: lambda field: (pa.timestamp(\"us\"), datetime.now()),\n+     # not used in SPARK, so not implementing for now\n+     # TypeID.TIME: pa.time64(None)\n+     }\n+\n+\n+class ParquetReader(object):\n+\n+    def __init__(self, input: InputFile, expected_schema: Schema, options, filter_expr: Expression,\n+                 case_sensitive: bool, start: int = None, end: int = None):\n+        self._stats: Dict[str, int] = dict()\n+\n+        self._input = input\n+        self._input_fo = input.new_fo()\n+\n+        self._arrow_file = pq.ParquetFile(self._input_fo)\n+        self._file_schema = convert_parquet_to_iceberg(self._arrow_file)\n+        self._expected_schema = expected_schema\n+        self._file_to_expected_name_map = ParquetReader.get_field_map(self._file_schema,\n+                                                                      self._expected_schema)\n+        self._options = options\n+        self._filter = get_dataset_filter(filter_expr, ParquetReader.get_reverse_field_map(self._file_schema,\n+                                                                                           self._expected_schema))\n+\n+        self._case_sensitive = case_sensitive\n+        if start is not None or end is not None:\n+            raise NotImplementedError(\"Partial file reads are not yet supported\")\n+            # self.start = start\n+            # self.end = end\n+\n+        self.materialized_table = False\n+        self.curr_iterator = None\n+        self._table = None\n+        self._df = None\n+\n+        _logger.debug(\"Reader initialized for %s\" % self._input.path)\n+\n+    @property\n+    def stats(self) -> dict:\n+        return dict(self._stats)\n+\n+    def to_pandas(self) -> Union[pd.Series, pd.DataFrame]:\n+        if not self.materialized_table:\n+            self._read_data()\n+            with profile(\"to_pandas\", self._stats):\n+                if self._table is not None:\n+                    self._df = self._table.to_pandas(use_threads=True)\n+                else:\n+                    self._df = None\n+\n+        return self._df\n+\n+    def to_arrow_table(self) -> pa.Table:\n+        if not self.materialized_table:\n+            self._read_data()\n+\n+        return self._table\n+\n+    def _read_data(self) -> None:\n+        _logger.debug(\"Starting data read\")\n+\n+        # only scan the columns projected and in our file\n+        cols_to_read = prune_columns(self._file_schema, self._expected_schema)\n+\n+        with profile(\"read data\", self._stats):\n+            arrow_dataset = ds.FileSystemDataset.from_paths([self._input.location()],", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00cdc50c58a68a5b2467ff99437cdb126f768fbb"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNTYwMA==", "bodyText": "Is LocalFileSystem correct here? It caught my eye and wasn't sure how we would read eg from S3 or hadoop", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r520525600", "createdAt": "2020-11-10T12:30:38Z", "author": {"login": "rymurr"}, "path": "python/iceberg/parquet/parquet_reader.py", "diffHunk": "@@ -0,0 +1,240 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\n+from datetime import datetime\n+import decimal\n+import logging\n+from typing import Any, Callable, Dict, List, Tuple, Union\n+\n+from iceberg.api import Schema\n+from iceberg.api.expressions import Expression\n+from iceberg.api.io import InputFile\n+from iceberg.api.types import NestedField, Type, TypeID\n+from iceberg.core.util.profile import profile\n+from iceberg.exceptions import InvalidCastException\n+import numpy as np\n+import pandas as pd\n+import pyarrow as pa\n+from pyarrow import fs\n+import pyarrow.dataset as ds\n+import pyarrow.parquet as pq\n+\n+from .dataset_utils import get_dataset_filter\n+from .parquet_schema_utils import prune_columns\n+from .parquet_to_iceberg import convert_parquet_to_iceberg\n+\n+_logger = logging.getLogger(__name__)\n+\n+DTYPE_MAP: Dict[TypeID,\n+                Callable[[NestedField], Tuple[pa.Field, Any]]] = \\\n+    {TypeID.BINARY: lambda field: pa.binary(),\n+     TypeID.BOOLEAN: lambda field: (pa.bool_(), False),\n+     TypeID.DATE: lambda field: (pa.date32(), datetime.now()),\n+     TypeID.DECIMAL: lambda field: (pa.decimal128(field.type.precision, field.type.scale),\n+                                    decimal.Decimal()),\n+     TypeID.DOUBLE: lambda field: (pa.float64(), np.nan),\n+     TypeID.FIXED: lambda field: pa.binary(field.length),\n+     TypeID.FLOAT: lambda field: (pa.float32(), np.nan),\n+     TypeID.INTEGER: lambda field: (pa.int32(), np.nan),\n+     TypeID.LIST: lambda field: (pa.list_(pa.field(\"element\",\n+                                                   DTYPE_MAP[field.type.element_type.type_id](field.type)[0])),\n+                                 None),\n+     TypeID.LONG: lambda field: (pa.int64(), np.nan),\n+     # To-Do: update to support reading map fields\n+     # TypeID.MAP: lambda field: (,),\n+     TypeID.STRING: lambda field: (pa.string(), \"\"),\n+     TypeID.STRUCT: lambda field: (pa.struct([(nested_field.name,\n+                                               DTYPE_MAP[nested_field.type.type_id](nested_field.type)[0])\n+                                              for nested_field in field.type.fields]), {}),\n+     TypeID.TIMESTAMP: lambda field: (pa.timestamp(\"us\"), datetime.now()),\n+     # not used in SPARK, so not implementing for now\n+     # TypeID.TIME: pa.time64(None)\n+     }\n+\n+\n+class ParquetReader(object):\n+\n+    def __init__(self, input: InputFile, expected_schema: Schema, options, filter_expr: Expression,\n+                 case_sensitive: bool, start: int = None, end: int = None):\n+        self._stats: Dict[str, int] = dict()\n+\n+        self._input = input\n+        self._input_fo = input.new_fo()\n+\n+        self._arrow_file = pq.ParquetFile(self._input_fo)\n+        self._file_schema = convert_parquet_to_iceberg(self._arrow_file)\n+        self._expected_schema = expected_schema\n+        self._file_to_expected_name_map = ParquetReader.get_field_map(self._file_schema,\n+                                                                      self._expected_schema)\n+        self._options = options\n+        self._filter = get_dataset_filter(filter_expr, ParquetReader.get_reverse_field_map(self._file_schema,\n+                                                                                           self._expected_schema))\n+\n+        self._case_sensitive = case_sensitive\n+        if start is not None or end is not None:\n+            raise NotImplementedError(\"Partial file reads are not yet supported\")\n+            # self.start = start\n+            # self.end = end\n+\n+        self.materialized_table = False\n+        self.curr_iterator = None\n+        self._table = None\n+        self._df = None\n+\n+        _logger.debug(\"Reader initialized for %s\" % self._input.path)\n+\n+    @property\n+    def stats(self) -> dict:\n+        return dict(self._stats)\n+\n+    def to_pandas(self) -> Union[pd.Series, pd.DataFrame]:\n+        if not self.materialized_table:\n+            self._read_data()\n+            with profile(\"to_pandas\", self._stats):\n+                if self._table is not None:\n+                    self._df = self._table.to_pandas(use_threads=True)\n+                else:\n+                    self._df = None\n+\n+        return self._df\n+\n+    def to_arrow_table(self) -> pa.Table:\n+        if not self.materialized_table:\n+            self._read_data()\n+\n+        return self._table\n+\n+    def _read_data(self) -> None:\n+        _logger.debug(\"Starting data read\")\n+\n+        # only scan the columns projected and in our file\n+        cols_to_read = prune_columns(self._file_schema, self._expected_schema)\n+\n+        with profile(\"read data\", self._stats):\n+            arrow_dataset = ds.FileSystemDataset.from_paths([self._input.location()],\n+                                                            schema=self._arrow_file.schema_arrow,\n+                                                            format=ds.ParquetFileFormat(),\n+                                                            filesystem=fs.LocalFileSystem())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00cdc50c58a68a5b2467ff99437cdb126f768fbb"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNjIzMw==", "bodyText": "why not pyarrow 2.0.0?", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r520526233", "createdAt": "2020-11-10T12:31:42Z", "author": {"login": "rymurr"}, "path": "python/setup.py", "diffHunk": "@@ -38,7 +37,7 @@\n                       'requests',\n                       'retrying',\n                       'pandas',\n-                      'pyarrow'\n+                      'pyarrow>=0.17.0'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "00cdc50c58a68a5b2467ff99437cdb126f768fbb"}, "originalPosition": 13}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "00cdc50c58a68a5b2467ff99437cdb126f768fbb", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/00cdc50c58a68a5b2467ff99437cdb126f768fbb", "committedDate": "2020-11-09T19:04:25Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}, "afterCommit": {"oid": "c2cd2e277c1d5ff2b6b18581ff1b511673cc0613", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/c2cd2e277c1d5ff2b6b18581ff1b511673cc0613", "committedDate": "2020-11-16T16:50:23Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c2cd2e277c1d5ff2b6b18581ff1b511673cc0613", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/c2cd2e277c1d5ff2b6b18581ff1b511673cc0613", "committedDate": "2020-11-16T16:50:23Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}, "afterCommit": {"oid": "0eca8f40cff2669627263ad38a93224e8e989058", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/0eca8f40cff2669627263ad38a93224e8e989058", "committedDate": "2020-11-16T16:51:51Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM2NTc0NTY2", "url": "https://github.com/apache/iceberg/pull/1727#pullrequestreview-536574566", "createdAt": "2020-11-23T15:21:03Z", "commit": {"oid": "0eca8f40cff2669627263ad38a93224e8e989058"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxNToyMTowM1rOH4SNTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxNToyMzo1NlrOH4SVsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc4MDYyMw==", "bodyText": "I think the import for the S3 filesystem should be down here right?", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r528780623", "createdAt": "2020-11-23T15:21:03Z", "author": {"login": "rymurr"}, "path": "python/iceberg/parquet/parquet_reader.py", "diffHunk": "@@ -0,0 +1,264 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\n+from collections import namedtuple\n+from datetime import datetime\n+import decimal\n+import logging\n+import typing\n+\n+from iceberg.api import Schema\n+from iceberg.api.expressions import Expression\n+from iceberg.api.io import InputFile\n+from iceberg.api.types import NestedField, Type, TypeID\n+from iceberg.core.filesystem import FileSystem, LocalFileSystem, S3FileSystem\n+from iceberg.core.util.profile import profile\n+from iceberg.exceptions import FileSystemNotFound, InvalidCastException\n+import numpy as np\n+import pandas as pd\n+import pyarrow as pa\n+from pyarrow import fs\n+import pyarrow.dataset as ds\n+import pyarrow.parquet as pq\n+\n+from .dataset_utils import get_dataset_filter\n+from .parquet_schema_utils import prune_columns\n+from .parquet_to_iceberg import convert_parquet_to_iceberg\n+\n+_logger = logging.getLogger(__name__)\n+\n+DTYPE_MAP: typing.Dict[TypeID,\n+                       typing.Callable[[NestedField], typing.Tuple[pa.Field, typing.Any]]] = \\\n+    {TypeID.BINARY: lambda field: pa.binary(),\n+     TypeID.BOOLEAN: lambda field: (pa.bool_(), False),\n+     TypeID.DATE: lambda field: (pa.date32(), datetime.now()),\n+     TypeID.DECIMAL: lambda field: (pa.decimal128(field.type.precision, field.type.scale),\n+                                    decimal.Decimal()),\n+     TypeID.DOUBLE: lambda field: (pa.float64(), np.nan),\n+     TypeID.FIXED: lambda field: pa.binary(field.length),\n+     TypeID.FLOAT: lambda field: (pa.float32(), np.nan),\n+     TypeID.INTEGER: lambda field: (pa.int32(), np.nan),\n+     TypeID.LIST: lambda field: (pa.list_(pa.field(\"element\",\n+                                                   DTYPE_MAP[field.type.element_type.type_id](field.type)[0])),\n+                                 None),\n+     TypeID.LONG: lambda field: (pa.int64(), np.nan),\n+     # To-Do: update to support reading map fields\n+     # TypeID.MAP: lambda field: (,),\n+     TypeID.STRING: lambda field: (pa.string(), \"\"),\n+     TypeID.STRUCT: lambda field: (pa.struct([(nested_field.name,\n+                                               DTYPE_MAP[nested_field.type.type_id](nested_field.type)[0])\n+                                              for nested_field in field.type.fields]), {}),\n+     TypeID.TIMESTAMP: lambda field: (pa.timestamp(\"us\"), datetime.now()),\n+     # not used in SPARK, so not implementing for now\n+     # TypeID.TIME: pa.time64(None)\n+     }\n+\n+FS_MAP: typing.Dict[typing.Type[FileSystem], typing.Type[fs.FileSystem]] = {LocalFileSystem: fs.LocalFileSystem}\n+\n+try:\n+    FS_MAP[S3FileSystem] = fs.S3FileSystem", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0eca8f40cff2669627263ad38a93224e8e989058"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc4Mjc3MA==", "bodyText": "were you still going to remove the pandas and iterator stuff? I was kinda thinking that we would have a class 1 level higher that would read across all tables and pandas would be accessed there", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r528782770", "createdAt": "2020-11-23T15:23:56Z", "author": {"login": "rymurr"}, "path": "python/iceberg/parquet/parquet_reader.py", "diffHunk": "@@ -0,0 +1,264 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\n+from collections import namedtuple\n+from datetime import datetime\n+import decimal\n+import logging\n+import typing\n+\n+from iceberg.api import Schema\n+from iceberg.api.expressions import Expression\n+from iceberg.api.io import InputFile\n+from iceberg.api.types import NestedField, Type, TypeID\n+from iceberg.core.filesystem import FileSystem, LocalFileSystem, S3FileSystem\n+from iceberg.core.util.profile import profile\n+from iceberg.exceptions import FileSystemNotFound, InvalidCastException\n+import numpy as np\n+import pandas as pd\n+import pyarrow as pa\n+from pyarrow import fs\n+import pyarrow.dataset as ds\n+import pyarrow.parquet as pq\n+\n+from .dataset_utils import get_dataset_filter\n+from .parquet_schema_utils import prune_columns\n+from .parquet_to_iceberg import convert_parquet_to_iceberg\n+\n+_logger = logging.getLogger(__name__)\n+\n+DTYPE_MAP: typing.Dict[TypeID,\n+                       typing.Callable[[NestedField], typing.Tuple[pa.Field, typing.Any]]] = \\\n+    {TypeID.BINARY: lambda field: pa.binary(),\n+     TypeID.BOOLEAN: lambda field: (pa.bool_(), False),\n+     TypeID.DATE: lambda field: (pa.date32(), datetime.now()),\n+     TypeID.DECIMAL: lambda field: (pa.decimal128(field.type.precision, field.type.scale),\n+                                    decimal.Decimal()),\n+     TypeID.DOUBLE: lambda field: (pa.float64(), np.nan),\n+     TypeID.FIXED: lambda field: pa.binary(field.length),\n+     TypeID.FLOAT: lambda field: (pa.float32(), np.nan),\n+     TypeID.INTEGER: lambda field: (pa.int32(), np.nan),\n+     TypeID.LIST: lambda field: (pa.list_(pa.field(\"element\",\n+                                                   DTYPE_MAP[field.type.element_type.type_id](field.type)[0])),\n+                                 None),\n+     TypeID.LONG: lambda field: (pa.int64(), np.nan),\n+     # To-Do: update to support reading map fields\n+     # TypeID.MAP: lambda field: (,),\n+     TypeID.STRING: lambda field: (pa.string(), \"\"),\n+     TypeID.STRUCT: lambda field: (pa.struct([(nested_field.name,\n+                                               DTYPE_MAP[nested_field.type.type_id](nested_field.type)[0])\n+                                              for nested_field in field.type.fields]), {}),\n+     TypeID.TIMESTAMP: lambda field: (pa.timestamp(\"us\"), datetime.now()),\n+     # not used in SPARK, so not implementing for now\n+     # TypeID.TIME: pa.time64(None)\n+     }\n+\n+FS_MAP: typing.Dict[typing.Type[FileSystem], typing.Type[fs.FileSystem]] = {LocalFileSystem: fs.LocalFileSystem}\n+\n+try:\n+    FS_MAP[S3FileSystem] = fs.S3FileSystem\n+except ImportError:\n+    _logger.warning(\"S3 Filesystem not available to arrow\")\n+\n+\n+class ParquetReader(object):\n+\n+    def __init__(self, input: InputFile, expected_schema: Schema, options, filter_expr: Expression,\n+                 case_sensitive: bool, start: int = None, end: int = None):\n+        self._stats: typing.Dict[str, int] = dict()\n+\n+        self._input = input\n+        self._input_fo = input.new_fo()\n+\n+        self._arrow_file = pq.ParquetFile(self._input_fo)\n+        self._file_schema = convert_parquet_to_iceberg(self._arrow_file)\n+        self._expected_schema = expected_schema\n+        self._file_to_expected_name_map = ParquetReader.get_field_map(self._file_schema,\n+                                                                      self._expected_schema)\n+        self._options = options\n+        self._filter = get_dataset_filter(filter_expr, ParquetReader.get_reverse_field_map(self._file_schema,\n+                                                                                           self._expected_schema))\n+\n+        self._case_sensitive = case_sensitive\n+        if start is not None or end is not None:\n+            raise NotImplementedError(\"Partial file reads are not yet supported\")\n+            # self.start = start\n+            # self.end = end\n+\n+        self.materialized_table = False\n+        self.curr_iterator = None\n+        self._table = None\n+        self._df = None\n+        self._batches = None\n+        self._row_tuple = None\n+\n+        _logger.debug(\"Reader initialized for %s\" % self._input.path)\n+\n+    @property\n+    def stats(self) -> dict:\n+        return dict(self._stats)\n+\n+    def to_pandas(self) -> typing.Union[pd.Series, pd.DataFrame]:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0eca8f40cff2669627263ad38a93224e8e989058"}, "originalPosition": 116}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0eca8f40cff2669627263ad38a93224e8e989058", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/0eca8f40cff2669627263ad38a93224e8e989058", "committedDate": "2020-11-16T16:51:51Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}, "afterCommit": {"oid": "544b90f4505e3292cda84d4c942caec8086a8c00", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/544b90f4505e3292cda84d4c942caec8086a8c00", "committedDate": "2020-11-27T22:46:12Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9d5d82ebed0b08771f52ba43166b6f1eb4c0614f", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/9d5d82ebed0b08771f52ba43166b6f1eb4c0614f", "committedDate": "2020-12-04T02:01:35Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "544b90f4505e3292cda84d4c942caec8086a8c00", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/544b90f4505e3292cda84d4c942caec8086a8c00", "committedDate": "2020-11-27T22:46:12Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}, "afterCommit": {"oid": "9d5d82ebed0b08771f52ba43166b6f1eb4c0614f", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/9d5d82ebed0b08771f52ba43166b6f1eb4c0614f", "committedDate": "2020-12-04T02:01:35Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NzQzOTgw", "url": "https://github.com/apache/iceberg/pull/1727#pullrequestreview-544743980", "createdAt": "2020-12-04T08:26:30Z", "commit": {"oid": "2beabcec423b377f9080068bdb613ca6060b94df"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2beabcec423b377f9080068bdb613ca6060b94df", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/2beabcec423b377f9080068bdb613ca6060b94df", "committedDate": "2020-12-04T02:23:32Z", "message": "trigger GitHub actions"}, "afterCommit": {"oid": "9d5d82ebed0b08771f52ba43166b6f1eb4c0614f", "author": {"user": {"login": "TGooch44", "name": "Ted Gooch"}}, "url": "https://github.com/apache/iceberg/commit/9d5d82ebed0b08771f52ba43166b6f1eb4c0614f", "committedDate": "2020-12-04T02:01:35Z", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3688, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}