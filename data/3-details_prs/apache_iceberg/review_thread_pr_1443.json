{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDg1MDQ0OTkw", "number": 1443, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQyMDoyMzoyNFrOEirjuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNjo1MDoyMVrOEjOoBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0ODAwNjk2OnYy", "diffSide": "LEFT", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQyMDoyMzoyNFrOHQsvzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwODo0NzozN1rOHRJ7ew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI3MjM5OA==", "bodyText": "Should we move Util.getFs out of this try/catch block? An IOException from not being able to load the FS should probably fail instead of returning a hint.", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r487272398", "createdAt": "2020-09-11T20:23:24Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "diffHunk": "@@ -290,8 +292,10 @@ private int readVersionHint() {\n         return Integer.parseInt(in.readLine().replace(\"\\n\", \"\"));\n       }\n \n-    } catch (IOException e) {\n-      throw new RuntimeIOException(e, \"Failed to get file system for path: %s\", versionHintFile);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64a736d8275c0493081c385a59514b072b134b18"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc1MDUyMw==", "bodyText": "Done", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r487750523", "createdAt": "2020-09-14T08:47:37Z", "author": {"login": "pvary"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "diffHunk": "@@ -290,8 +292,10 @@ private int readVersionHint() {\n         return Integer.parseInt(in.readLine().replace(\"\\n\", \"\"));\n       }\n \n-    } catch (IOException e) {\n-      throw new RuntimeIOException(e, \"Failed to get file system for path: %s\", versionHintFile);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI3MjM5OA=="}, "originalCommit": {"oid": "64a736d8275c0493081c385a59514b072b134b18"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0ODAxMTExOnYy", "diffSide": "RIGHT", "path": "core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCatalog.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMVQyMDoyNTowM1rOHQsybg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwODo1MDowMlrOHRKBMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI3MzA3MA==", "bodyText": "I don't think this is correct. If the hint file is missing, the table should still be readable.", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r487273070", "createdAt": "2020-09-11T20:25:03Z", "author": {"login": "rdblue"}, "path": "core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCatalog.java", "diffHunk": "@@ -441,4 +447,70 @@ public void testDropNamespace() throws IOException {\n     FileSystem fs = Util.getFs(new Path(metaLocation), conf);\n     Assert.assertFalse(fs.isDirectory(new Path(metaLocation)));\n   }\n+\n+  @Test\n+  public void testVersionHintFile() throws Exception {\n+    Configuration conf = new Configuration();\n+    String warehousePath = temp.newFolder().getAbsolutePath();\n+    HadoopCatalog catalog = new HadoopCatalog(conf, warehousePath);\n+\n+    // Create a test table with multiple versions\n+    TableIdentifier tableId = TableIdentifier.of(\"tbl\");\n+    Table table = catalog.createTable(tableId, SCHEMA, PartitionSpec.unpartitioned());\n+\n+    DataFile dataFile1 = DataFiles.builder(SPEC)\n+        .withPath(\"/a.parquet\")\n+        .withFileSizeInBytes(10)\n+        .withRecordCount(1)\n+        .build();\n+\n+    DataFile dataFile2 = DataFiles.builder(SPEC)\n+        .withPath(\"/b.parquet\")\n+        .withFileSizeInBytes(10)\n+        .withRecordCount(1)\n+        .build();\n+\n+    table.newAppend().appendFile(dataFile1).commit();\n+    table.newAppend().appendFile(dataFile2).commit();\n+    long secondSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    // Get the version-hint.text file location\n+    String versionHintLocation = ((HadoopTableOperations) catalog.newTableOps(tableId)).versionHintFile().toString();\n+\n+    // Write old data to confirm that we are writing the correct file\n+    FileIO io = new HadoopFileIO(conf);\n+    io.deleteFile(versionHintLocation);\n+    try (PositionOutputStream stream = io.newOutputFile(versionHintLocation).create()) {\n+      stream.write(\"1\".getBytes(StandardCharsets.UTF_8));\n+    }\n+\n+    // Load the table and check the current snapshotId\n+    Assert.assertEquals(secondSnapshotId, catalog.loadTable(tableId).currentSnapshot().snapshotId());\n+\n+    // Write newer data to confirm that we are writing the correct file\n+    io.deleteFile(versionHintLocation);\n+    try (PositionOutputStream stream = io.newOutputFile(versionHintLocation).create()) {\n+      stream.write(\"3\".getBytes(StandardCharsets.UTF_8));\n+    }\n+\n+    // Load the table and check the current snapshotId\n+    Assert.assertEquals(secondSnapshotId, catalog.loadTable(tableId).currentSnapshot().snapshotId());\n+\n+    // Write an empty version hint file\n+    io.deleteFile(versionHintLocation);\n+    io.newOutputFile(versionHintLocation).create().close();\n+\n+    // Load the table and check the current snapshotId\n+    Assert.assertEquals(secondSnapshotId, catalog.loadTable(tableId).currentSnapshot().snapshotId());\n+\n+    // Just delete the file - double check that we have manipulated the correct file\n+    io.deleteFile(versionHintLocation);\n+\n+    // Check that exception is thrown\n+    AssertHelpers.assertThrows(\n+        \"Should not be able to find the table\",\n+        NoSuchTableException.class,\n+        \"Table does not exist: tbl\",\n+        () -> catalog.loadTable(tableId));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64a736d8275c0493081c385a59514b072b134b18"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc1MTk4Nw==", "bodyText": "Thanks for the review @rdblue!\nThat was another codepath which I did not changed to keep the changes minimal, but I agree with you comment, and corrected that path as well.", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r487751987", "createdAt": "2020-09-14T08:50:02Z", "author": {"login": "pvary"}, "path": "core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCatalog.java", "diffHunk": "@@ -441,4 +447,70 @@ public void testDropNamespace() throws IOException {\n     FileSystem fs = Util.getFs(new Path(metaLocation), conf);\n     Assert.assertFalse(fs.isDirectory(new Path(metaLocation)));\n   }\n+\n+  @Test\n+  public void testVersionHintFile() throws Exception {\n+    Configuration conf = new Configuration();\n+    String warehousePath = temp.newFolder().getAbsolutePath();\n+    HadoopCatalog catalog = new HadoopCatalog(conf, warehousePath);\n+\n+    // Create a test table with multiple versions\n+    TableIdentifier tableId = TableIdentifier.of(\"tbl\");\n+    Table table = catalog.createTable(tableId, SCHEMA, PartitionSpec.unpartitioned());\n+\n+    DataFile dataFile1 = DataFiles.builder(SPEC)\n+        .withPath(\"/a.parquet\")\n+        .withFileSizeInBytes(10)\n+        .withRecordCount(1)\n+        .build();\n+\n+    DataFile dataFile2 = DataFiles.builder(SPEC)\n+        .withPath(\"/b.parquet\")\n+        .withFileSizeInBytes(10)\n+        .withRecordCount(1)\n+        .build();\n+\n+    table.newAppend().appendFile(dataFile1).commit();\n+    table.newAppend().appendFile(dataFile2).commit();\n+    long secondSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    // Get the version-hint.text file location\n+    String versionHintLocation = ((HadoopTableOperations) catalog.newTableOps(tableId)).versionHintFile().toString();\n+\n+    // Write old data to confirm that we are writing the correct file\n+    FileIO io = new HadoopFileIO(conf);\n+    io.deleteFile(versionHintLocation);\n+    try (PositionOutputStream stream = io.newOutputFile(versionHintLocation).create()) {\n+      stream.write(\"1\".getBytes(StandardCharsets.UTF_8));\n+    }\n+\n+    // Load the table and check the current snapshotId\n+    Assert.assertEquals(secondSnapshotId, catalog.loadTable(tableId).currentSnapshot().snapshotId());\n+\n+    // Write newer data to confirm that we are writing the correct file\n+    io.deleteFile(versionHintLocation);\n+    try (PositionOutputStream stream = io.newOutputFile(versionHintLocation).create()) {\n+      stream.write(\"3\".getBytes(StandardCharsets.UTF_8));\n+    }\n+\n+    // Load the table and check the current snapshotId\n+    Assert.assertEquals(secondSnapshotId, catalog.loadTable(tableId).currentSnapshot().snapshotId());\n+\n+    // Write an empty version hint file\n+    io.deleteFile(versionHintLocation);\n+    io.newOutputFile(versionHintLocation).create().close();\n+\n+    // Load the table and check the current snapshotId\n+    Assert.assertEquals(secondSnapshotId, catalog.loadTable(tableId).currentSnapshot().snapshotId());\n+\n+    // Just delete the file - double check that we have manipulated the correct file\n+    io.deleteFile(versionHintLocation);\n+\n+    // Check that exception is thrown\n+    AssertHelpers.assertThrows(\n+        \"Should not be able to find the table\",\n+        NoSuchTableException.class,\n+        \"Table does not exist: tbl\",\n+        () -> catalog.loadTable(tableId));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzI3MzA3MA=="}, "originalCommit": {"oid": "64a736d8275c0493081c385a59514b072b134b18"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA0OTgwMzc5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xM1QwODozOTo1N1rOHQ6qvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxMTozNTo1M1rOHSspPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzUwMDQ3OQ==", "bodyText": "Should this return 0 instead? Similar to when the version hint file does not exist?", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r487500479", "createdAt": "2020-09-13T08:39:57Z", "author": {"login": "shardulm94"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "diffHunk": "@@ -290,8 +292,10 @@ private int readVersionHint() {\n         return Integer.parseInt(in.readLine().replace(\"\\n\", \"\"));\n       }\n \n-    } catch (IOException e) {\n-      throw new RuntimeIOException(e, \"Failed to get file system for path: %s\", versionHintFile);\n+    } catch (Exception e) {\n+      LOG.warn(\"Error reading version hint file {}\", versionHintFile, e);\n+      // We just assume corrupted metadata and start to read from the first version file\n+      return 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "64a736d8275c0493081c385a59514b072b134b18"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzczNjgxNQ==", "bodyText": "When we are returning 0 the caller will end up throwing the following exception:\nTable does not exist: tbl\norg.apache.iceberg.exceptions.NoSuchTableException: Table does not exist: tbl\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:108)\n\tat org.apache.iceberg.hadoop.TestHadoopCatalog.testVersionHintFile(TestHadoopCatalog.java:508)\n\nThe goal of the PR is to recover from the corrupted version-hint.txt file, so I have returned 1 instead.\nBut this question made me wonder if returning 1 is a good idea or not, so I will check with Ryan", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r487736815", "createdAt": "2020-09-14T08:25:24Z", "author": {"login": "pvary"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "diffHunk": "@@ -290,8 +292,10 @@ private int readVersionHint() {\n         return Integer.parseInt(in.readLine().replace(\"\\n\", \"\"));\n       }\n \n-    } catch (IOException e) {\n-      throw new RuntimeIOException(e, \"Failed to get file system for path: %s\", versionHintFile);\n+    } catch (Exception e) {\n+      LOG.warn(\"Error reading version hint file {}\", versionHintFile, e);\n+      // We just assume corrupted metadata and start to read from the first version file\n+      return 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzUwMDQ3OQ=="}, "originalCommit": {"oid": "64a736d8275c0493081c385a59514b072b134b18"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4Nzc1NjUwMw==", "bodyText": "@rdblue: I have checked @shardulm94's comment and started to think about the possible problems/solutions.\nThe main question is:\n\nAre the metadataFiles kept forever? Or are they cleaned-up with compaction, or whatever other process?\n\nIf they are kept forever then it could be ok to start from 1. If they could be removed somehow then we might end up failing to read the removed version file and failing anyway. Could this be a problem? Could it be a good solution to list the metadata files for this case and trying to recover using that listing?\nThanks,\nPeter", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r487756503", "createdAt": "2020-09-14T08:56:58Z", "author": {"login": "pvary"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "diffHunk": "@@ -290,8 +292,10 @@ private int readVersionHint() {\n         return Integer.parseInt(in.readLine().replace(\"\\n\", \"\"));\n       }\n \n-    } catch (IOException e) {\n-      throw new RuntimeIOException(e, \"Failed to get file system for path: %s\", versionHintFile);\n+    } catch (Exception e) {\n+      LOG.warn(\"Error reading version hint file {}\", versionHintFile, e);\n+      // We just assume corrupted metadata and start to read from the first version file\n+      return 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzUwMDQ3OQ=="}, "originalCommit": {"oid": "64a736d8275c0493081c385a59514b072b134b18"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODA4ODI0NQ==", "bodyText": "0 is currently used to signal that the version hint doesn't exist. The refresh  method checks whether the version hint was missing so that it can throw an exception when the metadata is missing: if there is no metadata file and the version hint was 0, then the table doesn't exist yet. If the version hint is non-zero, then the problem is that the metadata file is missing.\nI like the solution here that does some recovery to find the right version. We should probably rename the method since it is doing recovery, but it's fine for now.\nThe recovery that is done in readVersionHint is something we can improve on. We could use file listing if the v1 metadata file doesn't exist. That would be a fairly safe way of recovering the version hint. I think that means there should be 2 follow-ups:\n\nImprove how we write the hint to avoid corruption in HDFS by writing a file to a unique name, then renaming it to version-hint.txt. That requires overwriting with the rename, so I'm not sure if we can actually do this.\nImprove version recovery when metadata files have been cleaned up, probably using list operations.", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r488088245", "createdAt": "2020-09-14T17:02:10Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "diffHunk": "@@ -290,8 +292,10 @@ private int readVersionHint() {\n         return Integer.parseInt(in.readLine().replace(\"\\n\", \"\"));\n       }\n \n-    } catch (IOException e) {\n-      throw new RuntimeIOException(e, \"Failed to get file system for path: %s\", versionHintFile);\n+    } catch (Exception e) {\n+      LOG.warn(\"Error reading version hint file {}\", versionHintFile, e);\n+      // We just assume corrupted metadata and start to read from the first version file\n+      return 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzUwMDQ3OQ=="}, "originalCommit": {"oid": "64a736d8275c0493081c385a59514b072b134b18"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTM2Nzg3MA==", "bodyText": "#1465 is created for the 2nd point", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r489367870", "createdAt": "2020-09-16T11:35:53Z", "author": {"login": "pvary"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "diffHunk": "@@ -290,8 +292,10 @@ private int readVersionHint() {\n         return Integer.parseInt(in.readLine().replace(\"\\n\", \"\"));\n       }\n \n-    } catch (IOException e) {\n-      throw new RuntimeIOException(e, \"Failed to get file system for path: %s\", versionHintFile);\n+    } catch (Exception e) {\n+      LOG.warn(\"Error reading version hint file {}\", versionHintFile, e);\n+      // We just assume corrupted metadata and start to read from the first version file\n+      return 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzUwMDQ3OQ=="}, "originalCommit": {"oid": "64a736d8275c0493081c385a59514b072b134b18"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1Mzc1MjM5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNjo1MDoyMVrOHReBuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNjo1MDoyMVrOHReBuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODA3OTgwMQ==", "bodyText": "Nit: unnecessary newline.", "url": "https://github.com/apache/iceberg/pull/1443#discussion_r488079801", "createdAt": "2020-09-14T16:50:21Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java", "diffHunk": "@@ -277,21 +279,30 @@ private void writeVersionHint(int versionToWrite) {\n     }\n   }\n \n-  private int readVersionHint() {\n+  @VisibleForTesting\n+  int readVersionHint() {\n     Path versionHintFile = versionHintFile();\n-    try {\n-      FileSystem fs = Util.getFs(versionHintFile, conf);\n-      if (!fs.exists(versionHintFile)) {\n-        return 0;\n-      }\n+    FileSystem fs = Util.getFs(versionHintFile, conf);\n+\n+    try (InputStreamReader fsr = new InputStreamReader(fs.open(versionHintFile), StandardCharsets.UTF_8);\n+         BufferedReader in = new BufferedReader(fsr)) {\n+      return Integer.parseInt(in.readLine().replace(\"\\n\", \"\"));\n \n-      try (InputStreamReader fsr = new InputStreamReader(fs.open(versionHintFile), StandardCharsets.UTF_8);\n-           BufferedReader in = new BufferedReader(fsr)) {\n-        return Integer.parseInt(in.readLine().replace(\"\\n\", \"\"));\n+    } catch (Exception e) {\n+      LOG.warn(\"Error reading version hint file {}\", versionHintFile, e);\n+      try {\n+        if (getMetadataFile(1) != null) {\n+          // We just assume corrupted metadata and start to read from the first version file\n+          return 1;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "68d5c482e1e71e9f87a3064beeb69703c50a9343"}, "originalPosition": 46}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3729, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}