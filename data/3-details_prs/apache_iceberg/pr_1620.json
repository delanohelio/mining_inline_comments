{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA0NzAzNTEy", "number": 1620, "title": "Hive: Fix TestHiveMetastore worker exhaustion", "bodyText": "Last night I was able to reproduce these failures for a while:\norg.apache.iceberg.mr.hive.TestHiveIcebergStorageHandlerWithHiveCatalog > testJoinTablesParquet FAILED\n   java.lang.IllegalArgumentException: Failed to executeQuery Hive query SHOW TABLES: Error while compiling statement: FAILED: SemanticException org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe (Write failed)\n       Caused by:\n       org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe (Write failed)\n           Caused by:\n           org.apache.hadoop.hive.ql.parse.SemanticException: org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe (Write failed)\n               Caused by:\n               org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe (Write failed)\n                   Caused by:\n                   org.apache.thrift.transport.TTransportException: java.net.SocketException: Broken pipe (Write failed)\n                       Caused by:\n                       java.net.SocketException: Broken pipe (Write failed)\n\nThe issue is that the number of worker threads is exhausted in the TestHiveMetastore.\nWe are creating HMSClients in several different places:\n\nTestHiveMetastore - we set iceberg.hive.client-pool-size to 2, so if HiveCatalog is used we create a pool with 2 connections\nTestHiveMetastore - own clientPool. The size is 1, so we create 1 more connection\nWhen we are initializing HiveServer2 for every worker and background thread we create 1-1 connections\nIn Hive3 HiveServer2 creates a NotificationEventPoll thread which also uses a connection\n\nWe initialize the TestHiveMetastore with maximum of 5 threads and the connections are not cleaned up consistently. The main culprit is that HiveMetastore and HiveServer2 is not designed as something which is started and stopped in the same thread/JVM multiple times, so cleaning up of the Metastore connections during HiveServer2 restart was never a priority. ThreadLocal connections are kept and reused for the worker threads, and the connections are only closed if the Finalizer thread destroys the object.\nI have tested 2 fixes which were working consistently when I have tested them (only one of them was enough to fix the issue, but I wanted to verify my theory of the RC):\n\nIncreased the threadpool size for the TestHiveMetastore\nAdded a System.gc() to the HiveIcebergStorageHandlerBaseTest.after() - As System.gc() is not guaranteed to run a GC this is just something which might work\n\nI have added one more fix where I have turned off the NotificationEventPoller, which saves 1 connection for sure.\nWhen I wanted to test this fix I was unable to reproduce the issue again - even without any changes.\nSince I have no way to test the fixes again and all of them are test only changes, I have created a pull request which contains all of them. I really hope this will fix all off the flakiness issues once and for all.\nCould you please check @massdosage, @marton-bod, @lcspinter, @rdblue", "createdAt": "2020-10-16T09:13:11Z", "url": "https://github.com/apache/iceberg/pull/1620", "merged": true, "mergeCommit": {"oid": "878d9a2df24a5469a2a4fa787237e2d0a939ab00"}, "closed": true, "closedAt": "2020-10-16T16:55:18Z", "author": {"login": "pvary"}, "timelineItems": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdS_xP4gH2gAyNTA0NzAzNTEyOjg3N2UxNDVlY2Q0ZDA2YTM0YWZkMGMwNzNmZmI5YTJjYzc2N2IyZWY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdTJWElgFqTUxMDY0MzYwMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "877e145ecd4d06a34afd0c073ffb9a2cc767b2ef", "author": {"user": {"login": "pvary", "name": null}}, "url": "https://github.com/apache/iceberg/commit/877e145ecd4d06a34afd0c073ffb9a2cc767b2ef", "committedDate": "2020-10-16T05:45:41Z", "message": "Fix TestHiveMetastore worker exhaustion"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEwNjQzNjAz", "url": "https://github.com/apache/iceberg/pull/1620#pullrequestreview-510643603", "createdAt": "2020-10-16T16:55:03Z", "commit": {"oid": "877e145ecd4d06a34afd0c073ffb9a2cc767b2ef"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQxNjo1NTowM1rOHjIg0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQxNjo1NTowM1rOHjIg0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjYwMTY4MQ==", "bodyText": "I don't think we should change these values for the majority of Hive tests, or at least Hive Metastore tests, because this has helped us track down connection leaks. It sounds like we should change these for HiveRunner tests, where we have less control of the connections. If Hive itself doesn't pool or close connections, then we would need to bump this up.\nCould you make this configurable so that we can set it for the tests that need it?", "url": "https://github.com/apache/iceberg/pull/1620#discussion_r506601681", "createdAt": "2020-10-16T16:55:03Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java", "diffHunk": "@@ -166,8 +166,8 @@ private TServer newThriftServer(TServerSocket socket, HiveConf conf) throws Exce\n         .processor(new TSetIpAddressProcessor<>(handler))\n         .transportFactory(new TTransportFactory())\n         .protocolFactory(new TBinaryProtocol.Factory())\n-        .minWorkerThreads(3)\n-        .maxWorkerThreads(5);\n+        .minWorkerThreads(5)\n+        .maxWorkerThreads(15);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "877e145ecd4d06a34afd0c073ffb9a2cc767b2ef"}, "originalPosition": 7}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3973, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}