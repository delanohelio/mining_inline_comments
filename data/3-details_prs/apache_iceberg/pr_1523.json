{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkzOTU1ODYy", "number": 1523, "title": "ISSUE-1520 Document writing against partitioned table in Spark", "bodyText": "Closes #1520\nThis PR documents how to write against partitioned table in Spark, as there doesn't look to be documented somewhere, and it's not a no-brainer thing to deal with.\nScreenshot of 230650d is follow:", "createdAt": "2020-09-28T07:49:32Z", "url": "https://github.com/apache/iceberg/pull/1523", "merged": true, "mergeCommit": {"oid": "dfaff0b3f772163081a287122d52e9eec973b8b5"}, "closed": true, "closedAt": "2020-09-29T17:14:25Z", "author": {"login": "HeartSaVioR"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdNOs3yAH2gAyNDkzOTU1ODYyOjIzMDY1MGQzNmY4MWQzOWQ1NWVhNDZhODVjODFjMjVjNGEzMmU0ODM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdNhqPDgH2gAyNDkzOTU1ODYyOmU2YWI4M2QzYTVjYWYzZGM5ZDk1ZGViODMwMGU2MDFhNTJlODE0MDI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "230650d36f81d39d55ea46a85c81c25c4a32e483", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/230650d36f81d39d55ea46a85c81c25c4a32e483", "committedDate": "2020-09-28T07:45:56Z", "message": "ISSUE-1520 Document writing against partitioned table in Spark"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3OTI1MDYz", "url": "https://github.com/apache/iceberg/pull/1523#pullrequestreview-497925063", "createdAt": "2020-09-28T21:49:41Z", "commit": {"oid": "230650d36f81d39d55ea46a85c81c25c4a32e483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMTo0OTo0MlrOHZRBuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMTo0OTo0MlrOHZRBuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI1NTQxNg==", "bodyText": "Could you add a SQL example as well?", "url": "https://github.com/apache/iceberg/pull/1523#discussion_r496255416", "createdAt": "2020-09-28T21:49:42Z", "author": {"login": "rdblue"}, "path": "site/docs/spark.md", "diffHunk": "@@ -519,6 +519,59 @@ data.writeTo(\"prod.db.table\")\n     .createOrReplace()\n ```\n \n+## Writing against partitioned table\n+\n+Iceberg requires the data to be sorted according to the partition spec in prior to write against partitioned table.\n+This applies both Writing with SQL and Writing with DataFrames.\n+\n+Assuming we would like to write the data against below sample table:\n+\n+```sql\n+CREATE TABLE prod.db.sample (\n+    id bigint,\n+    data string,\n+    category string,\n+    ts timestamp)\n+USING iceberg\n+PARTITIONED BY (bucket(16, id), days(ts), category)\n+```\n+\n+then your data needs to be sorted by `bucket(16, id), days(ts), category` before writing to the table, like below:\n+\n+```scala\n+val sorted = data.sortWithinPartitions(expr(\"iceberg_bucket16(id)\"), expr(\"iceberg_days(ts)\"), col(\"category\"))\n+```\n+\n+You can create a temporary view from the resulting DataFrame to write with SQL, or write with Dataframe directly.\n+\n+If the partition spec of the table consists of `transformation` (non-identity), you need to register the Iceberg\n+transform function in Spark to specify it during sort. For example, to register `iceberg_bucket16` function in above query:\n+\n+```scala\n+import org.apache.iceberg.transforms.Transforms\n+import org.apache.iceberg.types.Types\n+import org.apache.spark.sql.types.IntegerType\n+import org.apache.spark.sql.types.StringType\n+\n+// Load the bucket transform from Iceberg to use as a UDF.\n+// Here the source type is `Types.LongType`, and matching Java type is `java.lang.Long`.\n+val bucketTransform = Transforms.bucket[java.lang.Long](Types.LongType.get(), 16)\n+\n+// Needed because Scala has trouble with the Java transform type.\n+// The return type of `bucket` transform is int, so the method's return type is.\n+def bucketFunc(id: Long): Int = bucketTransform.apply(id)\n+\n+// create and register a UDF\n+spark.udf.register(\"iceberg_bucket16\", bucketFunc _)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "230650d36f81d39d55ea46a85c81c25c4a32e483"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3OTI1ODM0", "url": "https://github.com/apache/iceberg/pull/1523#pullrequestreview-497925834", "createdAt": "2020-09-28T21:51:21Z", "commit": {"oid": "230650d36f81d39d55ea46a85c81c25c4a32e483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMTo1MToyMVrOHZRHRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMTo1MToyMVrOHZRHRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI1NjgzOQ==", "bodyText": "This is a little to specific because Iceberg actually just requires that your data is clustered by partition in each task. Maybe this is the easiest way to get the idea across initially, but it would be nice to have a note box that explains that both global and local sorts will work.", "url": "https://github.com/apache/iceberg/pull/1523#discussion_r496256839", "createdAt": "2020-09-28T21:51:21Z", "author": {"login": "rdblue"}, "path": "site/docs/spark.md", "diffHunk": "@@ -519,6 +519,59 @@ data.writeTo(\"prod.db.table\")\n     .createOrReplace()\n ```\n \n+## Writing against partitioned table\n+\n+Iceberg requires the data to be sorted according to the partition spec in prior to write against partitioned table.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "230650d36f81d39d55ea46a85c81c25c4a32e483"}, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3OTI2ODk2", "url": "https://github.com/apache/iceberg/pull/1523#pullrequestreview-497926896", "createdAt": "2020-09-28T21:53:36Z", "commit": {"oid": "230650d36f81d39d55ea46a85c81c25c4a32e483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMTo1MzozNlrOHZRLSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMTo1MzozNlrOHZRLSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI1Nzg2NA==", "bodyText": "Should this example be a little simpler?\nI think it makes sense to have an example for bucketing, but the other two don't require creating a UDF with the Iceberg transform. Splitting this into two examples might make sense: one with days(ts) and category to show how to add the sort with a SQL ORDER BY and using sortWithinPartitions, and then a more complicated one for bucketing.", "url": "https://github.com/apache/iceberg/pull/1523#discussion_r496257864", "createdAt": "2020-09-28T21:53:36Z", "author": {"login": "rdblue"}, "path": "site/docs/spark.md", "diffHunk": "@@ -519,6 +519,59 @@ data.writeTo(\"prod.db.table\")\n     .createOrReplace()\n ```\n \n+## Writing against partitioned table\n+\n+Iceberg requires the data to be sorted according to the partition spec in prior to write against partitioned table.\n+This applies both Writing with SQL and Writing with DataFrames.\n+\n+Assuming we would like to write the data against below sample table:\n+\n+```sql\n+CREATE TABLE prod.db.sample (\n+    id bigint,\n+    data string,\n+    category string,\n+    ts timestamp)\n+USING iceberg\n+PARTITIONED BY (bucket(16, id), days(ts), category)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "230650d36f81d39d55ea46a85c81c25c4a32e483"}, "originalPosition": 18}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f4d4bcb56b466e50ddb7d2888f7c092c4689c19a", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/f4d4bcb56b466e50ddb7d2888f7c092c4689c19a", "committedDate": "2020-09-29T03:23:19Z", "message": "Reflect review comment, before having utility method for transform."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2b2e1772ae6d6783229daaed750c3bfb7ea01f74", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/2b2e1772ae6d6783229daaed750c3bfb7ea01f74", "committedDate": "2020-09-29T03:48:08Z", "message": "Add notes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "82915b6b50015520516b7875f5eba3250d25dfd0", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/82915b6b50015520516b7875f5eba3250d25dfd0", "committedDate": "2020-09-29T04:37:04Z", "message": "Introduce util class to ease use of registering bucket function to UDF"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f118cbcbd2c944974537fb5183f14d074dccd20d", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/f118cbcbd2c944974537fb5183f14d074dccd20d", "committedDate": "2020-09-29T04:50:38Z", "message": "Add test for IcebergSpark"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e6ab83d3a5caf3dc9d95deb8300e601a52e81402", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/e6ab83d3a5caf3dc9d95deb8300e601a52e81402", "committedDate": "2020-09-29T05:51:15Z", "message": "Fix test issue"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3859, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}