{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY4NjE0ODc4", "number": 1346, "title": "Flink: Introduce Flink InputFormat", "bodyText": "This is subtask of #1293\nIntroduce FlinkInputFormat to read records from Iceberg table by Flink engine. The FlinkInputFormat is the foundation of SQL reader and streaming reader.\nImplemented by:\n\nIntroduce RowDataIterator and DataIterator, just like Spark BaseDataReader and RowDataReader.\nThe DataIterator provides a union iterator to union FileScanTask iterators.\nIntroduce FlinkSplitGenerator, this generator just wrap Iceberg CombinedScanTask to FlinkInputSplit.\nIntroduce FlinkInputFormat, it provides a simple API: FlinkInputFormat.builder().tableLoader(..).build().\n\nThis PR is based on:\n\nIntroduce CatalogLoader. #1332\nIntroduce GenericAppenderFactory and GenericAppenderHelper for reusing and testing. #1340", "createdAt": "2020-08-17T06:22:54Z", "url": "https://github.com/apache/iceberg/pull/1346", "merged": true, "mergeCommit": {"oid": "1a797cd986c9193f2e422fec295aaf25ce7e1916"}, "closed": true, "closedAt": "2020-09-25T01:05:47Z", "author": {"login": "JingsongLi"}, "timelineItems": {"totalCount": 55, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc_uWWCgBqjM2NjA4MjQ1NDE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdT_V7ZAFqTUxMTQ5MzE4MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f5f7d7507c31ef04f1d2dc5445831987d468e413", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/f5f7d7507c31ef04f1d2dc5445831987d468e413", "committedDate": "2020-08-17T06:22:03Z", "message": "Flink: Introduce Flink InputFormat"}, "afterCommit": {"oid": "decf8f1136d6da4e024f5c91b00ddd4adf92f843", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/decf8f1136d6da4e024f5c91b00ddd4adf92f843", "committedDate": "2020-08-17T08:43:03Z", "message": "Flink: Introduce Flink InputFormat"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5MDQ1MzUy", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-469045352", "createdAt": "2020-08-18T07:06:49Z", "commit": {"oid": "decf8f1136d6da4e024f5c91b00ddd4adf92f843"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwNzowNjo0OVrOHCGIyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwNzo1NDowNFrOHCHw5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk1OTc1Mw==", "bodyText": "How about renaming this method to openTaskIterator  ?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r471959753", "createdAt": "2020-08-18T07:06:49Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+/**\n+ * Base class of Flink iterators.\n+ *\n+ * @param <T> is the Java class returned by this iterator whose objects contain one or more rows.\n+ */\n+abstract class DataIterator<T> implements CloseableIterator<T> {\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;\n+  private final EncryptionManager encryption;\n+  private final Schema projectedSchema;\n+  private final int[] fieldsReorder;\n+\n+  private CloseableIterator<T> currentIterator;\n+\n+  DataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption, Schema tableSchema,\n+               List<String> projectedFields) {\n+    this.fileIo = fileIo;\n+    this.tasks = task.files().iterator();\n+    this.encryption = encryption;\n+    this.currentIterator = CloseableIterator.empty();\n+\n+    this.projectedSchema = FlinkSchemaUtil.pruneWithoutReordering(tableSchema, projectedFields);\n+\n+    // The projected schema is the schema without reordering, but Flink wants its own order, so we need to reorder the\n+    // output row.\n+    List<String> projectedNames = projectedSchema.asStruct().fields().stream()\n+        .map(Types.NestedField::name).collect(Collectors.toList());\n+    this.fieldsReorder = projectedFields == null ?\n+        null : projectedFields.stream().mapToInt(projectedNames::indexOf).toArray();\n+  }\n+\n+  Schema projectedSchema() {\n+    return projectedSchema;\n+  }\n+\n+  int[] fieldsReorder() {\n+    return fieldsReorder;\n+  }\n+\n+  InputFile getInputFile(FileScanTask task) {\n+    Preconditions.checkArgument(!task.isDataTask(), \"Invalid task type\");\n+    return encryption.decrypt(EncryptedFiles.encryptedInput(\n+        this.fileIo.newInputFile(task.file().path().toString()),\n+        task.file().keyMetadata()));\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    updateCurrentIterator();\n+    return currentIterator.hasNext();\n+  }\n+\n+  @Override\n+  public T next() {\n+    updateCurrentIterator();\n+    return currentIterator.next();\n+  }\n+\n+  /**\n+   * Updates the current iterator field to ensure that the current Iterator\n+   * is not exhausted.\n+   */\n+  private void updateCurrentIterator() {\n+    try {\n+      while (!currentIterator.hasNext() && tasks.hasNext()) {\n+        currentIterator.close();\n+        currentIterator = nextTaskIterator(tasks.next());\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  abstract CloseableIterator<T> nextTaskIterator(FileScanTask scanTask) throws IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "decf8f1136d6da4e024f5c91b00ddd4adf92f843"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk3MTQ3MA==", "bodyText": "nit: is this comment still valuable ? Seems I did not get the point.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r471971470", "createdAt": "2020-08-18T07:29:09Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataIterator.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.data.FlinkAvroReader;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PartitionUtil;\n+\n+class RowDataIterator extends DataIterator<RowData> {\n+\n+  private final String nameMapping;\n+\n+  RowDataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption, Schema tableSchema,\n+                  List<String> projectedFields, String nameMapping) {\n+    super(task, fileIo, encryption, tableSchema, projectedFields);\n+    this.nameMapping = nameMapping;\n+  }\n+\n+  @Override\n+  protected CloseableIterator<RowData> nextTaskIterator(FileScanTask task) {\n+    // schema or rows returned by readers", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "decf8f1136d6da4e024f5c91b00ddd4adf92f843"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk4NTQ0Ng==", "bodyText": "I'm not quite sure whether flink support complex data type projection, if sure we may need more unit tests to address the projection cases, such as projection by a nested struct, map, list (similar to the spark's TestReadProjection ).", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r471985446", "createdAt": "2020-08-18T07:52:24Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TestHelpers;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericAppenderHelper;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.CatalogLoader;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestFlinkScan extends AbstractTestBase {\n+\n+  private static final Schema SCHEMA = new Schema(\n+          required(1, \"data\", Types.StringType.get()),\n+          required(2, \"id\", Types.LongType.get()),\n+          required(3, \"dt\", Types.StringType.get()));\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+          .identity(\"dt\")\n+          .bucket(\"id\", 1)\n+          .build();\n+\n+  // before variables\n+  private Configuration conf;\n+  String warehouse;\n+  private HadoopCatalog catalog;\n+\n+  // parametrized variables\n+  private final FileFormat fileFormat;\n+\n+  @Parameterized.Parameters\n+  public static Object[] parameters() {\n+    // TODO add orc and parquet\n+    return new Object[] {\"avro\"};\n+  }\n+\n+  TestFlinkScan(String fileFormat) {\n+    this.fileFormat = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File warehouseFile = TEMPORARY_FOLDER.newFolder();\n+    Assert.assertTrue(warehouseFile.delete());\n+    conf = new Configuration();\n+    warehouse = \"file:\" + warehouseFile;\n+    catalog = new HadoopCatalog(conf, warehouse);\n+  }\n+\n+  private List<Row> execute(Table table) throws IOException {\n+    return executeWithOptions(table, null, null, null, null, null, null, null, null);\n+  }\n+\n+  private List<Row> execute(Table table, List<String> projectFields) throws IOException {\n+    return executeWithOptions(table, projectFields, null, null, null, null, null, null, null);\n+  }\n+\n+  protected abstract List<Row> executeWithOptions(\n+      Table table, List<String> projectFields, CatalogLoader loader, Long snapshotId,\n+      Long startSnapshotId, Long endSnapshotId, Long asOfTimestamp, List<Expression> filters, String sqlFilter)\n+      throws IOException;\n+\n+  protected abstract void assertResiduals(List<Row> results, List<Record> writeRecords, List<Record> filteredRecords)\n+      throws IOException;\n+\n+  @Test\n+  public void testUnpartitionedTable() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA);\n+    List<Record> expectedRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(expectedRecords);\n+    assertRecords(execute(table), expectedRecords);\n+  }\n+\n+  @Test\n+  public void testPartitionedTable() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+    List<Record> expectedRecords = RandomGenericData.generate(SCHEMA, 1, 0L);\n+    expectedRecords.get(0).set(2, \"2020-03-20\");\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(\n+        org.apache.iceberg.TestHelpers.Row.of(\"2020-03-20\", 0), expectedRecords);\n+    assertRecords(execute(table), expectedRecords);\n+  }\n+\n+  @Test\n+  public void testProjection() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "decf8f1136d6da4e024f5c91b00ddd4adf92f843"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk4NjQwNQ==", "bodyText": "Another case: Project with a new renamed schema", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r471986405", "createdAt": "2020-08-18T07:54:04Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TestHelpers;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericAppenderHelper;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.CatalogLoader;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestFlinkScan extends AbstractTestBase {\n+\n+  private static final Schema SCHEMA = new Schema(\n+          required(1, \"data\", Types.StringType.get()),\n+          required(2, \"id\", Types.LongType.get()),\n+          required(3, \"dt\", Types.StringType.get()));\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+          .identity(\"dt\")\n+          .bucket(\"id\", 1)\n+          .build();\n+\n+  // before variables\n+  private Configuration conf;\n+  String warehouse;\n+  private HadoopCatalog catalog;\n+\n+  // parametrized variables\n+  private final FileFormat fileFormat;\n+\n+  @Parameterized.Parameters\n+  public static Object[] parameters() {\n+    // TODO add orc and parquet\n+    return new Object[] {\"avro\"};\n+  }\n+\n+  TestFlinkScan(String fileFormat) {\n+    this.fileFormat = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File warehouseFile = TEMPORARY_FOLDER.newFolder();\n+    Assert.assertTrue(warehouseFile.delete());\n+    conf = new Configuration();\n+    warehouse = \"file:\" + warehouseFile;\n+    catalog = new HadoopCatalog(conf, warehouse);\n+  }\n+\n+  private List<Row> execute(Table table) throws IOException {\n+    return executeWithOptions(table, null, null, null, null, null, null, null, null);\n+  }\n+\n+  private List<Row> execute(Table table, List<String> projectFields) throws IOException {\n+    return executeWithOptions(table, projectFields, null, null, null, null, null, null, null);\n+  }\n+\n+  protected abstract List<Row> executeWithOptions(\n+      Table table, List<String> projectFields, CatalogLoader loader, Long snapshotId,\n+      Long startSnapshotId, Long endSnapshotId, Long asOfTimestamp, List<Expression> filters, String sqlFilter)\n+      throws IOException;\n+\n+  protected abstract void assertResiduals(List<Row> results, List<Record> writeRecords, List<Record> filteredRecords)\n+      throws IOException;\n+\n+  @Test\n+  public void testUnpartitionedTable() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA);\n+    List<Record> expectedRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(expectedRecords);\n+    assertRecords(execute(table), expectedRecords);\n+  }\n+\n+  @Test\n+  public void testPartitionedTable() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+    List<Record> expectedRecords = RandomGenericData.generate(SCHEMA, 1, 0L);\n+    expectedRecords.get(0).set(2, \"2020-03-20\");\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(\n+        org.apache.iceberg.TestHelpers.Row.of(\"2020-03-20\", 0), expectedRecords);\n+    assertRecords(execute(table), expectedRecords);\n+  }\n+\n+  @Test\n+  public void testProjection() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk4NTQ0Ng=="}, "originalCommit": {"oid": "decf8f1136d6da4e024f5c91b00ddd4adf92f843"}, "originalPosition": 133}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5MTEwODQ2", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-469110846", "createdAt": "2020-08-18T08:36:22Z", "commit": {"oid": "decf8f1136d6da4e024f5c91b00ddd4adf92f843"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwODozNjoyM1rOHCJUcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwODozNjoyM1rOHCJUcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjAxMTg5MA==", "bodyText": "Continue with the question from here. If we could produce a  ordered & projected schema in this method (Saying if this method is pruneWithReordering), then seems we don't have to convert the read RowData to the correct order here ?\nI'd prefer to use the correct projected schema to read the target RowData if possible, rather than reading RowData in a disordered schema and then order them in an iterator transformation.  Because this is in the critical read path and an extra RowData transformation will cost more resources , also make the codes hard to follow.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r472011890", "createdAt": "2020-08-18T08:36:23Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java", "diffHunk": "@@ -98,4 +102,22 @@ public static TableSchema toSchema(RowType rowType) {\n     }\n     return builder.build();\n   }\n+\n+  /**\n+   * Prune columns from a {@link Schema} using a projected fields.\n+   *\n+   * @param schema a Schema\n+   * @param projectedFields projected fields from Flink\n+   * @return a Schema corresponding to the Flink projection\n+   * @throws IllegalArgumentException if the Flink type does not match the Schema\n+   */\n+  public static Schema pruneWithoutReordering(Schema schema, List<String> projectedFields) {\n+    if (projectedFields == null) {\n+      return schema;\n+    }\n+\n+    Map<String, Integer> indexByName = TypeUtil.indexByName(schema.asStruct());\n+    Set<Integer> projectedIds = projectedFields.stream().map(indexByName::get).collect(Collectors.toSet());\n+    return TypeUtil.select(schema, projectedIds);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "decf8f1136d6da4e024f5c91b00ddd4adf92f843"}, "originalPosition": 31}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "decf8f1136d6da4e024f5c91b00ddd4adf92f843", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/decf8f1136d6da4e024f5c91b00ddd4adf92f843", "committedDate": "2020-08-17T08:43:03Z", "message": "Flink: Introduce Flink InputFormat"}, "afterCommit": {"oid": "d327aa89c54d81a631a172ba10194225c9c06ded", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/d327aa89c54d81a631a172ba10194225c9c06ded", "committedDate": "2020-08-20T04:40:19Z", "message": "Address comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2f53fd94e55ed0c03da691979c5467886f393b64", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/2f53fd94e55ed0c03da691979c5467886f393b64", "committedDate": "2020-08-20T04:41:49Z", "message": "Address comments"}, "afterCommit": {"oid": "0b27405879bd2fdcd03a9fcc00236f5fd6ac9209", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/0b27405879bd2fdcd03a9fcc00236f5fd6ac9209", "committedDate": "2020-08-20T05:24:52Z", "message": "Address comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0b27405879bd2fdcd03a9fcc00236f5fd6ac9209", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/0b27405879bd2fdcd03a9fcc00236f5fd6ac9209", "committedDate": "2020-08-20T05:24:52Z", "message": "Address comments"}, "afterCommit": {"oid": "4259635402efef3df1c83140723d5968f0b11967", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/4259635402efef3df1c83140723d5968f0b11967", "committedDate": "2020-08-21T02:04:37Z", "message": "Address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyMzUyNTg5", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-472352589", "createdAt": "2020-08-21T09:43:28Z", "commit": {"oid": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQwOTo0MzoyOFrOHEmiew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQwOTo1NDo1MVrOHEnJfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU4Nzc3MQ==", "bodyText": "nit : projectedFields == null || projectFields.length ==0", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r474587771", "createdAt": "2020-08-21T09:43:28Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java", "diffHunk": "@@ -98,4 +101,26 @@ public static TableSchema toSchema(RowType rowType) {\n     }\n     return builder.build();\n   }\n+\n+  /**\n+   * Project columns from a {@link Schema} using a projected fields.\n+   *\n+   * @param schema a Schema\n+   * @param projectedFields projected fields from Flink\n+   * @return a Schema corresponding to the Flink projection\n+   */\n+  public static Schema projectWithReordering(Schema schema, List<String> projectedFields) {\n+    if (projectedFields == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5MzQwMg==", "bodyText": "we may need to add a comment to indicate that:  we don't support complex data type projection for flink now.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r474593402", "createdAt": "2020-08-21T09:49:30Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java", "diffHunk": "@@ -98,4 +101,26 @@ public static TableSchema toSchema(RowType rowType) {\n     }\n     return builder.build();\n   }\n+\n+  /**\n+   * Project columns from a {@link Schema} using a projected fields.\n+   *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5NTYzMw==", "bodyText": "We've already have a Schema#select, will it fit for your requirement ?\n  /**\n   * Creates a projection schema for a subset of columns, selected by name.\n   * <p>\n   * Names that identify nested fields will select part or all of the field's top-level column.\n   *\n   * @param names a List of String names for selected columns\n   * @return a projection schema from this schema, by name\n   */\n  public Schema select(Collection<String> names) {\n    return internalSelect(names, true);\n  }", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r474595633", "createdAt": "2020-08-21T09:51:48Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java", "diffHunk": "@@ -98,4 +101,26 @@ public static TableSchema toSchema(RowType rowType) {\n     }\n     return builder.build();\n   }\n+\n+  /**\n+   * Project columns from a {@link Schema} using a projected fields.\n+   *\n+   * @param schema a Schema\n+   * @param projectedFields projected fields from Flink\n+   * @return a Schema corresponding to the Flink projection\n+   */\n+  public static Schema projectWithReordering(Schema schema, List<String> projectedFields) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5Nzc1Nw==", "bodyText": "Here, you put all reader related classes inside the source package,  will we also need to put those writer related classes into sink package ?    I don't have strong feeling to do that, keeping consistence is OK for me.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r474597757", "createdAt": "2020-08-21T09:54:51Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135"}, "originalPosition": 20}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyMzYyNTQ1", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-472362545", "createdAt": "2020-08-21T09:57:47Z", "commit": {"oid": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQwOTo1Nzo0N1rOHEnPNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxMDowNjo0MFrOHEnfzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5OTIyMg==", "bodyText": "nit: better to keep the assign order with the arguments ?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r474599222", "createdAt": "2020-08-21T09:57:47Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+/**\n+ * Base class of Flink iterators.\n+ *\n+ * @param <T> is the Java class returned by this iterator whose objects contain one or more rows.\n+ */\n+abstract class DataIterator<T> implements CloseableIterator<T> {\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;\n+  private final EncryptionManager encryption;\n+\n+  private CloseableIterator<T> currentIterator;\n+\n+  DataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption) {\n+    this.fileIo = fileIo;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYwMzQ3MQ==", "bodyText": "the long value is surely microseconds ?  could we just return (int)((long)value/1_000) ?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r474603471", "createdAt": "2020-08-21T10:06:40Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+/**\n+ * Base class of Flink iterators.\n+ *\n+ * @param <T> is the Java class returned by this iterator whose objects contain one or more rows.\n+ */\n+abstract class DataIterator<T> implements CloseableIterator<T> {\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;\n+  private final EncryptionManager encryption;\n+\n+  private CloseableIterator<T> currentIterator;\n+\n+  DataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption) {\n+    this.fileIo = fileIo;\n+    this.tasks = task.files().iterator();\n+    this.encryption = encryption;\n+    this.currentIterator = CloseableIterator.empty();\n+  }\n+\n+  InputFile getInputFile(FileScanTask task) {\n+    Preconditions.checkArgument(!task.isDataTask(), \"Invalid task type\");\n+    return encryption.decrypt(EncryptedFiles.encryptedInput(\n+        fileIo.newInputFile(task.file().path().toString()),\n+        task.file().keyMetadata()));\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    updateCurrentIterator();\n+    return currentIterator.hasNext();\n+  }\n+\n+  @Override\n+  public T next() {\n+    updateCurrentIterator();\n+    return currentIterator.next();\n+  }\n+\n+  /**\n+   * Updates the current iterator field to ensure that the current Iterator\n+   * is not exhausted.\n+   */\n+  private void updateCurrentIterator() {\n+    try {\n+      while (!currentIterator.hasNext() && tasks.hasNext()) {\n+        currentIterator.close();\n+        currentIterator = openTaskIterator(tasks.next());\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  abstract CloseableIterator<T> openTaskIterator(FileScanTask scanTask) throws IOException;\n+\n+  @Override\n+  public void close() throws IOException {\n+    // close the current iterator\n+    this.currentIterator.close();\n+\n+    // exhaust the task iterator\n+    while (tasks.hasNext()) {\n+      tasks.next();\n+    }\n+  }\n+\n+  static Object convertConstant(Type type, Object value) {\n+    if (value == null) {\n+      return null;\n+    }\n+\n+    switch (type.typeId()) {\n+      case DECIMAL: // DecimalData\n+        Types.DecimalType decimal = (Types.DecimalType) type;\n+        return DecimalData.fromBigDecimal((BigDecimal) value, decimal.precision(), decimal.scale());\n+      case STRING: // StringData\n+        if (value instanceof Utf8) {\n+          Utf8 utf8 = (Utf8) value;\n+          return StringData.fromBytes(utf8.getBytes(), 0, utf8.getByteLength());\n+        }\n+        return StringData.fromString(value.toString());\n+      case FIXED: // byte[]\n+        if (value instanceof byte[]) {\n+          return value;\n+        } else if (value instanceof GenericData.Fixed) {\n+          return ((GenericData.Fixed) value).bytes();\n+        }\n+        return ByteBuffers.toByteArray((ByteBuffer) value);\n+      case BINARY: // byte[]\n+        return ByteBuffers.toByteArray((ByteBuffer) value);\n+      case TIME: // int instead of long\n+        return (int) (DateTimeUtil.timeFromMicros((Long) value).toNanoOfDay() / 1000_000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135"}, "originalPosition": 137}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyMzcxNDU0", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-472371454", "createdAt": "2020-08-21T10:11:52Z", "commit": {"oid": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxMDoxMTo1M1rOHEnpQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxMDoxMTo1M1rOHEnpQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYwNTg5MA==", "bodyText": "Em, seems I've missed to close the TableLoader  in IcebergFilesCommitter patch..", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r474605890", "createdAt": "2020-08-21T10:11:53Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.io.DefaultInputSplitAssigner;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+/**\n+ * Flink {@link InputFormat} for Iceberg.\n+ */\n+public class FlinkInputFormat extends RichInputFormat<RowData, FlinkInputSplit> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final ScanOptions options;\n+  private final List<Expression> filterExpressions;\n+  private final FileIO io;\n+  private final EncryptionManager encryption;\n+  private final SerializableConfiguration serializableConf;\n+\n+  private transient RowDataIterator iterator;\n+\n+  private FlinkInputFormat(\n+      TableLoader tableLoader, Schema projectedSchema, FileIO io, EncryptionManager encryption,\n+      List<Expression> filterExpressions, ScanOptions options, SerializableConfiguration serializableConf) {\n+    this.tableLoader = tableLoader;\n+    this.projectedSchema = projectedSchema;\n+    this.options = options;\n+    this.filterExpressions = filterExpressions;\n+    this.io = io;\n+    this.encryption = encryption;\n+    this.serializableConf = serializableConf;\n+  }\n+\n+  @VisibleForTesting\n+  Schema projectedSchema() {\n+    return projectedSchema;\n+  }\n+\n+  @Override\n+  public BaseStatistics getStatistics(BaseStatistics cachedStatistics) {\n+    // Legacy method, not be used.\n+    return null;\n+  }\n+\n+  @Override\n+  public FlinkInputSplit[] createInputSplits(int minNumSplits) throws IOException {\n+    // Called in Job manager, so it is OK to load table from catalog.\n+    tableLoader.open(serializableConf.get());\n+    try (TableLoader loader = tableLoader) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135"}, "originalPosition": 88}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyMzc1NjMx", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-472375631", "createdAt": "2020-08-21T10:19:01Z", "commit": {"oid": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxMDoxOTowMVrOHEn1lQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxMDoxOTowMVrOHEn1lQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYwOTA0NQ==", "bodyText": "Now the orc reader has been merge into master,  pls add the orc iterable here .", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r474609045", "createdAt": "2020-08-21T10:19:01Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataIterator.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.Map;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.data.FlinkAvroReader;\n+import org.apache.iceberg.flink.data.FlinkParquetReaders;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PartitionUtil;\n+\n+class RowDataIterator extends DataIterator<RowData> {\n+\n+  private final Schema projectedSchema;\n+  private final String nameMapping;\n+  private final boolean caseSensitive;\n+\n+  RowDataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption, Schema projectedSchema,\n+                  String nameMapping, boolean caseSensitive) {\n+    super(task, fileIo, encryption);\n+    this.projectedSchema = projectedSchema;\n+    this.nameMapping = nameMapping;\n+    this.caseSensitive = caseSensitive;\n+  }\n+\n+  @Override\n+  protected CloseableIterator<RowData> openTaskIterator(FileScanTask task) {\n+    Schema partitionSchema = TypeUtil.select(projectedSchema, task.spec().identitySourceIds());\n+\n+    Map<Integer, ?> idToConstant = partitionSchema.columns().isEmpty() ? ImmutableMap.of() :\n+        PartitionUtil.constantsMap(task, RowDataIterator::convertConstant);\n+    CloseableIterable<RowData> iterable = newIterable(task, idToConstant);\n+    return iterable.iterator();\n+  }\n+\n+  private CloseableIterable<RowData> newIterable(FileScanTask task, Map<Integer, ?> idToConstant) {\n+    CloseableIterable<RowData> iter;\n+    if (task.isDataTask()) {\n+      throw new UnsupportedOperationException(\"Cannot read data task.\");\n+    } else {\n+      switch (task.file().format()) {\n+        case PARQUET:\n+          iter = newParquetIterable(task, idToConstant);\n+          break;\n+\n+        case AVRO:\n+          iter = newAvroIterable(task, idToConstant);\n+          break;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135"}, "originalPosition": 77}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDczMDgzOTE4", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-473083918", "createdAt": "2020-08-24T02:25:45Z", "commit": {"oid": "3cdaad4303f41dd81647806db26acfdad3f615a3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwMjoyNTo0NVrOHFSnJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwMjoyNTo0NVrOHFSnJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMwOTg2MA==", "bodyText": "This fixes Flink Orc Reader (with partition) bug.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r475309860", "createdAt": "2020-08-24T02:25:45Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkOrcReaders.java", "diffHunk": "@@ -246,7 +256,7 @@ public void setBatchContext(long batchOffsetInFile) {\n \n     StructReader(List<OrcValueReader<?>> readers, Types.StructType struct, Map<Integer, ?> idToConstant) {\n       super(readers, struct, idToConstant);\n-      this.numFields = readers.size();\n+      this.numFields = struct.fields().size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3cdaad4303f41dd81647806db26acfdad3f615a3"}, "originalPosition": 36}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDczMDk4Mjcz", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-473098273", "createdAt": "2020-08-24T03:30:28Z", "commit": {"oid": "caaf5883be2532a1c5a9911e02609184ed02c289"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1MDgwMTA5", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-475080109", "createdAt": "2020-08-26T00:57:20Z", "commit": {"oid": "caaf5883be2532a1c5a9911e02609184ed02c289"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMDo1NzoyMFrOHG1h6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMDo1NzoyMFrOHG1h6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzMDUzNw==", "bodyText": "Was this not used anywhere?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r476930537", "createdAt": "2020-08-26T00:57:20Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkOrcReader.java", "diffHunk": "@@ -39,18 +39,14 @@\n public class FlinkOrcReader implements OrcRowReader<RowData> {\n   private final OrcValueReader<?> reader;\n \n-  private FlinkOrcReader(Schema iSchema, TypeDescription readSchema) {\n+  public FlinkOrcReader(Schema iSchema, TypeDescription readSchema) {\n     this(iSchema, readSchema, ImmutableMap.of());\n   }\n \n-  private FlinkOrcReader(Schema iSchema, TypeDescription readSchema, Map<Integer, ?> idToConstant) {\n+  public FlinkOrcReader(Schema iSchema, TypeDescription readSchema, Map<Integer, ?> idToConstant) {\n     this.reader = OrcSchemaWithTypeVisitor.visit(iSchema, readSchema, new ReadBuilder(idToConstant));\n   }\n \n-  public static OrcRowReader<RowData> buildReader(Schema schema, TypeDescription readSchema) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caaf5883be2532a1c5a9911e02609184ed02c289"}, "originalPosition": 14}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1MDgwMjU4", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-475080258", "createdAt": "2020-08-26T00:57:45Z", "commit": {"oid": "caaf5883be2532a1c5a9911e02609184ed02c289"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMDo1Nzo0NVrOHG1jNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMDo1Nzo0NVrOHG1jNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzMDg3MA==", "bodyText": "Does Flink require this?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r476930870", "createdAt": "2020-08-26T00:57:45Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkOrcReaders.java", "diffHunk": "@@ -127,6 +128,11 @@ public Integer nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public DecimalData nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n+\n+      // The hive ORC writer may will adjust the scale of decimal data.\n+      Preconditions.checkArgument(value.precision() <= precision,\n+          \"Cannot read value as decimal(%s,%s), too large: %s\", precision, scale, value);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caaf5883be2532a1c5a9911e02609184ed02c289"}, "originalPosition": 15}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1MDgxNjA5", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-475081609", "createdAt": "2020-08-26T01:02:11Z", "commit": {"oid": "caaf5883be2532a1c5a9911e02609184ed02c289"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMTowMjoxMVrOHG1vWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMTowMjoxMVrOHG1vWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzMzk3Nw==", "bodyText": "Nit: int in millis?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r476933977", "createdAt": "2020-08-26T01:02:11Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+/**\n+ * Base class of Flink iterators.\n+ *\n+ * @param <T> is the Java class returned by this iterator whose objects contain one or more rows.\n+ */\n+abstract class DataIterator<T> implements CloseableIterator<T> {\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;\n+  private final EncryptionManager encryption;\n+\n+  private CloseableIterator<T> currentIterator;\n+\n+  DataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption) {\n+    this.tasks = task.files().iterator();\n+    this.fileIo = fileIo;\n+    this.encryption = encryption;\n+    this.currentIterator = CloseableIterator.empty();\n+  }\n+\n+  InputFile getInputFile(FileScanTask task) {\n+    Preconditions.checkArgument(!task.isDataTask(), \"Invalid task type\");\n+    return encryption.decrypt(EncryptedFiles.encryptedInput(\n+        fileIo.newInputFile(task.file().path().toString()),\n+        task.file().keyMetadata()));\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    updateCurrentIterator();\n+    return currentIterator.hasNext();\n+  }\n+\n+  @Override\n+  public T next() {\n+    updateCurrentIterator();\n+    return currentIterator.next();\n+  }\n+\n+  /**\n+   * Updates the current iterator field to ensure that the current Iterator\n+   * is not exhausted.\n+   */\n+  private void updateCurrentIterator() {\n+    try {\n+      while (!currentIterator.hasNext() && tasks.hasNext()) {\n+        currentIterator.close();\n+        currentIterator = openTaskIterator(tasks.next());\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  abstract CloseableIterator<T> openTaskIterator(FileScanTask scanTask) throws IOException;\n+\n+  @Override\n+  public void close() throws IOException {\n+    // close the current iterator\n+    this.currentIterator.close();\n+\n+    // exhaust the task iterator\n+    while (tasks.hasNext()) {\n+      tasks.next();\n+    }\n+  }\n+\n+  static Object convertConstant(Type type, Object value) {\n+    if (value == null) {\n+      return null;\n+    }\n+\n+    switch (type.typeId()) {\n+      case DECIMAL: // DecimalData\n+        Types.DecimalType decimal = (Types.DecimalType) type;\n+        return DecimalData.fromBigDecimal((BigDecimal) value, decimal.precision(), decimal.scale());\n+      case STRING: // StringData\n+        if (value instanceof Utf8) {\n+          Utf8 utf8 = (Utf8) value;\n+          return StringData.fromBytes(utf8.getBytes(), 0, utf8.getByteLength());\n+        }\n+        return StringData.fromString(value.toString());\n+      case FIXED: // byte[]\n+        if (value instanceof byte[]) {\n+          return value;\n+        } else if (value instanceof GenericData.Fixed) {\n+          return ((GenericData.Fixed) value).bytes();\n+        }\n+        return ByteBuffers.toByteArray((ByteBuffer) value);\n+      case BINARY: // byte[]\n+        return ByteBuffers.toByteArray((ByteBuffer) value);\n+      case TIME: // int instead of long", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caaf5883be2532a1c5a9911e02609184ed02c289"}, "originalPosition": 136}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1MDgxODU1", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-475081855", "createdAt": "2020-08-26T01:03:03Z", "commit": {"oid": "caaf5883be2532a1c5a9911e02609184ed02c289"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMTowMzowM1rOHG1x2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMTowMzowM1rOHG1x2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzNDYxOA==", "bodyText": "Is this needed?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r476934618", "createdAt": "2020-08-26T01:03:03Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.io.DefaultInputSplitAssigner;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+/**\n+ * Flink {@link InputFormat} for Iceberg.\n+ */\n+public class FlinkInputFormat extends RichInputFormat<RowData, FlinkInputSplit> {\n+\n+  private static final long serialVersionUID = 1L;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caaf5883be2532a1c5a9911e02609184ed02c289"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1MTEzNDY0", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-475113464", "createdAt": "2020-08-26T02:47:59Z", "commit": {"oid": "caaf5883be2532a1c5a9911e02609184ed02c289"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMjo0ODowMFrOHG5yqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMjo0ODowMFrOHG5yqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwMDM2MQ==", "bodyText": "Q:  do you think whether there's need to abstract the common options builder sharing between flink and spark (maybe also hive/pig)  to validate and build those properties into a ScanOptions ?  If sure,  we may finish that in a new separate pr.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477000361", "createdAt": "2020-08-26T02:48:00Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanOptions.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Map;\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+import org.apache.flink.configuration.Configuration;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class ScanOptions implements Serializable {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  public static final ConfigOption<Long> SNAPSHOT_ID =\n+      ConfigOptions.key(\"snapshot-id\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Boolean> CASE_SENSITIVE =\n+      ConfigOptions.key(\"case-sensitive\").booleanType().defaultValue(false);\n+\n+  public static final ConfigOption<Long> AS_OF_TIMESTAMP =\n+      ConfigOptions.key(\"as-of-timestamp\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Long> START_SNAPSHOT_ID =\n+      ConfigOptions.key(\"start-snapshot-id\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Long> END_SNAPSHOT_ID =\n+      ConfigOptions.key(\"end-snapshot-id\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Long> SPLIT_SIZE =\n+      ConfigOptions.key(\"split-size\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Integer> SPLIT_LOOKBACK =\n+      ConfigOptions.key(\"split-lookback\").intType().defaultValue(null);\n+\n+  public static final ConfigOption<Long> SPLIT_FILE_OPEN_COST =\n+      ConfigOptions.key(\"split-file-open-cost\").longType().defaultValue(null);\n+\n+  private final boolean caseSensitive;\n+  private final Long snapshotId;\n+  private final Long startSnapshotId;\n+  private final Long endSnapshotId;\n+  private final Long asOfTimestamp;\n+  private final Long splitSize;\n+  private final Integer splitLookback;\n+  private final Long splitOpenFileCost;\n+  private final String nameMapping;\n+\n+  public ScanOptions(boolean caseSensitive, Long snapshotId, Long startSnapshotId, Long endSnapshotId,\n+                     Long asOfTimestamp, Long splitSize, Integer splitLookback, Long splitOpenFileCost,\n+                     String nameMapping) {\n+    this.caseSensitive = caseSensitive;\n+    this.snapshotId = snapshotId;\n+    this.startSnapshotId = startSnapshotId;\n+    this.endSnapshotId = endSnapshotId;\n+    this.asOfTimestamp = asOfTimestamp;\n+    this.splitSize = splitSize;\n+    this.splitLookback = splitLookback;\n+    this.splitOpenFileCost = splitOpenFileCost;\n+    this.nameMapping = nameMapping;\n+  }\n+\n+  public boolean isCaseSensitive() {\n+    return caseSensitive;\n+  }\n+\n+  public Long getSnapshotId() {\n+    return snapshotId;\n+  }\n+\n+  public Long getStartSnapshotId() {\n+    return startSnapshotId;\n+  }\n+\n+  public Long getEndSnapshotId() {\n+    return endSnapshotId;\n+  }\n+\n+  public Long getAsOfTimestamp() {\n+    return asOfTimestamp;\n+  }\n+\n+  public Long getSplitSize() {\n+    return splitSize;\n+  }\n+\n+  public Integer getSplitLookback() {\n+    return splitLookback;\n+  }\n+\n+  public Long getSplitOpenFileCost() {\n+    return splitOpenFileCost;\n+  }\n+\n+  public String getNameMapping() {\n+    return nameMapping;\n+  }\n+\n+  public static Builder builder() {\n+    return new Builder();\n+  }\n+\n+  public static ScanOptions of(Map<String, String> options) {\n+    return builder().options(options).build();\n+  }\n+\n+  public static final class Builder {\n+    private boolean caseSensitive = CASE_SENSITIVE.defaultValue();\n+    private Long snapshotId = SNAPSHOT_ID.defaultValue();\n+    private Long startSnapshotId = START_SNAPSHOT_ID.defaultValue();\n+    private Long endSnapshotId = END_SNAPSHOT_ID.defaultValue();\n+    private Long asOfTimestamp = AS_OF_TIMESTAMP.defaultValue();\n+    private Long splitSize = SPLIT_SIZE.defaultValue();\n+    private Integer splitLookback = SPLIT_LOOKBACK.defaultValue();\n+    private Long splitOpenFileCost = SPLIT_FILE_OPEN_COST.defaultValue();\n+    private String nameMapping;\n+\n+    private Builder() {\n+    }\n+\n+    public Builder options(Map<String, String> options) {\n+      Configuration config = new Configuration();\n+      options.forEach(config::setString);\n+      this.caseSensitive = config.get(CASE_SENSITIVE);\n+      this.snapshotId = config.get(SNAPSHOT_ID);\n+      this.asOfTimestamp = config.get(AS_OF_TIMESTAMP);\n+      this.startSnapshotId = config.get(START_SNAPSHOT_ID);\n+      this.endSnapshotId = config.get(END_SNAPSHOT_ID);\n+      this.splitSize = config.get(SPLIT_SIZE);\n+      this.splitLookback = config.get(SPLIT_LOOKBACK);\n+      this.splitOpenFileCost = config.get(SPLIT_FILE_OPEN_COST);\n+      this.nameMapping = options.get(DEFAULT_NAME_MAPPING);\n+      return this;\n+    }\n+\n+    public Builder caseSensitive(boolean newCaseSensitive) {\n+      this.caseSensitive = newCaseSensitive;\n+      return this;\n+    }\n+\n+    public Builder snapshotId(Long newSnapshotId) {\n+      this.snapshotId = newSnapshotId;\n+      return this;\n+    }\n+\n+    public Builder startSnapshotId(Long newStartSnapshotId) {\n+      this.startSnapshotId = newStartSnapshotId;\n+      return this;\n+    }\n+\n+    public Builder endSnapshotId(Long newEndSnapshotId) {\n+      this.endSnapshotId = newEndSnapshotId;\n+      return this;\n+    }\n+\n+    public Builder asOfTimestamp(Long newAsOfTimestamp) {\n+      this.asOfTimestamp = newAsOfTimestamp;\n+      return this;\n+    }\n+\n+    public Builder splitSize(Long newSplitSize) {\n+      this.splitSize = newSplitSize;\n+      return this;\n+    }\n+\n+    public Builder splitLookback(Integer newSplitLookback) {\n+      this.splitLookback = newSplitLookback;\n+      return this;\n+    }\n+\n+    public Builder splitOpenFileCost(Long newSplitOpenFileCost) {\n+      this.splitOpenFileCost = newSplitOpenFileCost;\n+      return this;\n+    }\n+\n+    public Builder nameMapping(String newNameMapping) {\n+      this.nameMapping = newNameMapping;\n+      return this;\n+    }\n+\n+    public ScanOptions build() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caaf5883be2532a1c5a9911e02609184ed02c289"}, "originalPosition": 200}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1MTE3MDQ3", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-475117047", "createdAt": "2020-08-26T02:59:55Z", "commit": {"oid": "caaf5883be2532a1c5a9911e02609184ed02c289"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMjo1OTo1NVrOHG5_MQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwMjo1OTo1NVrOHG5_MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwMzU2OQ==", "bodyText": "Q: Do we need more cases to address the other partitioned data type in https://github.com/apache/iceberg/pull/1346/files#diff-84728688cba8556f9ff91f32d3873efcR112 ?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477003569", "createdAt": "2020-08-26T02:59:55Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.conversion.DataStructureConverter;\n+import org.apache.flink.table.data.conversion.DataStructureConverters;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TestHelpers;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericAppenderHelper;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.CatalogLoader;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.RowDataConverter;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.DateTimeUtil;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestFlinkScan extends AbstractTestBase {\n+\n+  private static final Schema SCHEMA = new Schema(\n+          required(1, \"data\", Types.StringType.get()),\n+          required(2, \"id\", Types.LongType.get()),\n+          required(3, \"dt\", Types.StringType.get()));\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+          .identity(\"dt\")\n+          .bucket(\"id\", 1)\n+          .build();\n+\n+  // before variables\n+  private Configuration conf;\n+  String warehouse;\n+  private HadoopCatalog catalog;\n+\n+  // parametrized variables\n+  private final FileFormat fileFormat;\n+\n+  @Parameterized.Parameters(name = \"format={0}\")\n+  public static Object[] parameters() {\n+    return new Object[] {\"avro\", \"parquet\", \"orc\"};\n+  }\n+\n+  TestFlinkScan(String fileFormat) {\n+    this.fileFormat = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File warehouseFile = TEMPORARY_FOLDER.newFolder();\n+    Assert.assertTrue(warehouseFile.delete());\n+    conf = new Configuration();\n+    warehouse = \"file:\" + warehouseFile;\n+    catalog = new HadoopCatalog(conf, warehouse);\n+  }\n+\n+  private List<Row> execute(Table table) throws IOException {\n+    return executeWithOptions(table, null, null, null, null, null, null, null, null);\n+  }\n+\n+  private List<Row> execute(Table table, List<String> projectFields) throws IOException {\n+    return executeWithOptions(table, projectFields, null, null, null, null, null, null, null);\n+  }\n+\n+  protected abstract List<Row> executeWithOptions(\n+      Table table, List<String> projectFields, CatalogLoader loader, Long snapshotId,\n+      Long startSnapshotId, Long endSnapshotId, Long asOfTimestamp, List<Expression> filters, String sqlFilter)\n+      throws IOException;\n+\n+  protected abstract void assertResiduals(Schema schema, List<Row> results, List<Record> writeRecords,\n+                                          List<Record> filteredRecords) throws IOException;\n+\n+  @Test\n+  public void testUnpartitionedTable() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA);\n+    List<Record> expectedRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(expectedRecords);\n+    assertRecords(execute(table), expectedRecords, SCHEMA);\n+  }\n+\n+  @Test\n+  public void testPartitionedTable() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+    List<Record> expectedRecords = RandomGenericData.generate(SCHEMA, 1, 0L);\n+    expectedRecords.get(0).set(2, \"2020-03-20\");\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(\n+        org.apache.iceberg.TestHelpers.Row.of(\"2020-03-20\", 0), expectedRecords);\n+    assertRecords(execute(table), expectedRecords, SCHEMA);\n+  }\n+\n+  @Test\n+  public void testProjection() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+    List<Record> inputRecords = RandomGenericData.generate(SCHEMA, 1, 0L);\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(\n+        org.apache.iceberg.TestHelpers.Row.of(\"2020-03-20\", 0), inputRecords);\n+    assertRows(execute(table, Collections.singletonList(\"data\")), Row.of(inputRecords.get(0).get(0)));\n+  }\n+\n+  @Test\n+  public void testIdentityPartitionProjections() throws Exception {\n+    Schema logSchema = new Schema(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "caaf5883be2532a1c5a9911e02609184ed02c289"}, "originalPosition": 152}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1MjkyNTA4", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-475292508", "createdAt": "2020-08-26T09:01:13Z", "commit": {"oid": "8466f09738198c472132e4307f471b7c5740f9c3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwOTowMToxM1rOHHCvdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwOTowMToxM1rOHHCvdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE0Njk5OA==", "bodyText": "We need to move this line out of the for loop ?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477146998", "createdAt": "2020-08-26T09:01:13Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.io.DefaultInputSplitAssigner;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+/**\n+ * Flink {@link InputFormat} for Iceberg.\n+ */\n+public class FlinkInputFormat extends RichInputFormat<RowData, FlinkInputSplit> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final ScanOptions options;\n+  private final List<Expression> filterExpressions;\n+  private final FileIO io;\n+  private final EncryptionManager encryption;\n+  private final SerializableConfiguration serializableConf;\n+\n+  private transient RowDataIterator iterator;\n+\n+  private FlinkInputFormat(\n+      TableLoader tableLoader, Schema projectedSchema, FileIO io, EncryptionManager encryption,\n+      List<Expression> filterExpressions, ScanOptions options, SerializableConfiguration serializableConf) {\n+    this.tableLoader = tableLoader;\n+    this.projectedSchema = projectedSchema;\n+    this.options = options;\n+    this.filterExpressions = filterExpressions;\n+    this.io = io;\n+    this.encryption = encryption;\n+    this.serializableConf = serializableConf;\n+  }\n+\n+  @VisibleForTesting\n+  Schema projectedSchema() {\n+    return projectedSchema;\n+  }\n+\n+  @Override\n+  public BaseStatistics getStatistics(BaseStatistics cachedStatistics) {\n+    // Legacy method, not be used.\n+    return null;\n+  }\n+\n+  @Override\n+  public FlinkInputSplit[] createInputSplits(int minNumSplits) throws IOException {\n+    // Called in Job manager, so it is OK to load table from catalog.\n+    tableLoader.open(serializableConf.get());\n+    try (TableLoader loader = tableLoader) {\n+      Table table = loader.loadTable();\n+      FlinkSplitGenerator generator = new FlinkSplitGenerator(table, projectedSchema, options, filterExpressions);\n+      return generator.createInputSplits();\n+    }\n+  }\n+\n+  @Override\n+  public InputSplitAssigner getInputSplitAssigner(FlinkInputSplit[] inputSplits) {\n+    return new DefaultInputSplitAssigner(inputSplits);\n+  }\n+\n+  @Override\n+  public void configure(Configuration parameters) {\n+  }\n+\n+  @Override\n+  public void open(FlinkInputSplit split) {\n+    this.iterator = new RowDataIterator(split.getTask(), io, encryption, projectedSchema,\n+                                        options.getNameMapping(), options.isCaseSensitive());\n+  }\n+\n+  @Override\n+  public boolean reachedEnd() {\n+    return !iterator.hasNext();\n+  }\n+\n+  @Override\n+  public RowData nextRecord(RowData reuse) {\n+    return iterator.next();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (iterator != null) {\n+      iterator.close();\n+    }\n+  }\n+\n+  public static Builder builder() {\n+    return new Builder();\n+  }\n+\n+  public static final class Builder {\n+    private TableLoader tableLoader;\n+    private Schema icebergSchema;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private FileIO io;\n+    private EncryptionManager encryption;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private Builder() {\n+    }\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.icebergSchema = newTable.schema();\n+      this.io = newTable.io();\n+      this.encryption = newTable.encryption();\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder icebergSchema(Schema newSchema) {\n+      this.icebergSchema = newSchema;\n+      return this;\n+    }\n+\n+    public Builder io(FileIO newIO) {\n+      this.io = newIO;\n+      return this;\n+    }\n+\n+    public Builder encryption(EncryptionManager newEncryption) {\n+      this.encryption = newEncryption;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(org.apache.hadoop.conf.Configuration newConf) {\n+      this.hadoopConf = newConf;\n+      return this;\n+    }\n+\n+    public FlinkInputFormat build() {\n+      Preconditions.checkNotNull(tableLoader, \"TableLoader should not be null.\");\n+\n+      hadoopConf = hadoopConf == null ? FlinkCatalogFactory.clusterHadoopConf() : hadoopConf;\n+\n+      // load required fields by table loader.\n+      if (icebergSchema == null || io == null || encryption == null) {\n+        tableLoader.open(hadoopConf);\n+        try (TableLoader loader = tableLoader) {\n+          Table table = loader.loadTable();\n+          this.icebergSchema = table.schema();\n+          this.io = table.io();\n+          this.encryption = table.encryption();\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(e);\n+        }\n+      }\n+\n+      if (projectedSchema != null && selectedFields != null) {\n+        throw new IllegalArgumentException(\n+            \"Cannot using both requestedSchema and projectedFields to project.\");\n+      }\n+\n+      TableSchema flinkProjectedSchema = projectedSchema;\n+\n+      if (selectedFields != null) {\n+        TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(icebergSchema));\n+        TableSchema.Builder builder = TableSchema.builder();\n+        for (String field : selectedFields) {\n+          TableColumn column = tableSchema.getTableColumn(field).orElseThrow(() -> new IllegalArgumentException(\n+              \"The fields are illegal in projectedFields: \" + selectedFields));\n+          builder.field(column.getName(), column.getType());\n+          flinkProjectedSchema = builder.build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8466f09738198c472132e4307f471b7c5740f9c3"}, "originalPosition": 241}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1MjkzNjEx", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-475293611", "createdAt": "2020-08-26T09:02:36Z", "commit": {"oid": "8466f09738198c472132e4307f471b7c5740f9c3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwOTowMjozN1rOHHCyww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwOTowMjozN1rOHHCyww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE0Nzg0Mw==", "bodyText": "I think we need to point out which column is missing in the error message .", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477147843", "createdAt": "2020-08-26T09:02:37Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.io.DefaultInputSplitAssigner;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+/**\n+ * Flink {@link InputFormat} for Iceberg.\n+ */\n+public class FlinkInputFormat extends RichInputFormat<RowData, FlinkInputSplit> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final ScanOptions options;\n+  private final List<Expression> filterExpressions;\n+  private final FileIO io;\n+  private final EncryptionManager encryption;\n+  private final SerializableConfiguration serializableConf;\n+\n+  private transient RowDataIterator iterator;\n+\n+  private FlinkInputFormat(\n+      TableLoader tableLoader, Schema projectedSchema, FileIO io, EncryptionManager encryption,\n+      List<Expression> filterExpressions, ScanOptions options, SerializableConfiguration serializableConf) {\n+    this.tableLoader = tableLoader;\n+    this.projectedSchema = projectedSchema;\n+    this.options = options;\n+    this.filterExpressions = filterExpressions;\n+    this.io = io;\n+    this.encryption = encryption;\n+    this.serializableConf = serializableConf;\n+  }\n+\n+  @VisibleForTesting\n+  Schema projectedSchema() {\n+    return projectedSchema;\n+  }\n+\n+  @Override\n+  public BaseStatistics getStatistics(BaseStatistics cachedStatistics) {\n+    // Legacy method, not be used.\n+    return null;\n+  }\n+\n+  @Override\n+  public FlinkInputSplit[] createInputSplits(int minNumSplits) throws IOException {\n+    // Called in Job manager, so it is OK to load table from catalog.\n+    tableLoader.open(serializableConf.get());\n+    try (TableLoader loader = tableLoader) {\n+      Table table = loader.loadTable();\n+      FlinkSplitGenerator generator = new FlinkSplitGenerator(table, projectedSchema, options, filterExpressions);\n+      return generator.createInputSplits();\n+    }\n+  }\n+\n+  @Override\n+  public InputSplitAssigner getInputSplitAssigner(FlinkInputSplit[] inputSplits) {\n+    return new DefaultInputSplitAssigner(inputSplits);\n+  }\n+\n+  @Override\n+  public void configure(Configuration parameters) {\n+  }\n+\n+  @Override\n+  public void open(FlinkInputSplit split) {\n+    this.iterator = new RowDataIterator(split.getTask(), io, encryption, projectedSchema,\n+                                        options.getNameMapping(), options.isCaseSensitive());\n+  }\n+\n+  @Override\n+  public boolean reachedEnd() {\n+    return !iterator.hasNext();\n+  }\n+\n+  @Override\n+  public RowData nextRecord(RowData reuse) {\n+    return iterator.next();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (iterator != null) {\n+      iterator.close();\n+    }\n+  }\n+\n+  public static Builder builder() {\n+    return new Builder();\n+  }\n+\n+  public static final class Builder {\n+    private TableLoader tableLoader;\n+    private Schema icebergSchema;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private FileIO io;\n+    private EncryptionManager encryption;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private Builder() {\n+    }\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.icebergSchema = newTable.schema();\n+      this.io = newTable.io();\n+      this.encryption = newTable.encryption();\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder icebergSchema(Schema newSchema) {\n+      this.icebergSchema = newSchema;\n+      return this;\n+    }\n+\n+    public Builder io(FileIO newIO) {\n+      this.io = newIO;\n+      return this;\n+    }\n+\n+    public Builder encryption(EncryptionManager newEncryption) {\n+      this.encryption = newEncryption;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(org.apache.hadoop.conf.Configuration newConf) {\n+      this.hadoopConf = newConf;\n+      return this;\n+    }\n+\n+    public FlinkInputFormat build() {\n+      Preconditions.checkNotNull(tableLoader, \"TableLoader should not be null.\");\n+\n+      hadoopConf = hadoopConf == null ? FlinkCatalogFactory.clusterHadoopConf() : hadoopConf;\n+\n+      // load required fields by table loader.\n+      if (icebergSchema == null || io == null || encryption == null) {\n+        tableLoader.open(hadoopConf);\n+        try (TableLoader loader = tableLoader) {\n+          Table table = loader.loadTable();\n+          this.icebergSchema = table.schema();\n+          this.io = table.io();\n+          this.encryption = table.encryption();\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(e);\n+        }\n+      }\n+\n+      if (projectedSchema != null && selectedFields != null) {\n+        throw new IllegalArgumentException(\n+            \"Cannot using both requestedSchema and projectedFields to project.\");\n+      }\n+\n+      TableSchema flinkProjectedSchema = projectedSchema;\n+\n+      if (selectedFields != null) {\n+        TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(icebergSchema));\n+        TableSchema.Builder builder = TableSchema.builder();\n+        for (String field : selectedFields) {\n+          TableColumn column = tableSchema.getTableColumn(field).orElseThrow(() -> new IllegalArgumentException(\n+              \"The fields are illegal in projectedFields: \" + selectedFields));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8466f09738198c472132e4307f471b7c5740f9c3"}, "originalPosition": 239}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc1MzMwMzY4", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-475330368", "createdAt": "2020-08-26T09:49:39Z", "commit": {"oid": "20a81915385b70ec3de115e043700e3ee4ed8a1c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwOTo0OTozOVrOHHEoeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwOTo0OTozOVrOHHEoeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE3Nzk3Ng==", "bodyText": "Do we really need those three methods ?  I saw that we would loadTable and override all of the three if anyone is null, that says setting one or two of them won't work in this builder.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477177976", "createdAt": "2020-08-26T09:49:39Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.io.DefaultInputSplitAssigner;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+/**\n+ * Flink {@link InputFormat} for Iceberg.\n+ */\n+public class FlinkInputFormat extends RichInputFormat<RowData, FlinkInputSplit> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final ScanOptions options;\n+  private final List<Expression> filterExpressions;\n+  private final FileIO io;\n+  private final EncryptionManager encryption;\n+  private final SerializableConfiguration serializableConf;\n+\n+  private transient RowDataIterator iterator;\n+\n+  private FlinkInputFormat(\n+      TableLoader tableLoader, Schema projectedSchema, FileIO io, EncryptionManager encryption,\n+      List<Expression> filterExpressions, ScanOptions options, SerializableConfiguration serializableConf) {\n+    this.tableLoader = tableLoader;\n+    this.projectedSchema = projectedSchema;\n+    this.options = options;\n+    this.filterExpressions = filterExpressions;\n+    this.io = io;\n+    this.encryption = encryption;\n+    this.serializableConf = serializableConf;\n+  }\n+\n+  @VisibleForTesting\n+  Schema projectedSchema() {\n+    return projectedSchema;\n+  }\n+\n+  @Override\n+  public BaseStatistics getStatistics(BaseStatistics cachedStatistics) {\n+    // Legacy method, not be used.\n+    return null;\n+  }\n+\n+  @Override\n+  public FlinkInputSplit[] createInputSplits(int minNumSplits) throws IOException {\n+    // Called in Job manager, so it is OK to load table from catalog.\n+    tableLoader.open(serializableConf.get());\n+    try (TableLoader loader = tableLoader) {\n+      Table table = loader.loadTable();\n+      FlinkSplitGenerator generator = new FlinkSplitGenerator(table, projectedSchema, options, filterExpressions);\n+      return generator.createInputSplits();\n+    }\n+  }\n+\n+  @Override\n+  public InputSplitAssigner getInputSplitAssigner(FlinkInputSplit[] inputSplits) {\n+    return new DefaultInputSplitAssigner(inputSplits);\n+  }\n+\n+  @Override\n+  public void configure(Configuration parameters) {\n+  }\n+\n+  @Override\n+  public void open(FlinkInputSplit split) {\n+    this.iterator = new RowDataIterator(split.getTask(), io, encryption, projectedSchema,\n+                                        options.getNameMapping(), options.isCaseSensitive());\n+  }\n+\n+  @Override\n+  public boolean reachedEnd() {\n+    return !iterator.hasNext();\n+  }\n+\n+  @Override\n+  public RowData nextRecord(RowData reuse) {\n+    return iterator.next();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (iterator != null) {\n+      iterator.close();\n+    }\n+  }\n+\n+  public static Builder builder() {\n+    return new Builder();\n+  }\n+\n+  public static final class Builder {\n+    private TableLoader tableLoader;\n+    private Schema icebergSchema;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private FileIO io;\n+    private EncryptionManager encryption;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private Builder() {\n+    }\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.icebergSchema = newTable.schema();\n+      this.io = newTable.io();\n+      this.encryption = newTable.encryption();\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder icebergSchema(Schema newSchema) {\n+      this.icebergSchema = newSchema;\n+      return this;\n+    }\n+\n+    public Builder io(FileIO newIO) {\n+      this.io = newIO;\n+      return this;\n+    }\n+\n+    public Builder encryption(EncryptionManager newEncryption) {\n+      this.encryption = newEncryption;\n+      return this;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20a81915385b70ec3de115e043700e3ee4ed8a1c"}, "originalPosition": 202}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "41764797cfcdca315d9661b78964df7a07264710", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/41764797cfcdca315d9661b78964df7a07264710", "committedDate": "2020-08-27T02:28:00Z", "message": "checkstyle"}, "afterCommit": {"oid": "72d8fe47116364d66cfbf46bc703ce3e6d12a7d3", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/72d8fe47116364d66cfbf46bc703ce3e6d12a7d3", "committedDate": "2020-08-28T02:47:38Z", "message": "Flink: Introduce Flink InputFormat"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc5MzM2MDc5", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-479336079", "createdAt": "2020-09-01T02:06:11Z", "commit": {"oid": "72d8fe47116364d66cfbf46bc703ce3e6d12a7d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMjowNjoxMlrOHKVExw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMjowNjoxMlrOHKVExw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU5MzA5NQ==", "bodyText": "We have a discussion in #1302 for removing UUID. Is this a temporary solution?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r480593095", "createdAt": "2020-09-01T02:06:12Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkFixupTypes.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.FixupTypes;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+\n+/**\n+ * The uuid and fixed are converted to the same Flink type. Conversion back can produce only one,\n+ * which may not be correct.\n+ */\n+class FlinkFixupTypes extends FixupTypes {\n+\n+  private FlinkFixupTypes(Schema referenceSchema) {\n+    super(referenceSchema);\n+  }\n+\n+  static Schema fixup(Schema schema, Schema referenceSchema) {\n+    return new Schema(TypeUtil.visit(schema,\n+        new FlinkFixupTypes(referenceSchema)).asStructType().fields());\n+  }\n+\n+  @Override\n+  protected boolean fixupPrimitive(Type.PrimitiveType type, Type source) {\n+    if (type instanceof Types.FixedType) {\n+      int length = ((Types.FixedType) type).length();\n+      return source.typeId() == Type.TypeID.UUID && length == 16;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72d8fe47116364d66cfbf46bc703ce3e6d12a7d3"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc5MzQxMTY2", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-479341166", "createdAt": "2020-09-01T02:09:55Z", "commit": {"oid": "72d8fe47116364d66cfbf46bc703ce3e6d12a7d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMjowOTo1NVrOHKVThA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMjowOTo1NVrOHKVThA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU5Njg2OA==", "bodyText": "Why we need to exhaust tasks?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r480596868", "createdAt": "2020-09-01T02:09:55Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+/**\n+ * Base class of Flink iterators.\n+ *\n+ * @param <T> is the Java class returned by this iterator whose objects contain one or more rows.\n+ */\n+abstract class DataIterator<T> implements CloseableIterator<T> {\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;\n+  private final EncryptionManager encryption;\n+\n+  private CloseableIterator<T> currentIterator;\n+\n+  DataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption) {\n+    this.tasks = task.files().iterator();\n+    this.fileIo = fileIo;\n+    this.encryption = encryption;\n+    this.currentIterator = CloseableIterator.empty();\n+  }\n+\n+  InputFile getInputFile(FileScanTask task) {\n+    Preconditions.checkArgument(!task.isDataTask(), \"Invalid task type\");\n+    return encryption.decrypt(EncryptedFiles.encryptedInput(\n+        fileIo.newInputFile(task.file().path().toString()),\n+        task.file().keyMetadata()));\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    updateCurrentIterator();\n+    return currentIterator.hasNext();\n+  }\n+\n+  @Override\n+  public T next() {\n+    updateCurrentIterator();\n+    return currentIterator.next();\n+  }\n+\n+  /**\n+   * Updates the current iterator field to ensure that the current Iterator\n+   * is not exhausted.\n+   */\n+  private void updateCurrentIterator() {\n+    try {\n+      while (!currentIterator.hasNext() && tasks.hasNext()) {\n+        currentIterator.close();\n+        currentIterator = openTaskIterator(tasks.next());\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  abstract CloseableIterator<T> openTaskIterator(FileScanTask scanTask) throws IOException;\n+\n+  @Override\n+  public void close() throws IOException {\n+    // close the current iterator\n+    this.currentIterator.close();\n+\n+    // exhaust the task iterator", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72d8fe47116364d66cfbf46bc703ce3e6d12a7d3"}, "originalPosition": 106}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc5MzcwNDMx", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-479370431", "createdAt": "2020-09-01T03:09:54Z", "commit": {"oid": "72d8fe47116364d66cfbf46bc703ce3e6d12a7d3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMzowOTo1NFrOHKaFPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMzowOTo1NFrOHKaFPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY3NTEzNA==", "bodyText": "nit: typo shcema ->schema.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r480675134", "createdAt": "2020-09-01T03:09:54Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.conversion.DataStructureConverter;\n+import org.apache.flink.table.data.conversion.DataStructureConverters;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.CatalogLoader;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+/**\n+ * Test {@link FlinkInputFormat}.\n+ */\n+public class TestFlinkInputFormat extends TestFlinkScan {\n+\n+  private FlinkInputFormat.Builder builder;\n+\n+  public TestFlinkInputFormat(String fileFormat) {\n+    super(fileFormat);\n+  }\n+\n+  @Override\n+  public void before() throws IOException {\n+    super.before();\n+    builder = FlinkInputFormat.builder().tableLoader(TableLoader.fromHadoopTable(warehouse + \"/default/t\"));\n+  }\n+\n+  @Override\n+  protected List<Row> executeWithOptions(\n+      Table table, List<String> projectFields, CatalogLoader loader, Long snapshotId, Long startSnapshotId,\n+      Long endSnapshotId, Long asOfTimestamp, List<Expression> filters, String sqlFilter) throws IOException {\n+    ScanOptions options = ScanOptions.builder().snapshotId(snapshotId).startSnapshotId(startSnapshotId)\n+        .endSnapshotId(endSnapshotId).asOfTimestamp(asOfTimestamp).build();\n+    if (loader != null) {\n+      builder.tableLoader(TableLoader.fromCatalog(loader, TableIdentifier.of(\"default\", \"t\")));\n+    }\n+\n+    return run(builder.select(projectFields).filters(filters).options(options).build());\n+  }\n+\n+  @Override\n+  protected void assertResiduals(\n+      Schema shcema, List<Row> results, List<Record> writeRecords, List<Record> filteredRecords) {\n+    // can not filter the data.\n+    assertRecords(results, writeRecords, shcema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "72d8fe47116364d66cfbf46bc703ce3e6d12a7d3"}, "originalPosition": 76}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg1NzMwNTg2", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-485730586", "createdAt": "2020-09-10T09:04:03Z", "commit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwOTowNDowNFrOHPqLvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwOTowNDowNFrOHPqLvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjE4MTgyMQ==", "bodyText": "Q:  is it possible that  we will step into this if block ?  I saw PartitionData will transform the Utf8 to String ?\n\n  \n    \n      iceberg/core/src/main/java/org/apache/iceberg/PartitionData.java\n    \n    \n         Line 142\n      in\n      c28d1c8\n    \n    \n    \n    \n\n        \n          \n           data[pos] = value.toString();", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r486181821", "createdAt": "2020-09-10T09:04:04Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+/**\n+ * Base class of Flink iterators.\n+ *\n+ * @param <T> is the Java class returned by this iterator whose objects contain one or more rows.\n+ */\n+abstract class DataIterator<T> implements CloseableIterator<T> {\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;\n+  private final EncryptionManager encryption;\n+\n+  private CloseableIterator<T> currentIterator;\n+\n+  DataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption) {\n+    this.tasks = task.files().iterator();\n+    this.fileIo = fileIo;\n+    this.encryption = encryption;\n+    this.currentIterator = CloseableIterator.empty();\n+  }\n+\n+  InputFile getInputFile(FileScanTask task) {\n+    Preconditions.checkArgument(!task.isDataTask(), \"Invalid task type\");\n+    return encryption.decrypt(EncryptedFiles.encryptedInput(\n+        fileIo.newInputFile(task.file().path().toString()),\n+        task.file().keyMetadata()));\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    updateCurrentIterator();\n+    return currentIterator.hasNext();\n+  }\n+\n+  @Override\n+  public T next() {\n+    updateCurrentIterator();\n+    return currentIterator.next();\n+  }\n+\n+  /**\n+   * Updates the current iterator field to ensure that the current Iterator\n+   * is not exhausted.\n+   */\n+  private void updateCurrentIterator() {\n+    try {\n+      while (!currentIterator.hasNext() && tasks.hasNext()) {\n+        currentIterator.close();\n+        currentIterator = openTaskIterator(tasks.next());\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  abstract CloseableIterator<T> openTaskIterator(FileScanTask scanTask) throws IOException;\n+\n+  @Override\n+  public void close() throws IOException {\n+    // close the current iterator\n+    this.currentIterator.close();\n+\n+    // exhaust the task iterator\n+    while (tasks.hasNext()) {\n+      tasks.next();\n+    }\n+  }\n+\n+  static Object convertConstant(Type type, Object value) {\n+    if (value == null) {\n+      return null;\n+    }\n+\n+    switch (type.typeId()) {\n+      case DECIMAL: // DecimalData\n+        Types.DecimalType decimal = (Types.DecimalType) type;\n+        return DecimalData.fromBigDecimal((BigDecimal) value, decimal.precision(), decimal.scale());\n+      case STRING: // StringData\n+        if (value instanceof Utf8) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "originalPosition": 122}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg1NzMyMDk2", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-485732096", "createdAt": "2020-09-10T09:05:51Z", "commit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwOTowNTo1MVrOHPqQOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwOTowNTo1MVrOHPqQOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjE4Mjk3MA==", "bodyText": "Same question here,  would it be possible that the value is a GenericData.Fixed or ByteBuffer ?  At least the PartitionData will tranform the ByteBuffer to byte[] ?\n\n  \n    \n      iceberg/core/src/main/java/org/apache/iceberg/PartitionData.java\n    \n    \n         Line 148\n      in\n      c28d1c8\n    \n    \n    \n    \n\n        \n          \n           data[pos] = bytes;", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r486182970", "createdAt": "2020-09-10T09:05:51Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+/**\n+ * Base class of Flink iterators.\n+ *\n+ * @param <T> is the Java class returned by this iterator whose objects contain one or more rows.\n+ */\n+abstract class DataIterator<T> implements CloseableIterator<T> {\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;\n+  private final EncryptionManager encryption;\n+\n+  private CloseableIterator<T> currentIterator;\n+\n+  DataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption) {\n+    this.tasks = task.files().iterator();\n+    this.fileIo = fileIo;\n+    this.encryption = encryption;\n+    this.currentIterator = CloseableIterator.empty();\n+  }\n+\n+  InputFile getInputFile(FileScanTask task) {\n+    Preconditions.checkArgument(!task.isDataTask(), \"Invalid task type\");\n+    return encryption.decrypt(EncryptedFiles.encryptedInput(\n+        fileIo.newInputFile(task.file().path().toString()),\n+        task.file().keyMetadata()));\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    updateCurrentIterator();\n+    return currentIterator.hasNext();\n+  }\n+\n+  @Override\n+  public T next() {\n+    updateCurrentIterator();\n+    return currentIterator.next();\n+  }\n+\n+  /**\n+   * Updates the current iterator field to ensure that the current Iterator\n+   * is not exhausted.\n+   */\n+  private void updateCurrentIterator() {\n+    try {\n+      while (!currentIterator.hasNext() && tasks.hasNext()) {\n+        currentIterator.close();\n+        currentIterator = openTaskIterator(tasks.next());\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  abstract CloseableIterator<T> openTaskIterator(FileScanTask scanTask) throws IOException;\n+\n+  @Override\n+  public void close() throws IOException {\n+    // close the current iterator\n+    this.currentIterator.close();\n+\n+    // exhaust the task iterator\n+    while (tasks.hasNext()) {\n+      tasks.next();\n+    }\n+  }\n+\n+  static Object convertConstant(Type type, Object value) {\n+    if (value == null) {\n+      return null;\n+    }\n+\n+    switch (type.typeId()) {\n+      case DECIMAL: // DecimalData\n+        Types.DecimalType decimal = (Types.DecimalType) type;\n+        return DecimalData.fromBigDecimal((BigDecimal) value, decimal.precision(), decimal.scale());\n+      case STRING: // StringData\n+        if (value instanceof Utf8) {\n+          Utf8 utf8 = (Utf8) value;\n+          return StringData.fromBytes(utf8.getBytes(), 0, utf8.getByteLength());\n+        }\n+        return StringData.fromString(value.toString());\n+      case FIXED: // byte[]\n+        if (value instanceof byte[]) {\n+          return value;\n+        } else if (value instanceof GenericData.Fixed) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "originalPosition": 130}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg1NzMzNjY0", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-485733664", "createdAt": "2020-09-10T09:07:44Z", "commit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwOTowNzo0NVrOHPqVMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwOTowNzo0NVrOHPqVMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjE4NDI0MA==", "bodyText": "nit: could we align the assignment order with the arguments order ?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r486184240", "createdAt": "2020-09-10T09:07:45Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.flink.api.common.io.DefaultInputSplitAssigner;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+\n+/**\n+ * Flink {@link InputFormat} for Iceberg.\n+ */\n+public class FlinkInputFormat extends RichInputFormat<RowData, FlinkInputSplit> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final ScanOptions options;\n+  private final List<Expression> filterExpressions;\n+  private final FileIO io;\n+  private final EncryptionManager encryption;\n+  private final SerializableConfiguration serializableConf;\n+\n+  private transient RowDataIterator iterator;\n+\n+  FlinkInputFormat(\n+      TableLoader tableLoader, Schema projectedSchema, FileIO io, EncryptionManager encryption,\n+      List<Expression> filterExpressions, ScanOptions options, SerializableConfiguration serializableConf) {\n+    this.tableLoader = tableLoader;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "originalPosition": 60}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg1NzM5Mjg2", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-485739286", "createdAt": "2020-09-10T09:14:29Z", "commit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwOToxNDoyOVrOHPqmMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwOToxNDoyOVrOHPqmMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjE4ODU5NQ==", "bodyText": "We could also use this source to read multiple snapshots in bounded mode , right ?  Since it's a public interface/method exposed to user, I'd prefer to provide a more detailed javadoc.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r486188595", "createdAt": "2020-09-10T09:14:29Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table in bounded mode. Reading a snapshot of the table.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg1NzQxMDI1", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-485741025", "createdAt": "2020-09-10T09:16:36Z", "commit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwOToxNjozNlrOHPqrSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwOToxNjozNlrOHPqrSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjE4OTg5Ng==", "bodyText": "nit: how about importing this Configuration explicitly in the import part ?  I did not see the duplicated Configuration classes are used in this file.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r486189896", "createdAt": "2020-09-10T09:16:36Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table in bounded mode. Reading a snapshot of the table.\n+   *\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forBounded() {\n+    return new BoundedBuilder();\n+  }\n+\n+  /**\n+   * Source builder to build {@link DataStream}.\n+   */\n+  public abstract static class Builder {\n+    private StreamExecutionEnvironment env;\n+    private Table table;\n+    private TableLoader tableLoader;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private RowDataTypeInfo rowTypeInfo;\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(org.apache.hadoop.conf.Configuration newConf) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "originalPosition": 111}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MzYwNDgw", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-487360480", "createdAt": "2020-09-14T01:00:56Z", "commit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMTowMDo1N1rOHRA7rA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMTowMDo1N1rOHRA7rA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwMzExNg==", "bodyText": "Nit: we mostly use io for FileIO in other places.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487603116", "createdAt": "2020-09-14T01:00:57Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+/**\n+ * Base class of Flink iterators.\n+ *\n+ * @param <T> is the Java class returned by this iterator whose objects contain one or more rows.\n+ */\n+abstract class DataIterator<T> implements CloseableIterator<T> {\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MzYxNDk2", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-487361496", "createdAt": "2020-09-14T01:06:53Z", "commit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMTowNjo1M1rOHRA_Ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMTowNjo1M1rOHRA_Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwNDAyMg==", "bodyText": "So this class functions as both planner and reader?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487604022", "createdAt": "2020-09-14T01:06:53Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.flink.api.common.io.DefaultInputSplitAssigner;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+\n+/**\n+ * Flink {@link InputFormat} for Iceberg.\n+ */\n+public class FlinkInputFormat extends RichInputFormat<RowData, FlinkInputSplit> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final ScanOptions options;\n+  private final List<Expression> filterExpressions;\n+  private final FileIO io;\n+  private final EncryptionManager encryption;\n+  private final SerializableConfiguration serializableConf;\n+\n+  private transient RowDataIterator iterator;\n+\n+  FlinkInputFormat(\n+      TableLoader tableLoader, Schema projectedSchema, FileIO io, EncryptionManager encryption,\n+      List<Expression> filterExpressions, ScanOptions options, SerializableConfiguration serializableConf) {\n+    this.tableLoader = tableLoader;\n+    this.projectedSchema = projectedSchema;\n+    this.options = options;\n+    this.filterExpressions = filterExpressions;\n+    this.io = io;\n+    this.encryption = encryption;\n+    this.serializableConf = serializableConf;\n+  }\n+\n+  @VisibleForTesting\n+  Schema projectedSchema() {\n+    return projectedSchema;\n+  }\n+\n+  @Override\n+  public BaseStatistics getStatistics(BaseStatistics cachedStatistics) {\n+    // Legacy method, not be used.\n+    return null;\n+  }\n+\n+  @Override\n+  public FlinkInputSplit[] createInputSplits(int minNumSplits) throws IOException {\n+    // Called in Job manager, so it is OK to load table from catalog.\n+    tableLoader.open(serializableConf.get());\n+    try (TableLoader loader = tableLoader) {\n+      Table table = loader.loadTable();\n+      FlinkSplitGenerator generator = new FlinkSplitGenerator(table, projectedSchema, options, filterExpressions);\n+      return generator.createInputSplits();\n+    }\n+  }\n+\n+  @Override\n+  public InputSplitAssigner getInputSplitAssigner(FlinkInputSplit[] inputSplits) {\n+    return new DefaultInputSplitAssigner(inputSplits);\n+  }\n+\n+  @Override\n+  public void configure(Configuration parameters) {\n+  }\n+\n+  @Override\n+  public void open(FlinkInputSplit split) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "originalPosition": 101}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MzYxODIz", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-487361823", "createdAt": "2020-09-14T01:08:37Z", "commit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMTowODozN1rOHRBAUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMTowODozN1rOHRBAUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwNDMwNQ==", "bodyText": "It doesn't seem correct to ignore task in equals. This could lead to bugs in testing. For example, if someone uses this to assert that planned splits are equal to expected splits. Is this required by Flink?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487604305", "createdAt": "2020-09-14T01:08:37Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputSplit.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import org.apache.flink.core.io.InputSplit;\n+import org.apache.flink.core.io.LocatableInputSplit;\n+import org.apache.iceberg.CombinedScanTask;\n+\n+/**\n+ * TODO Implement {@link LocatableInputSplit}.\n+ */\n+public class FlinkInputSplit implements InputSplit {\n+\n+  private final int splitNumber;\n+  private final CombinedScanTask task;\n+\n+  FlinkInputSplit(int splitNumber, CombinedScanTask task) {\n+    this.splitNumber = splitNumber;\n+    this.task = task;\n+  }\n+\n+  @Override\n+  public int getSplitNumber() {\n+    return splitNumber;\n+  }\n+\n+  CombinedScanTask getTask() {\n+    return task;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    FlinkInputSplit that = (FlinkInputSplit) o;\n+    return splitNumber == that.splitNumber;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "originalPosition": 57}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MzYyMTU5", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-487362159", "createdAt": "2020-09-14T01:10:34Z", "commit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMToxMDozNFrOHRBBjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMToxMDozNFrOHRBBjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwNDYyMg==", "bodyText": "We don't normally include the type returned by the builder in the build method unless it is distinguishing between two options (like build and buildUnchecked). Could this just be build?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487604622", "createdAt": "2020-09-14T01:10:34Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table in bounded mode. Reading a snapshot of the table.\n+   *\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forBounded() {\n+    return new BoundedBuilder();\n+  }\n+\n+  /**\n+   * Source builder to build {@link DataStream}.\n+   */\n+  public abstract static class Builder {\n+    private StreamExecutionEnvironment env;\n+    private Table table;\n+    private TableLoader tableLoader;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private RowDataTypeInfo rowTypeInfo;\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(org.apache.hadoop.conf.Configuration newConf) {\n+      this.hadoopConf = newConf;\n+      return this;\n+    }\n+\n+    public Builder env(StreamExecutionEnvironment newEnv) {\n+      this.env = newEnv;\n+      return this;\n+    }\n+\n+    StreamExecutionEnvironment getEnv() {\n+      return env;\n+    }\n+\n+    RowDataTypeInfo getRowTypeInfo() {\n+      return rowTypeInfo;\n+    }\n+\n+    public FlinkInputFormat buildFormat() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "originalPosition": 129}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MzYyMzc1", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-487362375", "createdAt": "2020-09-14T01:11:45Z", "commit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMToxMTo0NVrOHRBCYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMToxMTo0NVrOHRBCYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwNDgzMg==", "bodyText": "Is this needed? If this delegated to when the scan is built, then the scan would do the check and users would get consistent exception messages.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487604832", "createdAt": "2020-09-14T01:11:45Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table in bounded mode. Reading a snapshot of the table.\n+   *\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forBounded() {\n+    return new BoundedBuilder();\n+  }\n+\n+  /**\n+   * Source builder to build {@link DataStream}.\n+   */\n+  public abstract static class Builder {\n+    private StreamExecutionEnvironment env;\n+    private Table table;\n+    private TableLoader tableLoader;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private RowDataTypeInfo rowTypeInfo;\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(org.apache.hadoop.conf.Configuration newConf) {\n+      this.hadoopConf = newConf;\n+      return this;\n+    }\n+\n+    public Builder env(StreamExecutionEnvironment newEnv) {\n+      this.env = newEnv;\n+      return this;\n+    }\n+\n+    StreamExecutionEnvironment getEnv() {\n+      return env;\n+    }\n+\n+    RowDataTypeInfo getRowTypeInfo() {\n+      return rowTypeInfo;\n+    }\n+\n+    public FlinkInputFormat buildFormat() {\n+      Preconditions.checkNotNull(tableLoader, \"TableLoader should not be null\");\n+\n+      hadoopConf = hadoopConf == null ? FlinkCatalogFactory.clusterHadoopConf() : hadoopConf;\n+\n+      Schema icebergSchema;\n+      FileIO io;\n+      EncryptionManager encryption;\n+      if (table == null) {\n+        // load required fields by table loader.\n+        tableLoader.open(hadoopConf);\n+        try (TableLoader loader = tableLoader) {\n+          table = loader.loadTable();\n+          icebergSchema = table.schema();\n+          io = table.io();\n+          encryption = table.encryption();\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(e);\n+        }\n+      } else {\n+        icebergSchema = table.schema();\n+        io = table.io();\n+        encryption = table.encryption();\n+      }\n+\n+      if (projectedSchema != null && selectedFields != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "originalPosition": 154}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MzYyNTY2", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-487362566", "createdAt": "2020-09-14T01:12:43Z", "commit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMToxMjo0M1rOHRBDDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMToxMjo0M1rOHRBDDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwNTAwNA==", "bodyText": "Why project the Flink schema manually rather than using icebergSchema.select(selectedFIelds) and converting the result?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487605004", "createdAt": "2020-09-14T01:12:43Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table in bounded mode. Reading a snapshot of the table.\n+   *\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forBounded() {\n+    return new BoundedBuilder();\n+  }\n+\n+  /**\n+   * Source builder to build {@link DataStream}.\n+   */\n+  public abstract static class Builder {\n+    private StreamExecutionEnvironment env;\n+    private Table table;\n+    private TableLoader tableLoader;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private RowDataTypeInfo rowTypeInfo;\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(org.apache.hadoop.conf.Configuration newConf) {\n+      this.hadoopConf = newConf;\n+      return this;\n+    }\n+\n+    public Builder env(StreamExecutionEnvironment newEnv) {\n+      this.env = newEnv;\n+      return this;\n+    }\n+\n+    StreamExecutionEnvironment getEnv() {\n+      return env;\n+    }\n+\n+    RowDataTypeInfo getRowTypeInfo() {\n+      return rowTypeInfo;\n+    }\n+\n+    public FlinkInputFormat buildFormat() {\n+      Preconditions.checkNotNull(tableLoader, \"TableLoader should not be null\");\n+\n+      hadoopConf = hadoopConf == null ? FlinkCatalogFactory.clusterHadoopConf() : hadoopConf;\n+\n+      Schema icebergSchema;\n+      FileIO io;\n+      EncryptionManager encryption;\n+      if (table == null) {\n+        // load required fields by table loader.\n+        tableLoader.open(hadoopConf);\n+        try (TableLoader loader = tableLoader) {\n+          table = loader.loadTable();\n+          icebergSchema = table.schema();\n+          io = table.io();\n+          encryption = table.encryption();\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(e);\n+        }\n+      } else {\n+        icebergSchema = table.schema();\n+        io = table.io();\n+        encryption = table.encryption();\n+      }\n+\n+      if (projectedSchema != null && selectedFields != null) {\n+        throw new IllegalArgumentException(\n+            \"Cannot using both requestedSchema and projectedFields to project\");\n+      }\n+\n+      TableSchema projectedTableSchema = projectedSchema;\n+      TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(icebergSchema));\n+      if (selectedFields != null) {\n+        TableSchema.Builder builder = TableSchema.builder();\n+        for (String field : selectedFields) {\n+          TableColumn column = tableSchema.getTableColumn(field).orElseThrow(\n+              () -> new IllegalArgumentException(String.format(\"The field(%s) can not be found in the table schema: %s\",\n+                  field, tableSchema)));\n+          builder.field(column.getName(), column.getType());\n+        }\n+        projectedTableSchema = builder.build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "originalPosition": 169}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MzY4OTI1", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-487368925", "createdAt": "2020-09-14T01:42:44Z", "commit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMTo0Mjo0NFrOHRBZLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMTo0Mjo0NFrOHRBZLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYxMDY3MQ==", "bodyText": "Will SQL use this or select?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487610671", "createdAt": "2020-09-14T01:42:44Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table in bounded mode. Reading a snapshot of the table.\n+   *\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forBounded() {\n+    return new BoundedBuilder();\n+  }\n+\n+  /**\n+   * Source builder to build {@link DataStream}.\n+   */\n+  public abstract static class Builder {\n+    private StreamExecutionEnvironment env;\n+    private Table table;\n+    private TableLoader tableLoader;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private RowDataTypeInfo rowTypeInfo;\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "originalPosition": 91}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MzY5NDQ5", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-487369449", "createdAt": "2020-09-14T01:45:06Z", "commit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMTo0NTowNlrOHRBbIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMTo0NTowNlrOHRBbIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYxMTE3MA==", "bodyText": "Since these are intended to be called from child classes, should they be protected?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487611170", "createdAt": "2020-09-14T01:45:06Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table in bounded mode. Reading a snapshot of the table.\n+   *\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forBounded() {\n+    return new BoundedBuilder();\n+  }\n+\n+  /**\n+   * Source builder to build {@link DataStream}.\n+   */\n+  public abstract static class Builder {\n+    private StreamExecutionEnvironment env;\n+    private Table table;\n+    private TableLoader tableLoader;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private RowDataTypeInfo rowTypeInfo;\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(org.apache.hadoop.conf.Configuration newConf) {\n+      this.hadoopConf = newConf;\n+      return this;\n+    }\n+\n+    public Builder env(StreamExecutionEnvironment newEnv) {\n+      this.env = newEnv;\n+      return this;\n+    }\n+\n+    StreamExecutionEnvironment getEnv() {\n+      return env;\n+    }\n+\n+    RowDataTypeInfo getRowTypeInfo() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "originalPosition": 125}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MzY5OTMw", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-487369930", "createdAt": "2020-09-14T01:47:13Z", "commit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMTo0NzoxM1rOHRBc1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMTo0NzoxM1rOHRBc1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYxMTYwNA==", "bodyText": "How is this different than an unbounded builder? I don't see anything that passes whether the stream should be bounded or unbounded. It seems like this should pass that information so that the input adapter can plan the current table scan, rather than checking for new data later.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487611604", "createdAt": "2020-09-14T01:47:13Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table in bounded mode. Reading a snapshot of the table.\n+   *\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forBounded() {\n+    return new BoundedBuilder();\n+  }\n+\n+  /**\n+   * Source builder to build {@link DataStream}.\n+   */\n+  public abstract static class Builder {\n+    private StreamExecutionEnvironment env;\n+    private Table table;\n+    private TableLoader tableLoader;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private RowDataTypeInfo rowTypeInfo;\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(org.apache.hadoop.conf.Configuration newConf) {\n+      this.hadoopConf = newConf;\n+      return this;\n+    }\n+\n+    public Builder env(StreamExecutionEnvironment newEnv) {\n+      this.env = newEnv;\n+      return this;\n+    }\n+\n+    StreamExecutionEnvironment getEnv() {\n+      return env;\n+    }\n+\n+    RowDataTypeInfo getRowTypeInfo() {\n+      return rowTypeInfo;\n+    }\n+\n+    public FlinkInputFormat buildFormat() {\n+      Preconditions.checkNotNull(tableLoader, \"TableLoader should not be null\");\n+\n+      hadoopConf = hadoopConf == null ? FlinkCatalogFactory.clusterHadoopConf() : hadoopConf;\n+\n+      Schema icebergSchema;\n+      FileIO io;\n+      EncryptionManager encryption;\n+      if (table == null) {\n+        // load required fields by table loader.\n+        tableLoader.open(hadoopConf);\n+        try (TableLoader loader = tableLoader) {\n+          table = loader.loadTable();\n+          icebergSchema = table.schema();\n+          io = table.io();\n+          encryption = table.encryption();\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(e);\n+        }\n+      } else {\n+        icebergSchema = table.schema();\n+        io = table.io();\n+        encryption = table.encryption();\n+      }\n+\n+      if (projectedSchema != null && selectedFields != null) {\n+        throw new IllegalArgumentException(\n+            \"Cannot using both requestedSchema and projectedFields to project\");\n+      }\n+\n+      TableSchema projectedTableSchema = projectedSchema;\n+      TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(icebergSchema));\n+      if (selectedFields != null) {\n+        TableSchema.Builder builder = TableSchema.builder();\n+        for (String field : selectedFields) {\n+          TableColumn column = tableSchema.getTableColumn(field).orElseThrow(\n+              () -> new IllegalArgumentException(String.format(\"The field(%s) can not be found in the table schema: %s\",\n+                  field, tableSchema)));\n+          builder.field(column.getName(), column.getType());\n+        }\n+        projectedTableSchema = builder.build();\n+      }\n+\n+      rowTypeInfo = RowDataTypeInfo.of((RowType) (projectedTableSchema == null ? tableSchema : projectedTableSchema)\n+              .toRowDataType().getLogicalType());\n+\n+      Schema expectedSchema = icebergSchema;\n+      if (projectedTableSchema != null) {\n+        expectedSchema = FlinkSchemaUtil.convert(icebergSchema, projectedTableSchema);\n+      }\n+\n+      return new FlinkInputFormat(tableLoader, expectedSchema, io, encryption, filterExpressions, options,\n+          new SerializableConfiguration(hadoopConf));\n+    }\n+\n+    public abstract DataStream<RowData> build();\n+  }\n+\n+  private static final class BoundedBuilder extends Builder {\n+    @Override\n+    public DataStream<RowData> build() {\n+      Preconditions.checkNotNull(getEnv(), \"StreamExecutionEnvironment should not be null\");\n+      FlinkInputFormat format = buildFormat();\n+      return getEnv().createInput(format, getRowTypeInfo());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "originalPosition": 192}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3MzcwODg4", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-487370888", "createdAt": "2020-09-14T01:51:34Z", "commit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMTo1MTozNFrOHRBgEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQwMTo1MTozNFrOHRBgEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYxMjQzMg==", "bodyText": "I'm not sure if this class should conform to the typical style used by Flink or Iceberg, but in Iceberg, we omit get from getter names because it doesn't add any helpful context and is awkward in non-Java languages where getter methods are named for fields.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487612432", "createdAt": "2020-09-14T01:51:34Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanOptions.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Map;\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+import org.apache.flink.configuration.Configuration;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class ScanOptions implements Serializable {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  public static final ConfigOption<Long> SNAPSHOT_ID =\n+      ConfigOptions.key(\"snapshot-id\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Boolean> CASE_SENSITIVE =\n+      ConfigOptions.key(\"case-sensitive\").booleanType().defaultValue(false);\n+\n+  public static final ConfigOption<Long> AS_OF_TIMESTAMP =\n+      ConfigOptions.key(\"as-of-timestamp\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Long> START_SNAPSHOT_ID =\n+      ConfigOptions.key(\"start-snapshot-id\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Long> END_SNAPSHOT_ID =\n+      ConfigOptions.key(\"end-snapshot-id\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Long> SPLIT_SIZE =\n+      ConfigOptions.key(\"split-size\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Integer> SPLIT_LOOKBACK =\n+      ConfigOptions.key(\"split-lookback\").intType().defaultValue(null);\n+\n+  public static final ConfigOption<Long> SPLIT_FILE_OPEN_COST =\n+      ConfigOptions.key(\"split-file-open-cost\").longType().defaultValue(null);\n+\n+  private final boolean caseSensitive;\n+  private final Long snapshotId;\n+  private final Long startSnapshotId;\n+  private final Long endSnapshotId;\n+  private final Long asOfTimestamp;\n+  private final Long splitSize;\n+  private final Integer splitLookback;\n+  private final Long splitOpenFileCost;\n+  private final String nameMapping;\n+\n+  public ScanOptions(boolean caseSensitive, Long snapshotId, Long startSnapshotId, Long endSnapshotId,\n+                     Long asOfTimestamp, Long splitSize, Integer splitLookback, Long splitOpenFileCost,\n+                     String nameMapping) {\n+    this.caseSensitive = caseSensitive;\n+    this.snapshotId = snapshotId;\n+    this.startSnapshotId = startSnapshotId;\n+    this.endSnapshotId = endSnapshotId;\n+    this.asOfTimestamp = asOfTimestamp;\n+    this.splitSize = splitSize;\n+    this.splitLookback = splitLookback;\n+    this.splitOpenFileCost = splitOpenFileCost;\n+    this.nameMapping = nameMapping;\n+  }\n+\n+  public boolean isCaseSensitive() {\n+    return caseSensitive;\n+  }\n+\n+  public Long getSnapshotId() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "originalPosition": 86}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg3OTk3NzQ1", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-487997745", "createdAt": "2020-09-14T17:34:27Z", "commit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNzozNDoyN1rOHRfrsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNFQxNzozNDoyN1rOHRfrsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODEwNjkzMQ==", "bodyText": "I think the tests would be much more readable if the ScanOptions builder were used directly. For example, this is hard to understand:\nassertRecords(executeWithOptions(table, null, null, null, snapshotId1, null, null, null, null), expected1, SCHEMA);\nBut you could rewrite that like this:\nScanOptions options = ScanOptions.builder().startSnapshotId(snapshotId1).build();\nassertRecords(executeWithOptions(table, options), expected1, SCHEMA);\nIn addition, passing null into this leaks the default state within the builder into the tests: test authors need to know that passing null for CatalogLoader is supported. I think it is better to let the test authors use the builder pattern.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r488106931", "createdAt": "2020-09-14T17:34:27Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.conversion.DataStructureConverter;\n+import org.apache.flink.table.data.conversion.DataStructureConverters;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.CatalogLoader;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+/**\n+ * Test {@link FlinkInputFormat}.\n+ */\n+public class TestFlinkInputFormat extends TestFlinkScan {\n+\n+  private FlinkSource.Builder builder;\n+\n+  public TestFlinkInputFormat(String fileFormat) {\n+    super(fileFormat);\n+  }\n+\n+  @Override\n+  public void before() throws IOException {\n+    super.before();\n+    builder = FlinkSource.forBounded().tableLoader(TableLoader.fromHadoopTable(warehouse + \"/default/t\"));\n+  }\n+\n+  @Override\n+  protected List<Row> executeWithOptions(\n+      Table table, List<String> projectFields, CatalogLoader loader, Long snapshotId, Long startSnapshotId,\n+      Long endSnapshotId, Long asOfTimestamp, List<Expression> filters, String sqlFilter) throws IOException {\n+    ScanOptions options = ScanOptions.builder().snapshotId(snapshotId).startSnapshotId(startSnapshotId)\n+        .endSnapshotId(endSnapshotId).asOfTimestamp(asOfTimestamp).build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657"}, "originalPosition": 64}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "committedDate": "2020-09-09T10:04:39Z", "message": "Checkstyle"}, "afterCommit": {"oid": "69e753a2cef87c39fe9171d617a185edc13548b3", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/69e753a2cef87c39fe9171d617a185edc13548b3", "committedDate": "2020-09-15T06:43:55Z", "message": "Address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e0fce7f3d8087cd4c527ea1b37163ab24b9a7d78", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/e0fce7f3d8087cd4c527ea1b37163ab24b9a7d78", "committedDate": "2020-09-24T02:19:26Z", "message": "Flink: Introduce Flink InputFormat"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b6e9ecd42ed8831efd95459bf21e502aba09d807", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/b6e9ecd42ed8831efd95459bf21e502aba09d807", "committedDate": "2020-09-24T02:19:26Z", "message": "Address comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b13ed9b5183f3a4ad3656a3bd8a5bbbc4a5f4745", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/b13ed9b5183f3a4ad3656a3bd8a5bbbc4a5f4745", "committedDate": "2020-09-24T02:19:27Z", "message": "checkstyles"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8d080ad9c7e2a8a30e4797401552c03abc81de96", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/8d080ad9c7e2a8a30e4797401552c03abc81de96", "committedDate": "2020-09-24T02:19:27Z", "message": "Introduce FlinkSource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3c727faafb1c097b5ac7248f7f575dabe2f38c95", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/3c727faafb1c097b5ac7248f7f575dabe2f38c95", "committedDate": "2020-09-24T02:19:27Z", "message": "Checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6c5f830244961f39f355d26529ce7d923908c121", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/6c5f830244961f39f355d26529ce7d923908c121", "committedDate": "2020-09-24T02:19:27Z", "message": "tmp"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e27e42149acabd483967dad6aae00c097d7ea44e", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/e27e42149acabd483967dad6aae00c097d7ea44e", "committedDate": "2020-09-24T02:19:27Z", "message": "Address comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "69e753a2cef87c39fe9171d617a185edc13548b3", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/69e753a2cef87c39fe9171d617a185edc13548b3", "committedDate": "2020-09-15T06:43:55Z", "message": "Address comments"}, "afterCommit": {"oid": "e27e42149acabd483967dad6aae00c097d7ea44e", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/e27e42149acabd483967dad6aae00c097d7ea44e", "committedDate": "2020-09-24T02:19:27Z", "message": "Address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "45af8c7b3377c310ea367958cbe63c2aadf50bdf", "author": {"user": {"login": "JingsongLi", "name": "Jingsong Lee"}}, "url": "https://github.com/apache/iceberg/commit/45af8c7b3377c310ea367958cbe63c2aadf50bdf", "committedDate": "2020-09-24T02:21:17Z", "message": "Set tasks to null in DataIterator"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk2MDQyOTUy", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-496042952", "createdAt": "2020-09-25T00:53:28Z", "commit": {"oid": "45af8c7b3377c310ea367958cbe63c2aadf50bdf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwMDo1MzoyOVrOHXxXgg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwMDo1MzoyOVrOHXxXgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4ODEzMA==", "bodyText": "We usually introduce a spin to avoid sleeping for long durations in lots of tests, like this: https://github.com/apache/iceberg/blob/master/core/src/test/java/org/apache/iceberg/TestRemoveSnapshots.java#L78", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r494688130", "createdAt": "2020-09-25T00:53:29Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.conversion.DataStructureConverter;\n+import org.apache.flink.table.data.conversion.DataStructureConverters;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TestHelpers;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericAppenderHelper;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.RowDataConverter;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.DateTimeUtil;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestFlinkScan extends AbstractTestBase {\n+\n+  private static final Schema SCHEMA = new Schema(\n+          required(1, \"data\", Types.StringType.get()),\n+          required(2, \"id\", Types.LongType.get()),\n+          required(3, \"dt\", Types.StringType.get()));\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+          .identity(\"dt\")\n+          .bucket(\"id\", 1)\n+          .build();\n+\n+  private HadoopCatalog catalog;\n+  protected String warehouse;\n+\n+  // parametrized variables\n+  private final FileFormat fileFormat;\n+\n+  @Parameterized.Parameters(name = \"format={0}\")\n+  public static Object[] parameters() {\n+    return new Object[] {\"avro\", \"parquet\", \"orc\"};\n+  }\n+\n+  TestFlinkScan(String fileFormat) {\n+    this.fileFormat = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File warehouseFile = TEMPORARY_FOLDER.newFolder();\n+    Assert.assertTrue(warehouseFile.delete());\n+    // before variables\n+    Configuration conf = new Configuration();\n+    warehouse = \"file:\" + warehouseFile;\n+    catalog = new HadoopCatalog(conf, warehouse);\n+  }\n+\n+  private List<Row> execute(Table table) throws IOException {\n+    return execute(table, ScanOptions.builder().build());\n+  }\n+\n+  protected abstract List<Row> execute(Table table, List<String> projectFields) throws IOException;\n+\n+  protected abstract List<Row> execute(Table table, ScanOptions options) throws IOException;\n+\n+  protected abstract List<Row> execute(Table table, List<Expression> filters, String sqlFilter) throws IOException;\n+\n+  /**\n+   * The Flink SQL has no residuals, because there will be operator to filter all the data that should be filtered.\n+   * But the FlinkInputFormat can't.\n+   */\n+  protected abstract void assertResiduals(Schema schema, List<Row> results, List<Record> writeRecords,\n+                                          List<Record> filteredRecords) throws IOException;\n+\n+  /**\n+   * Schema: [data, nested[f1, f2, f3], id]\n+   * Projection: [nested.f2, data]\n+   * The Flink SQL output: [f2, data]\n+   * The FlinkInputFormat output: [nested[f2], data].\n+   */\n+  protected abstract void assertNestedProjection(Table table, List<Record> records) throws IOException;\n+\n+  @Test\n+  public void testUnpartitionedTable() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA);\n+    List<Record> expectedRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(expectedRecords);\n+    assertRecords(execute(table), expectedRecords, SCHEMA);\n+  }\n+\n+  @Test\n+  public void testPartitionedTable() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+    List<Record> expectedRecords = RandomGenericData.generate(SCHEMA, 1, 0L);\n+    expectedRecords.get(0).set(2, \"2020-03-20\");\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(\n+        org.apache.iceberg.TestHelpers.Row.of(\"2020-03-20\", 0), expectedRecords);\n+    assertRecords(execute(table), expectedRecords, SCHEMA);\n+  }\n+\n+  @Test\n+  public void testProjection() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+    List<Record> inputRecords = RandomGenericData.generate(SCHEMA, 1, 0L);\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(\n+        org.apache.iceberg.TestHelpers.Row.of(\"2020-03-20\", 0), inputRecords);\n+    assertRows(execute(table, Collections.singletonList(\"data\")), Row.of(inputRecords.get(0).get(0)));\n+  }\n+\n+  @Test\n+  public void testIdentityPartitionProjections() throws Exception {\n+    Schema logSchema = new Schema(\n+        Types.NestedField.optional(1, \"id\", Types.IntegerType.get()),\n+        Types.NestedField.optional(2, \"dt\", Types.StringType.get()),\n+        Types.NestedField.optional(3, \"level\", Types.StringType.get()),\n+        Types.NestedField.optional(4, \"message\", Types.StringType.get())\n+    );\n+    PartitionSpec spec =\n+        PartitionSpec.builderFor(logSchema).identity(\"dt\").identity(\"level\").build();\n+\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), logSchema, spec);\n+    List<Record> inputRecords = RandomGenericData.generate(logSchema, 10, 0L);\n+\n+    int idx = 0;\n+    AppendFiles append = table.newAppend();\n+    for (Record record : inputRecords) {\n+      record.set(1, \"2020-03-2\" + idx);\n+      record.set(2, Integer.toString(idx));\n+      append.appendFile(new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).writeFile(\n+          org.apache.iceberg.TestHelpers.Row.of(\"2020-03-2\" + idx, Integer.toString(idx)), ImmutableList.of(record)));\n+      idx += 1;\n+    }\n+    append.commit();\n+\n+    // individual fields\n+    validateIdentityPartitionProjections(table, Collections.singletonList(\"dt\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Collections.singletonList(\"level\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Collections.singletonList(\"message\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Collections.singletonList(\"id\"), inputRecords);\n+    // field pairs\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"dt\", \"message\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"level\", \"message\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"dt\", \"level\"), inputRecords);\n+    // out-of-order pairs\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"message\", \"dt\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"message\", \"level\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"level\", \"dt\"), inputRecords);\n+    // out-of-order triplets\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"dt\", \"level\", \"message\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"level\", \"dt\", \"message\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"dt\", \"message\", \"level\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"level\", \"message\", \"dt\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"message\", \"dt\", \"level\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"message\", \"level\", \"dt\"), inputRecords);\n+  }\n+\n+  private void validateIdentityPartitionProjections(Table table, List<String> projectedFields,\n+      List<Record> inputRecords) throws IOException {\n+    List<Row> rows = execute(table, projectedFields);\n+\n+    for (int pos = 0; pos < inputRecords.size(); pos++) {\n+      Record inputRecord = inputRecords.get(pos);\n+      Row actualRecord = rows.get(pos);\n+\n+      for (int i = 0; i < projectedFields.size(); i++) {\n+        String name = projectedFields.get(i);\n+        Assert.assertEquals(\n+            \"Projected field \" + name + \" should match\", inputRecord.getField(name), actualRecord.getField(i));\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testSnapshotReads() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA);\n+\n+    GenericAppenderHelper helper = new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER);\n+\n+    List<Record> expectedRecords = RandomGenericData.generate(SCHEMA, 1, 0L);\n+    helper.appendToTable(expectedRecords);\n+    long snapshotId = table.currentSnapshot().snapshotId();\n+\n+    long timestampMillis = table.currentSnapshot().timestampMillis();\n+\n+    // produce another timestamp\n+    Thread.sleep(10);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "45af8c7b3377c310ea367958cbe63c2aadf50bdf"}, "originalPosition": 233}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk2MDQ1NDcw", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-496045470", "createdAt": "2020-09-25T01:02:32Z", "commit": {"oid": "45af8c7b3377c310ea367958cbe63c2aadf50bdf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwMTowMjozMlrOHXxgPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwMTowMjozMlrOHXxgPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MDM2Nw==", "bodyText": "I find it really strange that this is delegated to a subclass, given that it builds a very specific nested projection.\nWhy not make this use a method like execute(Table, List<String>), but pass in the projection instead of a list of fields?\nThen you could keep all of the schema details in the test method here, rather than delegating this assertion. I think it would be cleaner.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r494690367", "createdAt": "2020-09-25T01:02:32Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.conversion.DataStructureConverter;\n+import org.apache.flink.table.data.conversion.DataStructureConverters;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TestHelpers;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericAppenderHelper;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.RowDataConverter;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.DateTimeUtil;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestFlinkScan extends AbstractTestBase {\n+\n+  private static final Schema SCHEMA = new Schema(\n+          required(1, \"data\", Types.StringType.get()),\n+          required(2, \"id\", Types.LongType.get()),\n+          required(3, \"dt\", Types.StringType.get()));\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+          .identity(\"dt\")\n+          .bucket(\"id\", 1)\n+          .build();\n+\n+  private HadoopCatalog catalog;\n+  protected String warehouse;\n+\n+  // parametrized variables\n+  private final FileFormat fileFormat;\n+\n+  @Parameterized.Parameters(name = \"format={0}\")\n+  public static Object[] parameters() {\n+    return new Object[] {\"avro\", \"parquet\", \"orc\"};\n+  }\n+\n+  TestFlinkScan(String fileFormat) {\n+    this.fileFormat = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File warehouseFile = TEMPORARY_FOLDER.newFolder();\n+    Assert.assertTrue(warehouseFile.delete());\n+    // before variables\n+    Configuration conf = new Configuration();\n+    warehouse = \"file:\" + warehouseFile;\n+    catalog = new HadoopCatalog(conf, warehouse);\n+  }\n+\n+  private List<Row> execute(Table table) throws IOException {\n+    return execute(table, ScanOptions.builder().build());\n+  }\n+\n+  protected abstract List<Row> execute(Table table, List<String> projectFields) throws IOException;\n+\n+  protected abstract List<Row> execute(Table table, ScanOptions options) throws IOException;\n+\n+  protected abstract List<Row> execute(Table table, List<Expression> filters, String sqlFilter) throws IOException;\n+\n+  /**\n+   * The Flink SQL has no residuals, because there will be operator to filter all the data that should be filtered.\n+   * But the FlinkInputFormat can't.\n+   */\n+  protected abstract void assertResiduals(Schema schema, List<Row> results, List<Record> writeRecords,\n+                                          List<Record> filteredRecords) throws IOException;\n+\n+  /**\n+   * Schema: [data, nested[f1, f2, f3], id]\n+   * Projection: [nested.f2, data]\n+   * The Flink SQL output: [f2, data]\n+   * The FlinkInputFormat output: [nested[f2], data].\n+   */\n+  protected abstract void assertNestedProjection(Table table, List<Record> records) throws IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "45af8c7b3377c310ea367958cbe63c2aadf50bdf"}, "originalPosition": 128}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk2MDQ1NzMx", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-496045731", "createdAt": "2020-09-25T01:03:35Z", "commit": {"oid": "45af8c7b3377c310ea367958cbe63c2aadf50bdf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwMTowMzozNVrOHXxhIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwMTowMzozNVrOHXxhIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MDU5NQ==", "bodyText": "Why is this implemented by the subclass? Couldn't this just call assertRecords directly?", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r494690595", "createdAt": "2020-09-25T01:03:35Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.conversion.DataStructureConverter;\n+import org.apache.flink.table.data.conversion.DataStructureConverters;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TestHelpers;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericAppenderHelper;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.RowDataConverter;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.DateTimeUtil;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestFlinkScan extends AbstractTestBase {\n+\n+  private static final Schema SCHEMA = new Schema(\n+          required(1, \"data\", Types.StringType.get()),\n+          required(2, \"id\", Types.LongType.get()),\n+          required(3, \"dt\", Types.StringType.get()));\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+          .identity(\"dt\")\n+          .bucket(\"id\", 1)\n+          .build();\n+\n+  private HadoopCatalog catalog;\n+  protected String warehouse;\n+\n+  // parametrized variables\n+  private final FileFormat fileFormat;\n+\n+  @Parameterized.Parameters(name = \"format={0}\")\n+  public static Object[] parameters() {\n+    return new Object[] {\"avro\", \"parquet\", \"orc\"};\n+  }\n+\n+  TestFlinkScan(String fileFormat) {\n+    this.fileFormat = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File warehouseFile = TEMPORARY_FOLDER.newFolder();\n+    Assert.assertTrue(warehouseFile.delete());\n+    // before variables\n+    Configuration conf = new Configuration();\n+    warehouse = \"file:\" + warehouseFile;\n+    catalog = new HadoopCatalog(conf, warehouse);\n+  }\n+\n+  private List<Row> execute(Table table) throws IOException {\n+    return execute(table, ScanOptions.builder().build());\n+  }\n+\n+  protected abstract List<Row> execute(Table table, List<String> projectFields) throws IOException;\n+\n+  protected abstract List<Row> execute(Table table, ScanOptions options) throws IOException;\n+\n+  protected abstract List<Row> execute(Table table, List<Expression> filters, String sqlFilter) throws IOException;\n+\n+  /**\n+   * The Flink SQL has no residuals, because there will be operator to filter all the data that should be filtered.\n+   * But the FlinkInputFormat can't.\n+   */\n+  protected abstract void assertResiduals(Schema schema, List<Row> results, List<Record> writeRecords,\n+                                          List<Record> filteredRecords) throws IOException;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "45af8c7b3377c310ea367958cbe63c2aadf50bdf"}, "originalPosition": 120}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExNDkzMTgx", "url": "https://github.com/apache/iceberg/pull/1346#pullrequestreview-511493181", "createdAt": "2020-10-19T07:49:46Z", "commit": {"oid": "45af8c7b3377c310ea367958cbe63c2aadf50bdf"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwNzo0OTo0NlrOHkBzNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwNzo0OTo0NlrOHkBzNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0MDI3OA==", "bodyText": "Hi @JingsongLi , I'm testing Iceberg recently. Since the StreamExecutionEnvironment is a must-have parameter for FlinkSource, would it better to put it in the builder's constructor instead of FlinkSource.forRowData().env(xx)?\njust a minor improvement on user experience.", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r507540278", "createdAt": "2020-10-19T07:49:46Z", "author": {"login": "Jiayi-Liao"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table. Equivalent to {@link TableScan}.\n+   * See more options in {@link ScanOptions}.\n+   * <p>\n+   * The Source can be read static data in bounded mode. It can also continuously check the arrival of new data and\n+   * read records incrementally.\n+   * The Bounded and Unbounded depends on the {@link Builder#options(ScanOptions)}:\n+   * <ul>\n+   *   <li>Without startSnapshotId: Bounded</li>\n+   *   <li>With startSnapshotId and with endSnapshotId: Bounded</li>\n+   *   <li>With startSnapshotId (-1 means unbounded preceding) and Without endSnapshotId: Unbounded</li>\n+   * </ul>\n+   * <p>\n+   *\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRowData() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "45af8c7b3377c310ea367958cbe63c2aadf50bdf"}, "originalPosition": 64}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4059, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}