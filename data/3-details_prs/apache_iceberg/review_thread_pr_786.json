{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzcyNDE4MzE5", "number": 786, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMDozNzoyM1rODep_kg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwNTowMzowM1rODgj8sw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDcxODkwOnYy", "diffSide": "RIGHT", "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMDozNzoyM1rOFn6kbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMToyNzoxNVrOFn7VvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM5ODM4MA==", "bodyText": "I think it will be a good idea to have the spec argument before metricsConfig and conf as the last two are optional. I would make it consistent in all touched files (i.e. first spec then config).\n  def listPartition(\n      partition: Map[String, String],\n      uri: String,\n      format: String,\n      spec: PartitionSpec,\n      conf: Configuration = new Configuration(),\n      metricsConfig: MetricsConfig = MetricsConfig.getDefault): Seq[DataFile] = {", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377398380", "createdAt": "2020-02-11T00:37:23Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -174,22 +175,23 @@ object SparkTableUtil {\n    * @param format partition format, avro or parquet\n    * @param conf a Hadoop conf\n    * @param metricsConfig a metrics conf\n-   * @return a seq of [[SparkDataFile]]\n+   * @return a seq of [[DataFile]]\n    */\n   def listPartition(\n       partition: Map[String, String],\n       uri: String,\n       format: String,\n       conf: Configuration = new Configuration(),\n-      metricsConfig: MetricsConfig = MetricsConfig.getDefault): Seq[SparkDataFile] = {\n+      metricsConfig: MetricsConfig = MetricsConfig.getDefault,\n+      spec: PartitionSpec): Seq[DataFile] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d06c0724d5f02c55f479eafe8dfe21a471272626"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQxMTAwNQ==", "bodyText": "Make sense, will do.", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377411005", "createdAt": "2020-02-11T01:27:15Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -174,22 +175,23 @@ object SparkTableUtil {\n    * @param format partition format, avro or parquet\n    * @param conf a Hadoop conf\n    * @param metricsConfig a metrics conf\n-   * @return a seq of [[SparkDataFile]]\n+   * @return a seq of [[DataFile]]\n    */\n   def listPartition(\n       partition: Map[String, String],\n       uri: String,\n       format: String,\n       conf: Configuration = new Configuration(),\n-      metricsConfig: MetricsConfig = MetricsConfig.getDefault): Seq[SparkDataFile] = {\n+      metricsConfig: MetricsConfig = MetricsConfig.getDefault,\n+      spec: PartitionSpec): Seq[DataFile] = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM5ODM4MA=="}, "originalCommit": {"oid": "d06c0724d5f02c55f479eafe8dfe21a471272626"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDc0NTczOnYy", "diffSide": "RIGHT", "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMDo1MjozOFrOFn60YA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMTozODowM1rOFn7eRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMjQ2NA==", "bodyText": "Do we still need these methods?", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377402464", "createdAt": "2020-02-11T00:52:38Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -200,50 +202,6 @@ object SparkTableUtil {\n    */\n   case class SparkPartition(values: Map[String, String], uri: String, format: String)\n \n-  /**\n-   * Case class representing a data file.\n-   */\n-  case class SparkDataFile(\n-      path: String,\n-      partition: collection.Map[String, String],\n-      format: String,\n-      fileSize: Long,\n-      rowGroupSize: Long,\n-      rowCount: Long,\n-      columnSizes: Array[Long],\n-      valueCounts: Array[Long],\n-      nullValueCounts: Array[Long],\n-      lowerBounds: Seq[Array[Byte]],\n-      upperBounds: Seq[Array[Byte]]\n-    ) {\n-\n-    /**\n-     * Convert this to a [[DataFile]] that can be added to a [[org.apache.iceberg.Table]].\n-     *\n-     * @param spec a [[PartitionSpec]] that will be used to parse the partition key\n-     * @return a [[DataFile]] that can be passed to [[org.apache.iceberg.AppendFiles]]\n-     */\n-    def toDataFile(spec: PartitionSpec): DataFile = {\n-      // values are strings, so pass a path to let the builder coerce to the right types\n-      val partitionKey = spec.fields.asScala.map(_.name).map { name =>\n-        s\"$name=${partition(name)}\"\n-      }.mkString(\"/\")\n-\n-      DataFiles.builder(spec)\n-        .withPath(path)\n-        .withFormat(format)\n-        .withFileSizeInBytes(fileSize)\n-        .withMetrics(new Metrics(rowCount,\n-          arrayToMap(columnSizes),\n-          arrayToMap(valueCounts),\n-          arrayToMap(nullValueCounts),\n-          arrayToMap(lowerBounds),\n-          arrayToMap(upperBounds)))\n-        .withPartitionPath(partitionKey)\n-        .build()\n-    }\n-  }\n-\n   private def bytesMapToArray(map: java.util.Map[Integer, ByteBuffer]): Seq[Array[Byte]] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d06c0724d5f02c55f479eafe8dfe21a471272626"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQxMzE4OA==", "bodyText": "No, will delete them.", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377413188", "createdAt": "2020-02-11T01:38:03Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -200,50 +202,6 @@ object SparkTableUtil {\n    */\n   case class SparkPartition(values: Map[String, String], uri: String, format: String)\n \n-  /**\n-   * Case class representing a data file.\n-   */\n-  case class SparkDataFile(\n-      path: String,\n-      partition: collection.Map[String, String],\n-      format: String,\n-      fileSize: Long,\n-      rowGroupSize: Long,\n-      rowCount: Long,\n-      columnSizes: Array[Long],\n-      valueCounts: Array[Long],\n-      nullValueCounts: Array[Long],\n-      lowerBounds: Seq[Array[Byte]],\n-      upperBounds: Seq[Array[Byte]]\n-    ) {\n-\n-    /**\n-     * Convert this to a [[DataFile]] that can be added to a [[org.apache.iceberg.Table]].\n-     *\n-     * @param spec a [[PartitionSpec]] that will be used to parse the partition key\n-     * @return a [[DataFile]] that can be passed to [[org.apache.iceberg.AppendFiles]]\n-     */\n-    def toDataFile(spec: PartitionSpec): DataFile = {\n-      // values are strings, so pass a path to let the builder coerce to the right types\n-      val partitionKey = spec.fields.asScala.map(_.name).map { name =>\n-        s\"$name=${partition(name)}\"\n-      }.mkString(\"/\")\n-\n-      DataFiles.builder(spec)\n-        .withPath(path)\n-        .withFormat(format)\n-        .withFileSizeInBytes(fileSize)\n-        .withMetrics(new Metrics(rowCount,\n-          arrayToMap(columnSizes),\n-          arrayToMap(valueCounts),\n-          arrayToMap(nullValueCounts),\n-          arrayToMap(lowerBounds),\n-          arrayToMap(upperBounds)))\n-        .withPartitionPath(partitionKey)\n-        .build()\n-    }\n-  }\n-\n   private def bytesMapToArray(map: java.util.Map[Integer, ByteBuffer]): Seq[Array[Byte]] = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMjQ2NA=="}, "originalCommit": {"oid": "d06c0724d5f02c55f479eafe8dfe21a471272626"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDc1MjExOnYy", "diffSide": "RIGHT", "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMDo1NjowNFrOFn64Cw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMTozNTo0MlrOFn7cWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMzQwMw==", "bodyText": "What about initializing metrics before as a separate variable?\n    fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n      val metrics = new Metrics(-1L, arrayToMap(null), arrayToMap(null), arrayToMap(null))\n      val partitionKey = spec.fields.asScala.map(_.name).map { name =>\n        s\"$name=${partitionPath(name)}\"\n      }.mkString(\"/\")\n\n      DataFiles.builder(spec)\n        .withPath(stat.getPath.toString)\n        .withFormat(\"avro\")\n        .withFileSizeInBytes(stat.getLen)\n        .withMetrics(metrics)\n        .withPartitionPath(partitionKey)\n        .build()\n    }", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377403403", "createdAt": "2020-02-11T00:56:04Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -329,21 +287,26 @@ object SparkTableUtil {\n   private def listAvroPartition(\n       partitionPath: Map[String, String],\n       partitionUri: String,\n-      conf: Configuration): Seq[SparkDataFile] = {\n+      conf: Configuration,\n+      spec: PartitionSpec): Seq[DataFile] = {\n     val partition = new Path(partitionUri)\n     val fs = partition.getFileSystem(conf)\n \n     fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n-      SparkDataFile(\n-        stat.getPath.toString,\n-        partitionPath, \"avro\", stat.getLen,\n-        stat.getBlockSize,\n-        -1,\n-        null,\n-        null,\n-        null,\n-        null,\n-        null)\n+      val partitionKey = spec.fields.asScala.map(_.name).map { name =>\n+        s\"$name=${partitionPath(name)}\"\n+      }.mkString(\"/\")\n+\n+      DataFiles.builder(spec)\n+        .withPath(stat.getPath.toString)\n+        .withFormat(\"avro\")\n+        .withFileSizeInBytes(stat.getLen)\n+        .withMetrics(new Metrics(-1L,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d06c0724d5f02c55f479eafe8dfe21a471272626"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQxMjY5Nw==", "bodyText": "Sure, it looks better.", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377412697", "createdAt": "2020-02-11T01:35:42Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -329,21 +287,26 @@ object SparkTableUtil {\n   private def listAvroPartition(\n       partitionPath: Map[String, String],\n       partitionUri: String,\n-      conf: Configuration): Seq[SparkDataFile] = {\n+      conf: Configuration,\n+      spec: PartitionSpec): Seq[DataFile] = {\n     val partition = new Path(partitionUri)\n     val fs = partition.getFileSystem(conf)\n \n     fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n-      SparkDataFile(\n-        stat.getPath.toString,\n-        partitionPath, \"avro\", stat.getLen,\n-        stat.getBlockSize,\n-        -1,\n-        null,\n-        null,\n-        null,\n-        null,\n-        null)\n+      val partitionKey = spec.fields.asScala.map(_.name).map { name =>\n+        s\"$name=${partitionPath(name)}\"\n+      }.mkString(\"/\")\n+\n+      DataFiles.builder(spec)\n+        .withPath(stat.getPath.toString)\n+        .withFormat(\"avro\")\n+        .withFileSizeInBytes(stat.getLen)\n+        .withMetrics(new Metrics(-1L,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMzQwMw=="}, "originalCommit": {"oid": "d06c0724d5f02c55f479eafe8dfe21a471272626"}, "originalPosition": 137}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDc2MjYxOnYy", "diffSide": "RIGHT", "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMTowMTo0MFrOFn6-PQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMTowMTo0MFrOFn6-PQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwNDk4OQ==", "bodyText": "What about this?\nval partition = Map.empty[String, String]\nval spec = PartitionSpec.unpartitioned()\nval conf = spark.sessionState.newHadoopConf()\nval metricsConfig = MetricsConfig.fromProperties(targetTable.properties)\n\nval files = listPartition(partition, sourceTable.location.toString, format.get, spec, conf, metricsConfig)", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377404989", "createdAt": "2020-02-11T01:01:40Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -492,10 +454,11 @@ object SparkTableUtil {\n     val conf = spark.sessionState.newHadoopConf()\n     val metricsConfig = MetricsConfig.fromProperties(targetTable.properties)\n \n-    val files = listPartition(Map.empty, sourceTable.location.toString, format.get, conf, metricsConfig)\n+    val files = listPartition(Map.empty, sourceTable.location.toString, format.get, conf, metricsConfig,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d06c0724d5f02c55f479eafe8dfe21a471272626"}, "originalPosition": 243}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM0ODgwMDAxOnYy", "diffSide": "LEFT", "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxOTo0ODozNlrOFqBz9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDozNDoxOFrOFqyNJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYxNDE5Nw==", "bodyText": "Why do we want to remove the sort? I think it is needed to collocate files for the same partition next to each other so that partition skipping is quick.", "url": "https://github.com/apache/iceberg/pull/786#discussion_r379614197", "createdAt": "2020-02-14T19:48:36Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -527,9 +425,8 @@ object SparkTableUtil {\n     val metricsConfig = MetricsConfig.fromProperties(targetTable.properties)\n \n     val manifests = partitionDS\n-      .flatMap(partition => listPartition(partition, serializableConf, metricsConfig))\n+      .flatMap(partition => listPartition(partition, spec, serializableConf, metricsConfig))\n       .repartition(numShufflePartitions)\n-      .orderBy($\"path\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 333}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTc3NTMxMw==", "bodyText": "The GenericDataFile is not a case class so it doesn't have a schema with its fields. It just has one column named value. I tried to construct an extra column with UDF like:\n    val toPath = org.apache.spark.sql.functions.udf {s:DataFile => s.path().toString}\n\n    val manifests = partitionDS\n      .flatMap(partition => listPartition(partition, spec, serializableConf, metricsConfig))\n      .repartition(numShufflePartitions)\n      .orderBy(toPath($\"value\"))\n      .mapPartitions(buildManifest(serializableConf, spec, stagingDir))\n      .collect()\n\nbut it throws\njava.lang.ClassCastException: [B cannot be cast to org.apache.iceberg.DataFile. So I remove this, do you have any idea on this?", "url": "https://github.com/apache/iceberg/pull/786#discussion_r379775313", "createdAt": "2020-02-15T07:03:41Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -527,9 +425,8 @@ object SparkTableUtil {\n     val metricsConfig = MetricsConfig.fromProperties(targetTable.properties)\n \n     val manifests = partitionDS\n-      .flatMap(partition => listPartition(partition, serializableConf, metricsConfig))\n+      .flatMap(partition => listPartition(partition, spec, serializableConf, metricsConfig))\n       .repartition(numShufflePartitions)\n-      .orderBy($\"path\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYxNDE5Nw=="}, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 333}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTk2MTgyMQ==", "bodyText": "BTW, even we coalesce files in a manifest for same partition, I think we still have to iterate through all manifest entries in the manifest for partition skipping.  Please correct me if I am wrong.", "url": "https://github.com/apache/iceberg/pull/786#discussion_r379961821", "createdAt": "2020-02-17T02:21:33Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -527,9 +425,8 @@ object SparkTableUtil {\n     val metricsConfig = MetricsConfig.fromProperties(targetTable.properties)\n \n     val manifests = partitionDS\n-      .flatMap(partition => listPartition(partition, serializableConf, metricsConfig))\n+      .flatMap(partition => listPartition(partition, spec, serializableConf, metricsConfig))\n       .repartition(numShufflePartitions)\n-      .orderBy($\"path\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYxNDE5Nw=="}, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 333}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMwNzMzOA==", "bodyText": "What about this? It is a bit uglier than what we had before but should work.\nimplicit val manifestFileEncoder: Encoder[ManifestFile] = Encoders.javaSerialization[ManifestFile]\nimplicit val dataFileEncoder: Encoder[DataFile] = Encoders.javaSerialization[DataFile]\nimplicit val pathDataFileEncoder: Encoder[(String, DataFile)] = Encoders.tuple(Encoders.STRING, dataFileEncoder)\n\n....\n\nval manifests = partitionDS\n  .flatMap(partition => listPartition(partition, spec, serializableConf, metricsConfig))\n  .repartition(numShufflePartitions)\n  .map(file => (file.path.toString, file))\n  .orderBy($\"_1\")\n  .mapPartitions(files => buildManifest(serializableConf, spec, stagingDir)(files.map(_._2)))\n  .collect()\n\nThe reason sort is important is because we want to filter out irrelevant manifests when planning a job with a partition predicate. Each manifest keeps lower/upper bounds for partition values. If we place all files for the same partition in one manifest, we will need to read only one manifest to plan a job for that partition. If we don't sort files before creating manifests, we risk having files for one partition scattered across many manifests.", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380307338", "createdAt": "2020-02-17T17:41:20Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -527,9 +425,8 @@ object SparkTableUtil {\n     val metricsConfig = MetricsConfig.fromProperties(targetTable.properties)\n \n     val manifests = partitionDS\n-      .flatMap(partition => listPartition(partition, serializableConf, metricsConfig))\n+      .flatMap(partition => listPartition(partition, spec, serializableConf, metricsConfig))\n       .repartition(numShufflePartitions)\n-      .orderBy($\"path\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYxNDE5Nw=="}, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 333}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwNzA3Ng==", "bodyText": "Nice, I tried your way but forgot to define the tuple encoder. Thanks for the detailed explanation.", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380407076", "createdAt": "2020-02-18T00:34:18Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -527,9 +425,8 @@ object SparkTableUtil {\n     val metricsConfig = MetricsConfig.fromProperties(targetTable.properties)\n \n     val manifests = partitionDS\n-      .flatMap(partition => listPartition(partition, serializableConf, metricsConfig))\n+      .flatMap(partition => listPartition(partition, spec, serializableConf, metricsConfig))\n       .repartition(numShufflePartitions)\n-      .orderBy($\"path\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYxNDE5Nw=="}, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 333}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1MTIwODg5OnYy", "diffSide": "RIGHT", "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "isResolved": false, "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNlQxOTowMjoxNFrOFqUzBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDo1ODowM1rOFqycWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ==", "bodyText": "Shouldn't rowCount be a positive number?", "url": "https://github.com/apache/iceberg/pull/786#discussion_r379925255", "createdAt": "2020-02-16T19:02:14Z", "author": {"login": "rdsr"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -329,70 +222,74 @@ object SparkTableUtil {\n   private def listAvroPartition(\n       partitionPath: Map[String, String],\n       partitionUri: String,\n-      conf: Configuration): Seq[SparkDataFile] = {\n+      spec: PartitionSpec,\n+      conf: Configuration): Seq[DataFile] = {\n     val partition = new Path(partitionUri)\n     val fs = partition.getFileSystem(conf)\n \n     fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n-      SparkDataFile(\n-        stat.getPath.toString,\n-        partitionPath, \"avro\", stat.getLen,\n-        stat.getBlockSize,\n-        -1,\n-        null,\n-        null,\n-        null,\n-        null,\n-        null)\n+      val metrics = new Metrics(-1L, arrayToMap(null), arrayToMap(null), arrayToMap(null))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTk1Njc0NQ==", "bodyText": "I think the metric is not intended to be used so it is set to an invalid value. We might need to read through whole file to get row count, right?", "url": "https://github.com/apache/iceberg/pull/786#discussion_r379956745", "createdAt": "2020-02-17T01:45:39Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -329,70 +222,74 @@ object SparkTableUtil {\n   private def listAvroPartition(\n       partitionPath: Map[String, String],\n       partitionUri: String,\n-      conf: Configuration): Seq[SparkDataFile] = {\n+      spec: PartitionSpec,\n+      conf: Configuration): Seq[DataFile] = {\n     val partition = new Path(partitionUri)\n     val fs = partition.getFileSystem(conf)\n \n     fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n-      SparkDataFile(\n-        stat.getPath.toString,\n-        partitionPath, \"avro\", stat.getLen,\n-        stat.getBlockSize,\n-        -1,\n-        null,\n-        null,\n-        null,\n-        null,\n-        null)\n+      val metrics = new Metrics(-1L, arrayToMap(null), arrayToMap(null), arrayToMap(null))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDI2ODkxMw==", "bodyText": "Let's try to keep the logic as close to what we had before as possible.", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380268913", "createdAt": "2020-02-17T16:13:40Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -329,70 +222,74 @@ object SparkTableUtil {\n   private def listAvroPartition(\n       partitionPath: Map[String, String],\n       partitionUri: String,\n-      conf: Configuration): Seq[SparkDataFile] = {\n+      spec: PartitionSpec,\n+      conf: Configuration): Seq[DataFile] = {\n     val partition = new Path(partitionUri)\n     val fs = partition.getFileSystem(conf)\n \n     fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n-      SparkDataFile(\n-        stat.getPath.toString,\n-        partitionPath, \"avro\", stat.getLen,\n-        stat.getBlockSize,\n-        -1,\n-        null,\n-        null,\n-        null,\n-        null,\n-        null)\n+      val metrics = new Metrics(-1L, arrayToMap(null), arrayToMap(null), arrayToMap(null))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDI3Mjg2MQ==", "bodyText": "I think anything positive should do. keeping it <= 0 may possibly affect some scan planning code to filter out this particular file. e.g see org.apache.iceberg.expressions.InclusiveMetricsEvaluator\n@aokolnychyi , thoughts?", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380272861", "createdAt": "2020-02-17T16:21:30Z", "author": {"login": "rdsr"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -329,70 +222,74 @@ object SparkTableUtil {\n   private def listAvroPartition(\n       partitionPath: Map[String, String],\n       partitionUri: String,\n-      conf: Configuration): Seq[SparkDataFile] = {\n+      spec: PartitionSpec,\n+      conf: Configuration): Seq[DataFile] = {\n     val partition = new Path(partitionUri)\n     val fs = partition.getFileSystem(conf)\n \n     fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n-      SparkDataFile(\n-        stat.getPath.toString,\n-        partitionPath, \"avro\", stat.getLen,\n-        stat.getBlockSize,\n-        -1,\n-        null,\n-        null,\n-        null,\n-        null,\n-        null)\n+      val metrics = new Metrics(-1L, arrayToMap(null), arrayToMap(null), arrayToMap(null))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMyMDcxNA==", "bodyText": "Good catch, @rdsr. This is definitely a problem. Right now, the InclusiveMetricsEvaluator will remove files with negative or 0 row counts.\nI don't think that the solution is to use a positive number here. The reason why this was required is that we want good stats for job planning. Setting this to -1 causes a correctness bug, but setting it to some other constant will introduce bad behavior when using the stats that are provided by Iceberg. I think we should either count the number of records, use a heuristic (file size / est. row size?), or remove support for importing Avro tables. I'm leaning toward counting the number of records.\nWe should also change the check in InclusiveMetricsEvaluator to check for files with 0 rows and allow files with -1 rows through to fix the correctness bug for existing tables that used this path to import Avro data.", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380320714", "createdAt": "2020-02-17T18:22:39Z", "author": {"login": "rdblue"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -329,70 +222,74 @@ object SparkTableUtil {\n   private def listAvroPartition(\n       partitionPath: Map[String, String],\n       partitionUri: String,\n-      conf: Configuration): Seq[SparkDataFile] = {\n+      spec: PartitionSpec,\n+      conf: Configuration): Seq[DataFile] = {\n     val partition = new Path(partitionUri)\n     val fs = partition.getFileSystem(conf)\n \n     fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n-      SparkDataFile(\n-        stat.getPath.toString,\n-        partitionPath, \"avro\", stat.getLen,\n-        stat.getBlockSize,\n-        -1,\n-        null,\n-        null,\n-        null,\n-        null,\n-        null)\n+      val metrics = new Metrics(-1L, arrayToMap(null), arrayToMap(null), arrayToMap(null))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMyODY5OA==", "bodyText": "I think the number of records must be correct and precise as we want to answer some data queries with metadata (e.g. give me the number of records per partition). Updating our metrics evaluators to handle -1 seems reasonable to me as well.", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380328698", "createdAt": "2020-02-17T18:49:32Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -329,70 +222,74 @@ object SparkTableUtil {\n   private def listAvroPartition(\n       partitionPath: Map[String, String],\n       partitionUri: String,\n-      conf: Configuration): Seq[SparkDataFile] = {\n+      spec: PartitionSpec,\n+      conf: Configuration): Seq[DataFile] = {\n     val partition = new Path(partitionUri)\n     val fs = partition.getFileSystem(conf)\n \n     fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n-      SparkDataFile(\n-        stat.getPath.toString,\n-        partitionPath, \"avro\", stat.getLen,\n-        stat.getBlockSize,\n-        -1,\n-        null,\n-        null,\n-        null,\n-        null,\n-        null)\n+      val metrics = new Metrics(-1L, arrayToMap(null), arrayToMap(null), arrayToMap(null))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMyOTUzMw==", "bodyText": "@rdsr, could you create follow-up issues so that we don't forget?", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380329533", "createdAt": "2020-02-17T18:52:13Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -329,70 +222,74 @@ object SparkTableUtil {\n   private def listAvroPartition(\n       partitionPath: Map[String, String],\n       partitionUri: String,\n-      conf: Configuration): Seq[SparkDataFile] = {\n+      spec: PartitionSpec,\n+      conf: Configuration): Seq[DataFile] = {\n     val partition = new Path(partitionUri)\n     val fs = partition.getFileSystem(conf)\n \n     fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n-      SparkDataFile(\n-        stat.getPath.toString,\n-        partitionPath, \"avro\", stat.getLen,\n-        stat.getBlockSize,\n-        -1,\n-        null,\n-        null,\n-        null,\n-        null,\n-        null)\n+      val metrics = new Metrics(-1L, arrayToMap(null), arrayToMap(null), arrayToMap(null))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMzNzA1Mw==", "bodyText": "+1 I'll do that!", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380337053", "createdAt": "2020-02-17T19:18:24Z", "author": {"login": "rdsr"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -329,70 +222,74 @@ object SparkTableUtil {\n   private def listAvroPartition(\n       partitionPath: Map[String, String],\n       partitionUri: String,\n-      conf: Configuration): Seq[SparkDataFile] = {\n+      spec: PartitionSpec,\n+      conf: Configuration): Seq[DataFile] = {\n     val partition = new Path(partitionUri)\n     val fs = partition.getFileSystem(conf)\n \n     fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n-      SparkDataFile(\n-        stat.getPath.toString,\n-        partitionPath, \"avro\", stat.getLen,\n-        stat.getBlockSize,\n-        -1,\n-        null,\n-        null,\n-        null,\n-        null,\n-        null)\n+      val metrics = new Metrics(-1L, arrayToMap(null), arrayToMap(null), arrayToMap(null))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwNzQ2Mw==", "bodyText": "Thank you guys for the detail explanation!", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380407463", "createdAt": "2020-02-18T00:36:33Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -329,70 +222,74 @@ object SparkTableUtil {\n   private def listAvroPartition(\n       partitionPath: Map[String, String],\n       partitionUri: String,\n-      conf: Configuration): Seq[SparkDataFile] = {\n+      spec: PartitionSpec,\n+      conf: Configuration): Seq[DataFile] = {\n     val partition = new Path(partitionUri)\n     val fs = partition.getFileSystem(conf)\n \n     fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n-      SparkDataFile(\n-        stat.getPath.toString,\n-        partitionPath, \"avro\", stat.getLen,\n-        stat.getBlockSize,\n-        -1,\n-        null,\n-        null,\n-        null,\n-        null,\n-        null)\n+      val metrics = new Metrics(-1L, arrayToMap(null), arrayToMap(null), arrayToMap(null))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 192}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQxMDk2OA==", "bodyText": "Created #809 to track this.", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380410968", "createdAt": "2020-02-18T00:58:03Z", "author": {"login": "rdsr"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -329,70 +222,74 @@ object SparkTableUtil {\n   private def listAvroPartition(\n       partitionPath: Map[String, String],\n       partitionUri: String,\n-      conf: Configuration): Seq[SparkDataFile] = {\n+      spec: PartitionSpec,\n+      conf: Configuration): Seq[DataFile] = {\n     val partition = new Path(partitionUri)\n     val fs = partition.getFileSystem(conf)\n \n     fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n-      SparkDataFile(\n-        stat.getPath.toString,\n-        partitionPath, \"avro\", stat.getLen,\n-        stat.getBlockSize,\n-        -1,\n-        null,\n-        null,\n-        null,\n-        null,\n-        null)\n+      val metrics = new Metrics(-1L, arrayToMap(null), arrayToMap(null), arrayToMap(null))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 192}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM1NDcwMDAzOnYy", "diffSide": "RIGHT", "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwNTowMzowM1rOFq1Npw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwNTozNzowMlrOFq1nFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQ1NjM1OQ==", "bodyText": "nit: this can fit into 1 line if we reuse dataFileEncoder:\nimplicit val pathDataFileEncoder: Encoder[(String, DataFile)] = Encoders.tuple(Encoders.STRING, dataFileEncoder)", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380456359", "createdAt": "2020-02-18T05:03:03Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -516,6 +413,9 @@ object SparkTableUtil {\n       stagingDir: String): Unit = {\n \n     implicit val manifestFileEncoder: Encoder[ManifestFile] = Encoders.javaSerialization[ManifestFile]\n+    implicit val dataFileEncoder: Encoder[DataFile] = Encoders.javaSerialization[DataFile]\n+    implicit val pathDataFileEncoder: Encoder[(String, DataFile)] = Encoders.tuple(Encoders.STRING,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e0f4102aa11b6a2be56da23949f9ca81076923b"}, "originalPosition": 323}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQ2Mjg3MA==", "bodyText": "Oh, my bad.", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380462870", "createdAt": "2020-02-18T05:37:02Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -516,6 +413,9 @@ object SparkTableUtil {\n       stagingDir: String): Unit = {\n \n     implicit val manifestFileEncoder: Encoder[ManifestFile] = Encoders.javaSerialization[ManifestFile]\n+    implicit val dataFileEncoder: Encoder[DataFile] = Encoders.javaSerialization[DataFile]\n+    implicit val pathDataFileEncoder: Encoder[(String, DataFile)] = Encoders.tuple(Encoders.STRING,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQ1NjM1OQ=="}, "originalCommit": {"oid": "2e0f4102aa11b6a2be56da23949f9ca81076923b"}, "originalPosition": 323}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3022, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}