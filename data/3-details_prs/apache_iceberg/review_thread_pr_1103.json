{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMxMjU4NDQ0", "number": 1103, "reviewThreads": {"totalCount": 49, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNjo0NToyOVrOEDiSww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxNjo1Nzo1NFrOEKlw6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcyMTQzMDQzOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSchemaToTypeInfo.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNjo0NToyOVrOGgnCTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wOFQxNjo0NToyOVrOGgnCTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjg0NzE4MA==", "bodyText": "Can you add the log message you want now? Or just remove the try/catch as it's not adding anything right now.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r436847180", "createdAt": "2020-06-08T16:45:29Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSchemaToTypeInfo.java", "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+/**\n+ * Class to convert Iceberg types to Hive TypeInfo\n+ */\n+final class IcebergSchemaToTypeInfo {\n+\n+  private IcebergSchemaToTypeInfo() {\n+\n+  }\n+\n+  private static final ImmutableMap<Object, Object> primitiveTypeToTypeInfo = ImmutableMap.builder()\n+      .put(Types.BooleanType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BOOLEAN_TYPE_NAME))\n+      .put(Types.IntegerType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.INT_TYPE_NAME))\n+      .put(Types.LongType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.FloatType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.FLOAT_TYPE_NAME))\n+      .put(Types.DoubleType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DOUBLE_TYPE_NAME))\n+      .put(Types.BinaryType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BINARY_TYPE_NAME))\n+      .put(Types.StringType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME))\n+      .put(Types.DateType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DATE_TYPE_NAME))\n+      .put(Types.TimestampType.withoutZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.TimestampType.withZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .build();\n+\n+  public static List<TypeInfo> getColumnTypes(Schema schema) throws Exception {\n+    List<Types.NestedField> fields = schema.columns();\n+    List<TypeInfo> types = new ArrayList<>(fields.size());\n+    for (Types.NestedField field : fields) {\n+      types.add(generateTypeInfo(field.type()));\n+    }\n+    return types;\n+  }\n+\n+  private static TypeInfo generateTypeInfo(Type type) throws Exception {\n+    if (primitiveTypeToTypeInfo.containsKey(type)) {\n+      return (TypeInfo) primitiveTypeToTypeInfo.get(type);\n+    }\n+    switch (type.typeId()) {\n+      case UUID:\n+        return TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME);\n+      case FIXED:\n+        return TypeInfoFactory.getPrimitiveTypeInfo(\"binary\");\n+      case TIME:\n+        return TypeInfoFactory.getPrimitiveTypeInfo(\"long\");\n+      case DECIMAL:\n+        Types.DecimalType dec = (Types.DecimalType) type;\n+        int scale = dec.scale();\n+        int precision = dec.precision();\n+        try {\n+          HiveDecimalUtils.validateParameter(precision, scale);\n+        } catch (Exception e) {\n+          //TODO Log that precision / scale isn't valid", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "90311cc8ba9d11ace4e8c4237f5c908d95d91ce6"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDM5MDkzOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSchemaToTypeInfo.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoxODo1OVrOGh-43g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoxODo1OVrOGh-43g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI4NjU1OA==", "bodyText": "Nit: no need for a blank line here.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438286558", "createdAt": "2020-06-10T17:18:59Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSchemaToTypeInfo.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+/**\n+ * Class to convert Iceberg types to Hive TypeInfo\n+ */\n+final class IcebergSchemaToTypeInfo {\n+\n+  private IcebergSchemaToTypeInfo() {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDM5NTczOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSchemaToTypeInfo.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyMDoyMFrOGh-79A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyMDoyMFrOGh-79A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI4NzM0OA==", "bodyText": "Why not use serdeConstants here and for the fixed case?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438287348", "createdAt": "2020-06-10T17:20:20Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSchemaToTypeInfo.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+/**\n+ * Class to convert Iceberg types to Hive TypeInfo\n+ */\n+final class IcebergSchemaToTypeInfo {\n+\n+  private IcebergSchemaToTypeInfo() {\n+\n+  }\n+\n+  private static final ImmutableMap<Object, Object> primitiveTypeToTypeInfo = ImmutableMap.builder()\n+      .put(Types.BooleanType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BOOLEAN_TYPE_NAME))\n+      .put(Types.IntegerType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.INT_TYPE_NAME))\n+      .put(Types.LongType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.FloatType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.FLOAT_TYPE_NAME))\n+      .put(Types.DoubleType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DOUBLE_TYPE_NAME))\n+      .put(Types.BinaryType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BINARY_TYPE_NAME))\n+      .put(Types.StringType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME))\n+      .put(Types.DateType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DATE_TYPE_NAME))\n+      .put(Types.TimestampType.withoutZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.TimestampType.withZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .build();\n+\n+  public static List<TypeInfo> getColumnTypes(Schema schema) throws Exception {\n+    List<Types.NestedField> fields = schema.columns();\n+    List<TypeInfo> types = new ArrayList<>(fields.size());\n+    for (Types.NestedField field : fields) {\n+      types.add(generateTypeInfo(field.type()));\n+    }\n+    return types;\n+  }\n+\n+  private static TypeInfo generateTypeInfo(Type type) throws Exception {\n+    if (primitiveTypeToTypeInfo.containsKey(type)) {\n+      return (TypeInfo) primitiveTypeToTypeInfo.get(type);\n+    }\n+    switch (type.typeId()) {\n+      case UUID:\n+        return TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME);\n+      case FIXED:\n+        return TypeInfoFactory.getPrimitiveTypeInfo(\"binary\");\n+      case TIME:\n+        return TypeInfoFactory.getPrimitiveTypeInfo(\"long\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDQwMzE1OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergWritable.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyMjoyOFrOGh_Aug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyMjoyOFrOGh_Aug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI4ODU3MA==", "bodyText": "Since this is public, can you add Javadoc describing its purpose?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438288570", "createdAt": "2020-06-10T17:22:28Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergWritable.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.Record;\n+\n+public class IcebergWritable implements Writable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDQwMzY5OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergWritable.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyMjozNlrOGh_BEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyMjozNlrOGh_BEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI4ODY1Ng==", "bodyText": "No need to include empty public constructors.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438288656", "createdAt": "2020-06-10T17:22:36Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergWritable.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.Record;\n+\n+public class IcebergWritable implements Writable {\n+\n+  private Record record;\n+  private Schema schema;\n+\n+  public IcebergWritable() {}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDQwNTY1OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergWritable.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyMzoxMVrOGh_CUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyMzoxMVrOGh_CUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI4ODk3Nw==", "bodyText": "We typically use the verb wrap for this pattern.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438288977", "createdAt": "2020-06-10T17:23:11Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergWritable.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.Record;\n+\n+public class IcebergWritable implements Writable {\n+\n+  private Record record;\n+  private Schema schema;\n+\n+  public IcebergWritable() {}\n+\n+  public void setRecord(Record record) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDQwNjc1OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergWritable.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyMzozM1rOGh_DGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyMzozM1rOGh_DGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI4OTE3Nw==", "bodyText": "For getters, we omit get because it doesn't add any value.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438289177", "createdAt": "2020-06-10T17:23:33Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergWritable.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.Record;\n+\n+public class IcebergWritable implements Writable {\n+\n+  private Record record;\n+  private Schema schema;\n+\n+  public IcebergWritable() {}\n+\n+  public void setRecord(Record record) {\n+    this.record = record;\n+  }\n+\n+  public Record getRecord() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDQwODMxOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergWritable.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyNDowNFrOGh_EPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyNDowNFrOGh_EPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI4OTQ3MA==", "bodyText": "If this writable isn't actually writable, then I think this should throw UnsupportedOperationException here and in readFields.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438289470", "createdAt": "2020-06-10T17:24:04Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergWritable.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.Record;\n+\n+public class IcebergWritable implements Writable {\n+\n+  private Record record;\n+  private Schema schema;\n+\n+  public IcebergWritable() {}\n+\n+  public void setRecord(Record record) {\n+    this.record = record;\n+  }\n+\n+  public Record getRecord() {\n+    return record;\n+  }\n+\n+  public Schema getSchema() {\n+    return schema;\n+  }\n+\n+  public void setSchema(Schema schema) {\n+    this.schema = schema;\n+  }\n+\n+  @Override\n+  public void write(DataOutput dataOutput) throws IOException {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDQyMTQ3OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/SystemTableUtil.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyNzozNVrOGh_M2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQyMToxNDozM1rOGiu9kA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI5MTY3Mg==", "bodyText": "If this is configurable, then why use the double underscore name? Couldn't this use snapshot_id instead?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438291672", "createdAt": "2020-06-10T17:27:35Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/SystemTableUtil.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.types.Types;\n+\n+public class SystemTableUtil {\n+\n+  static final String VIRTUAL_COLUMN_NAME = \"iceberg.hive.snapshot.virtual.column.name\";\n+\n+  private static final String DEFAULT_SNAPSHOT_ID_COLUMN_NAME = \"snapshot__id\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODY0MzU1NA==", "bodyText": "We ended up going with the double underscore option to follow the convention of the inbuilt virtual columns in Hive: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+VirtualColumns\nI agree it does add a little bit of extra confusion with the slight difference between Iceberg's column name and our choice for the virtual column name, but the argument could go either way. We're happy to go for any option as it'll just be a matter of documenting - do you have a preference?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438643554", "createdAt": "2020-06-11T09:00:21Z", "author": {"login": "cmathiesen"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/SystemTableUtil.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.types.Types;\n+\n+public class SystemTableUtil {\n+\n+  static final String VIRTUAL_COLUMN_NAME = \"iceberg.hive.snapshot.virtual.column.name\";\n+\n+  private static final String DEFAULT_SNAPSHOT_ID_COLUMN_NAME = \"snapshot__id\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI5MTY3Mg=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA3NDE5Mg==", "bodyText": "If this is to follow an established pattern in Hive, I'm happy with it.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439074192", "createdAt": "2020-06-11T21:14:33Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/SystemTableUtil.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.types.Types;\n+\n+public class SystemTableUtil {\n+\n+  static final String VIRTUAL_COLUMN_NAME = \"iceberg.hive.snapshot.virtual.column.name\";\n+\n+  private static final String DEFAULT_SNAPSHOT_ID_COLUMN_NAME = \"snapshot__id\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI5MTY3Mg=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDQyNDc0OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/SystemTableUtil.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyODoyNVrOGh_O0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyODoyNVrOGh_O0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI5MjE3OA==", "bodyText": "We generally like to use Lists.newArrayList() to avoid depending on a specific implementation class.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438292178", "createdAt": "2020-06-10T17:28:25Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/SystemTableUtil.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.types.Types;\n+\n+public class SystemTableUtil {\n+\n+  static final String VIRTUAL_COLUMN_NAME = \"iceberg.hive.snapshot.virtual.column.name\";\n+\n+  private static final String DEFAULT_SNAPSHOT_ID_COLUMN_NAME = \"snapshot__id\";\n+\n+  private SystemTableUtil() {}\n+\n+  protected static Schema schemaWithVirtualColumn(Schema schema, String columnName) {\n+    List<Types.NestedField> columns = new ArrayList<>(schema.columns());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDQyOTg0OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/SystemTableUtil.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyOTo1MFrOGh_SPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzoyOTo1MFrOGh_SPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI5MzA1Mw==", "bodyText": "The field positions match, right? If so, then this could just iterate through positions in the original record instead of names. That would avoid a hashmap lookup of the position for both get and set.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438293053", "createdAt": "2020-06-10T17:29:50Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/SystemTableUtil.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.types.Types;\n+\n+public class SystemTableUtil {\n+\n+  static final String VIRTUAL_COLUMN_NAME = \"iceberg.hive.snapshot.virtual.column.name\";\n+\n+  private static final String DEFAULT_SNAPSHOT_ID_COLUMN_NAME = \"snapshot__id\";\n+\n+  private SystemTableUtil() {}\n+\n+  protected static Schema schemaWithVirtualColumn(Schema schema, String columnName) {\n+    List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+    columns.add(Types.NestedField.optional(Integer.MAX_VALUE, columnName, Types.LongType.get()));\n+    return new Schema(columns);\n+  }\n+\n+  protected static Record recordWithVirtualColumn(Record record, long snapshotId, Schema oldSchema,\n+                                                   String columnName) {\n+    Schema newSchema = schemaWithVirtualColumn(oldSchema, columnName);\n+    Record newRecord = GenericRecord.create(newSchema);\n+    for (Types.NestedField field : oldSchema.columns()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDQzMjc0OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/SystemTableUtil.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzozMDozM1rOGh_UAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNDozMzoyMlrOGkeUxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI5MzUwNQ==", "bodyText": "When are properties used and when is configuration used? I'm surprised that we need both.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438293505", "createdAt": "2020-06-10T17:30:33Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/SystemTableUtil.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.types.Types;\n+\n+public class SystemTableUtil {\n+\n+  static final String VIRTUAL_COLUMN_NAME = \"iceberg.hive.snapshot.virtual.column.name\";\n+\n+  private static final String DEFAULT_SNAPSHOT_ID_COLUMN_NAME = \"snapshot__id\";\n+\n+  private SystemTableUtil() {}\n+\n+  protected static Schema schemaWithVirtualColumn(Schema schema, String columnName) {\n+    List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+    columns.add(Types.NestedField.optional(Integer.MAX_VALUE, columnName, Types.LongType.get()));\n+    return new Schema(columns);\n+  }\n+\n+  protected static Record recordWithVirtualColumn(Record record, long snapshotId, Schema oldSchema,\n+                                                   String columnName) {\n+    Schema newSchema = schemaWithVirtualColumn(oldSchema, columnName);\n+    Record newRecord = GenericRecord.create(newSchema);\n+    for (Types.NestedField field : oldSchema.columns()) {\n+      newRecord.setField(field.name(), record.getField(field.name()));\n+    }\n+    newRecord.setField(columnName, snapshotId);\n+    return newRecord;\n+  }\n+\n+  protected static String getVirtualColumnName(Configuration conf) {\n+    String virtualColumnName = conf.get(VIRTUAL_COLUMN_NAME);\n+    if (virtualColumnName == null) {\n+      return DEFAULT_SNAPSHOT_ID_COLUMN_NAME;\n+    } else {\n+      return virtualColumnName;\n+    }\n+  }\n+\n+  protected static String getVirtualColumnName(Properties properties) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODY4ODg4MQ==", "bodyText": "Yeah we agree, we discovered this when adding the SerDe - the IF uses Configuration but the SerDe only uses Properties and we wanted to use the methods across both classes and it seemed simpler to overload a method rather than create new Properties from the Configuration in the IF", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438688881", "createdAt": "2020-06-11T10:24:48Z", "author": {"login": "cmathiesen"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/SystemTableUtil.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.types.Types;\n+\n+public class SystemTableUtil {\n+\n+  static final String VIRTUAL_COLUMN_NAME = \"iceberg.hive.snapshot.virtual.column.name\";\n+\n+  private static final String DEFAULT_SNAPSHOT_ID_COLUMN_NAME = \"snapshot__id\";\n+\n+  private SystemTableUtil() {}\n+\n+  protected static Schema schemaWithVirtualColumn(Schema schema, String columnName) {\n+    List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+    columns.add(Types.NestedField.optional(Integer.MAX_VALUE, columnName, Types.LongType.get()));\n+    return new Schema(columns);\n+  }\n+\n+  protected static Record recordWithVirtualColumn(Record record, long snapshotId, Schema oldSchema,\n+                                                   String columnName) {\n+    Schema newSchema = schemaWithVirtualColumn(oldSchema, columnName);\n+    Record newRecord = GenericRecord.create(newSchema);\n+    for (Types.NestedField field : oldSchema.columns()) {\n+      newRecord.setField(field.name(), record.getField(field.name()));\n+    }\n+    newRecord.setField(columnName, snapshotId);\n+    return newRecord;\n+  }\n+\n+  protected static String getVirtualColumnName(Configuration conf) {\n+    String virtualColumnName = conf.get(VIRTUAL_COLUMN_NAME);\n+    if (virtualColumnName == null) {\n+      return DEFAULT_SNAPSHOT_ID_COLUMN_NAME;\n+    } else {\n+      return virtualColumnName;\n+    }\n+  }\n+\n+  protected static String getVirtualColumnName(Properties properties) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI5MzUwNQ=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODY5MzY0Mg==", "bodyText": "Although is exactly what we're doing in the TableResolver class... :')", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438693642", "createdAt": "2020-06-11T10:34:59Z", "author": {"login": "cmathiesen"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/SystemTableUtil.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.types.Types;\n+\n+public class SystemTableUtil {\n+\n+  static final String VIRTUAL_COLUMN_NAME = \"iceberg.hive.snapshot.virtual.column.name\";\n+\n+  private static final String DEFAULT_SNAPSHOT_ID_COLUMN_NAME = \"snapshot__id\";\n+\n+  private SystemTableUtil() {}\n+\n+  protected static Schema schemaWithVirtualColumn(Schema schema, String columnName) {\n+    List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+    columns.add(Types.NestedField.optional(Integer.MAX_VALUE, columnName, Types.LongType.get()));\n+    return new Schema(columns);\n+  }\n+\n+  protected static Record recordWithVirtualColumn(Record record, long snapshotId, Schema oldSchema,\n+                                                   String columnName) {\n+    Schema newSchema = schemaWithVirtualColumn(oldSchema, columnName);\n+    Record newRecord = GenericRecord.create(newSchema);\n+    for (Types.NestedField field : oldSchema.columns()) {\n+      newRecord.setField(field.name(), record.getField(field.name()));\n+    }\n+    newRecord.setField(columnName, snapshotId);\n+    return newRecord;\n+  }\n+\n+  protected static String getVirtualColumnName(Configuration conf) {\n+    String virtualColumnName = conf.get(VIRTUAL_COLUMN_NAME);\n+    if (virtualColumnName == null) {\n+      return DEFAULT_SNAPSHOT_ID_COLUMN_NAME;\n+    } else {\n+      return virtualColumnName;\n+    }\n+  }\n+\n+  protected static String getVirtualColumnName(Properties properties) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI5MzUwNQ=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDg5ODc1OQ==", "bodyText": "The configuration contains all the configs we set in HiveConf and possible hadoop conf as well.\nThe properties are a  merged result of Hive table and partition properties.  We can see how Hive uses these as part of the initialize method of AbstractSerde\npublic void initialize(Configuration configuration, Properties tableProperties,\n                         Properties partitionProperties) throws SerDeException {\n    initialize(configuration,\n               SerDeUtils.createOverlayedProperties(tableProperties, partitionProperties));\n  }```", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r440898759", "createdAt": "2020-06-16T14:33:22Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/SystemTableUtil.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Properties;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.types.Types;\n+\n+public class SystemTableUtil {\n+\n+  static final String VIRTUAL_COLUMN_NAME = \"iceberg.hive.snapshot.virtual.column.name\";\n+\n+  private static final String DEFAULT_SNAPSHOT_ID_COLUMN_NAME = \"snapshot__id\";\n+\n+  private SystemTableUtil() {}\n+\n+  protected static Schema schemaWithVirtualColumn(Schema schema, String columnName) {\n+    List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+    columns.add(Types.NestedField.optional(Integer.MAX_VALUE, columnName, Types.LongType.get()));\n+    return new Schema(columns);\n+  }\n+\n+  protected static Record recordWithVirtualColumn(Record record, long snapshotId, Schema oldSchema,\n+                                                   String columnName) {\n+    Schema newSchema = schemaWithVirtualColumn(oldSchema, columnName);\n+    Record newRecord = GenericRecord.create(newSchema);\n+    for (Types.NestedField field : oldSchema.columns()) {\n+      newRecord.setField(field.name(), record.getField(field.name()));\n+    }\n+    newRecord.setField(columnName, snapshotId);\n+    return newRecord;\n+  }\n+\n+  protected static String getVirtualColumnName(Configuration conf) {\n+    String virtualColumnName = conf.get(VIRTUAL_COLUMN_NAME);\n+    if (virtualColumnName == null) {\n+      return DEFAULT_SNAPSHOT_ID_COLUMN_NAME;\n+    } else {\n+      return virtualColumnName;\n+    }\n+  }\n+\n+  protected static String getVirtualColumnName(Properties properties) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODI5MzUwNQ=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDQ3NDk2OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSchemaToTypeInfo.java", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzo0MjoxMFrOGh_u4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxMjo0MToyMVrOGjwa0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMwMDM4NQ==", "bodyText": "It looks like this would be easier to implement using the type visitors, which already have the logic to traverse a schema. A good example is converting a Type to Spark's DataType.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438300385", "createdAt": "2020-06-10T17:42:10Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSchemaToTypeInfo.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+/**\n+ * Class to convert Iceberg types to Hive TypeInfo\n+ */\n+final class IcebergSchemaToTypeInfo {\n+\n+  private IcebergSchemaToTypeInfo() {\n+\n+  }\n+\n+  private static final ImmutableMap<Object, Object> primitiveTypeToTypeInfo = ImmutableMap.builder()\n+      .put(Types.BooleanType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BOOLEAN_TYPE_NAME))\n+      .put(Types.IntegerType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.INT_TYPE_NAME))\n+      .put(Types.LongType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.FloatType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.FLOAT_TYPE_NAME))\n+      .put(Types.DoubleType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DOUBLE_TYPE_NAME))\n+      .put(Types.BinaryType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BINARY_TYPE_NAME))\n+      .put(Types.StringType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME))\n+      .put(Types.DateType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DATE_TYPE_NAME))\n+      .put(Types.TimestampType.withoutZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.TimestampType.withZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .build();\n+\n+  public static List<TypeInfo> getColumnTypes(Schema schema) throws Exception {\n+    List<Types.NestedField> fields = schema.columns();\n+    List<TypeInfo> types = new ArrayList<>(fields.size());\n+    for (Types.NestedField field : fields) {\n+      types.add(generateTypeInfo(field.type()));\n+    }\n+    return types;\n+  }\n+\n+  private static TypeInfo generateTypeInfo(Type type) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODY5NDQ4Nw==", "bodyText": "That looks way simpler, I'll get started on that", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438694487", "createdAt": "2020-06-11T10:36:46Z", "author": {"login": "cmathiesen"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSchemaToTypeInfo.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+/**\n+ * Class to convert Iceberg types to Hive TypeInfo\n+ */\n+final class IcebergSchemaToTypeInfo {\n+\n+  private IcebergSchemaToTypeInfo() {\n+\n+  }\n+\n+  private static final ImmutableMap<Object, Object> primitiveTypeToTypeInfo = ImmutableMap.builder()\n+      .put(Types.BooleanType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BOOLEAN_TYPE_NAME))\n+      .put(Types.IntegerType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.INT_TYPE_NAME))\n+      .put(Types.LongType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.FloatType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.FLOAT_TYPE_NAME))\n+      .put(Types.DoubleType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DOUBLE_TYPE_NAME))\n+      .put(Types.BinaryType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BINARY_TYPE_NAME))\n+      .put(Types.StringType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME))\n+      .put(Types.DateType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DATE_TYPE_NAME))\n+      .put(Types.TimestampType.withoutZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.TimestampType.withZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .build();\n+\n+  public static List<TypeInfo> getColumnTypes(Schema schema) throws Exception {\n+    List<Types.NestedField> fields = schema.columns();\n+    List<TypeInfo> types = new ArrayList<>(fields.size());\n+    for (Types.NestedField field : fields) {\n+      types.add(generateTypeInfo(field.type()));\n+    }\n+    return types;\n+  }\n+\n+  private static TypeInfo generateTypeInfo(Type type) throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMwMDM4NQ=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAwNTI2MA==", "bodyText": "I already have a type visitor somewhere from Schema to ObjectInspector. I can also submit that one on my PR so you can focus on the remaining things to do.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439005260", "createdAt": "2020-06-11T19:00:12Z", "author": {"login": "guilload"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSchemaToTypeInfo.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+/**\n+ * Class to convert Iceberg types to Hive TypeInfo\n+ */\n+final class IcebergSchemaToTypeInfo {\n+\n+  private IcebergSchemaToTypeInfo() {\n+\n+  }\n+\n+  private static final ImmutableMap<Object, Object> primitiveTypeToTypeInfo = ImmutableMap.builder()\n+      .put(Types.BooleanType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BOOLEAN_TYPE_NAME))\n+      .put(Types.IntegerType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.INT_TYPE_NAME))\n+      .put(Types.LongType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.FloatType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.FLOAT_TYPE_NAME))\n+      .put(Types.DoubleType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DOUBLE_TYPE_NAME))\n+      .put(Types.BinaryType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BINARY_TYPE_NAME))\n+      .put(Types.StringType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME))\n+      .put(Types.DateType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DATE_TYPE_NAME))\n+      .put(Types.TimestampType.withoutZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.TimestampType.withZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .build();\n+\n+  public static List<TypeInfo> getColumnTypes(Schema schema) throws Exception {\n+    List<Types.NestedField> fields = schema.columns();\n+    List<TypeInfo> types = new ArrayList<>(fields.size());\n+    for (Types.NestedField field : fields) {\n+      types.add(generateTypeInfo(field.type()));\n+    }\n+    return types;\n+  }\n+\n+  private static TypeInfo generateTypeInfo(Type type) throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMwMDM4NQ=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE5MDM1MQ==", "bodyText": "Yea +1 for a visitor. We have TypeInfo to Iceberg Type visitor for inspiration https://github.com/linkedin/iceberg/blob/master/hive/src/main/java/org/apache/iceberg/hive/legacy/HiveTypeToIcebergType.java", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439190351", "createdAt": "2020-06-12T03:42:12Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSchemaToTypeInfo.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+/**\n+ * Class to convert Iceberg types to Hive TypeInfo\n+ */\n+final class IcebergSchemaToTypeInfo {\n+\n+  private IcebergSchemaToTypeInfo() {\n+\n+  }\n+\n+  private static final ImmutableMap<Object, Object> primitiveTypeToTypeInfo = ImmutableMap.builder()\n+      .put(Types.BooleanType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BOOLEAN_TYPE_NAME))\n+      .put(Types.IntegerType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.INT_TYPE_NAME))\n+      .put(Types.LongType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.FloatType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.FLOAT_TYPE_NAME))\n+      .put(Types.DoubleType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DOUBLE_TYPE_NAME))\n+      .put(Types.BinaryType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BINARY_TYPE_NAME))\n+      .put(Types.StringType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME))\n+      .put(Types.DateType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DATE_TYPE_NAME))\n+      .put(Types.TimestampType.withoutZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.TimestampType.withZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .build();\n+\n+  public static List<TypeInfo> getColumnTypes(Schema schema) throws Exception {\n+    List<Types.NestedField> fields = schema.columns();\n+    List<TypeInfo> types = new ArrayList<>(fields.size());\n+    for (Types.NestedField field : fields) {\n+      types.add(generateTypeInfo(field.type()));\n+    }\n+    return types;\n+  }\n+\n+  private static TypeInfo generateTypeInfo(Type type) throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMwMDM4NQ=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTI5MTI5MA==", "bodyText": "@guilload that would be great, thank you!", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439291290", "createdAt": "2020-06-12T08:47:50Z", "author": {"login": "cmathiesen"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSchemaToTypeInfo.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+/**\n+ * Class to convert Iceberg types to Hive TypeInfo\n+ */\n+final class IcebergSchemaToTypeInfo {\n+\n+  private IcebergSchemaToTypeInfo() {\n+\n+  }\n+\n+  private static final ImmutableMap<Object, Object> primitiveTypeToTypeInfo = ImmutableMap.builder()\n+      .put(Types.BooleanType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BOOLEAN_TYPE_NAME))\n+      .put(Types.IntegerType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.INT_TYPE_NAME))\n+      .put(Types.LongType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.FloatType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.FLOAT_TYPE_NAME))\n+      .put(Types.DoubleType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DOUBLE_TYPE_NAME))\n+      .put(Types.BinaryType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BINARY_TYPE_NAME))\n+      .put(Types.StringType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME))\n+      .put(Types.DateType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DATE_TYPE_NAME))\n+      .put(Types.TimestampType.withoutZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.TimestampType.withZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .build();\n+\n+  public static List<TypeInfo> getColumnTypes(Schema schema) throws Exception {\n+    List<Types.NestedField> fields = schema.columns();\n+    List<TypeInfo> types = new ArrayList<>(fields.size());\n+    for (Types.NestedField field : fields) {\n+      types.add(generateTypeInfo(field.type()));\n+    }\n+    return types;\n+  }\n+\n+  private static TypeInfo generateTypeInfo(Type type) throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMwMDM4NQ=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTYwMDM2Ng==", "bodyText": "Still work in progress, things missing are mostly unit tests, but this is what it'll look like:\nguilload@3bffa7c", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439600366", "createdAt": "2020-06-12T19:16:41Z", "author": {"login": "guilload"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSchemaToTypeInfo.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+/**\n+ * Class to convert Iceberg types to Hive TypeInfo\n+ */\n+final class IcebergSchemaToTypeInfo {\n+\n+  private IcebergSchemaToTypeInfo() {\n+\n+  }\n+\n+  private static final ImmutableMap<Object, Object> primitiveTypeToTypeInfo = ImmutableMap.builder()\n+      .put(Types.BooleanType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BOOLEAN_TYPE_NAME))\n+      .put(Types.IntegerType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.INT_TYPE_NAME))\n+      .put(Types.LongType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.FloatType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.FLOAT_TYPE_NAME))\n+      .put(Types.DoubleType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DOUBLE_TYPE_NAME))\n+      .put(Types.BinaryType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BINARY_TYPE_NAME))\n+      .put(Types.StringType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME))\n+      .put(Types.DateType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DATE_TYPE_NAME))\n+      .put(Types.TimestampType.withoutZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.TimestampType.withZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .build();\n+\n+  public static List<TypeInfo> getColumnTypes(Schema schema) throws Exception {\n+    List<Types.NestedField> fields = schema.columns();\n+    List<TypeInfo> types = new ArrayList<>(fields.size());\n+    for (Types.NestedField field : fields) {\n+      types.add(generateTypeInfo(field.type()));\n+    }\n+    return types;\n+  }\n+\n+  private static TypeInfo generateTypeInfo(Type type) throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMwMDM4NQ=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDE0NjY0Mw==", "bodyText": "That looks promising, happy to move that in here when you're done if the others agree. Thanks!", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r440146643", "createdAt": "2020-06-15T12:41:21Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSchemaToTypeInfo.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+/**\n+ * Class to convert Iceberg types to Hive TypeInfo\n+ */\n+final class IcebergSchemaToTypeInfo {\n+\n+  private IcebergSchemaToTypeInfo() {\n+\n+  }\n+\n+  private static final ImmutableMap<Object, Object> primitiveTypeToTypeInfo = ImmutableMap.builder()\n+      .put(Types.BooleanType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BOOLEAN_TYPE_NAME))\n+      .put(Types.IntegerType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.INT_TYPE_NAME))\n+      .put(Types.LongType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.FloatType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.FLOAT_TYPE_NAME))\n+      .put(Types.DoubleType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DOUBLE_TYPE_NAME))\n+      .put(Types.BinaryType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BINARY_TYPE_NAME))\n+      .put(Types.StringType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME))\n+      .put(Types.DateType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DATE_TYPE_NAME))\n+      .put(Types.TimestampType.withoutZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.TimestampType.withZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .build();\n+\n+  public static List<TypeInfo> getColumnTypes(Schema schema) throws Exception {\n+    List<Types.NestedField> fields = schema.columns();\n+    List<TypeInfo> types = new ArrayList<>(fields.size());\n+    for (Types.NestedField field : fields) {\n+      types.add(generateTypeInfo(field.type()));\n+    }\n+    return types;\n+  }\n+\n+  private static TypeInfo generateTypeInfo(Type type) throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMwMDM4NQ=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDQ3NjY2OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSchemaToTypeInfo.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzo0MjozOFrOGh_v9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzo0MjozOFrOGh_v9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMwMDY2MQ==", "bodyText": "Doesn't Hive have timestamp types?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438300661", "createdAt": "2020-06-10T17:42:38Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSchemaToTypeInfo.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+/**\n+ * Class to convert Iceberg types to Hive TypeInfo\n+ */\n+final class IcebergSchemaToTypeInfo {\n+\n+  private IcebergSchemaToTypeInfo() {\n+\n+  }\n+\n+  private static final ImmutableMap<Object, Object> primitiveTypeToTypeInfo = ImmutableMap.builder()\n+      .put(Types.BooleanType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BOOLEAN_TYPE_NAME))\n+      .put(Types.IntegerType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.INT_TYPE_NAME))\n+      .put(Types.LongType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.FloatType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.FLOAT_TYPE_NAME))\n+      .put(Types.DoubleType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DOUBLE_TYPE_NAME))\n+      .put(Types.BinaryType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BINARY_TYPE_NAME))\n+      .put(Types.StringType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME))\n+      .put(Types.DateType.get(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DATE_TYPE_NAME))\n+      .put(Types.TimestampType.withoutZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))\n+      .put(Types.TimestampType.withZone(), TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.BIGINT_TYPE_NAME))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDQ4MzYxOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzo0NDozOVrOGh_0jg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzo0NDozOVrOGh_0jg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMwMTgzOA==", "bodyText": "I think getVritualColumnName should have a better method name. Here, it isn't clear what's happening because which virtual column is getting added is not obvious. If this were snapshotIdColumnName then I think it would be better.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438301838", "createdAt": "2020-06-10T17:44:39Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {\n+\n+  private Schema schema;\n+  private ObjectInspector inspector;\n+\n+  @Override\n+  public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    Table table = null;\n+    try {\n+      table = TableResolver.resolveTableFromConfiguration(configuration, serDeProperties);\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(\"Unable to resolve table from configuration: \", e);\n+    }\n+    this.schema = table.schema();\n+    if (table instanceof SnapshotsTable) {\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(schema);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    } else {\n+      List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+      columns.add(Types.NestedField.optional(Integer.MAX_VALUE, SystemTableUtil.getVirtualColumnName(serDeProperties),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDUwMjAyOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzo0OTo1NVrOGiAAtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzo0OTo1NVrOGiAAtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMwNDk1MA==", "bodyText": "Does this need to allocate a new ArrayList every time or can it reuse one?\nWe try to make the code called for every row in a tight loop (like this method) as small as possible for performance reasons. Ideally, we would be able to reuse this storage, have the list of columns already prepared, and access field values by column position instead of by name. Something like this:\npublic Object deserialize(Writable writable) {\n  Record record = ((IcebergWritable) writable).record();\n  for (int i = 0; i < recordSize; i += 1) {\n    reusedArray[i] = record.get(i, Object.class);\n  }\n  return reusedArrayAsList;\n}", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438304950", "createdAt": "2020-06-10T17:49:55Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {\n+\n+  private Schema schema;\n+  private ObjectInspector inspector;\n+\n+  @Override\n+  public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    Table table = null;\n+    try {\n+      table = TableResolver.resolveTableFromConfiguration(configuration, serDeProperties);\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(\"Unable to resolve table from configuration: \", e);\n+    }\n+    this.schema = table.schema();\n+    if (table instanceof SnapshotsTable) {\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(schema);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    } else {\n+      List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+      columns.add(Types.NestedField.optional(Integer.MAX_VALUE, SystemTableUtil.getVirtualColumnName(serDeProperties),\n+              Types.LongType.get()));\n+      Schema withVirtualColumn = new Schema(columns);\n+\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(withVirtualColumn);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Class<? extends Writable> getSerializedClass() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Writable serialize(Object o, ObjectInspector objectInspector) {\n+    return null;\n+  }\n+\n+  @Override\n+  public SerDeStats getSerDeStats() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Object deserialize(Writable writable) {\n+    IcebergWritable icebergWritable = (IcebergWritable) writable;\n+    List<Types.NestedField> fields = icebergWritable.getSchema().columns();\n+    List<Object> row = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDUxMjk1OnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSchemaToTypeInfo.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzo1MzowNFrOGiAILg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzo1MzowNFrOGiAILg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMwNjg2Mg==", "bodyText": "This should assert that the types are correct, not just that there is the correct number of types.\nAlso, please use context where possible to make it clear what the assertion expects, like \"Converted TypeInfo should have the same number of columns\". That makes it easier to understand when tests start to fail.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438306862", "createdAt": "2020-06-10T17:53:04Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSchemaToTypeInfo.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestIcebergSchemaToTypeInfo {\n+\n+  @Test\n+  public void testGeneratePrimitiveTypeInfo() throws Exception {\n+    Schema schema = new Schema(\n+        required(1, \"id\", Types.IntegerType.get()),\n+        optional(2, \"data\", Types.StringType.get()),\n+        required(8, \"feature1\", Types.BooleanType.get()),\n+        required(12, \"lat\", Types.FloatType.get()),\n+        required(15, \"x\", Types.LongType.get()),\n+        required(16, \"date\", Types.DateType.get()),\n+        required(17, \"double\", Types.DoubleType.get()),\n+        required(18, \"binary\", Types.BinaryType.get()));\n+    List<TypeInfo> types = IcebergSchemaToTypeInfo.getColumnTypes(schema);\n+\n+    assertEquals(8, types.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDUxOTg1OnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSchemaToTypeInfo.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzo1NTowNlrOGiANBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzo1NTowNlrOGiANBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMwODEwMg==", "bodyText": "No need for new ArrayList<>(...) here. Arrays.asList(...) should return a list.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438308102", "createdAt": "2020-06-10T17:55:06Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSchemaToTypeInfo.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestIcebergSchemaToTypeInfo {\n+\n+  @Test\n+  public void testGeneratePrimitiveTypeInfo() throws Exception {\n+    Schema schema = new Schema(\n+        required(1, \"id\", Types.IntegerType.get()),\n+        optional(2, \"data\", Types.StringType.get()),\n+        required(8, \"feature1\", Types.BooleanType.get()),\n+        required(12, \"lat\", Types.FloatType.get()),\n+        required(15, \"x\", Types.LongType.get()),\n+        required(16, \"date\", Types.DateType.get()),\n+        required(17, \"double\", Types.DoubleType.get()),\n+        required(18, \"binary\", Types.BinaryType.get()));\n+    List<TypeInfo> types = IcebergSchemaToTypeInfo.getColumnTypes(schema);\n+\n+    assertEquals(8, types.size());\n+  }\n+\n+  @Test\n+  public void testGenerateMapTypeInfo() throws Exception {\n+    TypeInfo expected = TypeInfoFactory.getMapTypeInfo(\n+        TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME),\n+        TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME));\n+\n+    Schema schema = new Schema(\n+        optional(7, \"properties\", Types.MapType.ofOptional(18, 19,\n+            Types.StringType.get(),\n+            Types.StringType.get()\n+        ), \"string map of properties\"));\n+\n+    List<TypeInfo> types = IcebergSchemaToTypeInfo.getColumnTypes(schema);\n+\n+    assertEquals(1, types.size());\n+    assertEquals(expected, types.get(0));\n+  }\n+\n+  @Test\n+  public void testGenerateListTypeInfo() throws Exception {\n+    TypeInfo expected = TypeInfoFactory\n+        .getListTypeInfo(TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DOUBLE_TYPE_NAME));\n+    Schema schema = new Schema(\n+        required(6, \"doubles\", Types.ListType.ofRequired(17,\n+            Types.DoubleType.get()\n+        )));\n+    List<TypeInfo> types = IcebergSchemaToTypeInfo.getColumnTypes(schema);\n+\n+    assertEquals(1, types.size());\n+    assertEquals(expected, types.get(0));\n+  }\n+\n+  @Test\n+  public void testGenerateMapAndStructTypeInfo() throws Exception {\n+    List<String> names1 = new ArrayList<>(Arrays.asList(\"address\", \"city\", \"state\", \"zip\"));\n+    List<TypeInfo> typeInfo1 = new ArrayList<>(Arrays.asList(\n+        TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME),\n+        TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME),\n+        TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME),\n+        TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.INT_TYPE_NAME)\n+    ));\n+    TypeInfo mapKeyStructExpected = TypeInfoFactory.getStructTypeInfo(names1, typeInfo1);\n+\n+    List<String> names2 = new ArrayList<>(Arrays.asList(\"lat\", \"long\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDUzMTE5OnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSchemaToTypeInfo.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzo1ODowNlrOGiAUXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzo1ODowNlrOGiAUXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMwOTk4Mg==", "bodyText": "Minor: these names could be better to make it clear what's happening. These are the types for the value struct, so you could name the variable valueTypes, and the names valueNames.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438309982", "createdAt": "2020-06-10T17:58:06Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSchemaToTypeInfo.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestIcebergSchemaToTypeInfo {\n+\n+  @Test\n+  public void testGeneratePrimitiveTypeInfo() throws Exception {\n+    Schema schema = new Schema(\n+        required(1, \"id\", Types.IntegerType.get()),\n+        optional(2, \"data\", Types.StringType.get()),\n+        required(8, \"feature1\", Types.BooleanType.get()),\n+        required(12, \"lat\", Types.FloatType.get()),\n+        required(15, \"x\", Types.LongType.get()),\n+        required(16, \"date\", Types.DateType.get()),\n+        required(17, \"double\", Types.DoubleType.get()),\n+        required(18, \"binary\", Types.BinaryType.get()));\n+    List<TypeInfo> types = IcebergSchemaToTypeInfo.getColumnTypes(schema);\n+\n+    assertEquals(8, types.size());\n+  }\n+\n+  @Test\n+  public void testGenerateMapTypeInfo() throws Exception {\n+    TypeInfo expected = TypeInfoFactory.getMapTypeInfo(\n+        TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME),\n+        TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME));\n+\n+    Schema schema = new Schema(\n+        optional(7, \"properties\", Types.MapType.ofOptional(18, 19,\n+            Types.StringType.get(),\n+            Types.StringType.get()\n+        ), \"string map of properties\"));\n+\n+    List<TypeInfo> types = IcebergSchemaToTypeInfo.getColumnTypes(schema);\n+\n+    assertEquals(1, types.size());\n+    assertEquals(expected, types.get(0));\n+  }\n+\n+  @Test\n+  public void testGenerateListTypeInfo() throws Exception {\n+    TypeInfo expected = TypeInfoFactory\n+        .getListTypeInfo(TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.DOUBLE_TYPE_NAME));\n+    Schema schema = new Schema(\n+        required(6, \"doubles\", Types.ListType.ofRequired(17,\n+            Types.DoubleType.get()\n+        )));\n+    List<TypeInfo> types = IcebergSchemaToTypeInfo.getColumnTypes(schema);\n+\n+    assertEquals(1, types.size());\n+    assertEquals(expected, types.get(0));\n+  }\n+\n+  @Test\n+  public void testGenerateMapAndStructTypeInfo() throws Exception {\n+    List<String> names1 = new ArrayList<>(Arrays.asList(\"address\", \"city\", \"state\", \"zip\"));\n+    List<TypeInfo> typeInfo1 = new ArrayList<>(Arrays.asList(\n+        TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME),\n+        TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME),\n+        TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.STRING_TYPE_NAME),\n+        TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.INT_TYPE_NAME)\n+    ));\n+    TypeInfo mapKeyStructExpected = TypeInfoFactory.getStructTypeInfo(names1, typeInfo1);\n+\n+    List<String> names2 = new ArrayList<>(Arrays.asList(\"lat\", \"long\"));\n+    List<TypeInfo> typeInfo2 = new ArrayList<>(Arrays.asList(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDUzNzA0OnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSerDe.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzo1OTo0NFrOGiAYMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxNzo1OTo0NFrOGiAYMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxMDk2MA==", "bodyText": "Minor: you might consider changing the signature of createCustomRecord to Schema, Object... and wrap the object array as a list internally. Then these would be shorter: createCustomRecord(schema, \"Michael\", 3000L).", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438310960", "createdAt": "2020-06-10T17:59:44Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSerDe.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestIcebergSerDe {\n+\n+  private File tableLocation;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @Before\n+  public void before() throws IOException {\n+    tableLocation = temp.newFolder();\n+    Schema schema = new Schema(optional(1, \"name\", Types.StringType.get()),\n+        optional(2, \"salary\", Types.LongType.get()));\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+\n+    Configuration conf = new Configuration();\n+    HadoopCatalog catalog = new HadoopCatalog(conf, tableLocation.getAbsolutePath());\n+    TableIdentifier id = TableIdentifier.parse(\"source_db.table_a\");\n+    Table table = catalog.createTable(id, schema, spec);\n+\n+    List<Record> data = new ArrayList<>();\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Michael\", 3000L)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDU0MTI5OnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSerDe.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODowMDo1M1rOGiAa7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQyMToxMzoyOFrOGiu8BQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxMTY2MA==", "bodyText": "Does Hive support non-string map keys? It looks like all of the test cases are string maps.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438311660", "createdAt": "2020-06-10T18:00:53Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSerDe.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestIcebergSerDe {\n+\n+  private File tableLocation;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @Before\n+  public void before() throws IOException {\n+    tableLocation = temp.newFolder();\n+    Schema schema = new Schema(optional(1, \"name\", Types.StringType.get()),\n+        optional(2, \"salary\", Types.LongType.get()));\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+\n+    Configuration conf = new Configuration();\n+    HadoopCatalog catalog = new HadoopCatalog(conf, tableLocation.getAbsolutePath());\n+    TableIdentifier id = TableIdentifier.parse(\"source_db.table_a\");\n+    Table table = catalog.createTable(id, schema, spec);\n+\n+    List<Record> data = new ArrayList<>();\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Michael\", 3000L)));\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Andy\", 3000L)));\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Berta\", 4000L)));\n+\n+    DataFile fileA = TestHelpers.writeFile(temp.newFile(), table, null, FileFormat.PARQUET, data);\n+\n+    table.newAppend().appendFile(fileA).commit();\n+  }\n+\n+  @Test\n+  public void testDeserializeMap() {\n+    Schema schema = new Schema(required(1, \"map_type\", Types.MapType\n+        .ofRequired(18, 19, Types.StringType.get(), Types.StringType.get())));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODcyODU5Mw==", "bodyText": "Yeah, it supports all primitive types as keys I believe - I've added an extra test case using an Integer key type, but should I add a larger range of tests like this one?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438728593", "createdAt": "2020-06-11T11:52:43Z", "author": {"login": "cmathiesen"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSerDe.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestIcebergSerDe {\n+\n+  private File tableLocation;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @Before\n+  public void before() throws IOException {\n+    tableLocation = temp.newFolder();\n+    Schema schema = new Schema(optional(1, \"name\", Types.StringType.get()),\n+        optional(2, \"salary\", Types.LongType.get()));\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+\n+    Configuration conf = new Configuration();\n+    HadoopCatalog catalog = new HadoopCatalog(conf, tableLocation.getAbsolutePath());\n+    TableIdentifier id = TableIdentifier.parse(\"source_db.table_a\");\n+    Table table = catalog.createTable(id, schema, spec);\n+\n+    List<Record> data = new ArrayList<>();\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Michael\", 3000L)));\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Andy\", 3000L)));\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Berta\", 4000L)));\n+\n+    DataFile fileA = TestHelpers.writeFile(temp.newFile(), table, null, FileFormat.PARQUET, data);\n+\n+    table.newAppend().appendFile(fileA).commit();\n+  }\n+\n+  @Test\n+  public void testDeserializeMap() {\n+    Schema schema = new Schema(required(1, \"map_type\", Types.MapType\n+        .ofRequired(18, 19, Types.StringType.get(), Types.StringType.get())));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxMTY2MA=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA3Mzc5Nw==", "bodyText": "No, one non-string test case is good.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439073797", "createdAt": "2020-06-11T21:13:28Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSerDe.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestIcebergSerDe {\n+\n+  private File tableLocation;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @Before\n+  public void before() throws IOException {\n+    tableLocation = temp.newFolder();\n+    Schema schema = new Schema(optional(1, \"name\", Types.StringType.get()),\n+        optional(2, \"salary\", Types.LongType.get()));\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+\n+    Configuration conf = new Configuration();\n+    HadoopCatalog catalog = new HadoopCatalog(conf, tableLocation.getAbsolutePath());\n+    TableIdentifier id = TableIdentifier.parse(\"source_db.table_a\");\n+    Table table = catalog.createTable(id, schema, spec);\n+\n+    List<Record> data = new ArrayList<>();\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Michael\", 3000L)));\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Andy\", 3000L)));\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Berta\", 4000L)));\n+\n+    DataFile fileA = TestHelpers.writeFile(temp.newFile(), table, null, FileFormat.PARQUET, data);\n+\n+    table.newAppend().appendFile(fileA).commit();\n+  }\n+\n+  @Test\n+  public void testDeserializeMap() {\n+    Schema schema = new Schema(required(1, \"map_type\", Types.MapType\n+        .ofRequired(18, 19, Types.StringType.get(), Types.StringType.get())));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxMTY2MA=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDU0NjQ1OnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSerDe.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODowMjoyOVrOGiAeWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODowMjoyOVrOGiAeWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxMjUzOQ==", "bodyText": "I think the assertEquals is sufficient since it checks deep equality.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438312539", "createdAt": "2020-06-10T18:02:29Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSerDe.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestIcebergSerDe {\n+\n+  private File tableLocation;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @Before\n+  public void before() throws IOException {\n+    tableLocation = temp.newFolder();\n+    Schema schema = new Schema(optional(1, \"name\", Types.StringType.get()),\n+        optional(2, \"salary\", Types.LongType.get()));\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+\n+    Configuration conf = new Configuration();\n+    HadoopCatalog catalog = new HadoopCatalog(conf, tableLocation.getAbsolutePath());\n+    TableIdentifier id = TableIdentifier.parse(\"source_db.table_a\");\n+    Table table = catalog.createTable(id, schema, spec);\n+\n+    List<Record> data = new ArrayList<>();\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Michael\", 3000L)));\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Andy\", 3000L)));\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Berta\", 4000L)));\n+\n+    DataFile fileA = TestHelpers.writeFile(temp.newFile(), table, null, FileFormat.PARQUET, data);\n+\n+    table.newAppend().appendFile(fileA).commit();\n+  }\n+\n+  @Test\n+  public void testDeserializeMap() {\n+    Schema schema = new Schema(required(1, \"map_type\", Types.MapType\n+        .ofRequired(18, 19, Types.StringType.get(), Types.StringType.get())));\n+    Map<String, String> expected = ImmutableMap.of(\"foo\", \"bar\");\n+    List<Map> data = new ArrayList<>();\n+    data.add(expected);\n+\n+    Record record = TestHelpers.createCustomRecord(schema, data);\n+    IcebergWritable writable = new IcebergWritable();\n+    writable.setRecord(record);\n+    writable.setSchema(schema);\n+\n+    IcebergSerDe serDe = new IcebergSerDe();\n+    List<Object> deserialized = (List<Object>) serDe.deserialize(writable);\n+    Map result = (Map) deserialized.get(0);\n+\n+    assertEquals(expected, result);\n+    assertTrue(result.containsKey(\"foo\"));\n+    assertTrue(result.containsValue(\"bar\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDU0NzAyOnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSerDe.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODowMjozOVrOGiAevw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODowMjozOVrOGiAevw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxMjYzOQ==", "bodyText": "Should this use the object inspectors?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438312639", "createdAt": "2020-06-10T18:02:39Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSerDe.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestIcebergSerDe {\n+\n+  private File tableLocation;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @Before\n+  public void before() throws IOException {\n+    tableLocation = temp.newFolder();\n+    Schema schema = new Schema(optional(1, \"name\", Types.StringType.get()),\n+        optional(2, \"salary\", Types.LongType.get()));\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+\n+    Configuration conf = new Configuration();\n+    HadoopCatalog catalog = new HadoopCatalog(conf, tableLocation.getAbsolutePath());\n+    TableIdentifier id = TableIdentifier.parse(\"source_db.table_a\");\n+    Table table = catalog.createTable(id, schema, spec);\n+\n+    List<Record> data = new ArrayList<>();\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Michael\", 3000L)));\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Andy\", 3000L)));\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Berta\", 4000L)));\n+\n+    DataFile fileA = TestHelpers.writeFile(temp.newFile(), table, null, FileFormat.PARQUET, data);\n+\n+    table.newAppend().appendFile(fileA).commit();\n+  }\n+\n+  @Test\n+  public void testDeserializeMap() {\n+    Schema schema = new Schema(required(1, \"map_type\", Types.MapType\n+        .ofRequired(18, 19, Types.StringType.get(), Types.StringType.get())));\n+    Map<String, String> expected = ImmutableMap.of(\"foo\", \"bar\");\n+    List<Map> data = new ArrayList<>();\n+    data.add(expected);\n+\n+    Record record = TestHelpers.createCustomRecord(schema, data);\n+    IcebergWritable writable = new IcebergWritable();\n+    writable.setRecord(record);\n+    writable.setSchema(schema);\n+\n+    IcebergSerDe serDe = new IcebergSerDe();\n+    List<Object> deserialized = (List<Object>) serDe.deserialize(writable);\n+    Map result = (Map) deserialized.get(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDU0OTc2OnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSerDe.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODowMzoyOFrOGiAgmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQyMToxMjozOVrOGiu6wQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxMzExMg==", "bodyText": "I'm not sure so many test cases are necessary since this is really just testing that a record is converted to a collection with the same field order. That's all deserialize is doing.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438313112", "createdAt": "2020-06-10T18:03:28Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSerDe.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestIcebergSerDe {\n+\n+  private File tableLocation;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @Before\n+  public void before() throws IOException {\n+    tableLocation = temp.newFolder();\n+    Schema schema = new Schema(optional(1, \"name\", Types.StringType.get()),\n+        optional(2, \"salary\", Types.LongType.get()));\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+\n+    Configuration conf = new Configuration();\n+    HadoopCatalog catalog = new HadoopCatalog(conf, tableLocation.getAbsolutePath());\n+    TableIdentifier id = TableIdentifier.parse(\"source_db.table_a\");\n+    Table table = catalog.createTable(id, schema, spec);\n+\n+    List<Record> data = new ArrayList<>();\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Michael\", 3000L)));\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Andy\", 3000L)));\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Berta\", 4000L)));\n+\n+    DataFile fileA = TestHelpers.writeFile(temp.newFile(), table, null, FileFormat.PARQUET, data);\n+\n+    table.newAppend().appendFile(fileA).commit();\n+  }\n+\n+  @Test\n+  public void testDeserializeMap() {\n+    Schema schema = new Schema(required(1, \"map_type\", Types.MapType\n+        .ofRequired(18, 19, Types.StringType.get(), Types.StringType.get())));\n+    Map<String, String> expected = ImmutableMap.of(\"foo\", \"bar\");\n+    List<Map> data = new ArrayList<>();\n+    data.add(expected);\n+\n+    Record record = TestHelpers.createCustomRecord(schema, data);\n+    IcebergWritable writable = new IcebergWritable();\n+    writable.setRecord(record);\n+    writable.setSchema(schema);\n+\n+    IcebergSerDe serDe = new IcebergSerDe();\n+    List<Object> deserialized = (List<Object>) serDe.deserialize(writable);\n+    Map result = (Map) deserialized.get(0);\n+\n+    assertEquals(expected, result);\n+    assertTrue(result.containsKey(\"foo\"));\n+    assertTrue(result.containsValue(\"bar\"));\n+  }\n+\n+  @Test\n+  public void testDeserializeList() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA3MzQ3Mw==", "bodyText": "And if we take the advice from @guilload and just return the record, then we don't need to test at all! We could convert the whole suite to testing the object inspectors against Iceberg generics.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439073473", "createdAt": "2020-06-11T21:12:39Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestIcebergSerDe.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TestIcebergSerDe {\n+\n+  private File tableLocation;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @Before\n+  public void before() throws IOException {\n+    tableLocation = temp.newFolder();\n+    Schema schema = new Schema(optional(1, \"name\", Types.StringType.get()),\n+        optional(2, \"salary\", Types.LongType.get()));\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+\n+    Configuration conf = new Configuration();\n+    HadoopCatalog catalog = new HadoopCatalog(conf, tableLocation.getAbsolutePath());\n+    TableIdentifier id = TableIdentifier.parse(\"source_db.table_a\");\n+    Table table = catalog.createTable(id, schema, spec);\n+\n+    List<Record> data = new ArrayList<>();\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Michael\", 3000L)));\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Andy\", 3000L)));\n+    data.add(TestHelpers.createCustomRecord(schema, Arrays.asList(\"Berta\", 4000L)));\n+\n+    DataFile fileA = TestHelpers.writeFile(temp.newFile(), table, null, FileFormat.PARQUET, data);\n+\n+    table.newAppend().appendFile(fileA).commit();\n+  }\n+\n+  @Test\n+  public void testDeserializeMap() {\n+    Schema schema = new Schema(required(1, \"map_type\", Types.MapType\n+        .ofRequired(18, 19, Types.StringType.get(), Types.StringType.get())));\n+    Map<String, String> expected = ImmutableMap.of(\"foo\", \"bar\");\n+    List<Map> data = new ArrayList<>();\n+    data.add(expected);\n+\n+    Record record = TestHelpers.createCustomRecord(schema, data);\n+    IcebergWritable writable = new IcebergWritable();\n+    writable.setRecord(record);\n+    writable.setSchema(schema);\n+\n+    IcebergSerDe serDe = new IcebergSerDe();\n+    List<Object> deserialized = (List<Object>) serDe.deserialize(writable);\n+    Map result = (Map) deserialized.get(0);\n+\n+    assertEquals(expected, result);\n+    assertTrue(result.containsKey(\"foo\"));\n+    assertTrue(result.containsValue(\"bar\"));\n+  }\n+\n+  @Test\n+  public void testDeserializeList() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxMzExMg=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDU1NTA1OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergObjectInspectorGenerator.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODowNTowM1rOGiAj-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQwMzo0MDo0OVrOGi2CJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxMzk3OA==", "bodyText": "Does Hive use LocalDate as its internal representation for date? That's what Iceberg generics are going to return. I think the object inspector needs to convert that correctly to the representation that Hive expects.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438313978", "createdAt": "2020-06-10T18:05:03Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergObjectInspectorGenerator.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n+\n+class IcebergObjectInspectorGenerator {\n+\n+  protected ObjectInspector createObjectInspector(Schema schema) throws Exception {\n+    List<String> columnNames = setColumnNames(schema);\n+    List<TypeInfo> columnTypes = IcebergSchemaToTypeInfo.getColumnTypes(schema);\n+\n+    List<ObjectInspector> columnOIs = new ArrayList<>(columnTypes.size());\n+    for (int i = 0; i < columnTypes.size(); i++) {\n+      columnOIs.add(createObjectInspectorWorker(columnTypes.get(i)));\n+    }\n+    return ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, columnOIs, null);\n+  }\n+\n+  protected ObjectInspector createObjectInspectorWorker(TypeInfo typeInfo) throws Exception {\n+    ObjectInspector.Category typeCategory = typeInfo.getCategory();\n+\n+    switch (typeCategory) {\n+      case PRIMITIVE:\n+        PrimitiveTypeInfo pti = (PrimitiveTypeInfo) typeInfo;\n+        return PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(pti);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg5NTY3Mw==", "bodyText": "Hive uses java.sql.Date and java.sql.Timestamp so I've been working on doing some converting between types to stop Hive complaining :')", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438895673", "createdAt": "2020-06-11T15:55:42Z", "author": {"login": "cmathiesen"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergObjectInspectorGenerator.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n+\n+class IcebergObjectInspectorGenerator {\n+\n+  protected ObjectInspector createObjectInspector(Schema schema) throws Exception {\n+    List<String> columnNames = setColumnNames(schema);\n+    List<TypeInfo> columnTypes = IcebergSchemaToTypeInfo.getColumnTypes(schema);\n+\n+    List<ObjectInspector> columnOIs = new ArrayList<>(columnTypes.size());\n+    for (int i = 0; i < columnTypes.size(); i++) {\n+      columnOIs.add(createObjectInspectorWorker(columnTypes.get(i)));\n+    }\n+    return ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, columnOIs, null);\n+  }\n+\n+  protected ObjectInspector createObjectInspectorWorker(TypeInfo typeInfo) throws Exception {\n+    ObjectInspector.Category typeCategory = typeInfo.getCategory();\n+\n+    switch (typeCategory) {\n+      case PRIMITIVE:\n+        PrimitiveTypeInfo pti = (PrimitiveTypeInfo) typeInfo;\n+        return PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(pti);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxMzk3OA=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA3MzI2OQ==", "bodyText": "It sounds like @guilload can help out with some of the object inspector questions.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439073269", "createdAt": "2020-06-11T21:12:09Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergObjectInspectorGenerator.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n+\n+class IcebergObjectInspectorGenerator {\n+\n+  protected ObjectInspector createObjectInspector(Schema schema) throws Exception {\n+    List<String> columnNames = setColumnNames(schema);\n+    List<TypeInfo> columnTypes = IcebergSchemaToTypeInfo.getColumnTypes(schema);\n+\n+    List<ObjectInspector> columnOIs = new ArrayList<>(columnTypes.size());\n+    for (int i = 0; i < columnTypes.size(); i++) {\n+      columnOIs.add(createObjectInspectorWorker(columnTypes.get(i)));\n+    }\n+    return ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, columnOIs, null);\n+  }\n+\n+  protected ObjectInspector createObjectInspectorWorker(TypeInfo typeInfo) throws Exception {\n+    ObjectInspector.Category typeCategory = typeInfo.getCategory();\n+\n+    switch (typeCategory) {\n+      case PRIMITIVE:\n+        PrimitiveTypeInfo pti = (PrimitiveTypeInfo) typeInfo;\n+        return PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(pti);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxMzk3OA=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE5MDA1NA==", "bodyText": "Maybe this can be a visitor?  We recently built a visitor the other way round Hive -> Iceberg. https://github.com/linkedin/iceberg/blob/master/hive/src/main/java/org/apache/iceberg/hive/legacy/HiveTypeToIcebergType.java", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439190054", "createdAt": "2020-06-12T03:40:49Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergObjectInspectorGenerator.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n+\n+class IcebergObjectInspectorGenerator {\n+\n+  protected ObjectInspector createObjectInspector(Schema schema) throws Exception {\n+    List<String> columnNames = setColumnNames(schema);\n+    List<TypeInfo> columnTypes = IcebergSchemaToTypeInfo.getColumnTypes(schema);\n+\n+    List<ObjectInspector> columnOIs = new ArrayList<>(columnTypes.size());\n+    for (int i = 0; i < columnTypes.size(); i++) {\n+      columnOIs.add(createObjectInspectorWorker(columnTypes.get(i)));\n+    }\n+    return ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, columnOIs, null);\n+  }\n+\n+  protected ObjectInspector createObjectInspectorWorker(TypeInfo typeInfo) throws Exception {\n+    ObjectInspector.Category typeCategory = typeInfo.getCategory();\n+\n+    switch (typeCategory) {\n+      case PRIMITIVE:\n+        PrimitiveTypeInfo pti = (PrimitiveTypeInfo) typeInfo;\n+        return PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(pti);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxMzk3OA=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDU1NzcwOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODowNTo0NlrOGiAllg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxMjozODo1OVrOGjwWDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxNDM5MA==", "bodyText": "Does this need to be removed from the InputFormat class?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438314390", "createdAt": "2020-06-10T18:05:46Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.expressions.Expression;\n+\n+public class InputFormatConfig {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODY1OTYwNA==", "bodyText": "This is from #933, specifically at https://github.com/apache/iceberg/pull/933/files#diff-0aa47c0ca21b715226d83ae55f4b4671 so we just pulled that class in here \"as is\" as it was easier. If you'd prefer we can trim this down to just the config needed by the SerDe or we can (temporarily) put the code elsewhere in this PR and then refactor it back into this class when we come back to#933 after this is merged.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438659604", "createdAt": "2020-06-11T09:29:32Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.expressions.Expression;\n+\n+public class InputFormatConfig {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxNDM5MA=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA3Mjg1Nw==", "bodyText": "I think it's safer to either reference it in place or to move it in this commit. Otherwise, changes to the other class may not show up as git commit conflicts and we could accidentally lose them by adding the class in one commit and removing the other copy later.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439072857", "createdAt": "2020-06-11T21:11:10Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.expressions.Expression;\n+\n+public class InputFormatConfig {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxNDM5MA=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ2MzgyNQ==", "bodyText": "I don't think I understand what you want us to do here. We need it here for this to compile and get merged in separately from #933. We basically cut and paste the class out of the other branch to here. We'll happily resolve any conflicts etc. in #933 later if needed.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439463825", "createdAt": "2020-06-12T14:46:25Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.expressions.Expression;\n+\n+public class InputFormatConfig {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxNDM5MA=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUwNzI1MA==", "bodyText": "If we are moving code into its own top-level class, then we need to delete the code that is moving from its original location so we don't drop changes that are introduced in the mean time.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439507250", "createdAt": "2020-06-12T16:02:57Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.expressions.Expression;\n+\n+public class InputFormatConfig {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxNDM5MA=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDE0NTQyMg==", "bodyText": "Ah, OK, now I see what you mean, yes, that makes sense. I've done that now", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r440145422", "createdAt": "2020-06-15T12:38:59Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/InputFormatConfig.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.expressions.Expression;\n+\n+public class InputFormatConfig {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxNDM5MA=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDU3MzI2OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoxMDoyMlrOGiAvyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoxMDoyMlrOGiAvyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxNzAwMA==", "bodyText": "Why does this coerce to a URI and get the path? The full path should be used so I don't see a reason to modify it.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438317000", "createdAt": "2020-06-10T18:10:22Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Properties;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+\n+final class TableResolver {\n+\n+  private TableResolver() {\n+  }\n+\n+  static Table resolveTableFromJob(JobConf conf) throws IOException {\n+    Properties properties = new Properties();\n+    properties.setProperty(InputFormatConfig.CATALOG_NAME, extractProperty(conf, InputFormatConfig.CATALOG_NAME));\n+    if (conf.get(InputFormatConfig.CATALOG_NAME).equals(InputFormatConfig.HADOOP_CATALOG)) {\n+      properties.setProperty(InputFormatConfig.SNAPSHOT_TABLE, conf.get(InputFormatConfig.SNAPSHOT_TABLE, \"true\"));\n+    }\n+    properties.setProperty(InputFormatConfig.TABLE_LOCATION, extractProperty(conf, InputFormatConfig.TABLE_LOCATION));\n+    properties.setProperty(InputFormatConfig.TABLE_NAME, extractProperty(conf, InputFormatConfig.TABLE_NAME));\n+    return resolveTableFromConfiguration(conf, properties);\n+  }\n+\n+  static Table resolveTableFromConfiguration(Configuration conf, Properties properties) throws IOException {\n+    String catalogName = properties.getProperty(InputFormatConfig.CATALOG_NAME);\n+    URI tableLocation = pathAsURI(properties.getProperty(InputFormatConfig.TABLE_LOCATION));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDU3Nzg3OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoxMTo0NFrOGiAyyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNjowNDo0MVrOGjJcbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxNzc3MQ==", "bodyText": "I think the warehouse path should be passed separately, possibly as table properties but maybe just defaulting to the Hive warehouse path. It doesn't make sense to me to pass a full URI and then use HadoopCatalog instead of HadoopTables, which handles tables at a specific path. There is no benefit to HadoopCatalog if you have the full table location.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438317771", "createdAt": "2020-06-10T18:11:44Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Properties;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+\n+final class TableResolver {\n+\n+  private TableResolver() {\n+  }\n+\n+  static Table resolveTableFromJob(JobConf conf) throws IOException {\n+    Properties properties = new Properties();\n+    properties.setProperty(InputFormatConfig.CATALOG_NAME, extractProperty(conf, InputFormatConfig.CATALOG_NAME));\n+    if (conf.get(InputFormatConfig.CATALOG_NAME).equals(InputFormatConfig.HADOOP_CATALOG)) {\n+      properties.setProperty(InputFormatConfig.SNAPSHOT_TABLE, conf.get(InputFormatConfig.SNAPSHOT_TABLE, \"true\"));\n+    }\n+    properties.setProperty(InputFormatConfig.TABLE_LOCATION, extractProperty(conf, InputFormatConfig.TABLE_LOCATION));\n+    properties.setProperty(InputFormatConfig.TABLE_NAME, extractProperty(conf, InputFormatConfig.TABLE_NAME));\n+    return resolveTableFromConfiguration(conf, properties);\n+  }\n+\n+  static Table resolveTableFromConfiguration(Configuration conf, Properties properties) throws IOException {\n+    String catalogName = properties.getProperty(InputFormatConfig.CATALOG_NAME);\n+    URI tableLocation = pathAsURI(properties.getProperty(InputFormatConfig.TABLE_LOCATION));\n+    if (catalogName == null) {\n+      throw new IllegalArgumentException(\"Catalog property: 'iceberg.catalog' not set in JobConf\");\n+    }\n+    switch (catalogName) {\n+      case InputFormatConfig.HADOOP_TABLES:\n+        HadoopTables tables = new HadoopTables(conf);\n+        return tables.load(tableLocation.getPath());\n+      case InputFormatConfig.HADOOP_CATALOG:\n+        String tableName = properties.getProperty(InputFormatConfig.TABLE_NAME);\n+        TableIdentifier id = TableIdentifier.parse(tableName);\n+        if (tableName.endsWith(InputFormatConfig.SNAPSHOT_TABLE_SUFFIX)) {\n+          if (!Boolean.parseBoolean(properties.getProperty(InputFormatConfig.SNAPSHOT_TABLE,\n+                  Boolean.TRUE.toString()))) {\n+            String tablePath = id.toString().replaceAll(\"\\\\.\", \"/\");\n+            URI warehouseLocation = pathAsURI(tableLocation.getPath().replaceAll(tablePath, \"\"));\n+            HadoopCatalog catalog = new HadoopCatalog(conf, warehouseLocation.getPath());\n+            return catalog.loadTable(id);\n+          } else {\n+            return resolveMetadataTable(conf, tableLocation.getPath(), tableName);\n+          }\n+        } else {\n+          URI warehouseLocation = pathAsURI(extractWarehousePath(tableLocation.getPath(), tableName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODc3NjUxOA==", "bodyText": "We had initially implemented the IF using a method similar to what you're describing: we required a user to add an extra table property called iceberg.warehouse.location I think and used that to resolve the table with HadoopCatalog. The LOCATION property of your Hive table has to be unique for each table (which is why we always have a property for a full table path), so we couldn't put a path to a common warehouse there. We initially tried using the same location for multiple tables and relied on the table name/ID to resolve the table correctly in Iceberg but that caused major confusion in Hive when trying to join tables :')\nSo originally we used an extra property for the warehouse location, but we realised we could extract this location from the full table path and save the user an extra configuration property. We don't have any strong preference either way, so I'm happy to change this as you've suggested\nAlthough thinking about this raises another point - I've just realised/tested that you could create an Iceberg table using HadoopCatalog but if you had the full path you can use HadoopTables to load the table again instead... And with Hive we can always expect to have the full path to the table because the location needs to be unique, so we could simplify here and only use HadoopTables for this IF? @massdosage", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438776518", "createdAt": "2020-06-11T13:22:37Z", "author": {"login": "cmathiesen"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Properties;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+\n+final class TableResolver {\n+\n+  private TableResolver() {\n+  }\n+\n+  static Table resolveTableFromJob(JobConf conf) throws IOException {\n+    Properties properties = new Properties();\n+    properties.setProperty(InputFormatConfig.CATALOG_NAME, extractProperty(conf, InputFormatConfig.CATALOG_NAME));\n+    if (conf.get(InputFormatConfig.CATALOG_NAME).equals(InputFormatConfig.HADOOP_CATALOG)) {\n+      properties.setProperty(InputFormatConfig.SNAPSHOT_TABLE, conf.get(InputFormatConfig.SNAPSHOT_TABLE, \"true\"));\n+    }\n+    properties.setProperty(InputFormatConfig.TABLE_LOCATION, extractProperty(conf, InputFormatConfig.TABLE_LOCATION));\n+    properties.setProperty(InputFormatConfig.TABLE_NAME, extractProperty(conf, InputFormatConfig.TABLE_NAME));\n+    return resolveTableFromConfiguration(conf, properties);\n+  }\n+\n+  static Table resolveTableFromConfiguration(Configuration conf, Properties properties) throws IOException {\n+    String catalogName = properties.getProperty(InputFormatConfig.CATALOG_NAME);\n+    URI tableLocation = pathAsURI(properties.getProperty(InputFormatConfig.TABLE_LOCATION));\n+    if (catalogName == null) {\n+      throw new IllegalArgumentException(\"Catalog property: 'iceberg.catalog' not set in JobConf\");\n+    }\n+    switch (catalogName) {\n+      case InputFormatConfig.HADOOP_TABLES:\n+        HadoopTables tables = new HadoopTables(conf);\n+        return tables.load(tableLocation.getPath());\n+      case InputFormatConfig.HADOOP_CATALOG:\n+        String tableName = properties.getProperty(InputFormatConfig.TABLE_NAME);\n+        TableIdentifier id = TableIdentifier.parse(tableName);\n+        if (tableName.endsWith(InputFormatConfig.SNAPSHOT_TABLE_SUFFIX)) {\n+          if (!Boolean.parseBoolean(properties.getProperty(InputFormatConfig.SNAPSHOT_TABLE,\n+                  Boolean.TRUE.toString()))) {\n+            String tablePath = id.toString().replaceAll(\"\\\\.\", \"/\");\n+            URI warehouseLocation = pathAsURI(tableLocation.getPath().replaceAll(tablePath, \"\"));\n+            HadoopCatalog catalog = new HadoopCatalog(conf, warehouseLocation.getPath());\n+            return catalog.loadTable(id);\n+          } else {\n+            return resolveMetadataTable(conf, tableLocation.getPath(), tableName);\n+          }\n+        } else {\n+          URI warehouseLocation = pathAsURI(extractWarehousePath(tableLocation.getPath(), tableName));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxNzc3MQ=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODgzNzcyMg==", "bodyText": "IIRC when the InputFormat \"runs\" there is always a value in the associated HiveConf named \"location\" (InputFormatConfig.TABLE_LOCATION above) which points to the \"base\" location of the table being queried. So I think we should be able to just use that and require that the location is set correctly when the Hive table is created which is the normal way of doing things for external tables. Then we can just use that here and not worry about additional table properties or the other code paths which hopefully simplifies things.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438837722", "createdAt": "2020-06-11T14:41:51Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Properties;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+\n+final class TableResolver {\n+\n+  private TableResolver() {\n+  }\n+\n+  static Table resolveTableFromJob(JobConf conf) throws IOException {\n+    Properties properties = new Properties();\n+    properties.setProperty(InputFormatConfig.CATALOG_NAME, extractProperty(conf, InputFormatConfig.CATALOG_NAME));\n+    if (conf.get(InputFormatConfig.CATALOG_NAME).equals(InputFormatConfig.HADOOP_CATALOG)) {\n+      properties.setProperty(InputFormatConfig.SNAPSHOT_TABLE, conf.get(InputFormatConfig.SNAPSHOT_TABLE, \"true\"));\n+    }\n+    properties.setProperty(InputFormatConfig.TABLE_LOCATION, extractProperty(conf, InputFormatConfig.TABLE_LOCATION));\n+    properties.setProperty(InputFormatConfig.TABLE_NAME, extractProperty(conf, InputFormatConfig.TABLE_NAME));\n+    return resolveTableFromConfiguration(conf, properties);\n+  }\n+\n+  static Table resolveTableFromConfiguration(Configuration conf, Properties properties) throws IOException {\n+    String catalogName = properties.getProperty(InputFormatConfig.CATALOG_NAME);\n+    URI tableLocation = pathAsURI(properties.getProperty(InputFormatConfig.TABLE_LOCATION));\n+    if (catalogName == null) {\n+      throw new IllegalArgumentException(\"Catalog property: 'iceberg.catalog' not set in JobConf\");\n+    }\n+    switch (catalogName) {\n+      case InputFormatConfig.HADOOP_TABLES:\n+        HadoopTables tables = new HadoopTables(conf);\n+        return tables.load(tableLocation.getPath());\n+      case InputFormatConfig.HADOOP_CATALOG:\n+        String tableName = properties.getProperty(InputFormatConfig.TABLE_NAME);\n+        TableIdentifier id = TableIdentifier.parse(tableName);\n+        if (tableName.endsWith(InputFormatConfig.SNAPSHOT_TABLE_SUFFIX)) {\n+          if (!Boolean.parseBoolean(properties.getProperty(InputFormatConfig.SNAPSHOT_TABLE,\n+                  Boolean.TRUE.toString()))) {\n+            String tablePath = id.toString().replaceAll(\"\\\\.\", \"/\");\n+            URI warehouseLocation = pathAsURI(tableLocation.getPath().replaceAll(tablePath, \"\"));\n+            HadoopCatalog catalog = new HadoopCatalog(conf, warehouseLocation.getPath());\n+            return catalog.loadTable(id);\n+          } else {\n+            return resolveMetadataTable(conf, tableLocation.getPath(), tableName);\n+          }\n+        } else {\n+          URI warehouseLocation = pathAsURI(extractWarehousePath(tableLocation.getPath(), tableName));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxNzc3MQ=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA3MjI5Nw==", "bodyText": "I've just realised/tested that you could create an Iceberg table using HadoopCatalog but if you had the full path you can use HadoopTables to load the table again instead...\n\nYes, this is why I said there is no benefit to using HadoopCatalog. If you already have the full path, you can open the table with HadoopTables and not worry about the warehouse path to pass to the catalog.\nThe only drawback is that this approach doesn't work for Hive tables. You'd need to know whether the table is a Hive table or a Hadoop table because they use different commit mechanisms.\nFor tables tracked by the iceberg-hive connector, you'd just need to instantiate the HiveCatalog and use the table name to load the table.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439072297", "createdAt": "2020-06-11T21:09:51Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Properties;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+\n+final class TableResolver {\n+\n+  private TableResolver() {\n+  }\n+\n+  static Table resolveTableFromJob(JobConf conf) throws IOException {\n+    Properties properties = new Properties();\n+    properties.setProperty(InputFormatConfig.CATALOG_NAME, extractProperty(conf, InputFormatConfig.CATALOG_NAME));\n+    if (conf.get(InputFormatConfig.CATALOG_NAME).equals(InputFormatConfig.HADOOP_CATALOG)) {\n+      properties.setProperty(InputFormatConfig.SNAPSHOT_TABLE, conf.get(InputFormatConfig.SNAPSHOT_TABLE, \"true\"));\n+    }\n+    properties.setProperty(InputFormatConfig.TABLE_LOCATION, extractProperty(conf, InputFormatConfig.TABLE_LOCATION));\n+    properties.setProperty(InputFormatConfig.TABLE_NAME, extractProperty(conf, InputFormatConfig.TABLE_NAME));\n+    return resolveTableFromConfiguration(conf, properties);\n+  }\n+\n+  static Table resolveTableFromConfiguration(Configuration conf, Properties properties) throws IOException {\n+    String catalogName = properties.getProperty(InputFormatConfig.CATALOG_NAME);\n+    URI tableLocation = pathAsURI(properties.getProperty(InputFormatConfig.TABLE_LOCATION));\n+    if (catalogName == null) {\n+      throw new IllegalArgumentException(\"Catalog property: 'iceberg.catalog' not set in JobConf\");\n+    }\n+    switch (catalogName) {\n+      case InputFormatConfig.HADOOP_TABLES:\n+        HadoopTables tables = new HadoopTables(conf);\n+        return tables.load(tableLocation.getPath());\n+      case InputFormatConfig.HADOOP_CATALOG:\n+        String tableName = properties.getProperty(InputFormatConfig.TABLE_NAME);\n+        TableIdentifier id = TableIdentifier.parse(tableName);\n+        if (tableName.endsWith(InputFormatConfig.SNAPSHOT_TABLE_SUFFIX)) {\n+          if (!Boolean.parseBoolean(properties.getProperty(InputFormatConfig.SNAPSHOT_TABLE,\n+                  Boolean.TRUE.toString()))) {\n+            String tablePath = id.toString().replaceAll(\"\\\\.\", \"/\");\n+            URI warehouseLocation = pathAsURI(tableLocation.getPath().replaceAll(tablePath, \"\"));\n+            HadoopCatalog catalog = new HadoopCatalog(conf, warehouseLocation.getPath());\n+            return catalog.loadTable(id);\n+          } else {\n+            return resolveMetadataTable(conf, tableLocation.getPath(), tableName);\n+          }\n+        } else {\n+          URI warehouseLocation = pathAsURI(extractWarehousePath(tableLocation.getPath(), tableName));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxNzc3MQ=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTMxMjQyNg==", "bodyText": "OK, may I suggest we just focus on supporting HadoopTables using the location for now and then add support for HiveCatalog in a future PR after this is merged? Just to keep the scope of this PR down etc.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439312426", "createdAt": "2020-06-12T09:31:06Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Properties;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+\n+final class TableResolver {\n+\n+  private TableResolver() {\n+  }\n+\n+  static Table resolveTableFromJob(JobConf conf) throws IOException {\n+    Properties properties = new Properties();\n+    properties.setProperty(InputFormatConfig.CATALOG_NAME, extractProperty(conf, InputFormatConfig.CATALOG_NAME));\n+    if (conf.get(InputFormatConfig.CATALOG_NAME).equals(InputFormatConfig.HADOOP_CATALOG)) {\n+      properties.setProperty(InputFormatConfig.SNAPSHOT_TABLE, conf.get(InputFormatConfig.SNAPSHOT_TABLE, \"true\"));\n+    }\n+    properties.setProperty(InputFormatConfig.TABLE_LOCATION, extractProperty(conf, InputFormatConfig.TABLE_LOCATION));\n+    properties.setProperty(InputFormatConfig.TABLE_NAME, extractProperty(conf, InputFormatConfig.TABLE_NAME));\n+    return resolveTableFromConfiguration(conf, properties);\n+  }\n+\n+  static Table resolveTableFromConfiguration(Configuration conf, Properties properties) throws IOException {\n+    String catalogName = properties.getProperty(InputFormatConfig.CATALOG_NAME);\n+    URI tableLocation = pathAsURI(properties.getProperty(InputFormatConfig.TABLE_LOCATION));\n+    if (catalogName == null) {\n+      throw new IllegalArgumentException(\"Catalog property: 'iceberg.catalog' not set in JobConf\");\n+    }\n+    switch (catalogName) {\n+      case InputFormatConfig.HADOOP_TABLES:\n+        HadoopTables tables = new HadoopTables(conf);\n+        return tables.load(tableLocation.getPath());\n+      case InputFormatConfig.HADOOP_CATALOG:\n+        String tableName = properties.getProperty(InputFormatConfig.TABLE_NAME);\n+        TableIdentifier id = TableIdentifier.parse(tableName);\n+        if (tableName.endsWith(InputFormatConfig.SNAPSHOT_TABLE_SUFFIX)) {\n+          if (!Boolean.parseBoolean(properties.getProperty(InputFormatConfig.SNAPSHOT_TABLE,\n+                  Boolean.TRUE.toString()))) {\n+            String tablePath = id.toString().replaceAll(\"\\\\.\", \"/\");\n+            URI warehouseLocation = pathAsURI(tableLocation.getPath().replaceAll(tablePath, \"\"));\n+            HadoopCatalog catalog = new HadoopCatalog(conf, warehouseLocation.getPath());\n+            return catalog.loadTable(id);\n+          } else {\n+            return resolveMetadataTable(conf, tableLocation.getPath(), tableName);\n+          }\n+        } else {\n+          URI warehouseLocation = pathAsURI(extractWarehousePath(tableLocation.getPath(), tableName));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxNzc3MQ=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUwODA3Nw==", "bodyText": "Yeah, that sounds good.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439508077", "createdAt": "2020-06-12T16:04:41Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Properties;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+\n+final class TableResolver {\n+\n+  private TableResolver() {\n+  }\n+\n+  static Table resolveTableFromJob(JobConf conf) throws IOException {\n+    Properties properties = new Properties();\n+    properties.setProperty(InputFormatConfig.CATALOG_NAME, extractProperty(conf, InputFormatConfig.CATALOG_NAME));\n+    if (conf.get(InputFormatConfig.CATALOG_NAME).equals(InputFormatConfig.HADOOP_CATALOG)) {\n+      properties.setProperty(InputFormatConfig.SNAPSHOT_TABLE, conf.get(InputFormatConfig.SNAPSHOT_TABLE, \"true\"));\n+    }\n+    properties.setProperty(InputFormatConfig.TABLE_LOCATION, extractProperty(conf, InputFormatConfig.TABLE_LOCATION));\n+    properties.setProperty(InputFormatConfig.TABLE_NAME, extractProperty(conf, InputFormatConfig.TABLE_NAME));\n+    return resolveTableFromConfiguration(conf, properties);\n+  }\n+\n+  static Table resolveTableFromConfiguration(Configuration conf, Properties properties) throws IOException {\n+    String catalogName = properties.getProperty(InputFormatConfig.CATALOG_NAME);\n+    URI tableLocation = pathAsURI(properties.getProperty(InputFormatConfig.TABLE_LOCATION));\n+    if (catalogName == null) {\n+      throw new IllegalArgumentException(\"Catalog property: 'iceberg.catalog' not set in JobConf\");\n+    }\n+    switch (catalogName) {\n+      case InputFormatConfig.HADOOP_TABLES:\n+        HadoopTables tables = new HadoopTables(conf);\n+        return tables.load(tableLocation.getPath());\n+      case InputFormatConfig.HADOOP_CATALOG:\n+        String tableName = properties.getProperty(InputFormatConfig.TABLE_NAME);\n+        TableIdentifier id = TableIdentifier.parse(tableName);\n+        if (tableName.endsWith(InputFormatConfig.SNAPSHOT_TABLE_SUFFIX)) {\n+          if (!Boolean.parseBoolean(properties.getProperty(InputFormatConfig.SNAPSHOT_TABLE,\n+                  Boolean.TRUE.toString()))) {\n+            String tablePath = id.toString().replaceAll(\"\\\\.\", \"/\");\n+            URI warehouseLocation = pathAsURI(tableLocation.getPath().replaceAll(tablePath, \"\"));\n+            HadoopCatalog catalog = new HadoopCatalog(conf, warehouseLocation.getPath());\n+            return catalog.loadTable(id);\n+          } else {\n+            return resolveMetadataTable(conf, tableLocation.getPath(), tableName);\n+          }\n+        } else {\n+          URI warehouseLocation = pathAsURI(extractWarehousePath(tableLocation.getPath(), tableName));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxNzc3MQ=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDU3OTc0OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoxMjoxM1rOGiAz9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMTowMTowMFrOGqn0WQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxODA2OQ==", "bodyText": "How are the db and table name passed?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438318069", "createdAt": "2020-06-10T18:12:13Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Properties;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+\n+final class TableResolver {\n+\n+  private TableResolver() {\n+  }\n+\n+  static Table resolveTableFromJob(JobConf conf) throws IOException {\n+    Properties properties = new Properties();\n+    properties.setProperty(InputFormatConfig.CATALOG_NAME, extractProperty(conf, InputFormatConfig.CATALOG_NAME));\n+    if (conf.get(InputFormatConfig.CATALOG_NAME).equals(InputFormatConfig.HADOOP_CATALOG)) {\n+      properties.setProperty(InputFormatConfig.SNAPSHOT_TABLE, conf.get(InputFormatConfig.SNAPSHOT_TABLE, \"true\"));\n+    }\n+    properties.setProperty(InputFormatConfig.TABLE_LOCATION, extractProperty(conf, InputFormatConfig.TABLE_LOCATION));\n+    properties.setProperty(InputFormatConfig.TABLE_NAME, extractProperty(conf, InputFormatConfig.TABLE_NAME));\n+    return resolveTableFromConfiguration(conf, properties);\n+  }\n+\n+  static Table resolveTableFromConfiguration(Configuration conf, Properties properties) throws IOException {\n+    String catalogName = properties.getProperty(InputFormatConfig.CATALOG_NAME);\n+    URI tableLocation = pathAsURI(properties.getProperty(InputFormatConfig.TABLE_LOCATION));\n+    if (catalogName == null) {\n+      throw new IllegalArgumentException(\"Catalog property: 'iceberg.catalog' not set in JobConf\");\n+    }\n+    switch (catalogName) {\n+      case InputFormatConfig.HADOOP_TABLES:\n+        HadoopTables tables = new HadoopTables(conf);\n+        return tables.load(tableLocation.getPath());\n+      case InputFormatConfig.HADOOP_CATALOG:\n+        String tableName = properties.getProperty(InputFormatConfig.TABLE_NAME);\n+        TableIdentifier id = TableIdentifier.parse(tableName);\n+        if (tableName.endsWith(InputFormatConfig.SNAPSHOT_TABLE_SUFFIX)) {\n+          if (!Boolean.parseBoolean(properties.getProperty(InputFormatConfig.SNAPSHOT_TABLE,\n+                  Boolean.TRUE.toString()))) {\n+            String tablePath = id.toString().replaceAll(\"\\\\.\", \"/\");\n+            URI warehouseLocation = pathAsURI(tableLocation.getPath().replaceAll(tablePath, \"\"));\n+            HadoopCatalog catalog = new HadoopCatalog(conf, warehouseLocation.getPath());\n+            return catalog.loadTable(id);\n+          } else {\n+            return resolveMetadataTable(conf, tableLocation.getPath(), tableName);\n+          }\n+        } else {\n+          URI warehouseLocation = pathAsURI(extractWarehousePath(tableLocation.getPath(), tableName));\n+          HadoopCatalog catalog = new HadoopCatalog(conf, warehouseLocation.getPath());\n+          return catalog.loadTable(id);\n+        }\n+      case InputFormatConfig.HIVE_CATALOG:\n+        //TODO Implement HiveCatalog", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODc3Nzc0MQ==", "bodyText": "Hive adds a property to the conf called name which is whatever you've defined in the CREATE TABLE name part, so that property will include db and table name", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r438777741", "createdAt": "2020-06-11T13:24:43Z", "author": {"login": "cmathiesen"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Properties;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+\n+final class TableResolver {\n+\n+  private TableResolver() {\n+  }\n+\n+  static Table resolveTableFromJob(JobConf conf) throws IOException {\n+    Properties properties = new Properties();\n+    properties.setProperty(InputFormatConfig.CATALOG_NAME, extractProperty(conf, InputFormatConfig.CATALOG_NAME));\n+    if (conf.get(InputFormatConfig.CATALOG_NAME).equals(InputFormatConfig.HADOOP_CATALOG)) {\n+      properties.setProperty(InputFormatConfig.SNAPSHOT_TABLE, conf.get(InputFormatConfig.SNAPSHOT_TABLE, \"true\"));\n+    }\n+    properties.setProperty(InputFormatConfig.TABLE_LOCATION, extractProperty(conf, InputFormatConfig.TABLE_LOCATION));\n+    properties.setProperty(InputFormatConfig.TABLE_NAME, extractProperty(conf, InputFormatConfig.TABLE_NAME));\n+    return resolveTableFromConfiguration(conf, properties);\n+  }\n+\n+  static Table resolveTableFromConfiguration(Configuration conf, Properties properties) throws IOException {\n+    String catalogName = properties.getProperty(InputFormatConfig.CATALOG_NAME);\n+    URI tableLocation = pathAsURI(properties.getProperty(InputFormatConfig.TABLE_LOCATION));\n+    if (catalogName == null) {\n+      throw new IllegalArgumentException(\"Catalog property: 'iceberg.catalog' not set in JobConf\");\n+    }\n+    switch (catalogName) {\n+      case InputFormatConfig.HADOOP_TABLES:\n+        HadoopTables tables = new HadoopTables(conf);\n+        return tables.load(tableLocation.getPath());\n+      case InputFormatConfig.HADOOP_CATALOG:\n+        String tableName = properties.getProperty(InputFormatConfig.TABLE_NAME);\n+        TableIdentifier id = TableIdentifier.parse(tableName);\n+        if (tableName.endsWith(InputFormatConfig.SNAPSHOT_TABLE_SUFFIX)) {\n+          if (!Boolean.parseBoolean(properties.getProperty(InputFormatConfig.SNAPSHOT_TABLE,\n+                  Boolean.TRUE.toString()))) {\n+            String tablePath = id.toString().replaceAll(\"\\\\.\", \"/\");\n+            URI warehouseLocation = pathAsURI(tableLocation.getPath().replaceAll(tablePath, \"\"));\n+            HadoopCatalog catalog = new HadoopCatalog(conf, warehouseLocation.getPath());\n+            return catalog.loadTable(id);\n+          } else {\n+            return resolveMetadataTable(conf, tableLocation.getPath(), tableName);\n+          }\n+        } else {\n+          URI warehouseLocation = pathAsURI(extractWarehousePath(tableLocation.getPath(), tableName));\n+          HadoopCatalog catalog = new HadoopCatalog(conf, warehouseLocation.getPath());\n+          return catalog.loadTable(id);\n+        }\n+      case InputFormatConfig.HIVE_CATALOG:\n+        //TODO Implement HiveCatalog", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxODA2OQ=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NTc1Mw==", "bodyText": "Should this throw an exception instead of returning null?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r447345753", "createdAt": "2020-06-30T01:01:00Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "diffHunk": "@@ -0,0 +1,121 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.Properties;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+\n+final class TableResolver {\n+\n+  private TableResolver() {\n+  }\n+\n+  static Table resolveTableFromJob(JobConf conf) throws IOException {\n+    Properties properties = new Properties();\n+    properties.setProperty(InputFormatConfig.CATALOG_NAME, extractProperty(conf, InputFormatConfig.CATALOG_NAME));\n+    if (conf.get(InputFormatConfig.CATALOG_NAME).equals(InputFormatConfig.HADOOP_CATALOG)) {\n+      properties.setProperty(InputFormatConfig.SNAPSHOT_TABLE, conf.get(InputFormatConfig.SNAPSHOT_TABLE, \"true\"));\n+    }\n+    properties.setProperty(InputFormatConfig.TABLE_LOCATION, extractProperty(conf, InputFormatConfig.TABLE_LOCATION));\n+    properties.setProperty(InputFormatConfig.TABLE_NAME, extractProperty(conf, InputFormatConfig.TABLE_NAME));\n+    return resolveTableFromConfiguration(conf, properties);\n+  }\n+\n+  static Table resolveTableFromConfiguration(Configuration conf, Properties properties) throws IOException {\n+    String catalogName = properties.getProperty(InputFormatConfig.CATALOG_NAME);\n+    URI tableLocation = pathAsURI(properties.getProperty(InputFormatConfig.TABLE_LOCATION));\n+    if (catalogName == null) {\n+      throw new IllegalArgumentException(\"Catalog property: 'iceberg.catalog' not set in JobConf\");\n+    }\n+    switch (catalogName) {\n+      case InputFormatConfig.HADOOP_TABLES:\n+        HadoopTables tables = new HadoopTables(conf);\n+        return tables.load(tableLocation.getPath());\n+      case InputFormatConfig.HADOOP_CATALOG:\n+        String tableName = properties.getProperty(InputFormatConfig.TABLE_NAME);\n+        TableIdentifier id = TableIdentifier.parse(tableName);\n+        if (tableName.endsWith(InputFormatConfig.SNAPSHOT_TABLE_SUFFIX)) {\n+          if (!Boolean.parseBoolean(properties.getProperty(InputFormatConfig.SNAPSHOT_TABLE,\n+                  Boolean.TRUE.toString()))) {\n+            String tablePath = id.toString().replaceAll(\"\\\\.\", \"/\");\n+            URI warehouseLocation = pathAsURI(tableLocation.getPath().replaceAll(tablePath, \"\"));\n+            HadoopCatalog catalog = new HadoopCatalog(conf, warehouseLocation.getPath());\n+            return catalog.loadTable(id);\n+          } else {\n+            return resolveMetadataTable(conf, tableLocation.getPath(), tableName);\n+          }\n+        } else {\n+          URI warehouseLocation = pathAsURI(extractWarehousePath(tableLocation.getPath(), tableName));\n+          HadoopCatalog catalog = new HadoopCatalog(conf, warehouseLocation.getPath());\n+          return catalog.loadTable(id);\n+        }\n+      case InputFormatConfig.HIVE_CATALOG:\n+        //TODO Implement HiveCatalog", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMxODA2OQ=="}, "originalCommit": {"oid": "c0cec1786f33ae4dafe46e0aa8be61e5acebea4e"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNDc4Nzc4OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "isResolved": true, "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxODo1NTowM1rOGiqlzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxMjo1Mzo1MFrOGpfxUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAwMjU3NA==", "bodyText": "There's actually no need to return anything other the Record object. Hive relies on the ObjectInspector class to retrieve values from arbitrary data structures and convert those values to the classes Hive expects.\nYou may want to take a look at https://cwiki.apache.org/confluence/display/Hive/DeveloperGuide#DeveloperGuide-ObjectInspector and then read the implementation of a few object inspectors in the Hive codebase.\nSo all we have to do here is return the Record object and ensure we have the right object inspectors for the Record class and other data types that are not represented in the same fashion in Iceberg vs. Hive.\nAs a matter of fact, I started working on an object inspector for the Record class last year. Why don't I submit a PR on top off this one to get you started?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439002574", "createdAt": "2020-06-11T18:55:03Z", "author": {"login": "guilload"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {\n+\n+  private Schema schema;\n+  private ObjectInspector inspector;\n+  private List<Object> row;\n+\n+  @Override\n+  public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    Table table = null;\n+    try {\n+      table = TableResolver.resolveTableFromConfiguration(configuration, serDeProperties);\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(\"Unable to resolve table from configuration: \", e);\n+    }\n+    this.schema = table.schema();\n+    if (table instanceof SnapshotsTable) {\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(schema);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    } else {\n+      List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+      columns.add(Types.NestedField.optional(Integer.MAX_VALUE,\n+          SystemTableUtil.snapshotIdVirtualColumnName(serDeProperties), Types.LongType.get()));\n+      Schema withVirtualColumn = new Schema(columns);\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(withVirtualColumn);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Class<? extends Writable> getSerializedClass() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Writable serialize(Object o, ObjectInspector objectInspector) {\n+    return null;\n+  }\n+\n+  @Override\n+  public SerDeStats getSerDeStats() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Object deserialize(Writable writable) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4ccfd7b996c68fb4ad82d7351c244baa43b4856"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTA4MDIzMA==", "bodyText": "Instead of using ObjectInspectors, do you think it's worth building a ReaderFunc which returns Hive compatible objects instead? We can save one conversion that way. e.g Avro objects -> Hive objects, instead of Avro objects -> Iceberg Generic objects -> Hive objects. The downside is we will have to build a Hive-compatible ReaderFunc for each file format but I don't expect it to be too hard.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439080230", "createdAt": "2020-06-11T21:28:49Z", "author": {"login": "shardulm94"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {\n+\n+  private Schema schema;\n+  private ObjectInspector inspector;\n+  private List<Object> row;\n+\n+  @Override\n+  public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    Table table = null;\n+    try {\n+      table = TableResolver.resolveTableFromConfiguration(configuration, serDeProperties);\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(\"Unable to resolve table from configuration: \", e);\n+    }\n+    this.schema = table.schema();\n+    if (table instanceof SnapshotsTable) {\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(schema);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    } else {\n+      List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+      columns.add(Types.NestedField.optional(Integer.MAX_VALUE,\n+          SystemTableUtil.snapshotIdVirtualColumnName(serDeProperties), Types.LongType.get()));\n+      Schema withVirtualColumn = new Schema(columns);\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(withVirtualColumn);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Class<? extends Writable> getSerializedClass() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Writable serialize(Object o, ObjectInspector objectInspector) {\n+    return null;\n+  }\n+\n+  @Override\n+  public SerDeStats getSerDeStats() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Object deserialize(Writable writable) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAwMjU3NA=="}, "originalCommit": {"oid": "e4ccfd7b996c68fb4ad82d7351c244baa43b4856"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE5MTI4NQ==", "bodyText": "+1. We should build this similar to reader Funcs. Please see examples of this in Spark module org.apache.iceberg.spark.data.SparkValueReaders [for Avro] and org.apache.iceberg.spark.data.SparkOrcValueReaders", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439191285", "createdAt": "2020-06-12T03:45:55Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {\n+\n+  private Schema schema;\n+  private ObjectInspector inspector;\n+  private List<Object> row;\n+\n+  @Override\n+  public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    Table table = null;\n+    try {\n+      table = TableResolver.resolveTableFromConfiguration(configuration, serDeProperties);\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(\"Unable to resolve table from configuration: \", e);\n+    }\n+    this.schema = table.schema();\n+    if (table instanceof SnapshotsTable) {\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(schema);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    } else {\n+      List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+      columns.add(Types.NestedField.optional(Integer.MAX_VALUE,\n+          SystemTableUtil.snapshotIdVirtualColumnName(serDeProperties), Types.LongType.get()));\n+      Schema withVirtualColumn = new Schema(columns);\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(withVirtualColumn);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Class<? extends Writable> getSerializedClass() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Writable serialize(Object o, ObjectInspector objectInspector) {\n+    return null;\n+  }\n+\n+  @Override\n+  public SerDeStats getSerDeStats() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Object deserialize(Writable writable) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAwMjU3NA=="}, "originalCommit": {"oid": "e4ccfd7b996c68fb4ad82d7351c244baa43b4856"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ2NzIyOQ==", "bodyText": "\"There's actually no need to return anything other the Record object.\"\n@guilload are you sure about this? Ultimately this all ends up being used by the old Map Reduce V1 API where serialization required the usage of Writables. Have you managed to get an InputFormat working end to end in an actual Hive 2.3.x client with a type that isn't a Writable? I see that the InputFormats etc. for Delta and Hudi both seem to use ArrayWritableand have Writables scattered over the code which I'm assuming is for this reason. I'd be happy to remove the Writable I'm just not sure it would actually work when run on a cluster.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439467229", "createdAt": "2020-06-12T14:51:59Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {\n+\n+  private Schema schema;\n+  private ObjectInspector inspector;\n+  private List<Object> row;\n+\n+  @Override\n+  public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    Table table = null;\n+    try {\n+      table = TableResolver.resolveTableFromConfiguration(configuration, serDeProperties);\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(\"Unable to resolve table from configuration: \", e);\n+    }\n+    this.schema = table.schema();\n+    if (table instanceof SnapshotsTable) {\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(schema);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    } else {\n+      List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+      columns.add(Types.NestedField.optional(Integer.MAX_VALUE,\n+          SystemTableUtil.snapshotIdVirtualColumnName(serDeProperties), Types.LongType.get()));\n+      Schema withVirtualColumn = new Schema(columns);\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(withVirtualColumn);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Class<? extends Writable> getSerializedClass() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Writable serialize(Object o, ObjectInspector objectInspector) {\n+    return null;\n+  }\n+\n+  @Override\n+  public SerDeStats getSerDeStats() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Object deserialize(Writable writable) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAwMjU3NA=="}, "originalCommit": {"oid": "e4ccfd7b996c68fb4ad82d7351c244baa43b4856"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUwOTE4Mg==", "bodyText": "Ultimately this all ends up being used by the old Map Reduce V1 API where serialization required the usage of Writables\n\nI think the suggestion was to return Record from deserialize, not to use a non-writable. After deserialize, I don't think there is a requirement for anything to be a Writable.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439509182", "createdAt": "2020-06-12T16:06:53Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {\n+\n+  private Schema schema;\n+  private ObjectInspector inspector;\n+  private List<Object> row;\n+\n+  @Override\n+  public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    Table table = null;\n+    try {\n+      table = TableResolver.resolveTableFromConfiguration(configuration, serDeProperties);\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(\"Unable to resolve table from configuration: \", e);\n+    }\n+    this.schema = table.schema();\n+    if (table instanceof SnapshotsTable) {\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(schema);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    } else {\n+      List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+      columns.add(Types.NestedField.optional(Integer.MAX_VALUE,\n+          SystemTableUtil.snapshotIdVirtualColumnName(serDeProperties), Types.LongType.get()));\n+      Schema withVirtualColumn = new Schema(columns);\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(withVirtualColumn);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Class<? extends Writable> getSerializedClass() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Writable serialize(Object o, ObjectInspector objectInspector) {\n+    return null;\n+  }\n+\n+  @Override\n+  public SerDeStats getSerDeStats() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Object deserialize(Writable writable) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAwMjU3NA=="}, "originalCommit": {"oid": "e4ccfd7b996c68fb4ad82d7351c244baa43b4856"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUxMDA3NA==", "bodyText": "Instead of using ObjectInspectors, do you think it's worth building a ReaderFunc which returns Hive compatible objects instead?\n\nThis seems like a larger effort that is probably something for a follow-up instead. It seems much easier to me to add a small translation layer just for the types that need it and use the current Iceberg generics, rather than building a new object model for data formats.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439510074", "createdAt": "2020-06-12T16:08:42Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {\n+\n+  private Schema schema;\n+  private ObjectInspector inspector;\n+  private List<Object> row;\n+\n+  @Override\n+  public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    Table table = null;\n+    try {\n+      table = TableResolver.resolveTableFromConfiguration(configuration, serDeProperties);\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(\"Unable to resolve table from configuration: \", e);\n+    }\n+    this.schema = table.schema();\n+    if (table instanceof SnapshotsTable) {\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(schema);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    } else {\n+      List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+      columns.add(Types.NestedField.optional(Integer.MAX_VALUE,\n+          SystemTableUtil.snapshotIdVirtualColumnName(serDeProperties), Types.LongType.get()));\n+      Schema withVirtualColumn = new Schema(columns);\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(withVirtualColumn);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Class<? extends Writable> getSerializedClass() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Writable serialize(Object o, ObjectInspector objectInspector) {\n+    return null;\n+  }\n+\n+  @Override\n+  public SerDeStats getSerDeStats() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Object deserialize(Writable writable) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAwMjU3NA=="}, "originalCommit": {"oid": "e4ccfd7b996c68fb4ad82d7351c244baa43b4856"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUxNzA0Mg==", "bodyText": "Ultimately this all ends up being used by the old Map Reduce V1 API where serialization required the usage of Writables\n\nI think the suggestion was to return Record from deserialize, not to use a non-writable. After deserialize, I don't think there is a requirement for anything to be a Writable.\n\nWe can certainly try, we should probably do it in Hiveberg first as we can then run a full HiveRunner test over it to make sure there aren't any surprises in the internals of Hive or MR.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439517042", "createdAt": "2020-06-12T16:18:01Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {\n+\n+  private Schema schema;\n+  private ObjectInspector inspector;\n+  private List<Object> row;\n+\n+  @Override\n+  public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    Table table = null;\n+    try {\n+      table = TableResolver.resolveTableFromConfiguration(configuration, serDeProperties);\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(\"Unable to resolve table from configuration: \", e);\n+    }\n+    this.schema = table.schema();\n+    if (table instanceof SnapshotsTable) {\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(schema);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    } else {\n+      List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+      columns.add(Types.NestedField.optional(Integer.MAX_VALUE,\n+          SystemTableUtil.snapshotIdVirtualColumnName(serDeProperties), Types.LongType.get()));\n+      Schema withVirtualColumn = new Schema(columns);\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(withVirtualColumn);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Class<? extends Writable> getSerializedClass() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Writable serialize(Object o, ObjectInspector objectInspector) {\n+    return null;\n+  }\n+\n+  @Override\n+  public SerDeStats getSerDeStats() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Object deserialize(Writable writable) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAwMjU3NA=="}, "originalCommit": {"oid": "e4ccfd7b996c68fb4ad82d7351c244baa43b4856"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY0NjE1Nw==", "bodyText": "Ultimately this all ends up being used by the old Map Reduce V1 API where serialization required the usage of Writables\n\nI think the suggestion was to return Record from deserialize, not to use a non-writable. After deserialize, I don't think there is a requirement for anything to be a Writable.\n\nExactly. Until getting to deserialize the object has to be a Writable but after that point, it can be anything really, as long as the right object inspector is provided. I have already successfully tested this approach on local branch where I built a simple Hive input format on top of #1104 and implemented my own SerDe with this object inspector.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439646157", "createdAt": "2020-06-12T21:17:43Z", "author": {"login": "guilload"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {\n+\n+  private Schema schema;\n+  private ObjectInspector inspector;\n+  private List<Object> row;\n+\n+  @Override\n+  public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    Table table = null;\n+    try {\n+      table = TableResolver.resolveTableFromConfiguration(configuration, serDeProperties);\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(\"Unable to resolve table from configuration: \", e);\n+    }\n+    this.schema = table.schema();\n+    if (table instanceof SnapshotsTable) {\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(schema);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    } else {\n+      List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+      columns.add(Types.NestedField.optional(Integer.MAX_VALUE,\n+          SystemTableUtil.snapshotIdVirtualColumnName(serDeProperties), Types.LongType.get()));\n+      Schema withVirtualColumn = new Schema(columns);\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(withVirtualColumn);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Class<? extends Writable> getSerializedClass() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Writable serialize(Object o, ObjectInspector objectInspector) {\n+    return null;\n+  }\n+\n+  @Override\n+  public SerDeStats getSerDeStats() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Object deserialize(Writable writable) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAwMjU3NA=="}, "originalCommit": {"oid": "e4ccfd7b996c68fb4ad82d7351c244baa43b4856"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDA1NTU0MQ==", "bodyText": "OK, sounds good, do you want to make this change as part of what you mention above with the Object Inspectors or later?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r440055541", "createdAt": "2020-06-15T09:43:23Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {\n+\n+  private Schema schema;\n+  private ObjectInspector inspector;\n+  private List<Object> row;\n+\n+  @Override\n+  public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    Table table = null;\n+    try {\n+      table = TableResolver.resolveTableFromConfiguration(configuration, serDeProperties);\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(\"Unable to resolve table from configuration: \", e);\n+    }\n+    this.schema = table.schema();\n+    if (table instanceof SnapshotsTable) {\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(schema);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    } else {\n+      List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+      columns.add(Types.NestedField.optional(Integer.MAX_VALUE,\n+          SystemTableUtil.snapshotIdVirtualColumnName(serDeProperties), Types.LongType.get()));\n+      Schema withVirtualColumn = new Schema(columns);\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(withVirtualColumn);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Class<? extends Writable> getSerializedClass() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Writable serialize(Object o, ObjectInspector objectInspector) {\n+    return null;\n+  }\n+\n+  @Override\n+  public SerDeStats getSerDeStats() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Object deserialize(Writable writable) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAwMjU3NA=="}, "originalCommit": {"oid": "e4ccfd7b996c68fb4ad82d7351c244baa43b4856"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE2NTMyOQ==", "bodyText": "Above changes have now been merged into this PR.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r446165329", "createdAt": "2020-06-26T12:53:50Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {\n+\n+  private Schema schema;\n+  private ObjectInspector inspector;\n+  private List<Object> row;\n+\n+  @Override\n+  public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    Table table = null;\n+    try {\n+      table = TableResolver.resolveTableFromConfiguration(configuration, serDeProperties);\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(\"Unable to resolve table from configuration: \", e);\n+    }\n+    this.schema = table.schema();\n+    if (table instanceof SnapshotsTable) {\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(schema);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    } else {\n+      List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+      columns.add(Types.NestedField.optional(Integer.MAX_VALUE,\n+          SystemTableUtil.snapshotIdVirtualColumnName(serDeProperties), Types.LongType.get()));\n+      Schema withVirtualColumn = new Schema(columns);\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(withVirtualColumn);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Class<? extends Writable> getSerializedClass() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Writable serialize(Object o, ObjectInspector objectInspector) {\n+    return null;\n+  }\n+\n+  @Override\n+  public SerDeStats getSerDeStats() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Object deserialize(Writable writable) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAwMjU3NA=="}, "originalCommit": {"oid": "e4ccfd7b996c68fb4ad82d7351c244baa43b4856"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNDc5NjAwOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxODo1NzozN1rOGiqrLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxODo1NzozN1rOGiqrLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAwMzk0OA==", "bodyText": "See this is where object inspectors come in handy, they'll take care of the conversion for us. Plus, you can clearly see that this current approach won't work for nested data structures.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439003948", "createdAt": "2020-06-11T18:57:37Z", "author": {"login": "guilload"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {\n+\n+  private Schema schema;\n+  private ObjectInspector inspector;\n+  private List<Object> row;\n+\n+  @Override\n+  public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    Table table = null;\n+    try {\n+      table = TableResolver.resolveTableFromConfiguration(configuration, serDeProperties);\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(\"Unable to resolve table from configuration: \", e);\n+    }\n+    this.schema = table.schema();\n+    if (table instanceof SnapshotsTable) {\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(schema);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    } else {\n+      List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+      columns.add(Types.NestedField.optional(Integer.MAX_VALUE,\n+          SystemTableUtil.snapshotIdVirtualColumnName(serDeProperties), Types.LongType.get()));\n+      Schema withVirtualColumn = new Schema(columns);\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(withVirtualColumn);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Class<? extends Writable> getSerializedClass() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Writable serialize(Object o, ObjectInspector objectInspector) {\n+    return null;\n+  }\n+\n+  @Override\n+  public SerDeStats getSerDeStats() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Object deserialize(Writable writable) {\n+    IcebergWritable icebergWritable = (IcebergWritable) writable;\n+    List<Types.NestedField> fields = icebergWritable.schema().columns();\n+\n+    if (row == null || row.size() != fields.size()) {\n+      row = new ArrayList<Object>(fields.size());\n+    } else {\n+      row.clear();\n+    }\n+    for (int i = 0; i < fields.size(); i++) {\n+      Object obj = ((IcebergWritable) writable).record().get(i);\n+      Type fieldType = fields.get(i).type();\n+      if (fieldType.equals(Types.DateType.get())) {\n+        row.add(Date.valueOf((LocalDate) obj));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4ccfd7b996c68fb4ad82d7351c244baa43b4856"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNTk2Njc2OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQwMzo0Mzo0N1rOGi2E-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQwMzo0Mzo0N1rOGi2E-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE5MDc3OA==", "bodyText": "throw Unsupported?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439190778", "createdAt": "2020-06-12T03:43:47Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {\n+\n+  private Schema schema;\n+  private ObjectInspector inspector;\n+  private List<Object> row;\n+\n+  @Override\n+  public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    Table table = null;\n+    try {\n+      table = TableResolver.resolveTableFromConfiguration(configuration, serDeProperties);\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(\"Unable to resolve table from configuration: \", e);\n+    }\n+    this.schema = table.schema();\n+    if (table instanceof SnapshotsTable) {\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(schema);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    } else {\n+      List<Types.NestedField> columns = new ArrayList<>(schema.columns());\n+      columns.add(Types.NestedField.optional(Integer.MAX_VALUE,\n+          SystemTableUtil.snapshotIdVirtualColumnName(serDeProperties), Types.LongType.get()));\n+      Schema withVirtualColumn = new Schema(columns);\n+      try {\n+        this.inspector = new IcebergObjectInspectorGenerator().createObjectInspector(withVirtualColumn);\n+      } catch (Exception e) {\n+        throw new SerDeException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Class<? extends Writable> getSerializedClass() {\n+    return null;\n+  }\n+\n+  @Override\n+  public Writable serialize(Object o, ObjectInspector objectInspector) {\n+    return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4ccfd7b996c68fb4ad82d7351c244baa43b4856"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNTk3Nzg0OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQwMzo1MToyMlrOGi2LnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNjoxMDozMVrOGjJouQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE5MjQ3Nw==", "bodyText": "@cmathiesen @rdblue . Do u folks see value in having Hive classes in a separate hive  module instead of MR?  Similar to Iceberg Pig ? I think we need to provide a reader func implementation for Hive to be used with the MR module", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439192477", "createdAt": "2020-06-12T03:51:22Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4ccfd7b996c68fb4ad82d7351c244baa43b4856"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ2NDkxMw==", "bodyText": "We discussed this with @rdblue and he didn't want to introduce another sub-package at this stage so we agreed to keep it all here. I don't mind either way to be honest.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439464913", "createdAt": "2020-06-12T14:48:15Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE5MjQ3Nw=="}, "originalCommit": {"oid": "e4ccfd7b996c68fb4ad82d7351c244baa43b4856"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUxMTIyNQ==", "bodyText": "As long as the Hive dependencies are provided/compileOnly and we can still use the MR models, I don't see much harm in keeping them in one module.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439511225", "createdAt": "2020-06-12T16:10:31Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SnapshotsTable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+public class IcebergSerDe extends AbstractSerDe {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE5MjQ3Nw=="}, "originalCommit": {"oid": "e4ccfd7b996c68fb4ad82d7351c244baa43b4856"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczODg3MjU3OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQyMTo1NjoxNlrOGjSmUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxMjo1NTozMlrOGlEY5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY1ODA2NQ==", "bodyText": "According to https://github.com/ExpediaGroup/hiveberg/blob/master/README.md#time-travel-and-system-tables it seems like to access a snapshot metadata table, we need to create a new table which ends in __snapshots. Instead of that, can we rather let the user create a table with any name but put the metadata table name as part of the location instead.\nCREATE TABLE source_db.table_a_snapshot_metadata_table\n  STORED BY 'com.expediagroup.hiveberg.IcebergStorageHandler'\n  LOCATION 'path_to_original_data_table#snapshots'\n    TBLPROPERTIES ('iceberg.catalog'='hadoop.catalog')\n\nThat way,\n\nYou don't need to reserve a table suffix\nYou don't need flags like iceberg.snapshots.table=false\nTableResolver code is also simplified as it does not need to understand the syntax for metadata tables in different catalogs. e.g. HadoopTables uses #snapshots, HiveCatalog uses .snapshots. It simply has to pass the location as-is to the catalog.\nIt will also be able to handle other metadata tables, like partitions, manifest, entries, etc. without any additional handling.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r439658065", "createdAt": "2020-06-12T21:56:16Z", "author": {"login": "shardulm94"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Properties;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+final class TableResolver {\n+\n+  private TableResolver() {\n+  }\n+\n+  static Table resolveTableFromJob(JobConf conf) throws IOException {\n+    Properties properties = new Properties();\n+    properties.setProperty(InputFormatConfig.CATALOG_NAME,\n+        conf.get(InputFormatConfig.CATALOG_NAME, InputFormatConfig.HADOOP_TABLES)); //Default to HadoopTables\n+    properties.setProperty(InputFormatConfig.SNAPSHOT_TABLE,\n+        conf.get(InputFormatConfig.SNAPSHOT_TABLE, \"true\"));\n+    properties.setProperty(InputFormatConfig.TABLE_LOCATION, extractProperty(conf, InputFormatConfig.TABLE_LOCATION));\n+    properties.setProperty(InputFormatConfig.TABLE_NAME, extractProperty(conf, InputFormatConfig.TABLE_NAME));\n+    return resolveTableFromConfiguration(conf, properties);\n+  }\n+\n+  static Table resolveTableFromConfiguration(Configuration conf, Properties properties) throws IOException {\n+    String catalogName = properties.getProperty(InputFormatConfig.CATALOG_NAME, InputFormatConfig.HADOOP_TABLES);\n+    String tableLocation = properties.getProperty(InputFormatConfig.TABLE_LOCATION);\n+    String tableName = properties.getProperty(InputFormatConfig.TABLE_NAME);\n+    Preconditions.checkNotNull(tableLocation, \"Table location is not set.\");\n+    Preconditions.checkNotNull(tableName, \"Table name is not set.\");\n+    switch (catalogName) {\n+      case InputFormatConfig.HADOOP_TABLES:\n+        HadoopTables tables = new HadoopTables(conf);\n+        if (tableName.endsWith(InputFormatConfig.SNAPSHOT_TABLE_SUFFIX)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eecb8336d66c3922e2b37552be35418abce5dd0b"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDAyODgzMw==", "bodyText": "I definitely think we should adopt the simplification offered by points 2,3,4. My only concern with point 1, is that it can often be useful to guide users into a good convention by making it a default behaviour. That said, table naming strategies are almost certainly the responsibility of the data lake owner and not the implementation - so perhaps best to drop the reserved suffix also. Thanks for this suggestion.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r440028833", "createdAt": "2020-06-15T08:58:01Z", "author": {"login": "teabot"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Properties;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+final class TableResolver {\n+\n+  private TableResolver() {\n+  }\n+\n+  static Table resolveTableFromJob(JobConf conf) throws IOException {\n+    Properties properties = new Properties();\n+    properties.setProperty(InputFormatConfig.CATALOG_NAME,\n+        conf.get(InputFormatConfig.CATALOG_NAME, InputFormatConfig.HADOOP_TABLES)); //Default to HadoopTables\n+    properties.setProperty(InputFormatConfig.SNAPSHOT_TABLE,\n+        conf.get(InputFormatConfig.SNAPSHOT_TABLE, \"true\"));\n+    properties.setProperty(InputFormatConfig.TABLE_LOCATION, extractProperty(conf, InputFormatConfig.TABLE_LOCATION));\n+    properties.setProperty(InputFormatConfig.TABLE_NAME, extractProperty(conf, InputFormatConfig.TABLE_NAME));\n+    return resolveTableFromConfiguration(conf, properties);\n+  }\n+\n+  static Table resolveTableFromConfiguration(Configuration conf, Properties properties) throws IOException {\n+    String catalogName = properties.getProperty(InputFormatConfig.CATALOG_NAME, InputFormatConfig.HADOOP_TABLES);\n+    String tableLocation = properties.getProperty(InputFormatConfig.TABLE_LOCATION);\n+    String tableName = properties.getProperty(InputFormatConfig.TABLE_NAME);\n+    Preconditions.checkNotNull(tableLocation, \"Table location is not set.\");\n+    Preconditions.checkNotNull(tableName, \"Table name is not set.\");\n+    switch (catalogName) {\n+      case InputFormatConfig.HADOOP_TABLES:\n+        HadoopTables tables = new HadoopTables(conf);\n+        if (tableName.endsWith(InputFormatConfig.SNAPSHOT_TABLE_SUFFIX)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY1ODA2NQ=="}, "originalCommit": {"oid": "eecb8336d66c3922e2b37552be35418abce5dd0b"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ4NTEzMQ==", "bodyText": "I think this is a good idea, especially if it supports all metadata tables without additional work.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r440485131", "createdAt": "2020-06-15T22:37:55Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Properties;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+final class TableResolver {\n+\n+  private TableResolver() {\n+  }\n+\n+  static Table resolveTableFromJob(JobConf conf) throws IOException {\n+    Properties properties = new Properties();\n+    properties.setProperty(InputFormatConfig.CATALOG_NAME,\n+        conf.get(InputFormatConfig.CATALOG_NAME, InputFormatConfig.HADOOP_TABLES)); //Default to HadoopTables\n+    properties.setProperty(InputFormatConfig.SNAPSHOT_TABLE,\n+        conf.get(InputFormatConfig.SNAPSHOT_TABLE, \"true\"));\n+    properties.setProperty(InputFormatConfig.TABLE_LOCATION, extractProperty(conf, InputFormatConfig.TABLE_LOCATION));\n+    properties.setProperty(InputFormatConfig.TABLE_NAME, extractProperty(conf, InputFormatConfig.TABLE_NAME));\n+    return resolveTableFromConfiguration(conf, properties);\n+  }\n+\n+  static Table resolveTableFromConfiguration(Configuration conf, Properties properties) throws IOException {\n+    String catalogName = properties.getProperty(InputFormatConfig.CATALOG_NAME, InputFormatConfig.HADOOP_TABLES);\n+    String tableLocation = properties.getProperty(InputFormatConfig.TABLE_LOCATION);\n+    String tableName = properties.getProperty(InputFormatConfig.TABLE_NAME);\n+    Preconditions.checkNotNull(tableLocation, \"Table location is not set.\");\n+    Preconditions.checkNotNull(tableName, \"Table name is not set.\");\n+    switch (catalogName) {\n+      case InputFormatConfig.HADOOP_TABLES:\n+        HadoopTables tables = new HadoopTables(conf);\n+        if (tableName.endsWith(InputFormatConfig.SNAPSHOT_TABLE_SUFFIX)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY1ODA2NQ=="}, "originalCommit": {"oid": "eecb8336d66c3922e2b37552be35418abce5dd0b"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTUyMjQwNA==", "bodyText": "We've made a note of all of these suggestions so thank you, first of all, and we're working on a new implementation for system tables via Hive. As was mentioned in the other PR, we'll take out all the system table + virtual column stuff from these initial PR's and will open another PR for just system tables after merging this and #933", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r441522404", "createdAt": "2020-06-17T12:55:32Z", "author": {"login": "cmathiesen"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Properties;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+final class TableResolver {\n+\n+  private TableResolver() {\n+  }\n+\n+  static Table resolveTableFromJob(JobConf conf) throws IOException {\n+    Properties properties = new Properties();\n+    properties.setProperty(InputFormatConfig.CATALOG_NAME,\n+        conf.get(InputFormatConfig.CATALOG_NAME, InputFormatConfig.HADOOP_TABLES)); //Default to HadoopTables\n+    properties.setProperty(InputFormatConfig.SNAPSHOT_TABLE,\n+        conf.get(InputFormatConfig.SNAPSHOT_TABLE, \"true\"));\n+    properties.setProperty(InputFormatConfig.TABLE_LOCATION, extractProperty(conf, InputFormatConfig.TABLE_LOCATION));\n+    properties.setProperty(InputFormatConfig.TABLE_NAME, extractProperty(conf, InputFormatConfig.TABLE_NAME));\n+    return resolveTableFromConfiguration(conf, properties);\n+  }\n+\n+  static Table resolveTableFromConfiguration(Configuration conf, Properties properties) throws IOException {\n+    String catalogName = properties.getProperty(InputFormatConfig.CATALOG_NAME, InputFormatConfig.HADOOP_TABLES);\n+    String tableLocation = properties.getProperty(InputFormatConfig.TABLE_LOCATION);\n+    String tableName = properties.getProperty(InputFormatConfig.TABLE_NAME);\n+    Preconditions.checkNotNull(tableLocation, \"Table location is not set.\");\n+    Preconditions.checkNotNull(tableName, \"Table name is not set.\");\n+    switch (catalogName) {\n+      case InputFormatConfig.HADOOP_TABLES:\n+        HadoopTables tables = new HadoopTables(conf);\n+        if (tableName.endsWith(InputFormatConfig.SNAPSHOT_TABLE_SUFFIX)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY1ODA2NQ=="}, "originalCommit": {"oid": "eecb8336d66c3922e2b37552be35418abce5dd0b"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MzMyOTc0OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQxNzoxMjoyMFrOGj7R8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNTo0MTo1NFrOGlLz_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDMyNDU5NQ==", "bodyText": "Seems like this class is shared by both mrv2 and mrv1 apis, similar to InputFormatConfig. We should have a consistent package name there..", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r440324595", "createdAt": "2020-06-15T17:12:20Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2350025f11bf3ce6df226d21a87d5c403ab50b53"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY0NDAzMA==", "bodyText": "In this PR it's only being used by the mrv1 API which is why it's in the mapred package. When we look at #933 after this we can discuss how much code is shared and then potentially move this up a package level and have it contain shared code etc.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r441644030", "createdAt": "2020-06-17T15:41:54Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/TableResolver.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDMyNDU5NQ=="}, "originalCommit": {"oid": "2350025f11bf3ce6df226d21a87d5c403ab50b53"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0Njk1MTYzOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergObjectInspectorGenerator.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNDo0MDoxOVrOGkepyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNDo0MDoxOVrOGkepyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDkwNDEzOQ==", "bodyText": "nit: maybe just rename this to columnNames.  Also why is this protected , seems like it can static like its counterpart in IcebergSchemaToTypeInfo", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r440904139", "createdAt": "2020-06-16T14:40:19Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergObjectInspectorGenerator.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n+\n+class IcebergObjectInspectorGenerator {\n+\n+  protected ObjectInspector createObjectInspector(Schema schema) throws Exception {\n+    List<String> columnNames = setColumnNames(schema);\n+    List<TypeInfo> columnTypes = IcebergSchemaToTypeInfo.getColumnTypes(schema);\n+\n+    List<ObjectInspector> columnOIs = new ArrayList<>(columnTypes.size());\n+    for (int i = 0; i < columnTypes.size(); i++) {\n+      columnOIs.add(createObjectInspectorWorker(columnTypes.get(i)));\n+    }\n+    return ObjectInspectorFactory.getStandardStructObjectInspector(columnNames, columnOIs, null);\n+  }\n+\n+  protected ObjectInspector createObjectInspectorWorker(TypeInfo typeInfo) throws Exception {\n+    ObjectInspector.Category typeCategory = typeInfo.getCategory();\n+\n+    switch (typeCategory) {\n+      case PRIMITIVE:\n+        PrimitiveTypeInfo pti = (PrimitiveTypeInfo) typeInfo;\n+        return PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(pti);\n+      case LIST:\n+        ListTypeInfo ati = (ListTypeInfo) typeInfo;\n+        return ObjectInspectorFactory\n+            .getStandardListObjectInspector(createObjectInspectorWorker(ati.getListElementTypeInfo()));\n+      case MAP:\n+        MapTypeInfo mti = (MapTypeInfo) typeInfo;\n+        return ObjectInspectorFactory.getStandardMapObjectInspector(\n+            createObjectInspectorWorker(mti.getMapKeyTypeInfo()),\n+            createObjectInspectorWorker(mti.getMapValueTypeInfo()));\n+      case STRUCT:\n+        StructTypeInfo sti = (StructTypeInfo) typeInfo;\n+        List<ObjectInspector> ois = new ArrayList<>(sti.getAllStructFieldTypeInfos().size());\n+        for (TypeInfo structTypeInfos : sti.getAllStructFieldTypeInfos()) {\n+          ois.add(createObjectInspectorWorker(structTypeInfos));\n+        }\n+        return ObjectInspectorFactory.getStandardStructObjectInspector(sti.getAllStructFieldNames(), ois);\n+      default:\n+        throw new SerDeException(\"Couldn't create Object Inspector for category: '\" + typeCategory + \"'\");\n+    }\n+  }\n+\n+  protected List<String> setColumnNames(Schema schema) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2350025f11bf3ce6df226d21a87d5c403ab50b53"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0ODg1NDMwOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwMDoyMDowNFrOGkxbNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNTo0Mjo1NlrOGlL20A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIxMTcwMg==", "bodyText": "I think the indentation was correct before. Can you revert this?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r441211702", "createdAt": "2020-06-17T00:20:04Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -265,9 +158,9 @@ private static void checkResiduals(CombinedScanTask task) {\n       Expression residual = fileScanTask.residual();\n       if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n         throw new UnsupportedOperationException(\n-            String.format(\n-                \"Filter expression %s is not completely satisfied. Additional rows \" +\n-                    \"can be returned not satisfied by the filter expression\", residual));\n+                String.format(\n+                        \"Filter expression %s is not completely satisfied. Additional rows \" +\n+                                \"can be returned not satisfied by the filter expression\", residual));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2350025f11bf3ce6df226d21a87d5c403ab50b53"}, "originalPosition": 202}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY0NDc1Mg==", "bodyText": "Yes, sorry!", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r441644752", "createdAt": "2020-06-17T15:42:56Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -265,9 +158,9 @@ private static void checkResiduals(CombinedScanTask task) {\n       Expression residual = fileScanTask.residual();\n       if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n         throw new UnsupportedOperationException(\n-            String.format(\n-                \"Filter expression %s is not completely satisfied. Additional rows \" +\n-                    \"can be returned not satisfied by the filter expression\", residual));\n+                String.format(\n+                        \"Filter expression %s is not completely satisfied. Additional rows \" +\n+                                \"can be returned not satisfied by the filter expression\", residual));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIxMTcwMg=="}, "originalCommit": {"oid": "2350025f11bf3ce6df226d21a87d5c403ab50b53"}, "originalPosition": 202}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0ODg1NTU4OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwMDoyMDo0NlrOGkxb9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNTo1MToxNlrOGlMMxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIxMTg5Mg==", "bodyText": "Can you revert the whitespace-only changes in this file? Otherwise this is going to have more conflicts than necessary.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r441211892", "createdAt": "2020-06-17T00:20:46Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -517,20 +411,20 @@ private Record withIdentityPartitionColumns(\n   }\n \n   private static Table findTable(Configuration conf) {\n-    String path = conf.get(TABLE_PATH);\n+    String path = conf.get(InputFormatConfig.TABLE_PATH);\n     Preconditions.checkArgument(path != null, \"Table path should not be null\");\n     if (path.contains(\"/\")) {\n       HadoopTables tables = new HadoopTables(conf);\n       return tables.load(path);\n     }\n \n-    String catalogFuncClass = conf.get(CATALOG);\n+    String catalogFuncClass = conf.get(InputFormatConfig.CATALOG);\n     if (catalogFuncClass != null) {\n       Function<Configuration, Catalog> catalogFunc = (Function<Configuration, Catalog>)\n-          DynConstructors.builder(Function.class)\n-                         .impl(catalogFuncClass)\n-                         .build()\n-                         .newInstance();\n+              DynConstructors.builder(Function.class)\n+                      .impl(catalogFuncClass)\n+                      .build()\n+                      .newInstance();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2350025f11bf3ce6df226d21a87d5c403ab50b53"}, "originalPosition": 361}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTY1MDM3Mg==", "bodyText": "Yes, definitely.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r441650372", "createdAt": "2020-06-17T15:51:16Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -517,20 +411,20 @@ private Record withIdentityPartitionColumns(\n   }\n \n   private static Table findTable(Configuration conf) {\n-    String path = conf.get(TABLE_PATH);\n+    String path = conf.get(InputFormatConfig.TABLE_PATH);\n     Preconditions.checkArgument(path != null, \"Table path should not be null\");\n     if (path.contains(\"/\")) {\n       HadoopTables tables = new HadoopTables(conf);\n       return tables.load(path);\n     }\n \n-    String catalogFuncClass = conf.get(CATALOG);\n+    String catalogFuncClass = conf.get(InputFormatConfig.CATALOG);\n     if (catalogFuncClass != null) {\n       Function<Configuration, Catalog> catalogFunc = (Function<Configuration, Catalog>)\n-          DynConstructors.builder(Function.class)\n-                         .impl(catalogFuncClass)\n-                         .build()\n-                         .newInstance();\n+              DynConstructors.builder(Function.class)\n+                      .impl(catalogFuncClass)\n+                      .build()\n+                      .newInstance();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIxMTg5Mg=="}, "originalCommit": {"oid": "2350025f11bf3ce6df226d21a87d5c403ab50b53"}, "originalPosition": 361}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4ODA0NjQxOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMDo1ODoyMVrOGqnxJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMDo1ODoyMVrOGqnxJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NDkzNA==", "bodyText": "I didn't realize Java added an UncheckedIOException in 8. We have one that is RuntimeIOException. We should probably convert Iceberg over to using the standard Java one.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r447344934", "createdAt": "2020-06-30T00:58:21Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Properties;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.serde2.AbstractSerDe;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeStats;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.mr.mapred.serde.objectinspector.IcebergObjectInspector;\n+\n+public class IcebergSerDe extends AbstractSerDe {\n+\n+  private ObjectInspector inspector;\n+\n+  @Override\n+  public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    final Table table;\n+\n+    try {\n+      table = TableResolver.resolveTableFromConfiguration(configuration, serDeProperties);\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(\"Unable to resolve table from configuration: \", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3a0cfe243b58719ab5b3b26f0dbd108b3f35c6d"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4ODA1ODI4OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/serde/objectinspector/IcebergBinaryObjectInspector.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMTowNDoyNVrOGqn35Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMTowNDoyNVrOGqn35Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0NjY2MQ==", "bodyText": "This isn't correct because it doesn't follow the contract of ByteBuffer. Avro will reuse byte buffers, so there is no guarantee that this array is the correct length. In addition, we want to generally follow the ByteBuffer contract so that we don't need to worry about whether an optimization later (buffer reuse) will break certain sections of code.\nAn easy fix is to use ByteBuffers.toByteArray here.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r447346661", "createdAt": "2020-06-30T01:04:25Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/serde/objectinspector/IcebergBinaryObjectInspector.java", "diffHunk": "@@ -0,0 +1,61 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred.serde.objectinspector;\n+\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.io.BytesWritable;\n+\n+public final class IcebergBinaryObjectInspector extends IcebergPrimitiveObjectInspector\n+                                                implements BinaryObjectInspector {\n+\n+  private static final IcebergBinaryObjectInspector INSTANCE = new IcebergBinaryObjectInspector();\n+\n+  public static IcebergBinaryObjectInspector get() {\n+    return INSTANCE;\n+  }\n+\n+  private IcebergBinaryObjectInspector() {\n+    super(TypeInfoFactory.binaryTypeInfo);\n+  }\n+\n+  @Override\n+  public byte[] getPrimitiveJavaObject(Object o) {\n+    return o == null ? null : ((ByteBuffer) o).array();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3a0cfe243b58719ab5b3b26f0dbd108b3f35c6d"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4ODA2OTAyOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/serde/objectinspector/IcebergObjectInspector.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMTowODo1MlrOGqn9jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxMzoyMzo0M1rOGrltmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0ODExMQ==", "bodyText": "Couldn't fixed by read as binary? And UUID as string? And doesn't Hive support time?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r447348111", "createdAt": "2020-06-30T01:08:52Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/serde/objectinspector/IcebergObjectInspector.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred.serde.objectinspector;\n+\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+\n+public final class IcebergObjectInspector extends TypeUtil.SchemaVisitor<ObjectInspector> {\n+\n+  public static ObjectInspector create(@Nullable Schema schema) {\n+    if (schema == null) {\n+      return IcebergRecordObjectInspector.empty();\n+    }\n+\n+    return TypeUtil.visit(schema, new IcebergObjectInspector());\n+  }\n+\n+  public static ObjectInspector create(Types.NestedField... fields) {\n+    return create(new Schema(fields));\n+  }\n+\n+  @Override\n+  public ObjectInspector field(Types.NestedField field, ObjectInspector fieldObjectInspector) {\n+    return fieldObjectInspector;\n+  }\n+\n+  @Override\n+  public ObjectInspector list(Types.ListType listTypeInfo, ObjectInspector listObjectInspector) {\n+    return ObjectInspectorFactory.getStandardListObjectInspector(listObjectInspector);\n+  }\n+\n+  @Override\n+  public ObjectInspector map(Types.MapType mapType,\n+                             ObjectInspector keyObjectInspector, ObjectInspector valueObjectInspector) {\n+    return ObjectInspectorFactory.getStandardMapObjectInspector(keyObjectInspector, valueObjectInspector);\n+  }\n+\n+  @Override\n+  public ObjectInspector primitive(Type.PrimitiveType primitiveType) {\n+    final PrimitiveTypeInfo primitiveTypeInfo;\n+\n+    switch (primitiveType.typeId()) {\n+      case BINARY:\n+        return IcebergBinaryObjectInspector.get();\n+      case BOOLEAN:\n+        primitiveTypeInfo = TypeInfoFactory.booleanTypeInfo;\n+        break;\n+      case DATE:\n+        return IcebergDateObjectInspector.get();\n+      case DECIMAL:\n+        Types.DecimalType type = (Types.DecimalType) primitiveType;\n+        return IcebergDecimalObjectInspector.get(type.precision(), type.scale());\n+      case DOUBLE:\n+        primitiveTypeInfo = TypeInfoFactory.doubleTypeInfo;\n+        break;\n+      case FLOAT:\n+        primitiveTypeInfo = TypeInfoFactory.floatTypeInfo;\n+        break;\n+      case INTEGER:\n+        primitiveTypeInfo = TypeInfoFactory.intTypeInfo;\n+        break;\n+      case LONG:\n+        primitiveTypeInfo = TypeInfoFactory.longTypeInfo;\n+        break;\n+      case STRING:\n+        primitiveTypeInfo = TypeInfoFactory.stringTypeInfo;\n+        break;\n+      case TIMESTAMP:\n+        boolean adjustToUTC = ((Types.TimestampType) primitiveType).shouldAdjustToUTC();\n+        return IcebergTimestampObjectInspector.get(adjustToUTC);\n+\n+      case FIXED:\n+      case TIME:\n+      case UUID:\n+      default:\n+        throw new IllegalArgumentException(primitiveType.typeId() + \" type is not supported\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3a0cfe243b58719ab5b3b26f0dbd108b3f35c6d"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODM1OTgzMg==", "bodyText": "First two done. As for TIME - Hive supports DATE and TIMESTAMP, I don't know enough about Iceberg's types to comment on how these differ from TIME but I'm guessing it doesn't?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r448359832", "createdAt": "2020-07-01T13:23:43Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/serde/objectinspector/IcebergObjectInspector.java", "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred.serde.objectinspector;\n+\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+\n+public final class IcebergObjectInspector extends TypeUtil.SchemaVisitor<ObjectInspector> {\n+\n+  public static ObjectInspector create(@Nullable Schema schema) {\n+    if (schema == null) {\n+      return IcebergRecordObjectInspector.empty();\n+    }\n+\n+    return TypeUtil.visit(schema, new IcebergObjectInspector());\n+  }\n+\n+  public static ObjectInspector create(Types.NestedField... fields) {\n+    return create(new Schema(fields));\n+  }\n+\n+  @Override\n+  public ObjectInspector field(Types.NestedField field, ObjectInspector fieldObjectInspector) {\n+    return fieldObjectInspector;\n+  }\n+\n+  @Override\n+  public ObjectInspector list(Types.ListType listTypeInfo, ObjectInspector listObjectInspector) {\n+    return ObjectInspectorFactory.getStandardListObjectInspector(listObjectInspector);\n+  }\n+\n+  @Override\n+  public ObjectInspector map(Types.MapType mapType,\n+                             ObjectInspector keyObjectInspector, ObjectInspector valueObjectInspector) {\n+    return ObjectInspectorFactory.getStandardMapObjectInspector(keyObjectInspector, valueObjectInspector);\n+  }\n+\n+  @Override\n+  public ObjectInspector primitive(Type.PrimitiveType primitiveType) {\n+    final PrimitiveTypeInfo primitiveTypeInfo;\n+\n+    switch (primitiveType.typeId()) {\n+      case BINARY:\n+        return IcebergBinaryObjectInspector.get();\n+      case BOOLEAN:\n+        primitiveTypeInfo = TypeInfoFactory.booleanTypeInfo;\n+        break;\n+      case DATE:\n+        return IcebergDateObjectInspector.get();\n+      case DECIMAL:\n+        Types.DecimalType type = (Types.DecimalType) primitiveType;\n+        return IcebergDecimalObjectInspector.get(type.precision(), type.scale());\n+      case DOUBLE:\n+        primitiveTypeInfo = TypeInfoFactory.doubleTypeInfo;\n+        break;\n+      case FLOAT:\n+        primitiveTypeInfo = TypeInfoFactory.floatTypeInfo;\n+        break;\n+      case INTEGER:\n+        primitiveTypeInfo = TypeInfoFactory.intTypeInfo;\n+        break;\n+      case LONG:\n+        primitiveTypeInfo = TypeInfoFactory.longTypeInfo;\n+        break;\n+      case STRING:\n+        primitiveTypeInfo = TypeInfoFactory.stringTypeInfo;\n+        break;\n+      case TIMESTAMP:\n+        boolean adjustToUTC = ((Types.TimestampType) primitiveType).shouldAdjustToUTC();\n+        return IcebergTimestampObjectInspector.get(adjustToUTC);\n+\n+      case FIXED:\n+      case TIME:\n+      case UUID:\n+      default:\n+        throw new IllegalArgumentException(primitiveType.typeId() + \" type is not supported\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0ODExMQ=="}, "originalCommit": {"oid": "b3a0cfe243b58719ab5b3b26f0dbd108b3f35c6d"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4ODA4MDU4OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/serde/objectinspector/IcebergRecordObjectInspector.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMToxNDo1OVrOGqoEbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMToxNDo1OVrOGqoEbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM0OTg2OA==", "bodyText": "@omalley, is the Iceberg field ID suitable to return as a Hive field ID here?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r447349868", "createdAt": "2020-06-30T01:14:59Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/serde/objectinspector/IcebergRecordObjectInspector.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred.serde.objectinspector;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+\n+public final class IcebergRecordObjectInspector extends StructObjectInspector {\n+\n+  private static final IcebergRecordObjectInspector EMPTY =\n+          new IcebergRecordObjectInspector(Types.StructType.of(), Collections.emptyList());\n+\n+  private final List<IcebergRecordStructField> structFields;\n+\n+  public IcebergRecordObjectInspector(Types.StructType structType, List<ObjectInspector> objectInspectors) {\n+    Preconditions.checkArgument(structType.fields().size() == objectInspectors.size());\n+\n+    this.structFields = Lists.newArrayListWithExpectedSize(structType.fields().size());\n+\n+    int position = 0;\n+\n+    for (Types.NestedField field : structType.fields()) {\n+      ObjectInspector oi = objectInspectors.get(position);\n+      IcebergRecordStructField structField = new IcebergRecordStructField(field, oi, position);\n+      structFields.add(structField);\n+      position++;\n+    }\n+  }\n+\n+  public static IcebergRecordObjectInspector empty() {\n+    return EMPTY;\n+  }\n+\n+  @Override\n+  public List<? extends StructField> getAllStructFieldRefs() {\n+    return structFields;\n+  }\n+\n+  @Override\n+  public StructField getStructFieldRef(String name) {\n+    return ObjectInspectorUtils.getStandardStructFieldRef(name, structFields);\n+  }\n+\n+  @Override\n+  public Object getStructFieldData(Object o, StructField structField) {\n+    return ((Record) o).get(((IcebergRecordStructField) structField).position());\n+  }\n+\n+  @Override\n+  public List<Object> getStructFieldsDataAsList(Object o) {\n+    Record record = (Record) o;\n+    return structFields\n+            .stream()\n+            .map(f -> record.get(f.position()))\n+            .collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public String getTypeName() {\n+    return ObjectInspectorUtils.getStandardStructTypeName(this);\n+  }\n+\n+  @Override\n+  public Category getCategory() {\n+    return Category.STRUCT;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+\n+    IcebergRecordObjectInspector that = (IcebergRecordObjectInspector) o;\n+    return structFields.equals(that.structFields);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return structFields.hashCode();\n+  }\n+\n+  private static class IcebergRecordStructField implements StructField {\n+\n+    private final Types.NestedField field;\n+    private final ObjectInspector oi;\n+    private final int position;\n+\n+    IcebergRecordStructField(Types.NestedField field, ObjectInspector oi, int position) {\n+      this.field = field;\n+      this.oi = oi;\n+      this.position = position; // position in the record\n+    }\n+\n+    @Override\n+    public String getFieldName() {\n+      return field.name();\n+    }\n+\n+    @Override\n+    public ObjectInspector getFieldObjectInspector() {\n+      return oi;\n+    }\n+\n+    @Override\n+    public int getFieldID() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3a0cfe243b58719ab5b3b26f0dbd108b3f35c6d"}, "originalPosition": 136}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4ODA4MjE2OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/serde/objectinspector/IcebergRecordObjectInspector.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMToxNTo0M1rOGqoFUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMToxNTo0M1rOGqoFUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1MDA5Ng==", "bodyText": "We typically prefer Objects.hash to this older pattern.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r447350096", "createdAt": "2020-06-30T01:15:43Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/serde/objectinspector/IcebergRecordObjectInspector.java", "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred.serde.objectinspector;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+\n+public final class IcebergRecordObjectInspector extends StructObjectInspector {\n+\n+  private static final IcebergRecordObjectInspector EMPTY =\n+          new IcebergRecordObjectInspector(Types.StructType.of(), Collections.emptyList());\n+\n+  private final List<IcebergRecordStructField> structFields;\n+\n+  public IcebergRecordObjectInspector(Types.StructType structType, List<ObjectInspector> objectInspectors) {\n+    Preconditions.checkArgument(structType.fields().size() == objectInspectors.size());\n+\n+    this.structFields = Lists.newArrayListWithExpectedSize(structType.fields().size());\n+\n+    int position = 0;\n+\n+    for (Types.NestedField field : structType.fields()) {\n+      ObjectInspector oi = objectInspectors.get(position);\n+      IcebergRecordStructField structField = new IcebergRecordStructField(field, oi, position);\n+      structFields.add(structField);\n+      position++;\n+    }\n+  }\n+\n+  public static IcebergRecordObjectInspector empty() {\n+    return EMPTY;\n+  }\n+\n+  @Override\n+  public List<? extends StructField> getAllStructFieldRefs() {\n+    return structFields;\n+  }\n+\n+  @Override\n+  public StructField getStructFieldRef(String name) {\n+    return ObjectInspectorUtils.getStandardStructFieldRef(name, structFields);\n+  }\n+\n+  @Override\n+  public Object getStructFieldData(Object o, StructField structField) {\n+    return ((Record) o).get(((IcebergRecordStructField) structField).position());\n+  }\n+\n+  @Override\n+  public List<Object> getStructFieldsDataAsList(Object o) {\n+    Record record = (Record) o;\n+    return structFields\n+            .stream()\n+            .map(f -> record.get(f.position()))\n+            .collect(Collectors.toList());\n+  }\n+\n+  @Override\n+  public String getTypeName() {\n+    return ObjectInspectorUtils.getStandardStructTypeName(this);\n+  }\n+\n+  @Override\n+  public Category getCategory() {\n+    return Category.STRUCT;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+\n+    IcebergRecordObjectInspector that = (IcebergRecordObjectInspector) o;\n+    return structFields.equals(that.structFields);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return structFields.hashCode();\n+  }\n+\n+  private static class IcebergRecordStructField implements StructField {\n+\n+    private final Types.NestedField field;\n+    private final ObjectInspector oi;\n+    private final int position;\n+\n+    IcebergRecordStructField(Types.NestedField field, ObjectInspector oi, int position) {\n+      this.field = field;\n+      this.oi = oi;\n+      this.position = position; // position in the record\n+    }\n+\n+    @Override\n+    public String getFieldName() {\n+      return field.name();\n+    }\n+\n+    @Override\n+    public ObjectInspector getFieldObjectInspector() {\n+      return oi;\n+    }\n+\n+    @Override\n+    public int getFieldID() {\n+      return field.fieldId();\n+    }\n+\n+    @Override\n+    public String getFieldComment() {\n+      return field.doc();\n+    }\n+\n+    int position() {\n+      return position;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+      if (this == o) {\n+        return true;\n+      }\n+\n+      if (o == null || getClass() != o.getClass()) {\n+        return false;\n+      }\n+\n+      IcebergRecordStructField that = (IcebergRecordStructField) o;\n+      return field.equals(that.field) && oi.equals(that.oi);\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+      return 31 * field.hashCode() + oi.hashCode();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3a0cfe243b58719ab5b3b26f0dbd108b3f35c6d"}, "originalPosition": 165}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4ODA4ODA2OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/serde/objectinspector/IcebergTimestampObjectInspector.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMToxOTowOVrOGqoI1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMToxOTowOVrOGqoI1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM1MDk5Nw==", "bodyText": "Minor: It seems like this would be a bit cleaner if the outer class was abstract and these were anonymous classes with an implementation for LocalDateTime convert(Object o) or something similar. Using Function is okay, but seems like it uses functions to avoid normal inheritance.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r447350997", "createdAt": "2020-06-30T01:19:09Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/serde/objectinspector/IcebergTimestampObjectInspector.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred.serde.objectinspector;\n+\n+import java.sql.Timestamp;\n+import java.time.LocalDateTime;\n+import java.time.OffsetDateTime;\n+import java.util.function.Function;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritable;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+public final class IcebergTimestampObjectInspector extends IcebergPrimitiveObjectInspector\n+        implements TimestampObjectInspector {\n+\n+  private static final IcebergTimestampObjectInspector INSTANCE_WITH_ZONE =\n+          new IcebergTimestampObjectInspector(o -> ((OffsetDateTime) o).toLocalDateTime());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3a0cfe243b58719ab5b3b26f0dbd108b3f35c6d"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4ODE3NzU2OnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/serde/objectinspector/TestIcebergBinaryObjectInspector.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMjowNDozOFrOGqo73A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMjowNDozOFrOGqo73A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM2NDA2MA==", "bodyText": "It would be nice to have more cases in this test suite:\n\nWhen the buffer's limit is less than array().length\nWhen the buffer's arrayOffset is non-zero\nWhen the buffer's position is non-zero", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r447364060", "createdAt": "2020-06-30T02:04:38Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/serde/objectinspector/TestIcebergBinaryObjectInspector.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred.serde.objectinspector;\n+\n+import java.nio.ByteBuffer;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.io.BytesWritable;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class TestIcebergBinaryObjectInspector {\n+\n+  @Test\n+  public void testIcebergBinaryObjectInspector() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3a0cfe243b58719ab5b3b26f0dbd108b3f35c6d"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc4ODE4MzYxOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/serde/objectinspector/IcebergDateObjectInspector.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMjowNzo0NVrOGqo_WA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQwMjowNzo0NVrOGqo_WA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzM2NDk1Mg==", "bodyText": "Instead of converting to Date and then wrapping with DateWritable, could we use the DateWritable constructor that accepts an integer? That would be more direct and we could convert using DateTimeUtil.daysFromDate(localDate) that we use elsewhere.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r447364952", "createdAt": "2020-06-30T02:07:45Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/serde/objectinspector/IcebergDateObjectInspector.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred.serde.objectinspector;\n+\n+import java.sql.Date;\n+import java.time.LocalDate;\n+import org.apache.hadoop.hive.serde2.io.DateWritable;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.DateObjectInspector;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+\n+public final class IcebergDateObjectInspector extends IcebergPrimitiveObjectInspector implements DateObjectInspector {\n+\n+  private static final IcebergDateObjectInspector INSTANCE = new IcebergDateObjectInspector();\n+\n+  public static IcebergDateObjectInspector get() {\n+    return INSTANCE;\n+  }\n+\n+  private IcebergDateObjectInspector() {\n+    super(TypeInfoFactory.dateTypeInfo);\n+  }\n+\n+  @Override\n+  public Date getPrimitiveJavaObject(Object o) {\n+    return o == null ? null : Date.valueOf((LocalDate) o);\n+  }\n+\n+  @Override\n+  public DateWritable getPrimitiveWritableObject(Object o) {\n+    Date date = getPrimitiveJavaObject(o);\n+    return date == null ? null : new DateWritable(date);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3a0cfe243b58719ab5b3b26f0dbd108b3f35c6d"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5NTI3MTA5OnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/serde/objectinspector/TestIcebergObjectInspector.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxNjoyMToyM1rOGrs1Vg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNzo1MDo1M1rOGsBa6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ3NjUwMg==", "bodyText": "Why was this introduced? It seems like relying on the same execution order between the schema creation and the test methods is brittle.\nI'd prefer to move back to fixed IDs since that's easier to test and more clear in assertions.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r448476502", "createdAt": "2020-07-01T16:21:23Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/serde/objectinspector/TestIcebergObjectInspector.java", "diffHunk": "@@ -38,31 +38,37 @@\n \n public class TestIcebergObjectInspector {\n \n+  private int id = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f0d8a1ba24f664fc4c07dd43988b06bb5a886e27"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUwOTg5Mw==", "bodyText": "That made my life easier when adding new field but I get your point, I'll fix it in a follow-up PR.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r448509893", "createdAt": "2020-07-01T17:22:51Z", "author": {"login": "guilload"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/serde/objectinspector/TestIcebergObjectInspector.java", "diffHunk": "@@ -38,31 +38,37 @@\n \n public class TestIcebergObjectInspector {\n \n+  private int id = 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ3NjUwMg=="}, "originalCommit": {"oid": "f0d8a1ba24f664fc4c07dd43988b06bb5a886e27"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUxNjA4OA==", "bodyText": "@rdblue , @guilload, @massdosage if there are many issues, open items, does it make sense to create a milestone with all the open tickets? So that it can be worked on in parallel by us and we don't duplicate effort or step on each other's toes?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r448516088", "createdAt": "2020-07-01T17:34:45Z", "author": {"login": "rdsr"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/serde/objectinspector/TestIcebergObjectInspector.java", "diffHunk": "@@ -38,31 +38,37 @@\n \n public class TestIcebergObjectInspector {\n \n+  private int id = 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ3NjUwMg=="}, "originalCommit": {"oid": "f0d8a1ba24f664fc4c07dd43988b06bb5a886e27"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU0MDc2Mg==", "bodyText": "Sounds good to me. You should be able to create and edit milestones.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r448540762", "createdAt": "2020-07-01T18:23:06Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/serde/objectinspector/TestIcebergObjectInspector.java", "diffHunk": "@@ -38,31 +38,37 @@\n \n public class TestIcebergObjectInspector {\n \n+  private int id = 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ3NjUwMg=="}, "originalCommit": {"oid": "f0d8a1ba24f664fc4c07dd43988b06bb5a886e27"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODgxMzgwMw==", "bodyText": "I think the only missing necessary follow up here already has a PR at #1157 so I'm not sure we need to do this? We're now co-ordinating with @guilload on the next steps for the mapred InputFormat.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r448813803", "createdAt": "2020-07-02T07:50:53Z", "author": {"login": "massdosage"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/serde/objectinspector/TestIcebergObjectInspector.java", "diffHunk": "@@ -38,31 +38,37 @@\n \n public class TestIcebergObjectInspector {\n \n+  private int id = 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ3NjUwMg=="}, "originalCommit": {"oid": "f0d8a1ba24f664fc4c07dd43988b06bb5a886e27"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5NTI3NzU1OnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestTableResolver.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxNjoyMzowMVrOGrs5Fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxOTo1Njo0NlrOGsbRDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ3NzQ2Mw==", "bodyText": "We usually prefer using AssertHelpers.assertThrows here, but this is minor since you don't need to check that other state has not been modified after the failure.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r448477463", "createdAt": "2020-07-01T16:23:01Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestTableResolver.java", "diffHunk": "@@ -111,4 +111,21 @@ public void resolveTableFromPropertiesDefault() throws IOException {\n     Assert.assertEquals(tableLocation.getAbsolutePath(), table.location());\n   }\n \n+  @Test(expected = UnsupportedOperationException.class)\n+  public void resolveTableFromConfigurationHiveCatalog() throws IOException {\n+    Configuration conf = new Configuration();\n+    conf.set(InputFormatConfig.CATALOG_NAME, InputFormatConfig.HIVE_CATALOG);\n+    conf.set(InputFormatConfig.TABLE_NAME, \"table_a\");\n+\n+    TableResolver.resolveTableFromConfiguration(conf);\n+  }\n+\n+  @Test(expected = NullPointerException.class)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de445c4aa7416a21b978b7f448b9d7fde27c7ced"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODgxNDI3NQ==", "bodyText": "Since the above is almost certainly going to be refactored when we merge more logic between the two InputFormats we can tackle this then.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r448814275", "createdAt": "2020-07-02T07:51:44Z", "author": {"login": "massdosage"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestTableResolver.java", "diffHunk": "@@ -111,4 +111,21 @@ public void resolveTableFromPropertiesDefault() throws IOException {\n     Assert.assertEquals(tableLocation.getAbsolutePath(), table.location());\n   }\n \n+  @Test(expected = UnsupportedOperationException.class)\n+  public void resolveTableFromConfigurationHiveCatalog() throws IOException {\n+    Configuration conf = new Configuration();\n+    conf.set(InputFormatConfig.CATALOG_NAME, InputFormatConfig.HIVE_CATALOG);\n+    conf.set(InputFormatConfig.TABLE_NAME, \"table_a\");\n+\n+    TableResolver.resolveTableFromConfiguration(conf);\n+  }\n+\n+  @Test(expected = NullPointerException.class)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ3NzQ2Mw=="}, "originalCommit": {"oid": "de445c4aa7416a21b978b7f448b9d7fde27c7ced"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTIzNzI2Mw==", "bodyText": "I agree. I didn't want to block progress on Hive just for this.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r449237263", "createdAt": "2020-07-02T19:56:46Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapred/TestTableResolver.java", "diffHunk": "@@ -111,4 +111,21 @@ public void resolveTableFromPropertiesDefault() throws IOException {\n     Assert.assertEquals(tableLocation.getAbsolutePath(), table.location());\n   }\n \n+  @Test(expected = UnsupportedOperationException.class)\n+  public void resolveTableFromConfigurationHiveCatalog() throws IOException {\n+    Configuration conf = new Configuration();\n+    conf.set(InputFormatConfig.CATALOG_NAME, InputFormatConfig.HIVE_CATALOG);\n+    conf.set(InputFormatConfig.TABLE_NAME, \"table_a\");\n+\n+    TableResolver.resolveTableFromConfiguration(conf);\n+  }\n+\n+  @Test(expected = NullPointerException.class)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ3NzQ2Mw=="}, "originalCommit": {"oid": "de445c4aa7416a21b978b7f448b9d7fde27c7ced"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5NTM5OTQ3OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxNjo1Nzo1NFrOGruFDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwNzo0NzoyNFrOGsBTwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5NjkwOA==", "bodyText": "can we put the hive classes in org.apache.iceberg.hive ? This is committed, but are you guys ok for this refactor?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r448496908", "createdAt": "2020-07-01T16:57:54Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "de445c4aa7416a21b978b7f448b9d7fde27c7ced"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUwMTEwOQ==", "bodyText": "I thought the convention was that the package name needed to match the subproject name and this isn't in the hive subproject but maybe that's not the case? Alternatively they could go in org.apache.iceberg.mr.hive?", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r448501109", "createdAt": "2020-07-01T17:05:34Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5NjkwOA=="}, "originalCommit": {"oid": "de445c4aa7416a21b978b7f448b9d7fde27c7ced"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUwNDk0OA==", "bodyText": "I prefer org.apache.iceberg.mr.hive", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r448504948", "createdAt": "2020-07-01T17:13:29Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5NjkwOA=="}, "originalCommit": {"oid": "de445c4aa7416a21b978b7f448b9d7fde27c7ced"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU0MTMyNA==", "bodyText": "mr.hive sounds good to me.\nI don't think we need to worry too much about this kind of refactor right now. We expect it to change rapidly as we build. We'll include a note in any release about how it is experimental and subject to change.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r448541324", "createdAt": "2020-07-01T18:24:13Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5NjkwOA=="}, "originalCommit": {"oid": "de445c4aa7416a21b978b7f448b9d7fde27c7ced"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODgxMTk2OQ==", "bodyText": "OK, my preference would be to leave this as it is for now and then do a review of all the packaging once we have the StorageHandler and InputFormat merged.", "url": "https://github.com/apache/iceberg/pull/1103#discussion_r448811969", "createdAt": "2020-07-02T07:47:24Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/IcebergSerDe.java", "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQ5NjkwOA=="}, "originalCommit": {"oid": "de445c4aa7416a21b978b7f448b9d7fde27c7ced"}, "originalPosition": 20}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3890, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}