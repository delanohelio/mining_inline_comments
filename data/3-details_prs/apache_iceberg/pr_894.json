{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk5NzIwNTI5", "number": 894, "title": "Spark: Implement an action to remove orphan files", "bodyText": "This PR adds a Spark action that removes orphan data and metadata files that can be left in some edge cases like executor preemption.", "createdAt": "2020-04-06T15:20:00Z", "url": "https://github.com/apache/iceberg/pull/894", "merged": true, "mergeCommit": {"oid": "bedc9c77036c882f6850fe3f3e6db93e4bb19efd"}, "closed": true, "closedAt": "2020-04-10T19:02:52Z", "author": {"login": "aokolnychyi"}, "timelineItems": {"totalCount": 26, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcVAScLAH2gAyMzk5NzIwNTI5OjMyYmQxYmQ3MmE5NTg2ZjU1OWIwZTMwYjM3OTc1YmFjNjRlNjg4ZTU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcWTwQJgFqTM5MTUzNTk0NQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/32bd1bd72a9586f559b0e30b37975bac64e688e5", "committedDate": "2020-04-06T15:18:38Z", "message": "Spark: Implement an action to remove orphan files"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg4NjU0NjY0", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-388654664", "createdAt": "2020-04-06T22:11:43Z", "commit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQyMjoxMTo0NFrOGBrz4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQyMjoxMTo0NFrOGBrz4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQxOTU1Mg==", "bodyText": "Can you also add a test for Write-Audit-Publish (WAP) workflow case where a snapshot can be staged (using the cherrypicking operation), where it's not part of the list of active snapshots. So expected behavior should be that this action should not delete those staged files as orphan files.\nThere are tests in TestWapWorkflow that illustrate this case.", "url": "https://github.com/apache/iceberg/pull/894#discussion_r404419552", "createdAt": "2020-04-06T22:11:44Z", "author": {"login": "prodeezy"}, "path": "spark/src/test/java/org/apache/iceberg/TestRemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestRemoveOrphanFilesAction {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+      .truncate(\"c2\", 2)\n+      .identity(\"c3\")\n+      .build();\n+\n+  private static SparkSession spark;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestRemoveOrphanFilesAction.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestRemoveOrphanFilesAction.spark;\n+    TestRemoveOrphanFilesAction.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+  private String tableLocation = null;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    File tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+  }\n+\n+  @Test\n+  public void testDryRun() throws IOException, InterruptedException {\n+    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap(), tableLocation);\n+\n+    List<ThreeColumnRecord> records = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(\"append\")\n+        .save(tableLocation);\n+\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(\"append\")\n+        .save(tableLocation);\n+\n+    List<String> validFiles = spark.read().format(\"iceberg\")\n+        .load(tableLocation + \"#files\")\n+        .select(\"file_path\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+    Assert.assertEquals(\"Should be 2 valid files\", 2, validFiles.size());\n+\n+    df.write().mode(\"append\").parquet(tableLocation + \"/data\");\n+\n+    Path dataPath = new Path(tableLocation + \"/data\");\n+    FileSystem fs = dataPath.getFileSystem(spark.sessionState().newHadoopConf());\n+    List<String> allFiles = Arrays.stream(fs.listStatus(dataPath, HiddenPathFilter.get()))\n+        .filter(FileStatus::isFile)\n+        .map(file -> file.getPath().toString())\n+        .collect(Collectors.toList());\n+    Assert.assertEquals(\"Should be 3 files\", 3, allFiles.size());\n+\n+    List<String> invalidFiles = Lists.newArrayList(allFiles);\n+    invalidFiles.removeAll(validFiles);\n+    Assert.assertEquals(\"Should be 1 invalid file\", 1, invalidFiles.size());\n+\n+    // sleep for 1 second to unsure files will be old enough\n+    Thread.sleep(1000);\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    RemoveOrphanFilesActionResult result1 = actions.removeOrphanFiles()\n+        .allDataFilesTable(tableLocation + \"#all_data_files\")\n+        .olderThan(System.currentTimeMillis())\n+        .dryRun(true)\n+        .execute();\n+    Assert.assertEquals(\"Action should find 1 data file\", invalidFiles, result1.dataFiles());\n+    Assert.assertTrue(\"Invalid file should be present\", fs.exists(new Path(invalidFiles.get(0))));\n+\n+    RemoveOrphanFilesActionResult result2 = actions.removeOrphanFiles()\n+        .allDataFilesTable(tableLocation + \"#all_data_files\")\n+        .olderThan(System.currentTimeMillis())\n+        .execute();\n+    Assert.assertEquals(\"Action should delete 1 data file\", invalidFiles, result2.dataFiles());\n+    Assert.assertFalse(\"Invalid file should not be present\", fs.exists(new Path(invalidFiles.get(0))));\n+\n+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n+    expectedRecords.addAll(records);\n+    expectedRecords.addAll(records);\n+\n+    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n+    List<ThreeColumnRecord> actualRecords = resultDF\n+        .as(Encoders.bean(ThreeColumnRecord.class))\n+        .collectAsList();\n+    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+  }\n+\n+  @Test\n+  public void testAllValidFilesAreKept() throws IOException, InterruptedException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "originalPosition": 166}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NTMxNTY2", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-389531566", "createdAt": "2020-04-07T22:21:54Z", "commit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyMTo1NFrOGCYQpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjoyMTo1NFrOGCYQpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE0NzgxNQ==", "bodyText": "What about providers that don't have a reliable location? Do they return null?", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405147815", "createdAt": "2020-04-07T22:21:54Z", "author": {"login": "rdblue"}, "path": "api/src/main/java/org/apache/iceberg/io/LocationProvider.java", "diffHunk": "@@ -29,6 +29,12 @@\n  * Implementations must be {@link Serializable} because instances will be serialized to tasks.\n  */\n public interface LocationProvider extends Serializable {\n+\n+  /**\n+   * Return a fully-qualified data location.\n+   */\n+  String dataLocation();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "originalPosition": 8}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NTM5NDM5", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-389539439", "createdAt": "2020-04-07T22:39:57Z", "commit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjozOTo1N1rOGCYsZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjozOTo1N1rOGCYsZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NDkxOQ==", "bodyText": "You probably don't need distinct if you're using this in a left anti-join. That just introduces an additional shuffle.", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405154919", "createdAt": "2020-04-07T22:39:57Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<RemoveOrphanFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final FileIO fileIO;\n+  private final int partitionDiscoveryParallelism;\n+  private final String dataLocation;\n+\n+  private String allDataFilesTable = null;\n+  private Long olderThanTimestamp = null;\n+  private boolean dryRun = false;\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.fileIO = table.io();\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.dataLocation = table.locationProvider().dataLocation();\n+  }\n+\n+  public RemoveOrphanFilesAction allDataFilesTable(String newAllDataFilesTable) {\n+    this.allDataFilesTable = newAllDataFilesTable;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction dryRun(boolean newDryRun) {\n+    this.dryRun = newDryRun;\n+    return this;\n+  }\n+\n+  @Override\n+  public RemoveOrphanFilesActionResult execute() {\n+    Preconditions.checkArgument(allDataFilesTable != null, \"allDataFilesTable must be set\");\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp should be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> actualDataFileDF = buildActualDataFileDF();\n+\n+    Column joinCond = validDataFileDF.col(\"file_path\").equalTo(actualDataFileDF.col(\"file_path\"));\n+    List<String> orphanDataFiles = actualDataFileDF.join(validDataFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    if (!dryRun) {\n+      Tasks.foreach(orphanDataFiles)\n+          .noRetry()\n+          .suppressFailureWhenFinished()\n+          .onFailure((file, exc) -> LOG.warn(\"Failed to delete data file: {}\", file, exc))\n+          .run(fileIO::deleteFile);\n+    }\n+\n+    return new RemoveOrphanFilesActionResult(orphanDataFiles);\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesTable)\n+        .select(\"file_path\")\n+        .distinct();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "originalPosition": 116}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NTM5ODIw", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-389539820", "createdAt": "2020-04-07T22:40:51Z", "commit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjo0MDo1MVrOGCYtpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjo0MDo1MVrOGCYtpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NTIzNw==", "bodyText": "Are you planning to make removeOrphanMetadataFiles as well, or would that be added to this?", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405155237", "createdAt": "2020-04-07T22:40:51Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/Actions.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import org.apache.spark.sql.SparkSession;\n+\n+public class Actions {\n+\n+  private SparkSession spark;\n+  private Table table;\n+\n+  private Actions(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+  }\n+\n+  public static Actions forTable(SparkSession spark, Table table) {\n+    return new Actions(spark, table);\n+  }\n+\n+  public static Actions forTable(Table table) {\n+    return new Actions(SparkSession.active(), table);\n+  }\n+\n+  public RemoveOrphanFilesAction removeOrphanFiles() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "originalPosition": 42}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NTQwMTc3", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-389540177", "createdAt": "2020-04-07T22:41:42Z", "commit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjo0MTo0MlrOGCYu5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjo0MTo0MlrOGCYu5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NTU1OA==", "bodyText": "Why return a result class when you could just return List<String>? Do we need these wrappers?", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405155558", "createdAt": "2020-04-07T22:41:42Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesActionResult.java", "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import java.util.List;\n+\n+public class RemoveOrphanFilesActionResult {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "originalPosition": 24}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NTQwNzY2", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-389540766", "createdAt": "2020-04-07T22:43:09Z", "commit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjo0MzowOVrOGCYxLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjo0MzowOVrOGCYxLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NjE0Mw==", "bodyText": "In other places, we use deleteWith instead of a dry run flag. Should we do that for consistency here?", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405156143", "createdAt": "2020-04-07T22:43:09Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<RemoveOrphanFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final FileIO fileIO;\n+  private final int partitionDiscoveryParallelism;\n+  private final String dataLocation;\n+\n+  private String allDataFilesTable = null;\n+  private Long olderThanTimestamp = null;\n+  private boolean dryRun = false;\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.fileIO = table.io();\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.dataLocation = table.locationProvider().dataLocation();\n+  }\n+\n+  public RemoveOrphanFilesAction allDataFilesTable(String newAllDataFilesTable) {\n+    this.allDataFilesTable = newAllDataFilesTable;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction dryRun(boolean newDryRun) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "originalPosition": 83}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NTQxMjAw", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-389541200", "createdAt": "2020-04-07T22:44:16Z", "commit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjo0NDoxNlrOGCYyyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjo0NDoxNlrOGCYyyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NjU1Mg==", "bodyText": "This seems awkward, but I'm not sure a better way to do it.", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405156552", "createdAt": "2020-04-07T22:44:16Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<RemoveOrphanFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final FileIO fileIO;\n+  private final int partitionDiscoveryParallelism;\n+  private final String dataLocation;\n+\n+  private String allDataFilesTable = null;\n+  private Long olderThanTimestamp = null;\n+  private boolean dryRun = false;\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.fileIO = table.io();\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.dataLocation = table.locationProvider().dataLocation();\n+  }\n+\n+  public RemoveOrphanFilesAction allDataFilesTable(String newAllDataFilesTable) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "originalPosition": 73}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NTQyNDgx", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-389542481", "createdAt": "2020-04-07T22:47:22Z", "commit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjo0NzoyMlrOGCY3WQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjo0NzoyMlrOGCY3WQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1NzcyMQ==", "bodyText": "We should add Javadoc here that explains what happens. That should note that the table location needs to be accessible for listing via Hadoop FileSystem, but the table's FileIO will be used to delete.", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405157721", "createdAt": "2020-04-07T22:47:22Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/Actions.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import org.apache.spark.sql.SparkSession;\n+\n+public class Actions {\n+\n+  private SparkSession spark;\n+  private Table table;\n+\n+  private Actions(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+  }\n+\n+  public static Actions forTable(SparkSession spark, Table table) {\n+    return new Actions(spark, table);\n+  }\n+\n+  public static Actions forTable(Table table) {\n+    return new Actions(SparkSession.active(), table);\n+  }\n+\n+  public RemoveOrphanFilesAction removeOrphanFiles() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "originalPosition": 42}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NTQyOTYy", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-389542962", "createdAt": "2020-04-07T22:48:29Z", "commit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjo0ODoyOVrOGCY5Mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjo0ODoyOVrOGCY5Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE1ODE5NQ==", "bodyText": "Do we?", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405158195", "createdAt": "2020-04-07T22:48:29Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<RemoveOrphanFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final FileIO fileIO;\n+  private final int partitionDiscoveryParallelism;\n+  private final String dataLocation;\n+\n+  private String allDataFilesTable = null;\n+  private Long olderThanTimestamp = null;\n+  private boolean dryRun = false;\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.fileIO = table.io();\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.dataLocation = table.locationProvider().dataLocation();\n+  }\n+\n+  public RemoveOrphanFilesAction allDataFilesTable(String newAllDataFilesTable) {\n+    this.allDataFilesTable = newAllDataFilesTable;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction dryRun(boolean newDryRun) {\n+    this.dryRun = newDryRun;\n+    return this;\n+  }\n+\n+  @Override\n+  public RemoveOrphanFilesActionResult execute() {\n+    Preconditions.checkArgument(allDataFilesTable != null, \"allDataFilesTable must be set\");\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp should be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> actualDataFileDF = buildActualDataFileDF();\n+\n+    Column joinCond = validDataFileDF.col(\"file_path\").equalTo(actualDataFileDF.col(\"file_path\"));\n+    List<String> orphanDataFiles = actualDataFileDF.join(validDataFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    if (!dryRun) {\n+      Tasks.foreach(orphanDataFiles)\n+          .noRetry()\n+          .suppressFailureWhenFinished()\n+          .onFailure((file, exc) -> LOG.warn(\"Failed to delete data file: {}\", file, exc))\n+          .run(fileIO::deleteFile);\n+    }\n+\n+    return new RemoveOrphanFilesActionResult(orphanDataFiles);\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesTable)\n+        .select(\"file_path\")\n+        .distinct();\n+  }\n+\n+  private Dataset<Row> buildActualDataFileDF() {\n+    List<String> topLevelDirs = Lists.newArrayList();\n+    List<String> matchingTopLevelFiles = Lists.newArrayList();\n+\n+    try {\n+      Path dataPath = new Path(dataLocation);\n+      FileSystem fs = dataPath.getFileSystem(hadoopConf.value());\n+\n+      for (FileStatus file : fs.listStatus(dataPath, HiddenPathFilter.get())) {\n+        // TODO: handle custom metadata folders\n+        // we need to ignore the metadata folder when data is written to the root table location", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "originalPosition": 129}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NTQ1MTg4", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-389545188", "createdAt": "2020-04-07T22:53:42Z", "commit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjo1Mzo0MlrOGCZBBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMjo1Mzo0MlrOGCZBBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTE2MDE5Ng==", "bodyText": "I would probably opt not to do this check. Instead, there is little cost to doing the anti-join with all known data and metadata file paths. I'd probably do that instead of worrying about metadata location.", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405160196", "createdAt": "2020-04-07T22:53:42Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<RemoveOrphanFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final FileIO fileIO;\n+  private final int partitionDiscoveryParallelism;\n+  private final String dataLocation;\n+\n+  private String allDataFilesTable = null;\n+  private Long olderThanTimestamp = null;\n+  private boolean dryRun = false;\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.fileIO = table.io();\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.dataLocation = table.locationProvider().dataLocation();\n+  }\n+\n+  public RemoveOrphanFilesAction allDataFilesTable(String newAllDataFilesTable) {\n+    this.allDataFilesTable = newAllDataFilesTable;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction dryRun(boolean newDryRun) {\n+    this.dryRun = newDryRun;\n+    return this;\n+  }\n+\n+  @Override\n+  public RemoveOrphanFilesActionResult execute() {\n+    Preconditions.checkArgument(allDataFilesTable != null, \"allDataFilesTable must be set\");\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp should be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> actualDataFileDF = buildActualDataFileDF();\n+\n+    Column joinCond = validDataFileDF.col(\"file_path\").equalTo(actualDataFileDF.col(\"file_path\"));\n+    List<String> orphanDataFiles = actualDataFileDF.join(validDataFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    if (!dryRun) {\n+      Tasks.foreach(orphanDataFiles)\n+          .noRetry()\n+          .suppressFailureWhenFinished()\n+          .onFailure((file, exc) -> LOG.warn(\"Failed to delete data file: {}\", file, exc))\n+          .run(fileIO::deleteFile);\n+    }\n+\n+    return new RemoveOrphanFilesActionResult(orphanDataFiles);\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesTable)\n+        .select(\"file_path\")\n+        .distinct();\n+  }\n+\n+  private Dataset<Row> buildActualDataFileDF() {\n+    List<String> topLevelDirs = Lists.newArrayList();\n+    List<String> matchingTopLevelFiles = Lists.newArrayList();\n+\n+    try {\n+      Path dataPath = new Path(dataLocation);\n+      FileSystem fs = dataPath.getFileSystem(hadoopConf.value());\n+\n+      for (FileStatus file : fs.listStatus(dataPath, HiddenPathFilter.get())) {\n+        // TODO: handle custom metadata folders\n+        // we need to ignore the metadata folder when data is written to the root table location\n+        if (file.isDirectory() && !\"metadata\".equals(file.getPath().getName())) {\n+          topLevelDirs.add(file.getPath().toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "32bd1bd72a9586f559b0e30b37975bac64e688e5"}, "originalPosition": 131}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2c25af79c5f646b342080aa8a0a414eed0b9f6d2", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/2c25af79c5f646b342080aa8a0a414eed0b9f6d2", "committedDate": "2020-04-09T00:18:43Z", "message": "Rework the action to clean the whole table location"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwNDIxNjEw", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-390421610", "createdAt": "2020-04-09T00:40:39Z", "commit": {"oid": "2c25af79c5f646b342080aa8a0a414eed0b9f6d2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwMDo0MDozOVrOGDFwLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwMDo0MDozOVrOGDFwLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5MzE2NA==", "bodyText": "We have to clean the location properly.", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405893164", "createdAt": "2020-04-09T00:40:39Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHiveTables.java", "diffHunk": "@@ -78,12 +81,12 @@ public static void stopMetastoreAndSpark() {\n   }\n \n   @After\n-  public void dropTable() throws Exception {\n-    clients.run(client -> {\n-      client.dropTable(TestIcebergSourceHiveTables.currentIdentifier.namespace().level(0),\n-          TestIcebergSourceHiveTables.currentIdentifier.name());\n-      return null;\n-    });\n+  public void dropTable() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c25af79c5f646b342080aa8a0a414eed0b9f6d2"}, "originalPosition": 21}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/e0c96f353f6abc6804bdf1944e8677c1a62c3191", "committedDate": "2020-04-09T00:41:26Z", "message": "Remove extra line"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwNDIzMzM1", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-390423335", "createdAt": "2020-04-09T00:46:22Z", "commit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwMDo0NjoyMlrOGDF2Kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQwMDo0NjoyMlrOGDF2Kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTg5NDY5OQ==", "bodyText": "I could probably add a condition to this branch:\nelse if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\"))\n\nAlso, we need a comment here.", "url": "https://github.com/apache/iceberg/pull/894#discussion_r405894699", "createdAt": "2020-04-09T00:46:22Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private Long olderThanTimestamp = null;\n+  private Consumer<String> deleteFunc = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      table.io().deleteFile(file);\n+    }\n+  };\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.location = table.location();\n+  }\n+\n+  public RemoveOrphanFilesAction location(String newLocation) {\n+    this.location = newLocation;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public List<String> execute() {\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp must be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> validMetadataFileDF = buildValidMetadataFileDF();\n+    Dataset<Row> validFileDF = validDataFileDF.union(validMetadataFileDF);\n+    Dataset<Row> actualFileDF = buildActualFileDF();\n+\n+    Column joinCond = validFileDF.col(\"file_path\").equalTo(actualFileDF.col(\"file_path\"));\n+    List<String> orphanFiles = actualFileDF.join(validFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    Tasks.foreach(orphanFiles)\n+        .noRetry()\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.warn(\"Failed to delete file: {}\", file, exc))\n+        .run(deleteFunc::accept);\n+\n+    return orphanFiles;\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  private Dataset<Row> buildValidMetadataFileDF() {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+\n+    List<String> otherMetadataFiles = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        otherMetadataFiles.add(manifestListLocation);\n+      }\n+    }\n+\n+    otherMetadataFiles.add(ops.metadataFileLocation(\"version-hint.text\"));\n+\n+    TableMetadata metadata = ops.current();\n+    otherMetadataFiles.add(metadata.file().location());\n+    for (TableMetadata.MetadataLogEntry previousMetadataFile : metadata.previousFiles()) {\n+      otherMetadataFiles.add(previousMetadataFile.file());\n+    }\n+\n+    Dataset<Row> otherMetadataFileDF = spark\n+        .createDataset(otherMetadataFiles, Encoders.STRING())\n+        .toDF(\"file_path\");\n+\n+    return manifestDF.union(otherMetadataFileDF);\n+  }\n+\n+  private Dataset<Row> buildActualFileDF() {\n+    List<String> topLevelDirs = Lists.newArrayList();\n+    List<String> matchingTopLevelFiles = Lists.newArrayList();\n+\n+    try {\n+      Path path = new Path(location);\n+      FileSystem fs = path.getFileSystem(hadoopConf.value());\n+\n+      for (FileStatus file : fs.listStatus(path, HiddenPathFilter.get())) {\n+        if (file.isDirectory()) {\n+          topLevelDirs.add(file.getPath().toString());\n+        } else if (file.isFile() && file.getModificationTime() < olderThanTimestamp) {\n+          matchingTopLevelFiles.add(file.getPath().toString());\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to determine top-level files and dirs in {}\", location);\n+    }\n+\n+    JavaRDD<String> matchingTopLevelFileRDD = sparkContext.parallelize(matchingTopLevelFiles, 1);\n+\n+    if (topLevelDirs.isEmpty()) {\n+      return spark.createDataset(matchingTopLevelFileRDD.rdd(), Encoders.STRING()).toDF(\"file_path\");\n+    }\n+\n+    int parallelism = Math.min(topLevelDirs.size(), partitionDiscoveryParallelism);\n+    JavaRDD<String> topLevelDirRDD = sparkContext.parallelize(topLevelDirs, parallelism);\n+\n+    Broadcast<SerializableConfiguration> conf = sparkContext.broadcast(hadoopConf);\n+    JavaRDD<String> matchingLeafFileRDD = topLevelDirRDD.mapPartitions(listDirsRecursively(conf, olderThanTimestamp));\n+\n+    JavaRDD<String> matchingFileRDD = matchingTopLevelFileRDD.union(matchingLeafFileRDD);\n+    return spark.createDataset(matchingFileRDD.rdd(), Encoders.STRING()).toDF(\"file_path\");\n+  }\n+\n+  private static FlatMapFunction<Iterator<String>, String> listDirsRecursively(\n+      Broadcast<SerializableConfiguration> conf,\n+      long olderThanTimestamp) {\n+\n+    return (FlatMapFunction<Iterator<String>, String>) dirs -> {\n+      List<String> files = Lists.newArrayList();\n+      Predicate<FileStatus> predicate = file -> file.getModificationTime() < olderThanTimestamp;\n+      dirs.forEachRemaining(dir -> {\n+        List<String> dirFiles = listDirRecursively(dir, predicate, conf.value().value());\n+        files.addAll(dirFiles);\n+      });\n+      return files.iterator();\n+    };\n+  }\n+\n+  private static List<String> listDirRecursively(String dir, Predicate<FileStatus> predicate, Configuration conf) {\n+    try {\n+      Path path = new Path(dir);\n+      FileSystem fs = path.getFileSystem(conf);\n+\n+      List<String> childDirs = Lists.newArrayList();\n+      List<String> matchingFiles = Lists.newArrayList();\n+\n+      for (FileStatus file : fs.listStatus(path, HiddenPathFilter.get())) {\n+        if (file.isDirectory()) {\n+          childDirs.add(file.getPath().toString());\n+        } else if (file.isFile() && predicate.test(file)) {\n+          matchingFiles.add(file.getPath().toString());\n+        }\n+      }\n+\n+      for (String childDir : childDirs) {\n+        List<String> childDirFiles = listDirRecursively(childDir, predicate, conf);\n+        matchingFiles.addAll(childDirFiles);\n+      }\n+\n+      return matchingFiles;\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e);\n+    }\n+  }\n+\n+  private String metadataTableName(MetadataTableType type) {\n+    String tableName = table.toString();\n+    if (tableName.contains(\"/\")) {\n+      return tableName + \"#\" + type;\n+    } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191"}, "originalPosition": 235}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwOTU0Mzgz", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-390954383", "createdAt": "2020-04-09T16:37:02Z", "commit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNjozNzowM1rOGDgjmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNjozNzowM1rOGDgjmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMzMjMxNA==", "bodyText": "Should we create an actions package?", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406332314", "createdAt": "2020-04-09T16:37:03Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/Action.java", "diffHunk": "@@ -0,0 +1,24 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+public interface Action<R> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191"}, "originalPosition": 22}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwOTU2Njc4", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-390956678", "createdAt": "2020-04-09T16:40:06Z", "commit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNjo0MDowN1rOGDgq9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNjo0MDowN1rOGDgq9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMzNDE5OA==", "bodyText": "Good catch!", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406334198", "createdAt": "2020-04-09T16:40:07Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private Long olderThanTimestamp = null;\n+  private Consumer<String> deleteFunc = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      table.io().deleteFile(file);\n+    }\n+  };\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.location = table.location();\n+  }\n+\n+  public RemoveOrphanFilesAction location(String newLocation) {\n+    this.location = newLocation;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public List<String> execute() {\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp must be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> validMetadataFileDF = buildValidMetadataFileDF();\n+    Dataset<Row> validFileDF = validDataFileDF.union(validMetadataFileDF);\n+    Dataset<Row> actualFileDF = buildActualFileDF();\n+\n+    Column joinCond = validFileDF.col(\"file_path\").equalTo(actualFileDF.col(\"file_path\"));\n+    List<String> orphanFiles = actualFileDF.join(validFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    Tasks.foreach(orphanFiles)\n+        .noRetry()\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.warn(\"Failed to delete file: {}\", file, exc))\n+        .run(deleteFunc::accept);\n+\n+    return orphanFiles;\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  private Dataset<Row> buildValidMetadataFileDF() {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+\n+    List<String> otherMetadataFiles = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        otherMetadataFiles.add(manifestListLocation);\n+      }\n+    }\n+\n+    otherMetadataFiles.add(ops.metadataFileLocation(\"version-hint.text\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191"}, "originalPosition": 139}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwOTYzNDQ5", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-390963449", "createdAt": "2020-04-09T16:49:26Z", "commit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNjo0OToyNlrOGDg_6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNjo0OToyNlrOGDg_6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjMzOTU2Mg==", "bodyText": "Now that this is the table's location, we expect this to contain just two directories: data and metadata. I think the intent of this methods was to parallelize on the first level of partition directories, but that's not what will happen here.\nIt's a bit more tricky because we don't know the convention actually matches the default structure, but I think it would be reasonable to traverse the first 2 layers of directories to build the top-level set. To do that, adding a depth parameter to the recursive traversal makes sense so you can use it here and return after 2 levels (or a configurable number). That would also be a good thing for the parallel traversal to ensure this won't get caught in a symlink loop.", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406339562", "createdAt": "2020-04-09T16:49:26Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private Long olderThanTimestamp = null;\n+  private Consumer<String> deleteFunc = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      table.io().deleteFile(file);\n+    }\n+  };\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.location = table.location();\n+  }\n+\n+  public RemoveOrphanFilesAction location(String newLocation) {\n+    this.location = newLocation;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public List<String> execute() {\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp must be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> validMetadataFileDF = buildValidMetadataFileDF();\n+    Dataset<Row> validFileDF = validDataFileDF.union(validMetadataFileDF);\n+    Dataset<Row> actualFileDF = buildActualFileDF();\n+\n+    Column joinCond = validFileDF.col(\"file_path\").equalTo(actualFileDF.col(\"file_path\"));\n+    List<String> orphanFiles = actualFileDF.join(validFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    Tasks.foreach(orphanFiles)\n+        .noRetry()\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.warn(\"Failed to delete file: {}\", file, exc))\n+        .run(deleteFunc::accept);\n+\n+    return orphanFiles;\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  private Dataset<Row> buildValidMetadataFileDF() {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+\n+    List<String> otherMetadataFiles = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        otherMetadataFiles.add(manifestListLocation);\n+      }\n+    }\n+\n+    otherMetadataFiles.add(ops.metadataFileLocation(\"version-hint.text\"));\n+\n+    TableMetadata metadata = ops.current();\n+    otherMetadataFiles.add(metadata.file().location());\n+    for (TableMetadata.MetadataLogEntry previousMetadataFile : metadata.previousFiles()) {\n+      otherMetadataFiles.add(previousMetadataFile.file());\n+    }\n+\n+    Dataset<Row> otherMetadataFileDF = spark\n+        .createDataset(otherMetadataFiles, Encoders.STRING())\n+        .toDF(\"file_path\");\n+\n+    return manifestDF.union(otherMetadataFileDF);\n+  }\n+\n+  private Dataset<Row> buildActualFileDF() {\n+    List<String> topLevelDirs = Lists.newArrayList();\n+    List<String> matchingTopLevelFiles = Lists.newArrayList();\n+\n+    try {\n+      Path path = new Path(location);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191"}, "originalPosition": 159}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwOTY2MDE2", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-390966016", "createdAt": "2020-04-09T16:53:06Z", "commit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNjo1MzowN1rOGDhH_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNjo1MzowN1rOGDhH_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM0MTYyOQ==", "bodyText": "Nit: predicate isn't a very descriptive name. Maybe pastOperationTimeLimit instead?", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406341629", "createdAt": "2020-04-09T16:53:07Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private Long olderThanTimestamp = null;\n+  private Consumer<String> deleteFunc = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      table.io().deleteFile(file);\n+    }\n+  };\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.location = table.location();\n+  }\n+\n+  public RemoveOrphanFilesAction location(String newLocation) {\n+    this.location = newLocation;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public List<String> execute() {\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp must be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> validMetadataFileDF = buildValidMetadataFileDF();\n+    Dataset<Row> validFileDF = validDataFileDF.union(validMetadataFileDF);\n+    Dataset<Row> actualFileDF = buildActualFileDF();\n+\n+    Column joinCond = validFileDF.col(\"file_path\").equalTo(actualFileDF.col(\"file_path\"));\n+    List<String> orphanFiles = actualFileDF.join(validFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    Tasks.foreach(orphanFiles)\n+        .noRetry()\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.warn(\"Failed to delete file: {}\", file, exc))\n+        .run(deleteFunc::accept);\n+\n+    return orphanFiles;\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  private Dataset<Row> buildValidMetadataFileDF() {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+\n+    List<String> otherMetadataFiles = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        otherMetadataFiles.add(manifestListLocation);\n+      }\n+    }\n+\n+    otherMetadataFiles.add(ops.metadataFileLocation(\"version-hint.text\"));\n+\n+    TableMetadata metadata = ops.current();\n+    otherMetadataFiles.add(metadata.file().location());\n+    for (TableMetadata.MetadataLogEntry previousMetadataFile : metadata.previousFiles()) {\n+      otherMetadataFiles.add(previousMetadataFile.file());\n+    }\n+\n+    Dataset<Row> otherMetadataFileDF = spark\n+        .createDataset(otherMetadataFiles, Encoders.STRING())\n+        .toDF(\"file_path\");\n+\n+    return manifestDF.union(otherMetadataFileDF);\n+  }\n+\n+  private Dataset<Row> buildActualFileDF() {\n+    List<String> topLevelDirs = Lists.newArrayList();\n+    List<String> matchingTopLevelFiles = Lists.newArrayList();\n+\n+    try {\n+      Path path = new Path(location);\n+      FileSystem fs = path.getFileSystem(hadoopConf.value());\n+\n+      for (FileStatus file : fs.listStatus(path, HiddenPathFilter.get())) {\n+        if (file.isDirectory()) {\n+          topLevelDirs.add(file.getPath().toString());\n+        } else if (file.isFile() && file.getModificationTime() < olderThanTimestamp) {\n+          matchingTopLevelFiles.add(file.getPath().toString());\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to determine top-level files and dirs in {}\", location);\n+    }\n+\n+    JavaRDD<String> matchingTopLevelFileRDD = sparkContext.parallelize(matchingTopLevelFiles, 1);\n+\n+    if (topLevelDirs.isEmpty()) {\n+      return spark.createDataset(matchingTopLevelFileRDD.rdd(), Encoders.STRING()).toDF(\"file_path\");\n+    }\n+\n+    int parallelism = Math.min(topLevelDirs.size(), partitionDiscoveryParallelism);\n+    JavaRDD<String> topLevelDirRDD = sparkContext.parallelize(topLevelDirs, parallelism);\n+\n+    Broadcast<SerializableConfiguration> conf = sparkContext.broadcast(hadoopConf);\n+    JavaRDD<String> matchingLeafFileRDD = topLevelDirRDD.mapPartitions(listDirsRecursively(conf, olderThanTimestamp));\n+\n+    JavaRDD<String> matchingFileRDD = matchingTopLevelFileRDD.union(matchingLeafFileRDD);\n+    return spark.createDataset(matchingFileRDD.rdd(), Encoders.STRING()).toDF(\"file_path\");\n+  }\n+\n+  private static FlatMapFunction<Iterator<String>, String> listDirsRecursively(\n+      Broadcast<SerializableConfiguration> conf,\n+      long olderThanTimestamp) {\n+\n+    return (FlatMapFunction<Iterator<String>, String>) dirs -> {\n+      List<String> files = Lists.newArrayList();\n+      Predicate<FileStatus> predicate = file -> file.getModificationTime() < olderThanTimestamp;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191"}, "originalPosition": 195}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwOTcwMTUz", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-390970153", "createdAt": "2020-04-09T16:58:25Z", "commit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNjo1ODoyNVrOGDhUog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNjo1ODoyNVrOGDhUog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM0NDg2Ng==", "bodyText": "What about passing a result list into this method to avoid creating lots of small lists and merging them together in every call?", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406344866", "createdAt": "2020-04-09T16:58:25Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private Long olderThanTimestamp = null;\n+  private Consumer<String> deleteFunc = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      table.io().deleteFile(file);\n+    }\n+  };\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.location = table.location();\n+  }\n+\n+  public RemoveOrphanFilesAction location(String newLocation) {\n+    this.location = newLocation;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public List<String> execute() {\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp must be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> validMetadataFileDF = buildValidMetadataFileDF();\n+    Dataset<Row> validFileDF = validDataFileDF.union(validMetadataFileDF);\n+    Dataset<Row> actualFileDF = buildActualFileDF();\n+\n+    Column joinCond = validFileDF.col(\"file_path\").equalTo(actualFileDF.col(\"file_path\"));\n+    List<String> orphanFiles = actualFileDF.join(validFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    Tasks.foreach(orphanFiles)\n+        .noRetry()\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.warn(\"Failed to delete file: {}\", file, exc))\n+        .run(deleteFunc::accept);\n+\n+    return orphanFiles;\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  private Dataset<Row> buildValidMetadataFileDF() {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+\n+    List<String> otherMetadataFiles = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        otherMetadataFiles.add(manifestListLocation);\n+      }\n+    }\n+\n+    otherMetadataFiles.add(ops.metadataFileLocation(\"version-hint.text\"));\n+\n+    TableMetadata metadata = ops.current();\n+    otherMetadataFiles.add(metadata.file().location());\n+    for (TableMetadata.MetadataLogEntry previousMetadataFile : metadata.previousFiles()) {\n+      otherMetadataFiles.add(previousMetadataFile.file());\n+    }\n+\n+    Dataset<Row> otherMetadataFileDF = spark\n+        .createDataset(otherMetadataFiles, Encoders.STRING())\n+        .toDF(\"file_path\");\n+\n+    return manifestDF.union(otherMetadataFileDF);\n+  }\n+\n+  private Dataset<Row> buildActualFileDF() {\n+    List<String> topLevelDirs = Lists.newArrayList();\n+    List<String> matchingTopLevelFiles = Lists.newArrayList();\n+\n+    try {\n+      Path path = new Path(location);\n+      FileSystem fs = path.getFileSystem(hadoopConf.value());\n+\n+      for (FileStatus file : fs.listStatus(path, HiddenPathFilter.get())) {\n+        if (file.isDirectory()) {\n+          topLevelDirs.add(file.getPath().toString());\n+        } else if (file.isFile() && file.getModificationTime() < olderThanTimestamp) {\n+          matchingTopLevelFiles.add(file.getPath().toString());\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to determine top-level files and dirs in {}\", location);\n+    }\n+\n+    JavaRDD<String> matchingTopLevelFileRDD = sparkContext.parallelize(matchingTopLevelFiles, 1);\n+\n+    if (topLevelDirs.isEmpty()) {\n+      return spark.createDataset(matchingTopLevelFileRDD.rdd(), Encoders.STRING()).toDF(\"file_path\");\n+    }\n+\n+    int parallelism = Math.min(topLevelDirs.size(), partitionDiscoveryParallelism);\n+    JavaRDD<String> topLevelDirRDD = sparkContext.parallelize(topLevelDirs, parallelism);\n+\n+    Broadcast<SerializableConfiguration> conf = sparkContext.broadcast(hadoopConf);\n+    JavaRDD<String> matchingLeafFileRDD = topLevelDirRDD.mapPartitions(listDirsRecursively(conf, olderThanTimestamp));\n+\n+    JavaRDD<String> matchingFileRDD = matchingTopLevelFileRDD.union(matchingLeafFileRDD);\n+    return spark.createDataset(matchingFileRDD.rdd(), Encoders.STRING()).toDF(\"file_path\");\n+  }\n+\n+  private static FlatMapFunction<Iterator<String>, String> listDirsRecursively(\n+      Broadcast<SerializableConfiguration> conf,\n+      long olderThanTimestamp) {\n+\n+    return (FlatMapFunction<Iterator<String>, String>) dirs -> {\n+      List<String> files = Lists.newArrayList();\n+      Predicate<FileStatus> predicate = file -> file.getModificationTime() < olderThanTimestamp;\n+      dirs.forEachRemaining(dir -> {\n+        List<String> dirFiles = listDirRecursively(dir, predicate, conf.value().value());\n+        files.addAll(dirFiles);\n+      });\n+      return files.iterator();\n+    };\n+  }\n+\n+  private static List<String> listDirRecursively(String dir, Predicate<FileStatus> predicate, Configuration conf) {\n+    try {\n+      Path path = new Path(dir);\n+      FileSystem fs = path.getFileSystem(conf);\n+\n+      List<String> childDirs = Lists.newArrayList();\n+      List<String> matchingFiles = Lists.newArrayList();\n+\n+      for (FileStatus file : fs.listStatus(path, HiddenPathFilter.get())) {\n+        if (file.isDirectory()) {\n+          childDirs.add(file.getPath().toString());\n+        } else if (file.isFile() && predicate.test(file)) {\n+          matchingFiles.add(file.getPath().toString());\n+        }\n+      }\n+\n+      for (String childDir : childDirs) {\n+        List<String> childDirFiles = listDirRecursively(childDir, predicate, conf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191"}, "originalPosition": 221}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkwOTc0NDYz", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-390974463", "createdAt": "2020-04-09T17:04:25Z", "commit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNzowNDoyNVrOGDhiMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNzowNDoyNVrOGDhiMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM0ODMzNw==", "bodyText": "Can we default it to System.currentTimeMillis() - THREE_DAYS_MS?", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406348337", "createdAt": "2020-04-09T17:04:25Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private Long olderThanTimestamp = null;\n+  private Consumer<String> deleteFunc = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      table.io().deleteFile(file);\n+    }\n+  };\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.location = table.location();\n+  }\n+\n+  public RemoveOrphanFilesAction location(String newLocation) {\n+    this.location = newLocation;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public List<String> execute() {\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp must be set\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191"}, "originalPosition": 96}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f3e694856dbf5aaa0c4577dcc38ae1fce413b3fe", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/f3e694856dbf5aaa0c4577dcc38ae1fce413b3fe", "committedDate": "2020-04-10T02:07:37Z", "message": "Rework listing and add javadocs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6c01b75d0d738bc4b227b3deb5ffb29e73071dc6", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/6c01b75d0d738bc4b227b3deb5ffb29e73071dc6", "committedDate": "2020-04-10T02:17:01Z", "message": "Explain metadataTableName"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxMjM4MTM2", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-391238136", "createdAt": "2020-04-10T02:30:54Z", "commit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQwMjozMDo1NFrOGDvVHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQwMjozMDo1NFrOGDvVHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjU3NDM2NA==", "bodyText": "a comment here on criteria for collecting all actual files would be helpful.", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406574364", "createdAt": "2020-04-10T02:30:54Z", "author": {"login": "prodeezy"}, "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,239 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import parquet.Preconditions;\n+\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private Long olderThanTimestamp = null;\n+  private Consumer<String> deleteFunc = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      table.io().deleteFile(file);\n+    }\n+  };\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.location = table.location();\n+  }\n+\n+  public RemoveOrphanFilesAction location(String newLocation) {\n+    this.location = newLocation;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public List<String> execute() {\n+    Preconditions.checkArgument(olderThanTimestamp != null, \"olderThanTimestamp must be set\");\n+\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> validMetadataFileDF = buildValidMetadataFileDF();\n+    Dataset<Row> validFileDF = validDataFileDF.union(validMetadataFileDF);\n+    Dataset<Row> actualFileDF = buildActualFileDF();\n+\n+    Column joinCond = validFileDF.col(\"file_path\").equalTo(actualFileDF.col(\"file_path\"));\n+    List<String> orphanFiles = actualFileDF.join(validFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    Tasks.foreach(orphanFiles)\n+        .noRetry()\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.warn(\"Failed to delete file: {}\", file, exc))\n+        .run(deleteFunc::accept);\n+\n+    return orphanFiles;\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  private Dataset<Row> buildValidMetadataFileDF() {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+\n+    List<String> otherMetadataFiles = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        otherMetadataFiles.add(manifestListLocation);\n+      }\n+    }\n+\n+    otherMetadataFiles.add(ops.metadataFileLocation(\"version-hint.text\"));\n+\n+    TableMetadata metadata = ops.current();\n+    otherMetadataFiles.add(metadata.file().location());\n+    for (TableMetadata.MetadataLogEntry previousMetadataFile : metadata.previousFiles()) {\n+      otherMetadataFiles.add(previousMetadataFile.file());\n+    }\n+\n+    Dataset<Row> otherMetadataFileDF = spark\n+        .createDataset(otherMetadataFiles, Encoders.STRING())\n+        .toDF(\"file_path\");\n+\n+    return manifestDF.union(otherMetadataFileDF);\n+  }\n+\n+  private Dataset<Row> buildActualFileDF() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e0c96f353f6abc6804bdf1944e8677c1a62c3191"}, "originalPosition": 154}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxNTMxNDgy", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-391531482", "createdAt": "2020-04-10T16:24:04Z", "commit": {"oid": "6c01b75d0d738bc4b227b3deb5ffb29e73071dc6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjoyNDowNFrOGD_GkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjoyNDowNFrOGD_GkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjgzMjc4NA==", "bodyText": "Nit: we should be able to write this a table.io()::deleteFile", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406832784", "createdAt": "2020-04-10T16:24:04Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action that removes orphan metadata and data files by listing a given location and comparing\n+ * the actual files in that location with data and metadata files referenced by all valid snapshots.\n+ * The location must be accessible for listing via the Hadoop {@link FileSystem}.\n+ * <p>\n+ * By default, this action cleans up the table location returned by {@link Table#location()} and\n+ * removes unreachable files that are older than 3 days using {@link Table#io()}. The behavior can be modified\n+ * by passing a custom location to {@link #location} and a custom timestamp to {@link #olderThan(long)}.\n+ * For example, someone might point this action to the data folder to clean up only orphan data files.\n+ * In addition, there is a way to configure an alternative delete method via {@link #deleteWith(Consumer)}.\n+ * <p>\n+ * <em>Note:</em> It is dangerous to call this action with a short retention interval as it might corrupt\n+ * the state of the table if another operation is writing at the same time.\n+ */\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private long olderThanTimestamp = System.currentTimeMillis() - TimeUnit.DAYS.toMillis(3);\n+  private Consumer<String> deleteFunc = new Consumer<String>() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c01b75d0d738bc4b227b3deb5ffb29e73071dc6"}, "originalPosition": 76}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxNTM1OTQ1", "url": "https://github.com/apache/iceberg/pull/894#pullrequestreview-391535945", "createdAt": "2020-04-10T16:33:19Z", "commit": {"oid": "6c01b75d0d738bc4b227b3deb5ffb29e73071dc6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjozMzoxOVrOGD_VTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjozMzoxOVrOGD_VTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjgzNjU1OA==", "bodyText": "This seems excessive, but not really that dangerous. When listing in executors, the purpose is to exit even if there is a reference cycle in the file system. This would technically do that, but would recurse 2 billion levels so the more likely failure is a stack overflow.\nThat's alright since it's the behavior that was here before, but I think it would be better to set this to 2,000 or something large but reasonable and then throw an exception if there are remaining directories when it returns.", "url": "https://github.com/apache/iceberg/pull/894#discussion_r406836558", "createdAt": "2020-04-10T16:33:19Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RemoveOrphanFilesAction.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Consumer;\n+import java.util.function.Predicate;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HiddenPathFilter;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action that removes orphan metadata and data files by listing a given location and comparing\n+ * the actual files in that location with data and metadata files referenced by all valid snapshots.\n+ * The location must be accessible for listing via the Hadoop {@link FileSystem}.\n+ * <p>\n+ * By default, this action cleans up the table location returned by {@link Table#location()} and\n+ * removes unreachable files that are older than 3 days using {@link Table#io()}. The behavior can be modified\n+ * by passing a custom location to {@link #location} and a custom timestamp to {@link #olderThan(long)}.\n+ * For example, someone might point this action to the data folder to clean up only orphan data files.\n+ * In addition, there is a way to configure an alternative delete method via {@link #deleteWith(Consumer)}.\n+ * <p>\n+ * <em>Note:</em> It is dangerous to call this action with a short retention interval as it might corrupt\n+ * the state of the table if another operation is writing at the same time.\n+ */\n+public class RemoveOrphanFilesAction implements Action<List<String>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RemoveOrphanFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final SerializableConfiguration hadoopConf;\n+  private final int partitionDiscoveryParallelism;\n+  private final Table table;\n+  private final TableOperations ops;\n+\n+  private String location = null;\n+  private long olderThanTimestamp = System.currentTimeMillis() - TimeUnit.DAYS.toMillis(3);\n+  private Consumer<String> deleteFunc = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      table.io().deleteFile(file);\n+    }\n+  };\n+\n+  RemoveOrphanFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.hadoopConf = new SerializableConfiguration(spark.sessionState().newHadoopConf());\n+    this.partitionDiscoveryParallelism = spark.sessionState().conf().parallelPartitionDiscoveryParallelism();\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.location = table.location();\n+  }\n+\n+  /**\n+   * Removes orphan files in the given location.\n+   *\n+   * @param newLocation a location\n+   * @return this for method chaining\n+   */\n+  public RemoveOrphanFilesAction location(String newLocation) {\n+    this.location = newLocation;\n+    return this;\n+  }\n+\n+  /**\n+   * Removes orphan files that are older than the given timestamp.\n+   *\n+   * @param newOlderThanTimestamp a timestamp in milliseconds\n+   * @return this for method chaining\n+   */\n+  public RemoveOrphanFilesAction olderThan(long newOlderThanTimestamp) {\n+    this.olderThanTimestamp = newOlderThanTimestamp;\n+    return this;\n+  }\n+\n+  /**\n+   * Passes an alternative delete implementation that will be used to delete orphan files.\n+   *\n+   * @param newDeleteFunc a delete func\n+   * @return this for method chaining\n+   */\n+  public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public List<String> execute() {\n+    Dataset<Row> validDataFileDF = buildValidDataFileDF();\n+    Dataset<Row> validMetadataFileDF = buildValidMetadataFileDF();\n+    Dataset<Row> validFileDF = validDataFileDF.union(validMetadataFileDF);\n+    Dataset<Row> actualFileDF = buildActualFileDF();\n+\n+    Column joinCond = validFileDF.col(\"file_path\").equalTo(actualFileDF.col(\"file_path\"));\n+    List<String> orphanFiles = actualFileDF.join(validFileDF, joinCond, \"leftanti\")\n+        .as(Encoders.STRING())\n+        .collectAsList();\n+\n+    Tasks.foreach(orphanFiles)\n+        .noRetry()\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.warn(\"Failed to delete file: {}\", file, exc))\n+        .run(deleteFunc::accept);\n+\n+    return orphanFiles;\n+  }\n+\n+  private Dataset<Row> buildValidDataFileDF() {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  private Dataset<Row> buildValidMetadataFileDF() {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+\n+    List<String> otherMetadataFiles = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        otherMetadataFiles.add(manifestListLocation);\n+      }\n+    }\n+\n+    otherMetadataFiles.add(ops.metadataFileLocation(\"version-hint.text\"));\n+\n+    TableMetadata metadata = ops.current();\n+    otherMetadataFiles.add(metadata.file().location());\n+    for (TableMetadata.MetadataLogEntry previousMetadataFile : metadata.previousFiles()) {\n+      otherMetadataFiles.add(previousMetadataFile.file());\n+    }\n+\n+    Dataset<Row> otherMetadataFileDF = spark\n+        .createDataset(otherMetadataFiles, Encoders.STRING())\n+        .toDF(\"file_path\");\n+\n+    return manifestDF.union(otherMetadataFileDF);\n+  }\n+\n+  private Dataset<Row> buildActualFileDF() {\n+    List<String> subDirs = Lists.newArrayList();\n+    List<String> matchingFiles = Lists.newArrayList();\n+\n+    Predicate<FileStatus> predicate = file -> file.getModificationTime() < olderThanTimestamp;\n+\n+    // list at most 3 levels and only dirs that have less than 10 direct sub dirs on the driver\n+    listDirRecursively(location, predicate, hadoopConf.value(), 3, 10, subDirs, matchingFiles);\n+\n+    JavaRDD<String> matchingFileRDD = sparkContext.parallelize(matchingFiles, 1);\n+\n+    if (subDirs.isEmpty()) {\n+      return spark.createDataset(matchingFileRDD.rdd(), Encoders.STRING()).toDF(\"file_path\");\n+    }\n+\n+    int parallelism = Math.min(subDirs.size(), partitionDiscoveryParallelism);\n+    JavaRDD<String> subDirRDD = sparkContext.parallelize(subDirs, parallelism);\n+\n+    Broadcast<SerializableConfiguration> conf = sparkContext.broadcast(hadoopConf);\n+    JavaRDD<String> matchingLeafFileRDD = subDirRDD.mapPartitions(listDirsRecursively(conf, olderThanTimestamp));\n+\n+    JavaRDD<String> completeMatchingFileRDD = matchingFileRDD.union(matchingLeafFileRDD);\n+    return spark.createDataset(completeMatchingFileRDD.rdd(), Encoders.STRING()).toDF(\"file_path\");\n+  }\n+\n+  private static void listDirRecursively(\n+      String dir, Predicate<FileStatus> predicate, Configuration conf, int maxDepth,\n+      int maxDirectSubDirs, List<String> remainingSubDirs, List<String> matchingFiles) {\n+\n+    // stop listing whenever we reach the max depth\n+    if (maxDepth <= 0) {\n+      remainingSubDirs.add(dir);\n+      return;\n+    }\n+\n+    try {\n+      Path path = new Path(dir);\n+      FileSystem fs = path.getFileSystem(conf);\n+\n+      List<String> subDirs = Lists.newArrayList();\n+\n+      for (FileStatus file : fs.listStatus(path, HiddenPathFilter.get())) {\n+        if (file.isDirectory()) {\n+          subDirs.add(file.getPath().toString());\n+        } else if (file.isFile() && predicate.test(file)) {\n+          matchingFiles.add(file.getPath().toString());\n+        }\n+      }\n+\n+      // stop listing if the number of direct sub dirs is bigger than maxDirectSubDirs\n+      if (subDirs.size() > maxDirectSubDirs) {\n+        remainingSubDirs.addAll(subDirs);\n+        return;\n+      }\n+\n+      for (String subDir : subDirs) {\n+        listDirRecursively(subDir, predicate, conf, maxDepth - 1, maxDirectSubDirs, remainingSubDirs, matchingFiles);\n+      }\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e);\n+    }\n+  }\n+\n+  private String metadataTableName(MetadataTableType type) {\n+    String tableName = table.toString();\n+    if (tableName.contains(\"/\")) {\n+      return tableName + \"#\" + type;\n+    } else if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\")) {\n+      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4\n+      return tableName.replaceFirst(\"(hadoop\\\\.)|(hive\\\\.)\", \"\") + \".\" + type;\n+    } else {\n+      return tableName + \".\" + type;\n+    }\n+  }\n+\n+  private static FlatMapFunction<Iterator<String>, String> listDirsRecursively(\n+      Broadcast<SerializableConfiguration> conf,\n+      long olderThanTimestamp) {\n+\n+    return (FlatMapFunction<Iterator<String>, String>) dirs -> {\n+      List<String> subDirs = Lists.newArrayList();\n+      List<String> files = Lists.newArrayList();\n+\n+      Predicate<FileStatus> predicate = file -> file.getModificationTime() < olderThanTimestamp;\n+\n+      int maxDepth = Integer.MAX_VALUE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c01b75d0d738bc4b227b3deb5ffb29e73071dc6"}, "originalPosition": 269}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4765, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}