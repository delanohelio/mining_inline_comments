{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE3OTI0MzM2", "number": 1743, "title": "Spark: Add stored procedure API", "bodyText": "This PR adds the stored procedure API.\nThe actual implementation in Iceberg and tests will come in a separate PR.\nResolves #1593.", "createdAt": "2020-11-09T17:33:12Z", "url": "https://github.com/apache/iceberg/pull/1743", "merged": true, "mergeCommit": {"oid": "d4a479147d3c7f4c061f56e7b9d7857a3b81060c"}, "closed": true, "closedAt": "2020-11-11T18:52:27Z", "author": {"login": "aokolnychyi"}, "timelineItems": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABda4RPhgH2gAyNTE3OTI0MzM2OjVkMjNlNzZjMjU5ZjhkNTAwMzQ3ODYzZTFiNzI5MzhjNzk0ZTAzYjQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdbgjI4AFqTUyODMzMjkxMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/5d23e76c259f8d500347863e1b72938c794e03b4", "committedDate": "2020-11-09T17:32:47Z", "message": "Spark: Add stored procedure API"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2NDkyNjU1", "url": "https://github.com/apache/iceberg/pull/1743#pullrequestreview-526492655", "createdAt": "2020-11-09T17:34:11Z", "commit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxNzozNDoxMlrOHv55uA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxNzozNDoxMlrOHv55uA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTk5Mzc4NA==", "bodyText": "This is ugly. I'd appreciate any ideas.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r519993784", "createdAt": "2020-11-09T17:34:12Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+\n+      validateParams(procedure)\n+      validateMethodHandle(procedure)\n+\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def validateMethodHandle(procedure: Procedure): Unit = {\n+    val params = procedure.parameters\n+    val outputType = procedure.outputType\n+\n+    val methodHandle = procedure.methodHandle\n+    val methodType = methodHandle.`type`\n+    val methodReturnType = methodType.returnType\n+\n+    // method cannot accept var ags\n+    if (methodHandle.isVarargsCollector) {\n+      throw new AnalysisException(\"Method must have fixed arity\")\n+    }\n+\n+    // verify the number of params in the procedure match the number of params in the method\n+    if (params.length != methodType.parameterCount) {\n+      throw new AnalysisException(\"Method parameter count must match the number of procedure parameters\")\n+    }\n+\n+    // the MethodHandle API does not allow us to check the generic type\n+    // so we only verify the return type is either void or iterable\n+\n+    if (outputType.nonEmpty && methodReturnType != classOf[java.lang.Iterable[_]]) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2NDk4NTMy", "url": "https://github.com/apache/iceberg/pull/1743#pullrequestreview-526498532", "createdAt": "2020-11-09T17:41:27Z", "commit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxNzo0MToyN1rOHv6Lqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxNzo0MToyN1rOHv6Lqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTk5ODM3OA==", "bodyText": "Presto uses method handles for stored procedures too.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r519998378", "createdAt": "2020-11-09T17:41:27Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/Procedure.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.catalog;\n+\n+import java.lang.invoke.MethodHandle;\n+import org.apache.spark.sql.types.StructType;\n+\n+public interface Procedure {\n+  ProcedureParameter[] parameters();\n+  StructType outputType();\n+  MethodHandle methodHandle();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 28}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2NTgzOTcz", "url": "https://github.com/apache/iceberg/pull/1743#pullrequestreview-526583973", "createdAt": "2020-11-09T19:26:53Z", "commit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToyNjo1M1rOHv-RXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToyNjo1M1rOHv-RXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NTM3Mg==", "bodyText": "The ignored argument is a SparkSession, which I think should be passed into ResolveProcedures so that it doesn't need to reference SparkSession.active.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520065372", "createdAt": "2020-11-09T19:26:53Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/iceberg/spark/extensions/IcebergSparkSessionExtensions.scala", "diffHunk": "@@ -20,13 +20,15 @@\n package org.apache.iceberg.spark.extensions\n \n import org.apache.spark.sql.SparkSessionExtensions\n+import org.apache.spark.sql.catalyst.analysis.ResolveProcedures\n import org.apache.spark.sql.catalyst.parser.extensions.IcebergSparkSqlExtensionsParser\n import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Strategy\n \n class IcebergSparkSessionExtensions extends (SparkSessionExtensions => Unit) {\n \n   override def apply(extensions: SparkSessionExtensions): Unit = {\n     extensions.injectParser { case (_, parser) => new IcebergSparkSqlExtensionsParser(parser) }\n+    extensions.injectResolutionRule { _ => ResolveProcedures }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2NTg0MjMz", "url": "https://github.com/apache/iceberg/pull/1743#pullrequestreview-526584233", "createdAt": "2020-11-09T19:27:13Z", "commit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToyNzoxM1rOHv-SIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToyNzoxM1rOHv-SIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NTU3MQ==", "bodyText": "The session should be passed in from rule injection.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520065571", "createdAt": "2020-11-09T19:27:13Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 31}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2NTg1MzUx", "url": "https://github.com/apache/iceberg/pull/1743#pullrequestreview-526585351", "createdAt": "2020-11-09T19:28:45Z", "commit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToyODo0NVrOHv-Vdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToyODo0NVrOHv-Vdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NjQyMg==", "bodyText": "Should this be in ExtendedDataSourceV2Strategy? It doesn't seem related to DSv2.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520066422", "createdAt": "2020-11-09T19:28:45Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedDataSourceV2Strategy.scala", "diffHunk": "@@ -20,14 +20,21 @@\n package org.apache.spark.sql.execution.datasources.v2\n \n import org.apache.spark.sql.{AnalysisException, Strategy}\n-import org.apache.spark.sql.catalyst.plans.logical.{CallStatement, LogicalPlan}\n+import org.apache.spark.sql.catalyst.CatalystTypeConverters\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, LogicalPlan}\n import org.apache.spark.sql.execution.SparkPlan\n \n object ExtendedDataSourceV2Strategy extends Strategy {\n \n   override def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {\n-    case _: CallStatement =>\n-      throw new AnalysisException(\"CALL statements are not currently supported\")\n+    case c @ Call(procedure, args) =>\n+      CallExec(c.output, procedure.methodHandle, args.map(toScalaValue)) :: Nil", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 16}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2NjY5MDM5", "url": "https://github.com/apache/iceberg/pull/1743#pullrequestreview-526669039", "createdAt": "2020-11-09T21:29:11Z", "commit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMToyOToxMVrOHwCVHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMToyOToxMVrOHwCVHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDEzMTg2OA==", "bodyText": "Does this need to be a struct?", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520131868", "createdAt": "2020-11-09T21:29:11Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/Procedure.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.catalog;\n+\n+import java.lang.invoke.MethodHandle;\n+import org.apache.spark.sql.types.StructType;\n+\n+public interface Procedure {\n+  ProcedureParameter[] parameters();\n+  StructType outputType();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 27}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2NzA4MzA1", "url": "https://github.com/apache/iceberg/pull/1743#pullrequestreview-526708305", "createdAt": "2020-11-09T22:31:48Z", "commit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjozMTo0OFrOHwEPBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjozMTo0OFrOHwEPBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MzA3Ng==", "bodyText": "This is going to force people to use value literals, like 12L. Can we insert a cast and execute it if the type is compatible and the cast is safe? I'm thinking it would be fine for int -> long, but not for string -> int.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520163076", "createdAt": "2020-11-09T22:31:48Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+\n+      validateParams(procedure)\n+      validateMethodHandle(procedure)\n+\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def validateMethodHandle(procedure: Procedure): Unit = {\n+    val params = procedure.parameters\n+    val outputType = procedure.outputType\n+\n+    val methodHandle = procedure.methodHandle\n+    val methodType = methodHandle.`type`\n+    val methodReturnType = methodType.returnType\n+\n+    // method cannot accept var ags\n+    if (methodHandle.isVarargsCollector) {\n+      throw new AnalysisException(\"Method must have fixed arity\")\n+    }\n+\n+    // verify the number of params in the procedure match the number of params in the method\n+    if (params.length != methodType.parameterCount) {\n+      throw new AnalysisException(\"Method parameter count must match the number of procedure parameters\")\n+    }\n+\n+    // the MethodHandle API does not allow us to check the generic type\n+    // so we only verify the return type is either void or iterable\n+\n+    if (outputType.nonEmpty && methodReturnType != classOf[java.lang.Iterable[_]]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines $outputType \" +\n+        \"as its output so must return java.lang.Iterable of Spark Rows\")\n+    }\n+\n+    if (outputType.isEmpty && methodReturnType != classOf[Void]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines no output columns \" +\n+        \"so must be void\")\n+    }\n+  }\n+\n+  private def buildArgExprs(procedure: Procedure, args: Seq[CallArgument]): Seq[Expression] = {\n+    val params = procedure.parameters\n+\n+    // build a map of declared parameter names to their positions\n+    val nameToPositionMap = params.map(_.name).zipWithIndex.toMap\n+\n+    // build a map of parameter names to args\n+    val nameToArgMap = buildNameToArgMap(params, args, nameToPositionMap)\n+\n+    // verify all required parameters are provided\n+    val missingParamNames = params.filter(_.required).collect {\n+      case param if !nameToArgMap.contains(param.name) => param.name\n+    }\n+\n+    if (missingParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Missing required parameters: ${missingParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    val argExprs = new Array[Expression](params.size)\n+\n+    nameToArgMap.foreach { case (name, arg) =>\n+      val position = nameToPositionMap(name)\n+      val param = params(position)\n+      val paramType = param.dataType\n+      val argType = arg.expr.dataType\n+      if (paramType != argType) {\n+        throw new AnalysisException(s\"Wrong arg type for ${param.name}: expected $paramType but got $argType\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 121}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI2NzA5Mjc5", "url": "https://github.com/apache/iceberg/pull/1743#pullrequestreview-526709279", "createdAt": "2020-11-09T22:33:33Z", "commit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjozMzozM1rOHwER6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjozMzozM1rOHwER6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MzgxOQ==", "bodyText": "This is Presto's current behavior?", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520163819", "createdAt": "2020-11-09T22:33:33Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+\n+      validateParams(procedure)\n+      validateMethodHandle(procedure)\n+\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def validateMethodHandle(procedure: Procedure): Unit = {\n+    val params = procedure.parameters\n+    val outputType = procedure.outputType\n+\n+    val methodHandle = procedure.methodHandle\n+    val methodType = methodHandle.`type`\n+    val methodReturnType = methodType.returnType\n+\n+    // method cannot accept var ags\n+    if (methodHandle.isVarargsCollector) {\n+      throw new AnalysisException(\"Method must have fixed arity\")\n+    }\n+\n+    // verify the number of params in the procedure match the number of params in the method\n+    if (params.length != methodType.parameterCount) {\n+      throw new AnalysisException(\"Method parameter count must match the number of procedure parameters\")\n+    }\n+\n+    // the MethodHandle API does not allow us to check the generic type\n+    // so we only verify the return type is either void or iterable\n+\n+    if (outputType.nonEmpty && methodReturnType != classOf[java.lang.Iterable[_]]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines $outputType \" +\n+        \"as its output so must return java.lang.Iterable of Spark Rows\")\n+    }\n+\n+    if (outputType.isEmpty && methodReturnType != classOf[Void]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines no output columns \" +\n+        \"so must be void\")\n+    }\n+  }\n+\n+  private def buildArgExprs(procedure: Procedure, args: Seq[CallArgument]): Seq[Expression] = {\n+    val params = procedure.parameters\n+\n+    // build a map of declared parameter names to their positions\n+    val nameToPositionMap = params.map(_.name).zipWithIndex.toMap\n+\n+    // build a map of parameter names to args\n+    val nameToArgMap = buildNameToArgMap(params, args, nameToPositionMap)\n+\n+    // verify all required parameters are provided\n+    val missingParamNames = params.filter(_.required).collect {\n+      case param if !nameToArgMap.contains(param.name) => param.name\n+    }\n+\n+    if (missingParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Missing required parameters: ${missingParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    val argExprs = new Array[Expression](params.size)\n+\n+    nameToArgMap.foreach { case (name, arg) =>\n+      val position = nameToPositionMap(name)\n+      val param = params(position)\n+      val paramType = param.dataType\n+      val argType = arg.expr.dataType\n+      if (paramType != argType) {\n+        throw new AnalysisException(s\"Wrong arg type for ${param.name}: expected $paramType but got $argType\")\n+      }\n+      argExprs(position) = arg.expr\n+    }\n+\n+    // assign nulls to optional params that were not set\n+    params.foreach {\n+      case p if !p.required && !nameToArgMap.contains(p.name) =>\n+        val position = nameToPositionMap(p.name)\n+        argExprs(position) = Literal.create(null, p.dataType)\n+      case _ =>\n+    }\n+\n+    argExprs\n+  }\n+\n+  private def buildNameToArgMap(\n+      params: Seq[ProcedureParameter],\n+      args: Seq[CallArgument],\n+      nameToPositionMap: Map[String, Int]): Map[String, CallArgument] = {\n+\n+    val containsNamedArg = args.exists(_.isInstanceOf[NamedArgument])\n+    val containsPositionalArg = args.exists(_.isInstanceOf[PositionalArgument])\n+\n+    if (containsNamedArg && containsPositionalArg) {\n+      throw new AnalysisException(\"Named and positional arguments cannot be mixed\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 146}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/588a116401751f3d59ed46aae74a38aa54947367", "committedDate": "2020-11-11T00:02:13Z", "message": "Get rid of MethodHandle & minor fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI3NzA0ODQx", "url": "https://github.com/apache/iceberg/pull/1743#pullrequestreview-527704841", "createdAt": "2020-11-11T00:03:30Z", "commit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDowMzozMFrOHw0ZNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDowMzozMFrOHw0ZNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1MjExNg==", "bodyText": "We've discussed how handy this rule is multiple times. I'd remove it if there are no objections.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520952116", "createdAt": "2020-11-11T00:03:30Z", "author": {"login": "aokolnychyi"}, "path": "project/scalastyle_config.xml", "diffHunk": "@@ -85,11 +85,6 @@\n             <parameter name=\"maxTypes\"><![CDATA[30]]></parameter>\n         </parameters>\n     </check>\n-    <check level=\"error\" class=\"org.scalastyle.scalariform.CyclomaticComplexityChecker\" enabled=\"true\">", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI3NzA2MjY5", "url": "https://github.com/apache/iceberg/pull/1743#pullrequestreview-527706269", "createdAt": "2020-11-11T00:07:03Z", "commit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDowNzowM1rOHw0dww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDowNzowM1rOHw0dww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1MzI4Mw==", "bodyText": "@rdblue, I do not pass a type alongside the row since the rules rely on parameters reported by the procedure to align input expressions.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520953283", "createdAt": "2020-11-11T00:07:03Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/CallExec.scala", "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.connector.catalog.Procedure\n+\n+case class CallExec(\n+    output: Seq[Attribute],\n+    procedure: Procedure,\n+    input: InternalRow) extends V2CommandExec {\n+\n+  override protected def run(): Seq[InternalRow] = {\n+    procedure.call(input)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI3NzU0MjU0", "url": "https://github.com/apache/iceberg/pull/1743#pullrequestreview-527754254", "createdAt": "2020-11-11T00:46:06Z", "commit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo0NjowNlrOHw1QLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo0NjowNlrOHw1QLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2NjE4OQ==", "bodyText": "We know from validation above that if the map doesn't contain an argument, that it must be required. Not a big deal to have the additional check, though.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520966189", "createdAt": "2020-11-11T00:46:06Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Cast, Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+case class ResolveProcedures(spark: SparkSession) extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = spark.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+      validateParams(procedure)\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def buildArgExprs(procedure: Procedure, args: Seq[CallArgument]): Seq[Expression] = {\n+    val params = procedure.parameters\n+\n+    // build a map of declared parameter names to their positions\n+    val nameToPositionMap = params.map(_.name).zipWithIndex.toMap\n+\n+    // build a map of parameter names to args\n+    val nameToArgMap = buildNameToArgMap(params, args, nameToPositionMap)\n+\n+    // verify all required parameters are provided\n+    val missingParamNames = params.filter(_.required).collect {\n+      case param if !nameToArgMap.contains(param.name) => param.name\n+    }\n+\n+    if (missingParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Missing required parameters: ${missingParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    val argExprs = new Array[Expression](params.size)\n+\n+    nameToArgMap.foreach { case (name, arg) =>\n+      val position = nameToPositionMap(name)\n+      val param = params(position)\n+      val paramType = param.dataType\n+      val argType = arg.expr.dataType\n+\n+      if (paramType != argType && !Cast.canUpCast(argType, paramType)) {\n+        throw new AnalysisException(s\"Wrong arg type for ${param.name}: expected $paramType but got $argType\")\n+      }\n+\n+      if (paramType != argType) {\n+        argExprs(position) = Cast(arg.expr, paramType)\n+      } else {\n+        argExprs(position) = arg.expr\n+      }\n+    }\n+\n+    // assign nulls to optional params that were not set\n+    params.foreach {\n+      case p if !p.required && !nameToArgMap.contains(p.name) =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "originalPosition": 97}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI3NzU3NjM1", "url": "https://github.com/apache/iceberg/pull/1743#pullrequestreview-527757635", "createdAt": "2020-11-11T00:47:50Z", "commit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo0Nzo1MFrOHw1UHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo0Nzo1MFrOHw1UHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2NzE5Nw==", "bodyText": "I think this needs to be a lazy val, or else the attributes will have different IDs every time it is called.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520967197", "createdAt": "2020-11-11T00:47:50Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/Call.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Expression}\n+import org.apache.spark.sql.connector.catalog.Procedure\n+import scala.collection.Seq\n+\n+case class Call(procedure: Procedure, args: Seq[Expression]) extends Command {\n+  override def output: Seq[Attribute] = procedure.outputType.toAttributes", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "originalPosition": 27}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI3NzU5OTc4", "url": "https://github.com/apache/iceberg/pull/1743#pullrequestreview-527759978", "createdAt": "2020-11-11T00:49:19Z", "commit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo0OToxOVrOHw1YQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo0OToxOVrOHw1YQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2ODI1Nw==", "bodyText": "I think for all of the connector interfaces, we should add clear Javadoc.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520968257", "createdAt": "2020-11-11T00:49:19Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/ProcedureParameter.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.catalog;\n+\n+import org.apache.spark.sql.types.DataType;\n+\n+public interface ProcedureParameter {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "originalPosition": 24}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI3NzYzMTYw", "url": "https://github.com/apache/iceberg/pull/1743#pullrequestreview-527763160", "createdAt": "2020-11-11T00:51:17Z", "commit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI3NzY2MDE3", "url": "https://github.com/apache/iceberg/pull/1743#pullrequestreview-527766017", "createdAt": "2020-11-11T00:53:11Z", "commit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo1MzoxMVrOHw1ekw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo1MzoxMVrOHw1ekw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2OTg3NQ==", "bodyText": "One more thing: we should also add a describe method so that we can show these correctly in plans.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520969875", "createdAt": "2020-11-11T00:53:11Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/Procedure.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.catalog;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.StructType;\n+\n+public interface Procedure {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "originalPosition": 25}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b6ade274936b6fb810d3999a8c95efb83cba6520", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/b6ade274936b6fb810d3999a8c95efb83cba6520", "committedDate": "2020-11-11T16:25:16Z", "message": "Lazy val & description"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI4MzMyOTEy", "url": "https://github.com/apache/iceberg/pull/1743#pullrequestreview-528332912", "createdAt": "2020-11-11T16:28:31Z", "commit": {"oid": "b6ade274936b6fb810d3999a8c95efb83cba6520"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxNjoyODozMVrOHxUk9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxNjoyODozMVrOHxUk9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQ3OTQxNQ==", "bodyText": "@rdblue, I implemented simpleString in Call and CallExec that use description in Procedure.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r521479415", "createdAt": "2020-11-11T16:28:31Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/Call.scala", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Expression}\n+import org.apache.spark.sql.catalyst.util.truncatedString\n+import org.apache.spark.sql.connector.catalog.Procedure\n+import scala.collection.Seq\n+\n+case class Call(procedure: Procedure, args: Seq[Expression]) extends Command {\n+  override lazy val output: Seq[Attribute] = procedure.outputType.toAttributes\n+\n+  override def simpleString(maxFields: Int): String = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b6ade274936b6fb810d3999a8c95efb83cba6520"}, "originalPosition": 30}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3720, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}