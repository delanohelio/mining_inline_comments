{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQxMDI5NzQ0", "number": 1939, "title": "Flink: Commit both data files and delete files to iceberg transaction.", "bodyText": "", "createdAt": "2020-12-16T09:44:50Z", "url": "https://github.com/apache/iceberg/pull/1939", "merged": true, "mergeCommit": {"oid": "77c5617c102c2ab27fbf35cac3fd75380a887d5d"}, "closed": true, "closedAt": "2020-12-18T17:57:05Z", "author": {"login": "openinx"}, "timelineItems": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdmru5dAH2gAyNTQxMDI5NzQ0OjE0YzAyOGJhMWNiZTFlYTdlM2M0Yjc5MzVmYTAwNTY2YThlMTNkNWQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdnaeXjgFqTU1NTYyMTA2NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "14c028ba1cbe1ea7e3c4b7935fa00566a8e13d5d", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/14c028ba1cbe1ea7e3c4b7935fa00566a8e13d5d", "committedDate": "2020-12-16T09:43:30Z", "message": "Flink: Commit both data files and delete files to iceberg transaction."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b5d2b09be04560167ccf2c5dd9cc10f3ba0d05a2", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/b5d2b09be04560167ccf2c5dd9cc10f3ba0d05a2", "committedDate": "2020-12-16T14:28:23Z", "message": "Minor changes."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUzNzQxMDIx", "url": "https://github.com/apache/iceberg/pull/1939#pullrequestreview-553741021", "createdAt": "2020-12-16T14:30:48Z", "commit": {"oid": "b5d2b09be04560167ccf2c5dd9cc10f3ba0d05a2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxNDozMDo0OFrOIHIFsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNlQxNDozMDo0OFrOIHIFsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDM0MzQ3Mg==", "bodyText": "I will provide an unit test to address it.", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r544343472", "createdAt": "2020-12-16T14:30:48Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -229,33 +232,71 @@ private void commitUpToCheckpoint(NavigableMap<Long, byte[]> manifestsMap,\n     }\n   }\n \n-  private void replacePartitions(List<DataFile> dataFiles, String newFlinkJobId, long checkpointId) {\n+  private void replacePartitions(NavigableMap<Long, WriteResult> pendingResults, String newFlinkJobId,\n+                                 long checkpointId) {\n+    // Merge all the pending results into a single write result.\n+    WriteResult result = WriteResult.builder().add(pendingResults.values()).build();\n+\n+    // Partition overwrite does not support delete files.\n+    Preconditions.checkArgument(result.deleteFiles().length == 0,\n+        \"Cannot overwrite partitions with delete files.\");\n     ReplacePartitions dynamicOverwrite = table.newReplacePartitions();\n \n+    // Commit the overwrite transaction.\n     int numFiles = 0;\n-    for (DataFile file : dataFiles) {\n+    for (DataFile file : result.dataFiles()) {\n       numFiles += 1;\n       dynamicOverwrite.addFile(file);\n     }\n \n-    commitOperation(dynamicOverwrite, numFiles, \"dynamic partition overwrite\", newFlinkJobId, checkpointId);\n+    commitOperation(dynamicOverwrite, numFiles, 0, \"dynamic partition overwrite\", newFlinkJobId, checkpointId);\n   }\n \n-  private void append(List<DataFile> dataFiles, String newFlinkJobId, long checkpointId) {\n-    AppendFiles appendFiles = table.newAppend();\n+  private void commitDeltaTxn(NavigableMap<Long, WriteResult> pendingResults, String newFlinkJobId, long checkpointId) {\n+    // Merge all pending results into a single write result.\n+    WriteResult mergedResult = WriteResult.builder().add(pendingResults.values()).build();\n \n-    int numFiles = 0;\n-    for (DataFile file : dataFiles) {\n-      numFiles += 1;\n-      appendFiles.appendFile(file);\n-    }\n+    if (mergedResult.deleteFiles().length < 1) {\n+      // To be compatible with iceberg format V1.\n+      AppendFiles appendFiles = table.newAppend();\n+\n+      int numFiles = 0;\n+      for (DataFile file : mergedResult.dataFiles()) {\n+        numFiles += 1;\n+        appendFiles.appendFile(file);\n+      }\n+\n+      commitOperation(appendFiles, numFiles, 0, \"append\", newFlinkJobId, checkpointId);\n+    } else {\n+      // To be compatible with iceberg format V2.\n+      for (Map.Entry<Long, WriteResult> e : pendingResults.entrySet()) {\n+        // We don't commit the merged result into a single transaction because for the sequential transaction txn1 and", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5d2b09be04560167ccf2c5dd9cc10f3ba0d05a2"}, "originalPosition": 155}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0MjE5ODgz", "url": "https://github.com/apache/iceberg/pull/1939#pullrequestreview-554219883", "createdAt": "2020-12-17T01:44:11Z", "commit": {"oid": "b5d2b09be04560167ccf2c5dd9cc10f3ba0d05a2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwMTo0NDoxMVrOIHg18g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwMTo0NDoxMVrOIHg18g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0OTA0Mg==", "bodyText": "Is this used?", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r544749042", "createdAt": "2020-12-17T01:44:11Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/DeltaManifests.java", "diffHunk": "@@ -19,30 +19,42 @@\n \n package org.apache.iceberg.flink.sink;\n \n-import java.io.IOException;\n-import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import java.util.Iterator;\n+import java.util.List;\n import org.apache.iceberg.ManifestFile;\n-import org.apache.iceberg.ManifestFiles;\n-import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.jetbrains.annotations.NotNull;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5d2b09be04560167ccf2c5dd9cc10f3ba0d05a2"}, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0MjIwNDQ2", "url": "https://github.com/apache/iceberg/pull/1939#pullrequestreview-554220446", "createdAt": "2020-12-17T01:45:52Z", "commit": {"oid": "b5d2b09be04560167ccf2c5dd9cc10f3ba0d05a2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwMTo0NTo1MlrOIHg4RA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwMTo0NTo1MlrOIHg4RA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0OTYzNg==", "bodyText": "Typically, we would follow the Java collection convention and use addAll.", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r544749636", "createdAt": "2020-12-17T01:45:52Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/io/WriteResult.java", "diffHunk": "@@ -76,6 +76,11 @@ public Builder add(WriteResult result) {\n       return this;\n     }\n \n+    public Builder add(Iterable<WriteResult> results) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5d2b09be04560167ccf2c5dd9cc10f3ba0d05a2"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0MjIyNDk4", "url": "https://github.com/apache/iceberg/pull/1939#pullrequestreview-554222498", "createdAt": "2020-12-17T01:52:05Z", "commit": {"oid": "b5d2b09be04560167ccf2c5dd9cc10f3ba0d05a2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwMTo1MjowNVrOIHhAxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwMTo1MjowNVrOIHhAxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc1MTgxMw==", "bodyText": "Is it correct for this to be a list of write results if a write result keeps track of a list of data files and a list of delete files?", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r544751813", "createdAt": "2020-12-17T01:52:05Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -85,9 +88,9 @@\n   // iceberg table when the next checkpoint happen.\n   private final NavigableMap<Long, byte[]> dataFilesPerCheckpoint = Maps.newTreeMap();\n \n-  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // The completed files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n   // 'dataFilesPerCheckpoint'.\n-  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+  private final List<WriteResult> writeResultsOfCurrentCkpt = Lists.newArrayList();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5d2b09be04560167ccf2c5dd9cc10f3ba0d05a2"}, "originalPosition": 38}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c00716509e57423ba3e7cf21212a0f674004a2c7", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/c00716509e57423ba3e7cf21212a0f674004a2c7", "committedDate": "2020-12-17T07:13:42Z", "message": "Addressing the comments."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0MzU3NzUz", "url": "https://github.com/apache/iceberg/pull/1939#pullrequestreview-554357753", "createdAt": "2020-12-17T08:04:55Z", "commit": {"oid": "c00716509e57423ba3e7cf21212a0f674004a2c7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwODowNDo1NVrOIHpLmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwODowNDo1NVrOIHpLmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg4NTY1OA==", "bodyText": "We will need to maintain the flink state's compatibility.  If the encoding version is 1, then we should use the FlinkManifestSerializer way to read the byte[].", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r544885658", "createdAt": "2020-12-17T08:04:55Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -184,36 +187,36 @@ public void notifyCheckpointComplete(long checkpointId) throws Exception {\n     }\n   }\n \n-  private void commitUpToCheckpoint(NavigableMap<Long, byte[]> manifestsMap,\n+  private void commitUpToCheckpoint(NavigableMap<Long, byte[]> deltaManifestsMap,\n                                     String newFlinkJobId,\n                                     long checkpointId) throws IOException {\n-    NavigableMap<Long, byte[]> pendingManifestMap = manifestsMap.headMap(checkpointId, true);\n+    NavigableMap<Long, byte[]> pendingMap = deltaManifestsMap.headMap(checkpointId, true);\n \n-    List<ManifestFile> manifestFiles = Lists.newArrayList();\n-    List<DataFile> pendingDataFiles = Lists.newArrayList();\n-    for (byte[] manifestData : pendingManifestMap.values()) {\n-      if (Arrays.equals(EMPTY_MANIFEST_DATA, manifestData)) {\n+    List<DeltaManifests> deltaManifestsList = Lists.newArrayList();\n+    NavigableMap<Long, WriteResult> pendingResults = Maps.newTreeMap();\n+    for (Map.Entry<Long, byte[]> e : pendingMap.entrySet()) {\n+      if (Arrays.equals(EMPTY_MANIFEST_DATA, e.getValue())) {\n         // Skip the empty flink manifest.\n         continue;\n       }\n \n-      ManifestFile manifestFile =\n-          SimpleVersionedSerialization.readVersionAndDeSerialize(FlinkManifestSerializer.INSTANCE, manifestData);\n+      DeltaManifests deltaManifests =\n+          SimpleVersionedSerialization.readVersionAndDeSerialize(DeltaManifestsSerializer.INSTANCE, e.getValue());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c00716509e57423ba3e7cf21212a0f674004a2c7"}, "originalPosition": 77}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1eae61a63938e85eb40eee9c27f39889b9bf2e07", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/1eae61a63938e85eb40eee9c27f39889b9bf2e07", "committedDate": "2020-12-17T09:15:18Z", "message": "Add unit tests to address the state compatibility issues."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0fdec6764e2b8d4962bc027512895581ba31b89f", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/0fdec6764e2b8d4962bc027512895581ba31b89f", "committedDate": "2020-12-17T09:43:33Z", "message": "Minor changes."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4e769b2c522c8ab7df7bf10eb267c758361e0a01", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/4e769b2c522c8ab7df7bf10eb267c758361e0a01", "committedDate": "2020-12-17T11:45:17Z", "message": "Add unit tests: addressing the case that commit two failure checkpoints in the lastest sucessful checkpoint."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0ODIxNTA3", "url": "https://github.com/apache/iceberg/pull/1939#pullrequestreview-554821507", "createdAt": "2020-12-17T17:23:20Z", "commit": {"oid": "4e769b2c522c8ab7df7bf10eb267c758361e0a01"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzoyMzoyMVrOIIAVeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzoyMzoyMVrOIIAVeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI2NTAxNg==", "bodyText": "does this need to extend from Iterable? It seems only needed for using Iterables.addAll(manifests, deltaManifests);. is it simpler to directly to cal the two getters?", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545265016", "createdAt": "2020-12-17T17:23:21Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/DeltaManifests.java", "diffHunk": "@@ -19,30 +19,40 @@\n \n package org.apache.iceberg.flink.sink;\n \n-import java.io.IOException;\n-import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import java.util.Iterator;\n+import java.util.List;\n import org.apache.iceberg.ManifestFile;\n-import org.apache.iceberg.ManifestFiles;\n-import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n \n-class FlinkManifestSerializer implements SimpleVersionedSerializer<ManifestFile> {\n-  private static final int VERSION_NUM = 1;\n-  static final FlinkManifestSerializer INSTANCE = new FlinkManifestSerializer();\n+class DeltaManifests implements Iterable<ManifestFile> {\n \n-  @Override\n-  public int getVersion() {\n-    return VERSION_NUM;\n+  private final ManifestFile dataManifest;\n+  private final ManifestFile deleteManifest;\n+\n+  DeltaManifests(ManifestFile dataManifest, ManifestFile deleteManifest) {\n+    this.dataManifest = dataManifest;\n+    this.deleteManifest = deleteManifest;\n   }\n \n-  @Override\n-  public byte[] serialize(ManifestFile manifestFile) throws IOException {\n-    Preconditions.checkNotNull(manifestFile, \"ManifestFile to be serialized should not be null\");\n+  ManifestFile dataManifest() {\n+    return dataManifest;\n+  }\n \n-    return ManifestFiles.encode(manifestFile);\n+  ManifestFile deleteManifest() {\n+    return deleteManifest;\n   }\n \n   @Override\n-  public ManifestFile deserialize(int version, byte[] serialized) throws IOException {\n-    return ManifestFiles.decode(serialized);\n+  public Iterator<ManifestFile> iterator() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e769b2c522c8ab7df7bf10eb267c758361e0a01"}, "originalPosition": 44}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0ODI3MjY1", "url": "https://github.com/apache/iceberg/pull/1939#pullrequestreview-554827265", "createdAt": "2020-12-17T17:29:37Z", "commit": {"oid": "4e769b2c522c8ab7df7bf10eb267c758361e0a01"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzoyOTozN1rOIIAmyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzoyOTozN1rOIIAmyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI2OTQ0OA==", "bodyText": "do we need to check if WriteResult is empty (no data and delete files)?", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545269448", "createdAt": "2020-12-17T17:29:37Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifestUtil.java", "diffHunk": "@@ -63,4 +66,53 @@ static ManifestOutputFileFactory createOutputFileFactory(Table table, String fli\n     TableOperations ops = ((HasTableOperations) table).operations();\n     return new ManifestOutputFileFactory(ops, table.io(), table.properties(), flinkJobId, subTaskId, attemptNumber);\n   }\n+\n+  static DeltaManifests writeCompletedFiles(WriteResult result,\n+                                            Supplier<OutputFile> outputFileSupplier,\n+                                            PartitionSpec spec) throws IOException {\n+\n+    ManifestFile dataManifest = null;\n+    ManifestFile deleteManifest = null;\n+\n+    // Write the completed data files into a newly created data manifest file.\n+    if (result.dataFiles() != null && result.dataFiles().length > 0) {\n+      dataManifest = writeDataFiles(outputFileSupplier.get(), spec, Lists.newArrayList(result.dataFiles()));\n+    }\n+\n+    // Write the completed delete files into a newly created delete manifest file.\n+    if (result.deleteFiles() != null && result.deleteFiles().length > 0) {\n+      OutputFile deleteManifestFile = outputFileSupplier.get();\n+\n+      ManifestWriter<DeleteFile> deleteManifestWriter = ManifestFiles.writeDeleteManifest(FORMAT_V2, spec,\n+          deleteManifestFile, DUMMY_SNAPSHOT_ID);\n+      try (ManifestWriter<DeleteFile> writer = deleteManifestWriter) {\n+        for (DeleteFile deleteFile : result.deleteFiles()) {\n+          writer.add(deleteFile);\n+        }\n+      }\n+\n+      deleteManifest = deleteManifestWriter.toManifestFile();\n+    }\n+\n+    return new DeltaManifests(dataManifest, deleteManifest);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e769b2c522c8ab7df7bf10eb267c758361e0a01"}, "originalPosition": 64}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU0ODMwNTQ3", "url": "https://github.com/apache/iceberg/pull/1939#pullrequestreview-554830547", "createdAt": "2020-12-17T17:33:01Z", "commit": {"oid": "4e769b2c522c8ab7df7bf10eb267c758361e0a01"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzozMzowMVrOIIAwBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QxNzozMzowMVrOIIAwBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3MTgxMg==", "bodyText": "just for my own education, referencedDataFiles from WriteResult doesn't seem to be used (except for unit test). What is it for? do we need to serialize it too?", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545271812", "createdAt": "2020-12-17T17:33:01Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifestUtil.java", "diffHunk": "@@ -63,4 +66,53 @@ static ManifestOutputFileFactory createOutputFileFactory(Table table, String fli\n     TableOperations ops = ((HasTableOperations) table).operations();\n     return new ManifestOutputFileFactory(ops, table.io(), table.properties(), flinkJobId, subTaskId, attemptNumber);\n   }\n+\n+  static DeltaManifests writeCompletedFiles(WriteResult result,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e769b2c522c8ab7df7bf10eb267c758361e0a01"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1MDY4MTY0", "url": "https://github.com/apache/iceberg/pull/1939#pullrequestreview-555068164", "createdAt": "2020-12-17T23:25:53Z", "commit": {"oid": "4e769b2c522c8ab7df7bf10eb267c758361e0a01"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMzoyNTo1NFrOIIM7fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMzoyNTo1NFrOIIM7fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ3MTM1OQ==", "bodyText": "We are using this API from AppendFiles interface. When we had an extended outage and accumulated a few hundreds of transactions/manifests in Flink checkpoint, this help avoiding rewrite of those manifest files. Otherwise, commit can take very long. @rdblue can probably explain it better than I do.\n  AppendFiles appendManifest(ManifestFile file);\n\nhere we are merging data files potentially from multiple checkpoint cycles/manifests into a single manifest file. Maybe we can add a similar API in DeleteFiles interface?\n  DeleteFiles deleteManifest(ManifestFile file);", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545471359", "createdAt": "2020-12-17T23:25:54Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -184,78 +185,106 @@ public void notifyCheckpointComplete(long checkpointId) throws Exception {\n     }\n   }\n \n-  private void commitUpToCheckpoint(NavigableMap<Long, byte[]> manifestsMap,\n+  private void commitUpToCheckpoint(NavigableMap<Long, byte[]> deltaManifestsMap,\n                                     String newFlinkJobId,\n                                     long checkpointId) throws IOException {\n-    NavigableMap<Long, byte[]> pendingManifestMap = manifestsMap.headMap(checkpointId, true);\n+    NavigableMap<Long, byte[]> pendingMap = deltaManifestsMap.headMap(checkpointId, true);\n \n-    List<ManifestFile> manifestFiles = Lists.newArrayList();\n-    List<DataFile> pendingDataFiles = Lists.newArrayList();\n-    for (byte[] manifestData : pendingManifestMap.values()) {\n-      if (Arrays.equals(EMPTY_MANIFEST_DATA, manifestData)) {\n+    List<ManifestFile> manifests = Lists.newArrayList();\n+    NavigableMap<Long, WriteResult> pendingResults = Maps.newTreeMap();\n+    for (Map.Entry<Long, byte[]> e : pendingMap.entrySet()) {\n+      if (Arrays.equals(EMPTY_MANIFEST_DATA, e.getValue())) {\n         // Skip the empty flink manifest.\n         continue;\n       }\n \n-      ManifestFile manifestFile =\n-          SimpleVersionedSerialization.readVersionAndDeSerialize(FlinkManifestSerializer.INSTANCE, manifestData);\n-\n-      manifestFiles.add(manifestFile);\n-      pendingDataFiles.addAll(FlinkManifestUtil.readDataFiles(manifestFile, table.io()));\n+      DeltaManifests deltaManifests = SimpleVersionedSerialization\n+          .readVersionAndDeSerialize(DeltaManifestsSerializer.INSTANCE, e.getValue());\n+      pendingResults.put(e.getKey(), FlinkManifestUtil.readCompletedFiles(deltaManifests, table.io()));\n+      Iterables.addAll(manifests, deltaManifests);\n     }\n \n     if (replacePartitions) {\n-      replacePartitions(pendingDataFiles, newFlinkJobId, checkpointId);\n+      replacePartitions(pendingResults, newFlinkJobId, checkpointId);\n     } else {\n-      append(pendingDataFiles, newFlinkJobId, checkpointId);\n+      commitDeltaTxn(pendingResults, newFlinkJobId, checkpointId);\n     }\n \n-    pendingManifestMap.clear();\n+    pendingMap.clear();\n \n-    // Delete the committed manifests and clear the committed data files from dataFilesPerCheckpoint.\n-    for (ManifestFile manifestFile : manifestFiles) {\n+    // Delete the committed manifests.\n+    for (ManifestFile manifest : manifests) {\n       try {\n-        table.io().deleteFile(manifestFile.path());\n+        table.io().deleteFile(manifest.path());\n       } catch (Exception e) {\n         // The flink manifests cleaning failure shouldn't abort the completed checkpoint.\n         String details = MoreObjects.toStringHelper(this)\n             .add(\"flinkJobId\", newFlinkJobId)\n             .add(\"checkpointId\", checkpointId)\n-            .add(\"manifestPath\", manifestFile.path())\n+            .add(\"manifestPath\", manifest.path())\n             .toString();\n         LOG.warn(\"The iceberg transaction has been committed, but we failed to clean the temporary flink manifests: {}\",\n             details, e);\n       }\n     }\n   }\n \n-  private void replacePartitions(List<DataFile> dataFiles, String newFlinkJobId, long checkpointId) {\n+  private void replacePartitions(NavigableMap<Long, WriteResult> pendingResults, String newFlinkJobId,\n+                                 long checkpointId) {\n+    // Partition overwrite does not support delete files.\n+    int deleteFilesNum = pendingResults.values().stream().mapToInt(r -> r.deleteFiles().length).sum();\n+    Preconditions.checkState(deleteFilesNum == 0, \"Cannot overwrite partitions with delete files.\");\n+\n+    // Commit the overwrite transaction.\n     ReplacePartitions dynamicOverwrite = table.newReplacePartitions();\n \n     int numFiles = 0;\n-    for (DataFile file : dataFiles) {\n-      numFiles += 1;\n-      dynamicOverwrite.addFile(file);\n+    for (WriteResult result : pendingResults.values()) {\n+      numFiles += result.dataFiles().length;\n+      Arrays.stream(result.dataFiles()).forEach(dynamicOverwrite::addFile);\n     }\n \n-    commitOperation(dynamicOverwrite, numFiles, \"dynamic partition overwrite\", newFlinkJobId, checkpointId);\n+    commitOperation(dynamicOverwrite, numFiles, 0, \"dynamic partition overwrite\", newFlinkJobId, checkpointId);\n   }\n \n-  private void append(List<DataFile> dataFiles, String newFlinkJobId, long checkpointId) {\n-    AppendFiles appendFiles = table.newAppend();\n+  private void commitDeltaTxn(NavigableMap<Long, WriteResult> pendingResults, String newFlinkJobId, long checkpointId) {\n+    int deleteFilesNum = pendingResults.values().stream().mapToInt(r -> r.deleteFiles().length).sum();\n \n-    int numFiles = 0;\n-    for (DataFile file : dataFiles) {\n-      numFiles += 1;\n-      appendFiles.appendFile(file);\n-    }\n+    if (deleteFilesNum == 0) {\n+      // To be compatible with iceberg format V1.\n+      AppendFiles appendFiles = table.newAppend();\n \n-    commitOperation(appendFiles, numFiles, \"append\", newFlinkJobId, checkpointId);\n+      int numFiles = 0;\n+      for (WriteResult result : pendingResults.values()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4e769b2c522c8ab7df7bf10eb267c758361e0a01"}, "originalPosition": 156}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0c6d008515af75c9f2cc13864c6fed4ead6b4915", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/0c6d008515af75c9f2cc13864c6fed4ead6b4915", "committedDate": "2020-12-18T09:11:59Z", "message": "Address the comments and add more unit tests."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1NjIxMDY0", "url": "https://github.com/apache/iceberg/pull/1939#pullrequestreview-555621064", "createdAt": "2020-12-18T16:10:59Z", "commit": {"oid": "0c6d008515af75c9f2cc13864c6fed4ead6b4915"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3271, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}