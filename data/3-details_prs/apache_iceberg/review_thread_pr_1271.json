{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU5MDIyNDAw", "number": 1271, "reviewThreads": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMDoyMzoxOVrOEUzN6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMToyODoyM1rOEVlX8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjQ2MTIwOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMDoyMzoxOVrOG7M11g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNjo0MToyMFrOG7THAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTU1OA==", "bodyText": "data.getNano() always returns positive integer, so is this change required?", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464729558", "createdAt": "2020-08-04T00:23:19Z", "author": {"login": "shardulm94"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -288,8 +289,10 @@ public void nonNullWrite(int rowId, LocalDate data, ColumnVector output) {\n     @Override\n     public void nonNullWrite(int rowId, OffsetDateTime data, ColumnVector output) {\n       TimestampColumnVector cv = (TimestampColumnVector) output;\n-      cv.time[rowId] = data.toInstant().toEpochMilli(); // millis\n-      cv.nanos[rowId] = (data.getNano() / 1_000) * 1_000; // truncate nanos to only keep microsecond precision\n+      // millis\n+      cv.time[rowId] = data.toInstant().toEpochMilli();\n+      // truncate nanos to only keep microsecond precision\n+      cv.nanos[rowId] = Math.floorDiv(data.getNano(), 1_000) * 1_000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzMjI1OQ==", "bodyText": "OK, I saw that the javadoc says the nano-of-second is from 0 to 999,999,999.  you're right, we don't need the floorDiv here.", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464832259", "createdAt": "2020-08-04T06:41:20Z", "author": {"login": "openinx"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -288,8 +289,10 @@ public void nonNullWrite(int rowId, LocalDate data, ColumnVector output) {\n     @Override\n     public void nonNullWrite(int rowId, OffsetDateTime data, ColumnVector output) {\n       TimestampColumnVector cv = (TimestampColumnVector) output;\n-      cv.time[rowId] = data.toInstant().toEpochMilli(); // millis\n-      cv.nanos[rowId] = (data.getNano() / 1_000) * 1_000; // truncate nanos to only keep microsecond precision\n+      // millis\n+      cv.time[rowId] = data.toInstant().toEpochMilli();\n+      // truncate nanos to only keep microsecond precision\n+      cv.nanos[rowId] = Math.floorDiv(data.getNano(), 1_000) * 1_000;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTU1OA=="}, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjY4OTE0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMjozMzo0N1rOG7O53Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNjo0ODozMVrOG7TSaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2MzM1Nw==", "bodyText": "Nit: precision <= 18 check can be moved into the constructor", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464763357", "createdAt": "2020-08-04T02:33:47Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -195,7 +196,12 @@ public Long nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      return new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\n+      BigDecimal decimal = new BigDecimal(BigInteger.valueOf(value.serialize64(value.scale())), value.scale());\n+\n+      Preconditions.checkArgument(value.precision() <= precision && precision <= 18,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzNTE3Nw==", "bodyText": "ditto.", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464835177", "createdAt": "2020-08-04T06:48:31Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -195,7 +196,12 @@ public Long nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      return new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\n+      BigDecimal decimal = new BigDecimal(BigInteger.valueOf(value.serialize64(value.scale())), value.scale());\n+\n+      Preconditions.checkArgument(value.precision() <= precision && precision <= 18,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2MzM1Nw=="}, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjY4OTM1OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMjozMzo1NFrOG7O5-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNjo1MjozM1rOG7TZRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2MzM4NA==", "bodyText": "Nit: precision <= 38 check can be moved into the constructor", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464763384", "createdAt": "2020-08-04T02:33:54Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -212,6 +218,10 @@ public Decimal nonNullRead(ColumnVector vector, int row) {\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       BigDecimal value = ((DecimalColumnVector) vector).vector[row]\n           .getHiveDecimal().bigDecimalValue();\n+\n+      Preconditions.checkArgument(value.precision() <= precision && precision <= 38,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzNjkzMw==", "bodyText": "ditto.", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464836933", "createdAt": "2020-08-04T06:52:33Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -212,6 +218,10 @@ public Decimal nonNullRead(ColumnVector vector, int row) {\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       BigDecimal value = ((DecimalColumnVector) vector).vector[row]\n           .getHiveDecimal().bigDecimalValue();\n+\n+      Preconditions.checkArgument(value.precision() <= precision && precision <= 38,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2MzM4NA=="}, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjc1OTQ3OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMzoxNToxM1rOG7Pjcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMTo1NTowNVrOG73Xtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NDAwMg==", "bodyText": "value.serialize64() will take in an expected scale as a parameter, so I think the only change required to the original code is to pass our expected reader scale into value.serialize64() instead of passing value.scale() and passing expected precision and scale to Decimal.set.\nSo this would look like return new Decimal().set(value.serialize64(scale), precision, scale);", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464774002", "createdAt": "2020-08-04T03:15:13Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -195,7 +196,12 @@ public Long nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      return new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\n+      BigDecimal decimal = new BigDecimal(BigInteger.valueOf(value.serialize64(value.scale())), value.scale());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzNTA3OA==", "bodyText": "Sounds great.  The essential purpose here is to construct a Decimal with the correct precision and scale ( instead of the value.precision() and value.scale().", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464835078", "createdAt": "2020-08-04T06:48:19Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -195,7 +196,12 @@ public Long nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      return new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\n+      BigDecimal decimal = new BigDecimal(BigInteger.valueOf(value.serialize64(value.scale())), value.scale());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NDAwMg=="}, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg0OTUyMA==", "bodyText": "Oh,  seems it's still incorrect.  Because the value.serialize64(scale) is still encoded by value.precision()  and value.scale(). we use the given precision and scale to parse this long value,  it will be messed up.  Notice, the value.precision is not equals to precision, similar to scale.\nThe correct way should be:\nDecimal decimal = new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\ndecimal.changePrecision(precision, scale);", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464849520", "createdAt": "2020-08-04T07:19:51Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -195,7 +196,12 @@ public Long nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      return new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\n+      BigDecimal decimal = new BigDecimal(BigInteger.valueOf(value.serialize64(value.scale())), value.scale());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NDAwMg=="}, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTE5OTg4Mg==", "bodyText": "I believe value.serialize64 returns the raw long value adjusted for the requested scale (and since precision <= 18, it always fits in long), I don't think it is tied to any precision. That being said, I am not very familiar with using decimals, so maybe I am missing something. Can you give an example of the case you are referring to?", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r465199882", "createdAt": "2020-08-04T17:06:03Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -195,7 +196,12 @@ public Long nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      return new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\n+      BigDecimal decimal = new BigDecimal(BigInteger.valueOf(value.serialize64(value.scale())), value.scale());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NDAwMg=="}, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQyNjM1OA==", "bodyText": "Checked this again,  I wrongly used the return new Decimal().set(value.serialize64(value.scale()), precision, scale) to construct the decimal before, which broken the unit tests.  You are right,  the long value is not tied to any precision.  Sorry for the noisy.", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r465426358", "createdAt": "2020-08-05T01:55:05Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -195,7 +196,12 @@ public Long nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      return new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\n+      BigDecimal decimal = new BigDecimal(BigInteger.valueOf(value.serialize64(value.scale())), value.scale());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NDAwMg=="}, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjc2OTkxOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMzoyMTo0M1rOG7Ppew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNjo0MzowNVrOG7TJ6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NTU0Nw==", "bodyText": "Nit: precision <= 18 check can be moved into the constructor", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464775547", "createdAt": "2020-08-04T03:21:43Z", "author": {"login": "shardulm94"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -324,14 +329,24 @@ public void nonNullWrite(int rowId, LocalDateTime data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n-      // TODO: validate precision and scale from schema\n+      Preconditions.checkArgument(data.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, data);\n+      Preconditions.checkArgument(data.precision() <= precision && precision <= 18,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzMzAwMA==", "bodyText": "the precision <=18  can be removed now, because we've checked it here.", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464833000", "createdAt": "2020-08-04T06:43:05Z", "author": {"login": "openinx"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -324,14 +329,24 @@ public void nonNullWrite(int rowId, LocalDateTime data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n-      // TODO: validate precision and scale from schema\n+      Preconditions.checkArgument(data.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, data);\n+      Preconditions.checkArgument(data.precision() <= precision && precision <= 18,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NTU0Nw=="}, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjc3MDI4OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMzoyMTo1M1rOG7PprA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNjo0NDo1MFrOG7TMbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NTU5Ng==", "bodyText": "Nit: precision <= 38 check can be moved into the constructor", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464775596", "createdAt": "2020-08-04T03:21:53Z", "author": {"login": "shardulm94"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -340,7 +355,11 @@ public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n-      // TODO: validate precision and scale from schema\n+      Preconditions.checkArgument(data.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, data);\n+      Preconditions.checkArgument(data.precision() <= precision && precision <= 38,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzMzY0Ng==", "bodyText": "ditto.", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464833646", "createdAt": "2020-08-04T06:44:50Z", "author": {"login": "openinx"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -340,7 +355,11 @@ public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n-      // TODO: validate precision and scale from schema\n+      Preconditions.checkArgument(data.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, data);\n+      Preconditions.checkArgument(data.precision() <= precision && precision <= 38,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NTU5Ng=="}, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjc4MTY5OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMzoyOToxNFrOG7Pwrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNjo1NzowNFrOG7ThPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NzM5MQ==", "bodyText": "This check seems redundant to me. If we are already passing our expected precision and scale to data.getDecimal(), wont the scale and precision of the returned decimal always match?", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464777391", "createdAt": "2020-08-04T03:29:14Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java", "diffHunk": "@@ -237,9 +239,14 @@ public void addValue(int rowId, int column, SpecializedGetters data,\n         output.noNulls = false;\n         output.isNull[rowId] = true;\n       } else {\n+        Decimal decimal = data.getDecimal(column, precision, scale);\n+        Preconditions.checkArgument(scale == decimal.scale(),\n+            \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+        Preconditions.checkArgument(decimal.precision() <= precision && precision <= 18,\n+            \"Cannot write value as decimal(%s,%s), invalid precision: %s\", decimal);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "042670ffd50e8b8a111bba93a2875ee357509137"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDgzODk3NA==", "bodyText": "Make sense.  they could be removed now.", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464838974", "createdAt": "2020-08-04T06:57:04Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java", "diffHunk": "@@ -237,9 +239,14 @@ public void addValue(int rowId, int column, SpecializedGetters data,\n         output.noNulls = false;\n         output.isNull[rowId] = true;\n       } else {\n+        Decimal decimal = data.getDecimal(column, precision, scale);\n+        Preconditions.checkArgument(scale == decimal.scale(),\n+            \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+        Preconditions.checkArgument(decimal.precision() <= precision && precision <= 18,\n+            \"Cannot write value as decimal(%s,%s), invalid precision: %s\", decimal);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NzM5MQ=="}, "originalCommit": {"oid": "042670ffd50e8b8a111bba93a2875ee357509137"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMjc4MjA1OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMzoyOToyNFrOG7Pw3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMzoyOToyNFrOG7Pw3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NzQzOQ==", "bodyText": "This check seems redundant to me. If we are already passing our expected precision and scale to data.getDecimal(), wont the scale and precision of the returned decimal always match?", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464777439", "createdAt": "2020-08-04T03:29:24Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java", "diffHunk": "@@ -261,9 +268,14 @@ public void addValue(int rowId, int column, SpecializedGetters data,\n         output.isNull[rowId] = true;\n       } else {\n         output.isNull[rowId] = false;\n-        ((DecimalColumnVector) output).vector[rowId].set(\n-            HiveDecimal.create(data.getDecimal(column, precision, scale)\n-                .toJavaBigDecimal()));\n+\n+        Decimal decimal = data.getDecimal(column, precision, scale);\n+        Preconditions.checkArgument(scale == decimal.scale(),\n+            \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+        Preconditions.checkArgument(decimal.precision() <= precision && precision <= 38,\n+            \"Cannot write value as decimal(%s,%s), invalid precision: %s\", precision, scale, decimal);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "042670ffd50e8b8a111bba93a2875ee357509137"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDYzMTgxOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMToxMzoxNFrOG8a1EA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNzozMzowNlrOG8mxeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAwNzMxMg==", "bodyText": "I'm not sure we need to check the precision either. If we read a value, then we should return it, right?", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r466007312", "createdAt": "2020-08-05T21:13:14Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -195,7 +197,15 @@ public Long nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      return new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\n+\n+      // The scale of decimal read from hive ORC file may be not equals to the expected scale. For data type\n+      // decimal(10,3) and the value 10.100, the hive ORC writer will remove its trailing zero and store it\n+      // as 101*10^(-1), its scale will adjust from 3 to 1. So here we could not assert that value.scale() == scale.\n+      // we also need to convert the hive orc decimal to a decimal with expected precision and scale.\n+      Preconditions.checkArgument(value.precision() <= precision,\n+          \"Cannot read value as decimal(%s,%s), too large: %s\", precision, scale, value);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1f7a489efa28ae39dc367230fa81882f78d2cc3"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIwMzAwMQ==", "bodyText": "It is necessary to do this check. we need to make sure that there's no bug when written a decimal into ORC. For example,  for decimal(3, 0) data type we encounter a hive decimal 10000 (whose precision is 5), that should be something wrong.  Throwing an exception is the correct way in that case.", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r466203001", "createdAt": "2020-08-06T07:33:06Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -195,7 +197,15 @@ public Long nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      return new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\n+\n+      // The scale of decimal read from hive ORC file may be not equals to the expected scale. For data type\n+      // decimal(10,3) and the value 10.100, the hive ORC writer will remove its trailing zero and store it\n+      // as 101*10^(-1), its scale will adjust from 3 to 1. So here we could not assert that value.scale() == scale.\n+      // we also need to convert the hive orc decimal to a decimal with expected precision and scale.\n+      Preconditions.checkArgument(value.precision() <= precision,\n+          \"Cannot read value as decimal(%s,%s), too large: %s\", precision, scale, value);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAwNzMxMg=="}, "originalCommit": {"oid": "d1f7a489efa28ae39dc367230fa81882f78d2cc3"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDY3ODkwOnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkRecordOrcReaderWriter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMToyODoyM1rOG8bRug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNzoyNDo1OVrOG8mhfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAxNDY1MA==", "bodyText": "Validation should be done against this data, not data that has been read from a file. That way the test won't be broken by a problem with the reader or writer that produces the expected rows. To validate against these, use the GenericsHelpers.assertEqualsUnsafe methods.", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r466014650", "createdAt": "2020-08-05T21:28:23Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkRecordOrcReaderWriter.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.orc.GenericOrcReader;\n+import org.apache.iceberg.data.orc.GenericOrcWriter;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.junit.Assert;\n+\n+public class TestSparkRecordOrcReaderWriter extends AvroDataTest {\n+  private static final int NUM_RECORDS = 200;\n+\n+  @Override\n+  protected void writeAndValidate(Schema schema) throws IOException {\n+    List<Record> records = RandomGenericData.generate(schema, NUM_RECORDS, 1992L);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1f7a489efa28ae39dc367230fa81882f78d2cc3"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE5ODkwOA==", "bodyText": "It make sense.", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r466198908", "createdAt": "2020-08-06T07:24:59Z", "author": {"login": "openinx"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkRecordOrcReaderWriter.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.orc.GenericOrcReader;\n+import org.apache.iceberg.data.orc.GenericOrcWriter;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.junit.Assert;\n+\n+public class TestSparkRecordOrcReaderWriter extends AvroDataTest {\n+  private static final int NUM_RECORDS = 200;\n+\n+  @Override\n+  protected void writeAndValidate(Schema schema) throws IOException {\n+    List<Record> records = RandomGenericData.generate(schema, NUM_RECORDS, 1992L);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAxNDY1MA=="}, "originalCommit": {"oid": "d1f7a489efa28ae39dc367230fa81882f78d2cc3"}, "originalPosition": 44}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3815, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}