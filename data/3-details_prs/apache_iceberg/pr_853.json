{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzkwNjg0NjAw", "number": 853, "title": "Task data reader changes for vectorized reads", "bodyText": "", "createdAt": "2020-03-18T21:54:31Z", "url": "https://github.com/apache/iceberg/pull/853", "merged": true, "mergeCommit": {"oid": "1bc64553aa07d9a3bade5928e4fa4f5bc8bb8c4e"}, "closed": true, "closedAt": "2020-03-21T02:11:48Z", "author": {"login": "samarthjain"}, "timelineItems": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcPQ5vrAH2gAyMzkwNjg0NjAwOmI0Y2M0YzU5ODYxOWE2NTU0NDA3YjYxYzNkMzA3NmIwNjdjZWFkOTA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcPraqNgFqTM3ODg4MzQyNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "b4cc4c598619a6554407b61c3d3076b067cead90", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/b4cc4c598619a6554407b61c3d3076b067cead90", "committedDate": "2020-03-19T19:16:30Z", "message": "Refactor TaskDataReader"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "230f62d489ff90b8f49ea9dc826271f0afadfe4c", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/230f62d489ff90b8f49ea9dc826271f0afadfe4c", "committedDate": "2020-03-18T21:54:04Z", "message": "Task data reader changes for vectorized reads"}, "afterCommit": {"oid": "b4cc4c598619a6554407b61c3d3076b067cead90", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/b4cc4c598619a6554407b61c3d3076b067cead90", "committedDate": "2020-03-19T19:16:30Z", "message": "Refactor TaskDataReader"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4NjE5MzUz", "url": "https://github.com/apache/iceberg/pull/853#pullrequestreview-378619353", "createdAt": "2020-03-20T16:10:11Z", "commit": {"oid": "b4cc4c598619a6554407b61c3d3076b067cead90"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNjoxMDoxMVrOF5aHQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNjoxMDoxMVrOF5aHQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0MDk5Mg==", "bodyText": "This will only be used by the row reader, right?", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395740992", "createdAt": "2020-03-20T16:10:11Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseTaskDataReader.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.common.DynMethods;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+abstract class BaseTaskDataReader<T> implements Closeable {\n+  // for some reason, the apply method can't be called from Java without reflection\n+  static final DynMethods.UnboundMethod APPLY_PROJECTION = DynMethods.builder(\"apply\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4cc4c598619a6554407b61c3d3076b067cead90"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4NjIwMzMz", "url": "https://github.com/apache/iceberg/pull/853#pullrequestreview-378620333", "createdAt": "2020-03-20T16:11:22Z", "commit": {"oid": "b4cc4c598619a6554407b61c3d3076b067cead90"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNjoxMToyM1rOF5aKNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNjoxMToyM1rOF5aKNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0MTc0OQ==", "bodyText": "This should implement InputPartitionReader<T> and document what T is in javadoc.", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395741749", "createdAt": "2020-03-20T16:11:23Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseTaskDataReader.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.common.DynMethods;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+abstract class BaseTaskDataReader<T> implements Closeable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4cc4c598619a6554407b61c3d3076b067cead90"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4NjIwNjUz", "url": "https://github.com/apache/iceberg/pull/853#pullrequestreview-378620653", "createdAt": "2020-03-20T16:11:45Z", "commit": {"oid": "b4cc4c598619a6554407b61c3d3076b067cead90"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNjoxMTo0NVrOF5aLHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNjoxMTo0NVrOF5aLHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0MTk4Mg==", "bodyText": "Needs @Override.", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395741982", "createdAt": "2020-03-20T16:11:45Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseTaskDataReader.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.common.DynMethods;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+abstract class BaseTaskDataReader<T> implements Closeable {\n+  // for some reason, the apply method can't be called from Java without reflection\n+  static final DynMethods.UnboundMethod APPLY_PROJECTION = DynMethods.builder(\"apply\")\n+      .impl(UnsafeProjection.class, InternalRow.class)\n+      .build();\n+\n+  final Iterator<FileScanTask> tasks;\n+  final Schema tableSchema;\n+  final Schema expectedSchema;\n+  final FileIO fileIo;\n+  final Map<String, InputFile> inputFiles;\n+  final boolean caseSensitive;\n+\n+  Iterator<T> currentIterator;\n+  Closeable currentCloseable = null;\n+  T current = null;\n+  final int batchSize;\n+\n+  BaseTaskDataReader(\n+      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n+      EncryptionManager encryptionManager, boolean caseSensitive) {\n+    this(task, tableSchema, expectedSchema, fileIo, encryptionManager, caseSensitive, -1);\n+  }\n+\n+  BaseTaskDataReader(\n+      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n+      EncryptionManager encryptionManager, boolean caseSensitive, int bSize) {\n+    this.fileIo = fileIo;\n+    this.tasks = task.files().iterator();\n+    this.tableSchema = tableSchema;\n+    this.expectedSchema = expectedSchema;\n+    Iterable<InputFile> decryptedFiles = encryptionManager.decrypt(Iterables.transform(\n+        task.files(),\n+        fileScanTask ->\n+            EncryptedFiles.encryptedInput(\n+                this.fileIo.newInputFile(fileScanTask.file().path().toString()),\n+                fileScanTask.file().keyMetadata())));\n+    ImmutableMap.Builder<String, InputFile> inputFileBuilder = ImmutableMap.builder();\n+    decryptedFiles.forEach(decrypted -> inputFileBuilder.put(decrypted.location(), decrypted));\n+    this.inputFiles = inputFileBuilder.build();\n+    this.caseSensitive = caseSensitive;\n+    this.batchSize = bSize;\n+    // open last because the schemas, fileIo and batchSize must be set\n+    this.currentIterator = open(tasks.next());\n+  }\n+\n+  public boolean next() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4cc4c598619a6554407b61c3d3076b067cead90"}, "originalPosition": 87}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4NjIxOTAx", "url": "https://github.com/apache/iceberg/pull/853#pullrequestreview-378621901", "createdAt": "2020-03-20T16:13:14Z", "commit": {"oid": "b4cc4c598619a6554407b61c3d3076b067cead90"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNjoxMzoxNFrOF5aOsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNjoxMzoxNFrOF5aOsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0Mjg5OQ==", "bodyText": "Why isn't T get() { return current; } implemented in this base class?", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395742899", "createdAt": "2020-03-20T16:13:14Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseTaskDataReader.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.common.DynMethods;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+abstract class BaseTaskDataReader<T> implements Closeable {\n+  // for some reason, the apply method can't be called from Java without reflection\n+  static final DynMethods.UnboundMethod APPLY_PROJECTION = DynMethods.builder(\"apply\")\n+      .impl(UnsafeProjection.class, InternalRow.class)\n+      .build();\n+\n+  final Iterator<FileScanTask> tasks;\n+  final Schema tableSchema;\n+  final Schema expectedSchema;\n+  final FileIO fileIo;\n+  final Map<String, InputFile> inputFiles;\n+  final boolean caseSensitive;\n+\n+  Iterator<T> currentIterator;\n+  Closeable currentCloseable = null;\n+  T current = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4cc4c598619a6554407b61c3d3076b067cead90"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4NjIyOTM0", "url": "https://github.com/apache/iceberg/pull/853#pullrequestreview-378622934", "createdAt": "2020-03-20T16:14:30Z", "commit": {"oid": "b4cc4c598619a6554407b61c3d3076b067cead90"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNjoxNDozMFrOF5aRnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNjoxNDozMFrOF5aRnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0MzY0NQ==", "bodyText": "How about RowDataReader here instead?", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395743645", "createdAt": "2020-03-20T16:14:30Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowTaskDataReader.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.SparkAvroReader;\n+import org.apache.iceberg.spark.data.SparkOrcReader;\n+import org.apache.iceberg.spark.data.SparkParquetReaders;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.catalyst.expressions.JoinedRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n+import org.apache.spark.sql.types.BinaryType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.types.DecimalType;\n+import org.apache.spark.sql.types.StringType;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.unsafe.types.UTF8String;\n+import scala.collection.JavaConverters;\n+\n+class RowTaskDataReader extends BaseTaskDataReader<InternalRow> implements InputPartitionReader<InternalRow> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4cc4c598619a6554407b61c3d3076b067cead90"}, "originalPosition": 70}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4NjI0MDEw", "url": "https://github.com/apache/iceberg/pull/853#pullrequestreview-378624010", "createdAt": "2020-03-20T16:15:50Z", "commit": {"oid": "b4cc4c598619a6554407b61c3d3076b067cead90"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNjoxNTo1MFrOF5aUrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNjoxNTo1MFrOF5aUrQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0NDQyOQ==", "bodyText": "Can we move this into a separate file while we are moving classes?", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395744429", "createdAt": "2020-03-20T16:15:50Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowTaskDataReader.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.SparkAvroReader;\n+import org.apache.iceberg.spark.data.SparkOrcReader;\n+import org.apache.iceberg.spark.data.SparkParquetReaders;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.catalyst.expressions.JoinedRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n+import org.apache.spark.sql.types.BinaryType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.types.DecimalType;\n+import org.apache.spark.sql.types.StringType;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.unsafe.types.UTF8String;\n+import scala.collection.JavaConverters;\n+\n+class RowTaskDataReader extends BaseTaskDataReader<InternalRow> implements InputPartitionReader<InternalRow> {\n+\n+  RowTaskDataReader(\n+      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n+      EncryptionManager encryptionManager, boolean caseSensitive) {\n+    super(task, tableSchema, expectedSchema, fileIo, encryptionManager, caseSensitive);\n+  }\n+\n+  @Override\n+  public InternalRow get() {\n+    return current;\n+  }\n+\n+  @Override\n+  Iterator<InternalRow> open(FileScanTask task) {\n+    DataFile file = task.file();\n+\n+    // update the current file for Spark's filename() function\n+    InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n+\n+    // schema or rows returned by readers\n+    Schema finalSchema = expectedSchema;\n+    PartitionSpec spec = task.spec();\n+    Set<Integer> idColumns = spec.identitySourceIds();\n+\n+    // schema needed for the projection and filtering\n+    StructType sparkType = SparkSchemaUtil.convert(finalSchema);\n+    Schema requiredSchema = SparkSchemaUtil.prune(tableSchema, sparkType, task.residual(), caseSensitive);\n+    boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+    boolean hasExtraFilterColumns = requiredSchema.columns().size() != finalSchema.columns().size();\n+\n+    Schema iterSchema;\n+    Iterator<InternalRow> iter;\n+\n+    if (hasJoinedPartitionColumns) {\n+      // schema used to read data files\n+      Schema readSchema = TypeUtil.selectNot(requiredSchema, idColumns);\n+      Schema partitionSchema = TypeUtil.select(requiredSchema, idColumns);\n+      PartitionRowConverter convertToRow = new PartitionRowConverter(partitionSchema, spec);\n+      JoinedRow joined = new JoinedRow();\n+\n+      InternalRow partition = convertToRow.apply(file.partition());\n+      joined.withRight(partition);\n+\n+      // create joined rows and project from the joined schema to the final schema\n+      iterSchema = TypeUtil.join(readSchema, partitionSchema);\n+      iter = Iterators.transform(open(task, readSchema), joined::withLeft);\n+    } else if (hasExtraFilterColumns) {\n+      // add projection to the final schema\n+      iterSchema = requiredSchema;\n+      iter = open(task, requiredSchema);\n+    } else {\n+      // return the base iterator\n+      iterSchema = finalSchema;\n+      iter = open(task, finalSchema);\n+    }\n+\n+    // TODO: remove the projection by reporting the iterator's schema back to Spark\n+    return Iterators.transform(\n+        iter,\n+        APPLY_PROJECTION.bind(projection(finalSchema, iterSchema))::invoke);\n+  }\n+\n+  private Iterator<InternalRow> open(FileScanTask task, Schema readSchema) {\n+    CloseableIterable<InternalRow> iter;\n+    if (task.isDataTask()) {\n+      iter = newDataIterable(task.asDataTask(), readSchema);\n+    } else {\n+      InputFile location = inputFiles.get(task.file().path().toString());\n+      Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n+\n+      switch (task.file().format()) {\n+        case PARQUET:\n+          iter = newParquetIterable(location, task, readSchema);\n+          break;\n+\n+        case AVRO:\n+          iter = newAvroIterable(location, task, readSchema);\n+          break;\n+\n+        case ORC:\n+          iter = newOrcIterable(location, task, readSchema);\n+          break;\n+\n+        default:\n+          throw new UnsupportedOperationException(\n+              \"Cannot read unknown format: \" + task.file().format());\n+      }\n+    }\n+\n+    this.currentCloseable = iter;\n+\n+    return iter.iterator();\n+  }\n+\n+  private CloseableIterable<InternalRow> newAvroIterable(\n+      InputFile location,\n+      FileScanTask task,\n+      Schema readSchema) {\n+    return Avro.read(location)\n+        .reuseContainers()\n+        .project(readSchema)\n+        .split(task.start(), task.length())\n+        .createReaderFunc(SparkAvroReader::new)\n+        .build();\n+  }\n+\n+  private CloseableIterable<InternalRow> newParquetIterable(\n+      InputFile location,\n+      FileScanTask task,\n+      Schema readSchema) {\n+    return Parquet.read(location)\n+        .project(readSchema)\n+        .split(task.start(), task.length())\n+        .createReaderFunc(fileSchema -> SparkParquetReaders.buildReader(readSchema, fileSchema))\n+        .filter(task.residual())\n+        .caseSensitive(caseSensitive)\n+        .build();\n+  }\n+\n+  private CloseableIterable<InternalRow> newOrcIterable(\n+      InputFile location,\n+      FileScanTask task,\n+      Schema readSchema) {\n+    return ORC.read(location)\n+        .schema(readSchema)\n+        .split(task.start(), task.length())\n+        .createReaderFunc(SparkOrcReader::new)\n+        .caseSensitive(caseSensitive)\n+        .build();\n+  }\n+\n+  private CloseableIterable<InternalRow> newDataIterable(DataTask task, Schema readSchema) {\n+    StructInternalRow row = new StructInternalRow(tableSchema.asStruct());\n+    CloseableIterable<InternalRow> asSparkRows = CloseableIterable.transform(\n+        task.asDataTask().rows(), row::setStruct);\n+    return CloseableIterable.transform(\n+        asSparkRows, APPLY_PROJECTION.bind(projection(readSchema, tableSchema))::invoke);\n+  }\n+\n+  private static UnsafeProjection projection(Schema finalSchema, Schema readSchema) {\n+    StructType struct = SparkSchemaUtil.convert(readSchema);\n+\n+    List<AttributeReference> refs = JavaConverters.seqAsJavaListConverter(struct.toAttributes()).asJava();\n+    List<Attribute> attrs = Lists.newArrayListWithExpectedSize(struct.fields().length);\n+    List<org.apache.spark.sql.catalyst.expressions.Expression> exprs =\n+        Lists.newArrayListWithExpectedSize(struct.fields().length);\n+\n+    for (AttributeReference ref : refs) {\n+      attrs.add(ref.toAttribute());\n+    }\n+\n+    for (Types.NestedField field : finalSchema.columns()) {\n+      int indexInReadSchema = struct.fieldIndex(field.name());\n+      exprs.add(refs.get(indexInReadSchema));\n+    }\n+\n+    return UnsafeProjection.create(\n+        JavaConverters.asScalaBufferConverter(exprs).asScala().toSeq(),\n+        JavaConverters.asScalaBufferConverter(attrs).asScala().toSeq());\n+  }\n+\n+  private static class PartitionRowConverter implements Function<StructLike, InternalRow> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b4cc4c598619a6554407b61c3d3076b067cead90"}, "originalPosition": 232}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dfaf7114160cc554c7f830f9d7ae2183508c0eb3", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/dfaf7114160cc554c7f830f9d7ae2183508c0eb3", "committedDate": "2020-03-20T18:44:42Z", "message": "Address code review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4NzM4MTIy", "url": "https://github.com/apache/iceberg/pull/853#pullrequestreview-378738122", "createdAt": "2020-03-20T18:55:40Z", "commit": {"oid": "dfaf7114160cc554c7f830f9d7ae2183508c0eb3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxODo1NTo0MFrOF5fuxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxODo1NTo0MFrOF5fuxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTgzMzAyOQ==", "bodyText": "T is the Java class returned by this reader that contains one or more rows.", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395833029", "createdAt": "2020-03-20T18:55:40Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseDataReader.java", "diffHunk": "@@ -28,22 +28,18 @@\n import org.apache.iceberg.CombinedScanTask;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.common.DynMethods;\n import org.apache.iceberg.encryption.EncryptedFiles;\n import org.apache.iceberg.encryption.EncryptionManager;\n import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.io.InputFile;\n import org.apache.spark.rdd.InputFileBlockHolder;\n-import org.apache.spark.sql.catalyst.InternalRow;\n-import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n \n+/**\n+ * @param <T> Base class of readers to read data as objects of type @param &lt;T&gt;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dfaf7114160cc554c7f830f9d7ae2183508c0eb3"}, "originalPosition": 15}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4NzQyNTgz", "url": "https://github.com/apache/iceberg/pull/853#pullrequestreview-378742583", "createdAt": "2020-03-20T19:02:32Z", "commit": {"oid": "dfaf7114160cc554c7f830f9d7ae2183508c0eb3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxOTowMjozMlrOF5f8QQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxOTowMjozMlrOF5f8QQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTgzNjQ4MQ==", "bodyText": "We need to clean up how these instance fields are shared. Some, like tableSchema, expectedSchema, caseSensitive don't need to be held by this class and should be removed. I think we need a protected method to get an InputFile from inputFiles. And I think that tasks isn't shared and can be private.", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395836481", "createdAt": "2020-03-20T19:02:32Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseDataReader.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n+\n+/**\n+ * @param <T> Base class of readers to read data as objects of type @param &lt;T&gt;\n+ */\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+abstract class BaseDataReader<T> implements InputPartitionReader<T> {\n+  final Iterator<FileScanTask> tasks;\n+  final Schema tableSchema;\n+  final Schema expectedSchema;\n+  final FileIO fileIo;\n+  final Map<String, InputFile> inputFiles;\n+  final boolean caseSensitive;\n+\n+  Iterator<T> currentIterator;\n+  Closeable currentCloseable = null;\n+  T current = null;\n+  final int batchSize;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dfaf7114160cc554c7f830f9d7ae2183508c0eb3"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4NzQyOTY3", "url": "https://github.com/apache/iceberg/pull/853#pullrequestreview-378742967", "createdAt": "2020-03-20T19:03:10Z", "commit": {"oid": "dfaf7114160cc554c7f830f9d7ae2183508c0eb3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxOTowMzoxMFrOF5f9iQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxOTowMzoxMFrOF5f9iQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTgzNjgwOQ==", "bodyText": "These can be private right?", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395836809", "createdAt": "2020-03-20T19:03:10Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseDataReader.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n+\n+/**\n+ * @param <T> Base class of readers to read data as objects of type @param &lt;T&gt;\n+ */\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+abstract class BaseDataReader<T> implements InputPartitionReader<T> {\n+  final Iterator<FileScanTask> tasks;\n+  final Schema tableSchema;\n+  final Schema expectedSchema;\n+  final FileIO fileIo;\n+  final Map<String, InputFile> inputFiles;\n+  final boolean caseSensitive;\n+\n+  Iterator<T> currentIterator;\n+  Closeable currentCloseable = null;\n+  T current = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dfaf7114160cc554c7f830f9d7ae2183508c0eb3"}, "originalPosition": 52}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b6c18780cc3ba6c1e9d8723aed7efabef9a915c1", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/b6c18780cc3ba6c1e9d8723aed7efabef9a915c1", "committedDate": "2020-03-20T22:52:40Z", "message": "Some more cleanup"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4ODgzNDI0", "url": "https://github.com/apache/iceberg/pull/853#pullrequestreview-378883424", "createdAt": "2020-03-21T02:09:58Z", "commit": {"oid": "b6c18780cc3ba6c1e9d8723aed7efabef9a915c1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMVQwMjowOTo1OVrOF5m-Iw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMVQwMjowOTo1OVrOF5m-Iw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTk1MTY1MQ==", "bodyText": "Can this be private?", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395951651", "createdAt": "2020-03-21T02:09:59Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseDataReader.java", "diffHunk": "@@ -23,48 +23,38 @@\n import com.google.common.collect.Iterables;\n import java.io.Closeable;\n import java.io.IOException;\n+import java.util.Collections;\n import java.util.Iterator;\n import java.util.Map;\n+import org.apache.arrow.util.Preconditions;\n import org.apache.iceberg.CombinedScanTask;\n import org.apache.iceberg.FileScanTask;\n-import org.apache.iceberg.Schema;\n import org.apache.iceberg.encryption.EncryptedFiles;\n import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.io.InputFile;\n import org.apache.spark.rdd.InputFileBlockHolder;\n import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n \n /**\n- * @param <T> Base class of readers to read data as objects of type @param &lt;T&gt;\n+ * Base class of readers of type {@link InputPartitionReader} to read data as objects of type @param &lt;T&gt;\n+ *\n+ * @param <T> is the Java class returned by this reader whose objects contain one or more rows.\n  */\n @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n abstract class BaseDataReader<T> implements InputPartitionReader<T> {\n-  final Iterator<FileScanTask> tasks;\n-  final Schema tableSchema;\n-  final Schema expectedSchema;\n-  final FileIO fileIo;\n-  final Map<String, InputFile> inputFiles;\n-  final boolean caseSensitive;\n-\n-  Iterator<T> currentIterator;\n-  Closeable currentCloseable = null;\n-  T current = null;\n-  final int batchSize;\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;\n+  private final Map<String, InputFile> inputFiles;\n \n-  BaseDataReader(\n-      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n-      EncryptionManager encryptionManager, boolean caseSensitive) {\n-    this(task, tableSchema, expectedSchema, fileIo, encryptionManager, caseSensitive, -1);\n-  }\n+  private Iterator<T> currentIterator;\n+  Closeable currentCloseable;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b6c18780cc3ba6c1e9d8723aed7efabef9a915c1"}, "originalPosition": 48}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4728, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}