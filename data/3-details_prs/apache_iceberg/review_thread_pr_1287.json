{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDYyMjc4NjQx", "number": 1287, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNzo0NjoyNFrOEUszTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxODo1MDoxMFrOEUuCtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMTQxMDA3OnYy", "diffSide": "RIGHT", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNzo0NjoyNFrOG7C98g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNzo1MTo0OFrOG7DIvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2Nzc5NA==", "bodyText": "Could we rename this to ConstantVectorHolder instead of \"dummy\"? Now that it returns a constant value, I think it's more accurate to use \"constant\".", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464567794", "createdAt": "2020-08-03T17:46:24Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java", "diffHunk": "@@ -91,17 +91,44 @@ public int numValues() {\n     return vector.getValueCount();\n   }\n \n+  public static VectorHolder constantHolder(int numRows, Object constantValue) {\n+    return new DummyVectorHolder(numRows, constantValue);\n+  }\n+\n   public static VectorHolder dummyHolder(int numRows) {\n-    return new VectorHolder() {\n-      @Override\n-      public int numValues() {\n-        return numRows;\n-      }\n-    };\n+    return new DummyVectorHolder(numRows);\n   }\n \n   public boolean isDummy() {\n     return vector == null;\n   }\n \n+  /**\n+   * A Vector Holder which does not actually produce values, consumers of this class should\n+   * use the constantValue to populate their ColumnVector implementation.\n+   */\n+  public static class DummyVectorHolder extends VectorHolder {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "71fe66885ce7099230441d4312e2c1e672c61d1c"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3MDU1OQ==", "bodyText": "Makes sense to me", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464570559", "createdAt": "2020-08-03T17:51:48Z", "author": {"login": "RussellSpitzer"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java", "diffHunk": "@@ -91,17 +91,44 @@ public int numValues() {\n     return vector.getValueCount();\n   }\n \n+  public static VectorHolder constantHolder(int numRows, Object constantValue) {\n+    return new DummyVectorHolder(numRows, constantValue);\n+  }\n+\n   public static VectorHolder dummyHolder(int numRows) {\n-    return new VectorHolder() {\n-      @Override\n-      public int numValues() {\n-        return numRows;\n-      }\n-    };\n+    return new DummyVectorHolder(numRows);\n   }\n \n   public boolean isDummy() {\n     return vector == null;\n   }\n \n+  /**\n+   * A Vector Holder which does not actually produce values, consumers of this class should\n+   * use the constantValue to populate their ColumnVector implementation.\n+   */\n+  public static class DummyVectorHolder extends VectorHolder {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2Nzc5NA=="}, "originalCommit": {"oid": "71fe66885ce7099230441d4312e2c1e672c61d1c"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMTQxNzI3OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNzo0ODozNVrOG7DCMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTo1ODoyMlrOG7KDgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2ODg4Mw==", "bodyText": "Nit: Looks like this Javadoc is missing the normal *  at the start of these lines?", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464568883", "createdAt": "2020-08-03T17:48:35Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "diffHunk": "@@ -110,24 +113,52 @@ public static void stopSpark() {\n   private Table table = null;\n   private Dataset<Row> logs = null;\n \n-  @Before\n-  public void setupTable() throws Exception {\n+  /*\n+  Use the Hive Based table to make Identity Partition Columns with no duplication of the data in the underlying\n+  parquet files. This makes sure that if the identity mapping fails, the test will also fail.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "71fe66885ce7099230441d4312e2c1e672c61d1c"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MzkwNQ==", "bodyText": "I was just leaving a multiline comment, but I'll make it a javadoc", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464683905", "createdAt": "2020-08-03T21:58:22Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "diffHunk": "@@ -110,24 +113,52 @@ public static void stopSpark() {\n   private Table table = null;\n   private Dataset<Row> logs = null;\n \n-  @Before\n-  public void setupTable() throws Exception {\n+  /*\n+  Use the Hive Based table to make Identity Partition Columns with no duplication of the data in the underlying\n+  parquet files. This makes sure that if the identity mapping fails, the test will also fail.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2ODg4Mw=="}, "originalCommit": {"oid": "71fe66885ce7099230441d4312e2c1e672c61d1c"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMTQyMDk5OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxNzo0OTo0MVrOG7DEkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMTowNjozMlrOG72kDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2OTQ4OQ==", "bodyText": "Isn't this the default? Why was it necessary to add select?", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464569489", "createdAt": "2020-08-03T17:49:41Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "diffHunk": "@@ -110,24 +113,52 @@ public static void stopSpark() {\n   private Table table = null;\n   private Dataset<Row> logs = null;\n \n-  @Before\n-  public void setupTable() throws Exception {\n+  /*\n+  Use the Hive Based table to make Identity Partition Columns with no duplication of the data in the underlying\n+  parquet files. This makes sure that if the identity mapping fails, the test will also fail.\n+   */\n+  private void setupParquet() throws Exception {\n     File location = temp.newFolder(\"logs\");\n+    File hiveLocation = temp.newFolder(\"hive\");\n+    String hiveTable = \"hivetable\";\n     Assert.assertTrue(\"Temp folder should exist\", location.exists());\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);\n-    this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n     this.logs = spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", hiveTable));\n+    logs.orderBy(\"date\", \"level\", \"id\").write().partitionBy(\"date\", \"level\").format(\"parquet\")\n+        .option(\"path\", hiveLocation.toString()).saveAsTable(hiveTable);\n+\n+    this.table = TABLES.create(SparkSchemaUtil.schemaForTable(spark, hiveTable),\n+        SparkSchemaUtil.specForTable(spark, hiveTable), properties, location.toString());\n+\n+    SparkTableUtil.importSparkTable(spark, new TableIdentifier(hiveTable), table, location.toString());\n+  }\n \n-    logs.orderBy(\"date\", \"level\", \"id\").write().format(\"iceberg\").mode(\"append\").save(location.toString());\n+  @Before\n+  public void setupTable() throws Exception {\n+    if (format.equals(\"parquet\")) {\n+      setupParquet();\n+    } else {\n+      File location = temp.newFolder(\"logs\");\n+      Assert.assertTrue(\"Temp folder should exist\", location.exists());\n+\n+      Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);\n+      this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n+      this.logs = spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n+\n+      logs.orderBy(\"date\", \"level\", \"id\").write().format(\"iceberg\").mode(\"append\").save(location.toString());\n+    }\n   }\n \n   @Test\n   public void testFullProjection() {\n     List<Row> expected = logs.orderBy(\"id\").collectAsList();\n     List<Row> actual = spark.read().format(\"iceberg\")\n         .option(\"vectorization-enabled\", String.valueOf(vectorized))\n-        .load(table.location()).orderBy(\"id\").collectAsList();\n+        .load(table.location()).orderBy(\"id\")\n+        .select(\"id\", \"date\", \"level\", \"message\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "71fe66885ce7099230441d4312e2c1e672c61d1c"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3MDE4Ng==", "bodyText": "When I added in the Hive Import it gets the schema in a different order, I think this may be an issue with the import code? I'm not sure, but I know the default column order does not come out the same way :/", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464570186", "createdAt": "2020-08-03T17:51:02Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "diffHunk": "@@ -110,24 +113,52 @@ public static void stopSpark() {\n   private Table table = null;\n   private Dataset<Row> logs = null;\n \n-  @Before\n-  public void setupTable() throws Exception {\n+  /*\n+  Use the Hive Based table to make Identity Partition Columns with no duplication of the data in the underlying\n+  parquet files. This makes sure that if the identity mapping fails, the test will also fail.\n+   */\n+  private void setupParquet() throws Exception {\n     File location = temp.newFolder(\"logs\");\n+    File hiveLocation = temp.newFolder(\"hive\");\n+    String hiveTable = \"hivetable\";\n     Assert.assertTrue(\"Temp folder should exist\", location.exists());\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);\n-    this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n     this.logs = spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", hiveTable));\n+    logs.orderBy(\"date\", \"level\", \"id\").write().partitionBy(\"date\", \"level\").format(\"parquet\")\n+        .option(\"path\", hiveLocation.toString()).saveAsTable(hiveTable);\n+\n+    this.table = TABLES.create(SparkSchemaUtil.schemaForTable(spark, hiveTable),\n+        SparkSchemaUtil.specForTable(spark, hiveTable), properties, location.toString());\n+\n+    SparkTableUtil.importSparkTable(spark, new TableIdentifier(hiveTable), table, location.toString());\n+  }\n \n-    logs.orderBy(\"date\", \"level\", \"id\").write().format(\"iceberg\").mode(\"append\").save(location.toString());\n+  @Before\n+  public void setupTable() throws Exception {\n+    if (format.equals(\"parquet\")) {\n+      setupParquet();\n+    } else {\n+      File location = temp.newFolder(\"logs\");\n+      Assert.assertTrue(\"Temp folder should exist\", location.exists());\n+\n+      Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);\n+      this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n+      this.logs = spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n+\n+      logs.orderBy(\"date\", \"level\", \"id\").write().format(\"iceberg\").mode(\"append\").save(location.toString());\n+    }\n   }\n \n   @Test\n   public void testFullProjection() {\n     List<Row> expected = logs.orderBy(\"id\").collectAsList();\n     List<Row> actual = spark.read().format(\"iceberg\")\n         .option(\"vectorization-enabled\", String.valueOf(vectorized))\n-        .load(table.location()).orderBy(\"id\").collectAsList();\n+        .load(table.location()).orderBy(\"id\")\n+        .select(\"id\", \"date\", \"level\", \"message\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2OTQ4OQ=="}, "originalCommit": {"oid": "71fe66885ce7099230441d4312e2c1e672c61d1c"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU3OTc1OQ==", "bodyText": "That's suspicious. We'll have to look into why the schema has the wrong order. I see select before all the writes, so it shouldn't need the reorder here.", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464579759", "createdAt": "2020-08-03T18:10:02Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "diffHunk": "@@ -110,24 +113,52 @@ public static void stopSpark() {\n   private Table table = null;\n   private Dataset<Row> logs = null;\n \n-  @Before\n-  public void setupTable() throws Exception {\n+  /*\n+  Use the Hive Based table to make Identity Partition Columns with no duplication of the data in the underlying\n+  parquet files. This makes sure that if the identity mapping fails, the test will also fail.\n+   */\n+  private void setupParquet() throws Exception {\n     File location = temp.newFolder(\"logs\");\n+    File hiveLocation = temp.newFolder(\"hive\");\n+    String hiveTable = \"hivetable\";\n     Assert.assertTrue(\"Temp folder should exist\", location.exists());\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);\n-    this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n     this.logs = spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", hiveTable));\n+    logs.orderBy(\"date\", \"level\", \"id\").write().partitionBy(\"date\", \"level\").format(\"parquet\")\n+        .option(\"path\", hiveLocation.toString()).saveAsTable(hiveTable);\n+\n+    this.table = TABLES.create(SparkSchemaUtil.schemaForTable(spark, hiveTable),\n+        SparkSchemaUtil.specForTable(spark, hiveTable), properties, location.toString());\n+\n+    SparkTableUtil.importSparkTable(spark, new TableIdentifier(hiveTable), table, location.toString());\n+  }\n \n-    logs.orderBy(\"date\", \"level\", \"id\").write().format(\"iceberg\").mode(\"append\").save(location.toString());\n+  @Before\n+  public void setupTable() throws Exception {\n+    if (format.equals(\"parquet\")) {\n+      setupParquet();\n+    } else {\n+      File location = temp.newFolder(\"logs\");\n+      Assert.assertTrue(\"Temp folder should exist\", location.exists());\n+\n+      Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);\n+      this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n+      this.logs = spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n+\n+      logs.orderBy(\"date\", \"level\", \"id\").write().format(\"iceberg\").mode(\"append\").save(location.toString());\n+    }\n   }\n \n   @Test\n   public void testFullProjection() {\n     List<Row> expected = logs.orderBy(\"id\").collectAsList();\n     List<Row> actual = spark.read().format(\"iceberg\")\n         .option(\"vectorization-enabled\", String.valueOf(vectorized))\n-        .load(table.location()).orderBy(\"id\").collectAsList();\n+        .load(table.location()).orderBy(\"id\")\n+        .select(\"id\", \"date\", \"level\", \"message\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2OTQ4OQ=="}, "originalCommit": {"oid": "71fe66885ce7099230441d4312e2c1e672c61d1c"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU5MDU5NA==", "bodyText": "I'll try to figure out the actual issue today, but I agree it shouldn't work this way. My assumption is that the Hive table schema is just being listed in a different order or when we use SparkSchemaUtil the order is getting scrambled.", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464590594", "createdAt": "2020-08-03T18:31:49Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "diffHunk": "@@ -110,24 +113,52 @@ public static void stopSpark() {\n   private Table table = null;\n   private Dataset<Row> logs = null;\n \n-  @Before\n-  public void setupTable() throws Exception {\n+  /*\n+  Use the Hive Based table to make Identity Partition Columns with no duplication of the data in the underlying\n+  parquet files. This makes sure that if the identity mapping fails, the test will also fail.\n+   */\n+  private void setupParquet() throws Exception {\n     File location = temp.newFolder(\"logs\");\n+    File hiveLocation = temp.newFolder(\"hive\");\n+    String hiveTable = \"hivetable\";\n     Assert.assertTrue(\"Temp folder should exist\", location.exists());\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);\n-    this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n     this.logs = spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", hiveTable));\n+    logs.orderBy(\"date\", \"level\", \"id\").write().partitionBy(\"date\", \"level\").format(\"parquet\")\n+        .option(\"path\", hiveLocation.toString()).saveAsTable(hiveTable);\n+\n+    this.table = TABLES.create(SparkSchemaUtil.schemaForTable(spark, hiveTable),\n+        SparkSchemaUtil.specForTable(spark, hiveTable), properties, location.toString());\n+\n+    SparkTableUtil.importSparkTable(spark, new TableIdentifier(hiveTable), table, location.toString());\n+  }\n \n-    logs.orderBy(\"date\", \"level\", \"id\").write().format(\"iceberg\").mode(\"append\").save(location.toString());\n+  @Before\n+  public void setupTable() throws Exception {\n+    if (format.equals(\"parquet\")) {\n+      setupParquet();\n+    } else {\n+      File location = temp.newFolder(\"logs\");\n+      Assert.assertTrue(\"Temp folder should exist\", location.exists());\n+\n+      Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);\n+      this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n+      this.logs = spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n+\n+      logs.orderBy(\"date\", \"level\", \"id\").write().format(\"iceberg\").mode(\"append\").save(location.toString());\n+    }\n   }\n \n   @Test\n   public void testFullProjection() {\n     List<Row> expected = logs.orderBy(\"id\").collectAsList();\n     List<Row> actual = spark.read().format(\"iceberg\")\n         .option(\"vectorization-enabled\", String.valueOf(vectorized))\n-        .load(table.location()).orderBy(\"id\").collectAsList();\n+        .load(table.location()).orderBy(\"id\")\n+        .select(\"id\", \"date\", \"level\", \"message\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2OTQ4OQ=="}, "originalCommit": {"oid": "71fe66885ce7099230441d4312e2c1e672c61d1c"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY4MzE5NA==", "bodyText": "I spent some time digging into this,\nWhen you call saveAsTable it ends up in this bit of code in DataFrameWriter\n    val tableDesc = CatalogTable(\n      identifier = tableIdent,\n      tableType = tableType,\n      storage = storage,\n      schema = new StructType,\n      provider = Some(source),\n      partitionColumnNames = partitioningColumns.getOrElse(Nil),\n      bucketSpec = getBucketSpec)\nWhich strips out whatever incoming schema you have. So the new table is created without any information about the actual ordering of columns you used in the create.\nThen when the Relation is resolved, that's when the attributes are looked up again and the schema is created from the Attribute output. So long story short, saveAsTable doesn't care about your field ordering as far as I can tell. This is all in Spark and I'm not sure we can do anything about it here.", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464683194", "createdAt": "2020-08-03T21:56:23Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "diffHunk": "@@ -110,24 +113,52 @@ public static void stopSpark() {\n   private Table table = null;\n   private Dataset<Row> logs = null;\n \n-  @Before\n-  public void setupTable() throws Exception {\n+  /*\n+  Use the Hive Based table to make Identity Partition Columns with no duplication of the data in the underlying\n+  parquet files. This makes sure that if the identity mapping fails, the test will also fail.\n+   */\n+  private void setupParquet() throws Exception {\n     File location = temp.newFolder(\"logs\");\n+    File hiveLocation = temp.newFolder(\"hive\");\n+    String hiveTable = \"hivetable\";\n     Assert.assertTrue(\"Temp folder should exist\", location.exists());\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);\n-    this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n     this.logs = spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", hiveTable));\n+    logs.orderBy(\"date\", \"level\", \"id\").write().partitionBy(\"date\", \"level\").format(\"parquet\")\n+        .option(\"path\", hiveLocation.toString()).saveAsTable(hiveTable);\n+\n+    this.table = TABLES.create(SparkSchemaUtil.schemaForTable(spark, hiveTable),\n+        SparkSchemaUtil.specForTable(spark, hiveTable), properties, location.toString());\n+\n+    SparkTableUtil.importSparkTable(spark, new TableIdentifier(hiveTable), table, location.toString());\n+  }\n \n-    logs.orderBy(\"date\", \"level\", \"id\").write().format(\"iceberg\").mode(\"append\").save(location.toString());\n+  @Before\n+  public void setupTable() throws Exception {\n+    if (format.equals(\"parquet\")) {\n+      setupParquet();\n+    } else {\n+      File location = temp.newFolder(\"logs\");\n+      Assert.assertTrue(\"Temp folder should exist\", location.exists());\n+\n+      Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);\n+      this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n+      this.logs = spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n+\n+      logs.orderBy(\"date\", \"level\", \"id\").write().format(\"iceberg\").mode(\"append\").save(location.toString());\n+    }\n   }\n \n   @Test\n   public void testFullProjection() {\n     List<Row> expected = logs.orderBy(\"id\").collectAsList();\n     List<Row> actual = spark.read().format(\"iceberg\")\n         .option(\"vectorization-enabled\", String.valueOf(vectorized))\n-        .load(table.location()).orderBy(\"id\").collectAsList();\n+        .load(table.location()).orderBy(\"id\")\n+        .select(\"id\", \"date\", \"level\", \"message\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2OTQ4OQ=="}, "originalCommit": {"oid": "71fe66885ce7099230441d4312e2c1e672c61d1c"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMzEzMg==", "bodyText": "I'm fine with this, then. Thanks for looking into it!", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r465413132", "createdAt": "2020-08-05T01:06:32Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "diffHunk": "@@ -110,24 +113,52 @@ public static void stopSpark() {\n   private Table table = null;\n   private Dataset<Row> logs = null;\n \n-  @Before\n-  public void setupTable() throws Exception {\n+  /*\n+  Use the Hive Based table to make Identity Partition Columns with no duplication of the data in the underlying\n+  parquet files. This makes sure that if the identity mapping fails, the test will also fail.\n+   */\n+  private void setupParquet() throws Exception {\n     File location = temp.newFolder(\"logs\");\n+    File hiveLocation = temp.newFolder(\"hive\");\n+    String hiveTable = \"hivetable\";\n     Assert.assertTrue(\"Temp folder should exist\", location.exists());\n \n     Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);\n-    this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n     this.logs = spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", hiveTable));\n+    logs.orderBy(\"date\", \"level\", \"id\").write().partitionBy(\"date\", \"level\").format(\"parquet\")\n+        .option(\"path\", hiveLocation.toString()).saveAsTable(hiveTable);\n+\n+    this.table = TABLES.create(SparkSchemaUtil.schemaForTable(spark, hiveTable),\n+        SparkSchemaUtil.specForTable(spark, hiveTable), properties, location.toString());\n+\n+    SparkTableUtil.importSparkTable(spark, new TableIdentifier(hiveTable), table, location.toString());\n+  }\n \n-    logs.orderBy(\"date\", \"level\", \"id\").write().format(\"iceberg\").mode(\"append\").save(location.toString());\n+  @Before\n+  public void setupTable() throws Exception {\n+    if (format.equals(\"parquet\")) {\n+      setupParquet();\n+    } else {\n+      File location = temp.newFolder(\"logs\");\n+      Assert.assertTrue(\"Temp folder should exist\", location.exists());\n+\n+      Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);\n+      this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n+      this.logs = spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n+\n+      logs.orderBy(\"date\", \"level\", \"id\").write().format(\"iceberg\").mode(\"append\").save(location.toString());\n+    }\n   }\n \n   @Test\n   public void testFullProjection() {\n     List<Row> expected = logs.orderBy(\"id\").collectAsList();\n     List<Row> actual = spark.read().format(\"iceberg\")\n         .option(\"vectorization-enabled\", String.valueOf(vectorized))\n-        .load(table.location()).orderBy(\"id\").collectAsList();\n+        .load(table.location()).orderBy(\"id\")\n+        .select(\"id\", \"date\", \"level\", \"message\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU2OTQ4OQ=="}, "originalCommit": {"oid": "71fe66885ce7099230441d4312e2c1e672c61d1c"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkwMTYxMzMyOnYy", "diffSide": "RIGHT", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxODo1MDoxMFrOG7E4-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMDoyNzoyMlrOG7HnmQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU5OTI4OQ==", "bodyText": "VectorHolder.constantHolder and DummyVectorHolder aren't parameterized. It would make sense to have them accept value of type T as well.", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464599289", "createdAt": "2020-08-03T18:50:10Z", "author": {"login": "samarthjain"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -356,5 +356,36 @@ public String toString() {\n     public void setBatchSize(int batchSize) {}\n   }\n \n+  /**\n+   * A Dummy Vector Reader which doesn't actually read files, instead it returns a dummy\n+   * VectorHolder which indicates the constant value which should be used for this column.\n+   * @param <T> The constant value to use\n+   */\n+  public static class ConstantVectorReader<T> extends VectorizedArrowReader {\n+    private final T value;\n+\n+    public ConstantVectorReader(T value) {\n+      this.value = value;\n+    }\n+\n+    @Override\n+    public VectorHolder read(VectorHolder reuse, int numValsToRead) {\n+      return VectorHolder.constantHolder(numValsToRead, value);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "71fe66885ce7099230441d4312e2c1e672c61d1c"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDYwNzU0OQ==", "bodyText": "Of course, I was thinking about that but I got a little worried about the \"null\" version since I felt a little weird about creating a parameterized type with null. I forgot how Java handles that but I'll double check and change the class if possible. IE what happens with\nfoo<t> ( T bar) when you invoke foo(null)", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464607549", "createdAt": "2020-08-03T19:07:13Z", "author": {"login": "RussellSpitzer"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -356,5 +356,36 @@ public String toString() {\n     public void setBatchSize(int batchSize) {}\n   }\n \n+  /**\n+   * A Dummy Vector Reader which doesn't actually read files, instead it returns a dummy\n+   * VectorHolder which indicates the constant value which should be used for this column.\n+   * @param <T> The constant value to use\n+   */\n+  public static class ConstantVectorReader<T> extends VectorizedArrowReader {\n+    private final T value;\n+\n+    public ConstantVectorReader(T value) {\n+      this.value = value;\n+    }\n+\n+    @Override\n+    public VectorHolder read(VectorHolder reuse, int numValsToRead) {\n+      return VectorHolder.constantHolder(numValsToRead, value);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU5OTI4OQ=="}, "originalCommit": {"oid": "71fe66885ce7099230441d4312e2c1e672c61d1c"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY0Mzk5Mw==", "bodyText": "If it is always null, you can use the Void type.", "url": "https://github.com/apache/iceberg/pull/1287#discussion_r464643993", "createdAt": "2020-08-03T20:27:22Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -356,5 +356,36 @@ public String toString() {\n     public void setBatchSize(int batchSize) {}\n   }\n \n+  /**\n+   * A Dummy Vector Reader which doesn't actually read files, instead it returns a dummy\n+   * VectorHolder which indicates the constant value which should be used for this column.\n+   * @param <T> The constant value to use\n+   */\n+  public static class ConstantVectorReader<T> extends VectorizedArrowReader {\n+    private final T value;\n+\n+    public ConstantVectorReader(T value) {\n+      this.value = value;\n+    }\n+\n+    @Override\n+    public VectorHolder read(VectorHolder reuse, int numValsToRead) {\n+      return VectorHolder.constantHolder(numValsToRead, value);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU5OTI4OQ=="}, "originalCommit": {"oid": "71fe66885ce7099230441d4312e2c1e672c61d1c"}, "originalPosition": 18}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3826, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}