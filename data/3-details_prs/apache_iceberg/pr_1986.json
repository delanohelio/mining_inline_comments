{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQ1NjkyNDMx", "number": 1986, "title": "Spark: Add a rule to align updates in MERGE operations", "bodyText": "This PR adds a rule that aligns assignments in MERGE operations with target table columns.", "createdAt": "2020-12-26T13:13:04Z", "url": "https://github.com/apache/iceberg/pull/1986", "merged": true, "mergeCommit": {"oid": "74bb38d810a0e046caa47e771804d4d14c06ff57"}, "closed": true, "closedAt": "2021-01-07T01:24:58Z", "author": {"login": "aokolnychyi"}, "timelineItems": {"totalCount": 37, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdp9AslABqjQxNDk2MTEyNzU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdtpx5RAFqTU2MzE1MjA3Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9ce69b6b2605d2dfdf90ea067c38a76dc580b79c", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/9ce69b6b2605d2dfdf90ea067c38a76dc580b79c", "committedDate": "2020-12-26T13:05:25Z", "message": "Spark: Add a rule to align updates in MERGE operations"}, "afterCommit": {"oid": "84a4d5e619edabf8baf373a73ee6b00b710681e7", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/84a4d5e619edabf8baf373a73ee6b00b710681e7", "committedDate": "2020-12-26T13:32:50Z", "message": "Spark: Add a rule to align updates in MERGE operations"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "84a4d5e619edabf8baf373a73ee6b00b710681e7", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/84a4d5e619edabf8baf373a73ee6b00b710681e7", "committedDate": "2020-12-26T13:32:50Z", "message": "Spark: Add a rule to align updates in MERGE operations"}, "afterCommit": {"oid": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/54e5f1530d8b1aa3dd83be96dc812ee435a37e5c", "committedDate": "2020-12-28T09:25:04Z", "message": "Spark: Add a rule to align updates in MERGE operations"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5MzQ5MTI5", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-559349129", "createdAt": "2020-12-29T01:27:02Z", "commit": {"oid": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMToyNzowMlrOIMFCMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMToyNzowMlrOIMFCMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzNjMwNQ==", "bodyText": "Can you be more specific about \"inspired by\"? Did you use the code from Delta as a reference? Did you use the same test cases?", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549536305", "createdAt": "2020-12-29T01:27:02Z", "author": {"login": "rdblue"}, "path": "LICENSE", "diffHunk": "@@ -289,3 +289,12 @@ Copyright: 2011-2018 The Apache Software Foundation\n Home page: https://spark.apache.org/\n License: https://www.apache.org/licenses/LICENSE-2.0\n \n+--------------------------------------------------------------------------------\n+\n+This product includes code from Delta Lake.\n+\n+* AssignmentAlignmentSupport is an independent development but was inspired by an equivalent rule in Delta.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c"}, "originalPosition": 8}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5MzQ5NDAz", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-559349403", "createdAt": "2020-12-29T01:29:16Z", "commit": {"oid": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMToyOToxNlrOIMFDhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMToyOToxNlrOIMFDhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzNjY0NA==", "bodyText": "Nit: I would probably add a newline before this case since the last block was so large.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549536644", "createdAt": "2020-12-29T01:29:16Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))\n+        case d: DeleteAction => d\n+      }\n+\n+      val alignedNotMatchedActions = m.notMatchedActions.map {\n+        case i @ InsertAction(_, assignments) =>\n+          // check no nested columns are present\n+          val namePartsSeq = assignments.map(_.key).map(getNameParts)\n+          namePartsSeq.foreach { nameParts =>\n+            if (nameParts.size > 1) {\n+              throw new AnalysisException(\n+                \"Nested fields are not supported inside INSERT clauses of MERGE operations: \" +\n+                s\"${nameParts.mkString(\"`\", \"`.`\", \"`\")}\")\n+            }\n+          }\n+\n+          val colNames = namePartsSeq.map(_.head)\n+\n+          // check there are no duplicates\n+          val duplicateColNames = colNames.groupBy(identity).collect {\n+            case (name, matchingNames) if matchingNames.size > 1 => name\n+          }\n+\n+          if (duplicateColNames.nonEmpty) {\n+            throw new AnalysisException(\n+              \"Duplicate column names inside INSERT clause: \" +\n+              duplicateColNames.mkString(\"[\", \",\", \"]\"))\n+          }\n+\n+          // reorder assignments by the target table column order\n+          i.copy(assignments = alignInsertActionAssignments(m.targetTable, colNames, assignments))\n+        case _ =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c"}, "originalPosition": 64}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5MzUwMjc0", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-559350274", "createdAt": "2020-12-29T01:35:41Z", "commit": {"oid": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMTozNTo0MlrOIMFHMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMTozNTo0MlrOIMFHMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzNzU4NA==", "bodyText": "What about getInsertReference or asInsertReference? Seems like this is actually quite specific to the references that we expect in the left side of an INSERT clause.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549537584", "createdAt": "2020-12-29T01:35:42Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AttributeReference, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.types.{DataType, NullType, StructField, StructType}\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(nameParts: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(getNameParts(a.key), a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.nameParts.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(update.expr, col.dataType, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.nameParts).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(nameParts = u.nameParts.tail))\n+    val updatedFieldExprs = applyUpdates(fieldExprs, newUpdates, resolver, namePrefix :+ col.name)\n+\n+    // construct a new struct with updated field expressions\n+    toNamedStruct(fields, updatedFieldExprs)\n+  }\n+\n+  private def toNamedStruct(fields: Seq[StructField], fieldExprs: Seq[Expression]): Expression = {\n+    val namedStructExprs = fields.zip(fieldExprs).flatMap { case (field, expr) =>\n+      Seq(Literal(field.name), expr)\n+    }\n+    CreateNamedStruct(namedStructExprs)\n+  }\n+\n+  private def hasExactMatch(\n+      updates: Seq[ColumnUpdate],\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    updates.exists(assignment => isExactMatch(assignment, col, resolver))\n+  }\n+\n+  private def isExactMatch(\n+      update: ColumnUpdate,\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    update.nameParts match {\n+      case Seq(namePart) if resolver(namePart, col.name) => true\n+      case _ => false\n+    }\n+  }\n+\n+  protected def castIfNeeded(\n+      expr: Expression,\n+      dataType: DataType,\n+      resolver: Resolver): Expression = expr match {\n+    // some types cannot be casted from NullType (e.g. StructType)\n+    case Literal(value, NullType) => Literal(value, dataType)\n+    case _ =>\n+      (expr.dataType, dataType) match {\n+        // resolve structs by name if they they have the same number of fields and their names match\n+        // e.g., it is ok to set a struct with fields (a, b) as another struct with fields (b, a)\n+        // it is invalid to a set a struct with fields (a, d) as another struct with fields (a, b)\n+        case (from: StructType, to: StructType) if requiresCast(from, to) && isValidCast(from, to, resolver) =>\n+          val fieldExprs = to.flatMap { field =>\n+            val fieldName = Literal(field.name)\n+            val fieldExpr = ExtractValue(expr, fieldName, resolver)\n+            Seq(fieldName, castIfNeeded(fieldExpr, field.dataType, resolver))\n+          }\n+          CreateNamedStruct(fieldExprs)\n+        case (from: StructType, to: StructType) if requiresCast(from, to) =>\n+          throw new AnalysisException(s\"Invalid assignments: cannot cast $from to $to\")\n+        case (from, to) if requiresCast(from, to) => cast(expr, dataType)\n+        case _ => expr\n+      }\n+  }\n+\n+  private def requiresCast(from: DataType, to: DataType): Boolean = {\n+    !DataType.equalsIgnoreCaseAndNullability(from, to)\n+  }\n+\n+  private def isValidCast(from: StructType, to: StructType, resolver: Resolver): Boolean = {\n+    from.length == to.length && from.exists { f => to.exists(t => resolver(f.name, t.name))}\n+  }\n+\n+  protected def getNameParts(expr: Expression): Seq[String] = expr match {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c"}, "originalPosition": 184}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5MzUwNTA2", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-559350506", "createdAt": "2020-12-29T01:37:17Z", "commit": {"oid": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMTozNzoxN1rOIMFIWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMTozNzoxN1rOIMFIWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzNzg4Mw==", "bodyText": "Why add [ and ]? Those seem distracting to me. I'd also add a space after , to make it more readable.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549537883", "createdAt": "2020-12-29T01:37:17Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))\n+        case d: DeleteAction => d\n+      }\n+\n+      val alignedNotMatchedActions = m.notMatchedActions.map {\n+        case i @ InsertAction(_, assignments) =>\n+          // check no nested columns are present\n+          val namePartsSeq = assignments.map(_.key).map(getNameParts)\n+          namePartsSeq.foreach { nameParts =>\n+            if (nameParts.size > 1) {\n+              throw new AnalysisException(\n+                \"Nested fields are not supported inside INSERT clauses of MERGE operations: \" +\n+                s\"${nameParts.mkString(\"`\", \"`.`\", \"`\")}\")\n+            }\n+          }\n+\n+          val colNames = namePartsSeq.map(_.head)\n+\n+          // check there are no duplicates\n+          val duplicateColNames = colNames.groupBy(identity).collect {\n+            case (name, matchingNames) if matchingNames.size > 1 => name\n+          }\n+\n+          if (duplicateColNames.nonEmpty) {\n+            throw new AnalysisException(\n+              \"Duplicate column names inside INSERT clause: \" +\n+              duplicateColNames.mkString(\"[\", \",\", \"]\"))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5MzUxMTg3", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-559351187", "createdAt": "2020-12-29T01:42:17Z", "commit": {"oid": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMTo0MjoxN1rOIMFLEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMTo0MjoxN1rOIMFLEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzODU3OQ==", "bodyText": "This assumes that the rewrite was needed, but we expect the MergeIntoTable plan to be resolved after this rule runs. As a result, this rule will always produce a new MergeIntoTable instance and fastEquals used by the rule executor will always compare the full tree.\nThis shouldn't affect correctness, but I think it is a best practice to detect whether anything has changed and avoid creating a new node if it is the same.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549538579", "createdAt": "2020-12-29T01:42:17Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c"}, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5MzUxNzQz", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-559351743", "createdAt": "2020-12-29T01:46:19Z", "commit": {"oid": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMTo0NjoxOVrOIMFNhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMTo0NjoxOVrOIMFNhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzOTIwNw==", "bodyText": "This could avoid calling getNameParts by creating a map out of insertCols and assignments:\nval assignmentMap = insertCols.zip(assigments).toMap\n\nThat would be better since this is going to call getNameParts for every assignment until a matching one is found.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549539207", "createdAt": "2020-12-29T01:46:19Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))\n+        case d: DeleteAction => d\n+      }\n+\n+      val alignedNotMatchedActions = m.notMatchedActions.map {\n+        case i @ InsertAction(_, assignments) =>\n+          // check no nested columns are present\n+          val namePartsSeq = assignments.map(_.key).map(getNameParts)\n+          namePartsSeq.foreach { nameParts =>\n+            if (nameParts.size > 1) {\n+              throw new AnalysisException(\n+                \"Nested fields are not supported inside INSERT clauses of MERGE operations: \" +\n+                s\"${nameParts.mkString(\"`\", \"`.`\", \"`\")}\")\n+            }\n+          }\n+\n+          val colNames = namePartsSeq.map(_.head)\n+\n+          // check there are no duplicates\n+          val duplicateColNames = colNames.groupBy(identity).collect {\n+            case (name, matchingNames) if matchingNames.size > 1 => name\n+          }\n+\n+          if (duplicateColNames.nonEmpty) {\n+            throw new AnalysisException(\n+              \"Duplicate column names inside INSERT clause: \" +\n+              duplicateColNames.mkString(\"[\", \",\", \"]\"))\n+          }\n+\n+          // reorder assignments by the target table column order\n+          i.copy(assignments = alignInsertActionAssignments(m.targetTable, colNames, assignments))\n+        case _ =>\n+          throw new AnalysisException(\"Not matched actions can only contain INSERT\")\n+      }\n+\n+      m.copy(matchedActions = alignedMatchedActions, notMatchedActions = alignedNotMatchedActions)\n+  }\n+\n+  private def alignInsertActionAssignments(\n+      targetTable: LogicalPlan,\n+      insertCols: Seq[String],\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val resolver = conf.resolver\n+    targetTable.output.map { targetAttr =>\n+      val assignment = assignments.find(a => resolver(targetAttr.name, getNameParts(a.key).head))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c"}, "originalPosition": 78}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5MzUyNjk2", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-559352696", "createdAt": "2020-12-29T01:53:23Z", "commit": {"oid": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMTo1MzoyM1rOIMFRYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwMTo1MzoyM1rOIMFRYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU0MDE5NA==", "bodyText": "Why not cast directly to the output column's data type?\nSeems like this should cast to the types from MergeIntoTable.output so that all of the assignments produce the same type. Are all of the assignment keys for a column guaranteed to have the same output type?\nAlso, if column resolution adds a projection with a cast to the a table output type, can these expressions be optimized to remove the duplicate cast? If not, we may want to cast directly to the table's type and update the output of the MergeIntoTable node to be identical to the table so column resolution is done here rather than in the normal write resolution rule.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549540194", "createdAt": "2020-12-29T01:53:23Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))\n+        case d: DeleteAction => d\n+      }\n+\n+      val alignedNotMatchedActions = m.notMatchedActions.map {\n+        case i @ InsertAction(_, assignments) =>\n+          // check no nested columns are present\n+          val namePartsSeq = assignments.map(_.key).map(getNameParts)\n+          namePartsSeq.foreach { nameParts =>\n+            if (nameParts.size > 1) {\n+              throw new AnalysisException(\n+                \"Nested fields are not supported inside INSERT clauses of MERGE operations: \" +\n+                s\"${nameParts.mkString(\"`\", \"`.`\", \"`\")}\")\n+            }\n+          }\n+\n+          val colNames = namePartsSeq.map(_.head)\n+\n+          // check there are no duplicates\n+          val duplicateColNames = colNames.groupBy(identity).collect {\n+            case (name, matchingNames) if matchingNames.size > 1 => name\n+          }\n+\n+          if (duplicateColNames.nonEmpty) {\n+            throw new AnalysisException(\n+              \"Duplicate column names inside INSERT clause: \" +\n+              duplicateColNames.mkString(\"[\", \",\", \"]\"))\n+          }\n+\n+          // reorder assignments by the target table column order\n+          i.copy(assignments = alignInsertActionAssignments(m.targetTable, colNames, assignments))\n+        case _ =>\n+          throw new AnalysisException(\"Not matched actions can only contain INSERT\")\n+      }\n+\n+      m.copy(matchedActions = alignedMatchedActions, notMatchedActions = alignedNotMatchedActions)\n+  }\n+\n+  private def alignInsertActionAssignments(\n+      targetTable: LogicalPlan,\n+      insertCols: Seq[String],\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val resolver = conf.resolver\n+    targetTable.output.map { targetAttr =>\n+      val assignment = assignments.find(a => resolver(targetAttr.name, getNameParts(a.key).head))\n+      if (assignment.isEmpty) {\n+        throw new AnalysisException(\n+          s\"Cannot find column '${targetAttr.name}' of the target table among \" +\n+          s\"the INSERT columns: ${insertCols.mkString(\", \")}. \" +\n+          \"INSERT clauses must provide values for all columns of the target table.\")\n+      }\n+\n+      val key = assignment.get.key\n+      val value = assignment.get.value\n+      Assignment(key, castIfNeeded(value, key.dataType, resolver))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c"}, "originalPosition": 88}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5NDU3NzYw", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-559457760", "createdAt": "2020-12-29T09:55:04Z", "commit": {"oid": "2f0f1303fb0f96512736a553c1efe8c9e5b13552"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwOTo1NTowNFrOIMLejw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwOTo1NTowNFrOIMLejw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTY0MTg3MQ==", "bodyText": "Here is what I came up with, @rdblue.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549641871", "createdAt": "2020-12-29T09:55:04Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))\n+        case d: DeleteAction => d\n+      }\n+\n+      val alignedNotMatchedActions = m.notMatchedActions.map {\n+        case i @ InsertAction(_, assignments) =>\n+          // check no nested columns are present\n+          val namePartsSeq = assignments.map(_.key).map(getNameParts)\n+          namePartsSeq.foreach { nameParts =>\n+            if (nameParts.size > 1) {\n+              throw new AnalysisException(\n+                \"Nested fields are not supported inside INSERT clauses of MERGE operations: \" +\n+                s\"${nameParts.mkString(\"`\", \"`.`\", \"`\")}\")\n+            }\n+          }\n+\n+          val colNames = namePartsSeq.map(_.head)\n+\n+          // check there are no duplicates\n+          val duplicateColNames = colNames.groupBy(identity).collect {\n+            case (name, matchingNames) if matchingNames.size > 1 => name\n+          }\n+\n+          if (duplicateColNames.nonEmpty) {\n+            throw new AnalysisException(\n+              s\"Duplicate column names inside INSERT clause: ${duplicateColNames.mkString(\", \")}\")\n+          }\n+\n+          // reorder assignments by the target table column order\n+          i.copy(assignments = alignInsertActionAssignments(m.targetTable, colNames, assignments))\n+\n+        case _ =>\n+          throw new AnalysisException(\"Not matched actions can only contain INSERT\")\n+      }\n+\n+      m.copy(matchedActions = alignedMatchedActions, notMatchedActions = alignedNotMatchedActions)\n+  }\n+\n+  private def alignInsertActionAssignments(\n+      targetTable: LogicalPlan,\n+      insertCols: Seq[String],\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val resolver = conf.resolver\n+\n+    // init a map of assignments to avoid calling getNameParts many times\n+    val assignmentMap = assignments.map(a => getNameParts(a.key).head -> a).toMap", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f0f1303fb0f96512736a553c1efe8c9e5b13552"}, "originalPosition": 79}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5NTE5Njg5", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-559519689", "createdAt": "2020-12-29T12:54:07Z", "commit": {"oid": "2f0f1303fb0f96512736a553c1efe8c9e5b13552"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQxMjo1NDowN1rOIMOpnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQxMjo1NDowN1rOIMOpnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTY5Mzg1Mg==", "bodyText": "This one is redundant, I'll remove it.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549693852", "createdAt": "2020-12-29T12:54:07Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java", "diffHunk": "@@ -0,0 +1,204 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+public abstract class TestMerge extends SparkRowLevelOperationsTestBase {\n+\n+  public TestMerge(String catalogName, String implementation, Map<String, String> config,\n+                   String fileFormat, boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected abstract Map<String, String> extraTableProperties();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f0f1303fb0f96512736a553c1efe8c9e5b13552"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5NjI3MTUx", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-559627151", "createdAt": "2020-12-29T17:18:00Z", "commit": {"oid": "2f0f1303fb0f96512736a553c1efe8c9e5b13552"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQxNzoxODowMFrOIMUI3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQxNzoxODowMFrOIMUI3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTc4Mzc3Mw==", "bodyText": "I think would be a bit easier to read this code if the isValidCast check were in a single case for structs. That way you don't have to compare the case clauses to see what's different. It also avoids running requiresCast twice.\n  if (!isValidCast(from, to resolver)) {\n    throw new AnalysisException(...)\n  }\n  ...", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549783773", "createdAt": "2020-12-29T17:18:00Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AttributeReference, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.types.{DataType, NullType, StructField, StructType}\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(nameParts: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(getNameParts(a.key), a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.nameParts.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(update.expr, col.dataType, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.nameParts).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(nameParts = u.nameParts.tail))\n+    val updatedFieldExprs = applyUpdates(fieldExprs, newUpdates, resolver, namePrefix :+ col.name)\n+\n+    // construct a new struct with updated field expressions\n+    toNamedStruct(fields, updatedFieldExprs)\n+  }\n+\n+  private def toNamedStruct(fields: Seq[StructField], fieldExprs: Seq[Expression]): Expression = {\n+    val namedStructExprs = fields.zip(fieldExprs).flatMap { case (field, expr) =>\n+      Seq(Literal(field.name), expr)\n+    }\n+    CreateNamedStruct(namedStructExprs)\n+  }\n+\n+  private def hasExactMatch(\n+      updates: Seq[ColumnUpdate],\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    updates.exists(assignment => isExactMatch(assignment, col, resolver))\n+  }\n+\n+  private def isExactMatch(\n+      update: ColumnUpdate,\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    update.nameParts match {\n+      case Seq(namePart) if resolver(namePart, col.name) => true\n+      case _ => false\n+    }\n+  }\n+\n+  protected def castIfNeeded(\n+      expr: Expression,\n+      dataType: DataType,\n+      resolver: Resolver): Expression = expr match {\n+    // some types cannot be casted from NullType (e.g. StructType)\n+    case Literal(value, NullType) => Literal(value, dataType)\n+    case _ =>\n+      (expr.dataType, dataType) match {\n+        // resolve structs by name if they they have the same number of fields and their names match\n+        // e.g., it is ok to set a struct with fields (a, b) as another struct with fields (b, a)\n+        // it is invalid to a set a struct with fields (a, d) as another struct with fields (a, b)\n+        case (from: StructType, to: StructType) if requiresCast(from, to) && isValidCast(from, to, resolver) =>\n+          val fieldExprs = to.flatMap { field =>\n+            val fieldName = Literal(field.name)\n+            val fieldExpr = ExtractValue(expr, fieldName, resolver)\n+            Seq(fieldName, castIfNeeded(fieldExpr, field.dataType, resolver))\n+          }\n+          CreateNamedStruct(fieldExprs)\n+        case (from: StructType, to: StructType) if requiresCast(from, to) =>\n+          throw new AnalysisException(s\"Invalid assignments: cannot cast $from to $to\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f0f1303fb0f96512736a553c1efe8c9e5b13552"}, "originalPosition": 170}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5NjI5ODY0", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-559629864", "createdAt": "2020-12-29T17:25:40Z", "commit": {"oid": "2f0f1303fb0f96512736a553c1efe8c9e5b13552"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQxNzoyNTo0MFrOIMUSHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQxNzoyNTo0MFrOIMUSHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTc4NjE0Mg==", "bodyText": "This inserts a cast regardless of whether it is safe to cast. There is also now an assignment policy that changes which casts should be inserted. I think this should apply the same logic as the table output resolver, just using the assignment names. What do you think of the logic in TableOutputResolver.checkField?", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549786142", "createdAt": "2020-12-29T17:25:40Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AttributeReference, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.types.{DataType, NullType, StructField, StructType}\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(nameParts: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(getNameParts(a.key), a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.nameParts.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(update.expr, col.dataType, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.nameParts).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(nameParts = u.nameParts.tail))\n+    val updatedFieldExprs = applyUpdates(fieldExprs, newUpdates, resolver, namePrefix :+ col.name)\n+\n+    // construct a new struct with updated field expressions\n+    toNamedStruct(fields, updatedFieldExprs)\n+  }\n+\n+  private def toNamedStruct(fields: Seq[StructField], fieldExprs: Seq[Expression]): Expression = {\n+    val namedStructExprs = fields.zip(fieldExprs).flatMap { case (field, expr) =>\n+      Seq(Literal(field.name), expr)\n+    }\n+    CreateNamedStruct(namedStructExprs)\n+  }\n+\n+  private def hasExactMatch(\n+      updates: Seq[ColumnUpdate],\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    updates.exists(assignment => isExactMatch(assignment, col, resolver))\n+  }\n+\n+  private def isExactMatch(\n+      update: ColumnUpdate,\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    update.nameParts match {\n+      case Seq(namePart) if resolver(namePart, col.name) => true\n+      case _ => false\n+    }\n+  }\n+\n+  protected def castIfNeeded(\n+      expr: Expression,\n+      dataType: DataType,\n+      resolver: Resolver): Expression = expr match {\n+    // some types cannot be casted from NullType (e.g. StructType)\n+    case Literal(value, NullType) => Literal(value, dataType)\n+    case _ =>\n+      (expr.dataType, dataType) match {\n+        // resolve structs by name if they they have the same number of fields and their names match\n+        // e.g., it is ok to set a struct with fields (a, b) as another struct with fields (b, a)\n+        // it is invalid to a set a struct with fields (a, d) as another struct with fields (a, b)\n+        case (from: StructType, to: StructType) if requiresCast(from, to) && isValidCast(from, to, resolver) =>\n+          val fieldExprs = to.flatMap { field =>\n+            val fieldName = Literal(field.name)\n+            val fieldExpr = ExtractValue(expr, fieldName, resolver)\n+            Seq(fieldName, castIfNeeded(fieldExpr, field.dataType, resolver))\n+          }\n+          CreateNamedStruct(fieldExprs)\n+        case (from: StructType, to: StructType) if requiresCast(from, to) =>\n+          throw new AnalysisException(s\"Invalid assignments: cannot cast $from to $to\")\n+        case (from, to) if requiresCast(from, to) => cast(expr, dataType)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f0f1303fb0f96512736a553c1efe8c9e5b13552"}, "originalPosition": 171}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5NjMzMTAw", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-559633100", "createdAt": "2020-12-29T17:35:03Z", "commit": {"oid": "2f0f1303fb0f96512736a553c1efe8c9e5b13552"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQxNzozNTowNFrOIMUdbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQxNzozNTowNFrOIMUdbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTc4OTAzOQ==", "bodyText": "This means that assignments within a struct are always done by name and never by position? In table output resolution, it looks like structs must match the expected order and if writing by name then the names must match. I would probably use the same logic as in canWrite.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549789039", "createdAt": "2020-12-29T17:35:04Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AttributeReference, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.types.{DataType, NullType, StructField, StructType}\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(nameParts: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(getNameParts(a.key), a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.nameParts.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(update.expr, col.dataType, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.nameParts).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(nameParts = u.nameParts.tail))\n+    val updatedFieldExprs = applyUpdates(fieldExprs, newUpdates, resolver, namePrefix :+ col.name)\n+\n+    // construct a new struct with updated field expressions\n+    toNamedStruct(fields, updatedFieldExprs)\n+  }\n+\n+  private def toNamedStruct(fields: Seq[StructField], fieldExprs: Seq[Expression]): Expression = {\n+    val namedStructExprs = fields.zip(fieldExprs).flatMap { case (field, expr) =>\n+      Seq(Literal(field.name), expr)\n+    }\n+    CreateNamedStruct(namedStructExprs)\n+  }\n+\n+  private def hasExactMatch(\n+      updates: Seq[ColumnUpdate],\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    updates.exists(assignment => isExactMatch(assignment, col, resolver))\n+  }\n+\n+  private def isExactMatch(\n+      update: ColumnUpdate,\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    update.nameParts match {\n+      case Seq(namePart) if resolver(namePart, col.name) => true\n+      case _ => false\n+    }\n+  }\n+\n+  protected def castIfNeeded(\n+      expr: Expression,\n+      dataType: DataType,\n+      resolver: Resolver): Expression = expr match {\n+    // some types cannot be casted from NullType (e.g. StructType)\n+    case Literal(value, NullType) => Literal(value, dataType)\n+    case _ =>\n+      (expr.dataType, dataType) match {\n+        // resolve structs by name if they they have the same number of fields and their names match\n+        // e.g., it is ok to set a struct with fields (a, b) as another struct with fields (b, a)\n+        // it is invalid to a set a struct with fields (a, d) as another struct with fields (a, b)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f0f1303fb0f96512736a553c1efe8c9e5b13552"}, "originalPosition": 161}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYwMDYzOTg2", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-560063986", "createdAt": "2020-12-30T14:14:45Z", "commit": {"oid": "35180160909b685a2b3ac070aafda2ddaabc91b4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDoxNDo0NVrOIMuEmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDoxNDo0NVrOIMuEmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIwODY2Nw==", "bodyText": "We build the map a bit earlier now.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550208667", "createdAt": "2020-12-30T14:14:45Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))\n+        case d: DeleteAction => d\n+      }\n+\n+      val alignedNotMatchedActions = m.notMatchedActions.map {\n+        case i @ InsertAction(_, assignments) =>\n+          // check no nested columns are present\n+          val refs = assignments.map(_.key).map(asAssignmentReference)\n+          refs.foreach { ref =>\n+            if (ref.size > 1) {\n+              throw new AnalysisException(\n+                \"Nested fields are not supported inside INSERT clauses of MERGE operations: \" +\n+                s\"${ref.mkString(\"`\", \"`.`\", \"`\")}\")\n+            }\n+          }\n+\n+          val colNames = refs.map(_.head)\n+\n+          // check there are no duplicates\n+          val duplicateColNames = colNames.groupBy(identity).collect {\n+            case (name, matchingNames) if matchingNames.size > 1 => name\n+          }\n+\n+          if (duplicateColNames.nonEmpty) {\n+            throw new AnalysisException(\n+              s\"Duplicate column names inside INSERT clause: ${duplicateColNames.mkString(\", \")}\")\n+          }\n+\n+          // reorder assignments by the target table column order\n+          val assignmentMap = colNames.zip(assignments).toMap", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35180160909b685a2b3ac070aafda2ddaabc91b4"}, "originalPosition": 62}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYwMDY0NDA2", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-560064406", "createdAt": "2020-12-30T14:15:51Z", "commit": {"oid": "35180160909b685a2b3ac070aafda2ddaabc91b4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDoxNTo1MVrOIMuGCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDoxNTo1MVrOIMuGCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIwOTAzMg==", "bodyText": "I've reworked castIfNeeded completely. Now, it is closer to checkField in TableOutputResolver.\nI should probably add this to the licence.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550209032", "createdAt": "2020-12-30T14:15:51Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AnsiCast, AttributeReference, Cast, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import scala.collection.mutable\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(ref: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(a.key, a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.ref.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(col, update.expr, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.ref).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(ref = u.ref.tail))\n+    val updatedFieldExprs = applyUpdates(fieldExprs, newUpdates, resolver, namePrefix :+ col.name)\n+\n+    // construct a new struct with updated field expressions\n+    toNamedStruct(fields, updatedFieldExprs)\n+  }\n+\n+  private def toNamedStruct(fields: Seq[StructField], fieldExprs: Seq[Expression]): Expression = {\n+    val namedStructExprs = fields.zip(fieldExprs).flatMap { case (field, expr) =>\n+      Seq(Literal(field.name), expr)\n+    }\n+    CreateNamedStruct(namedStructExprs)\n+  }\n+\n+  private def hasExactMatch(\n+      updates: Seq[ColumnUpdate],\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    updates.exists(assignment => isExactMatch(assignment, col, resolver))\n+  }\n+\n+  private def isExactMatch(\n+      update: ColumnUpdate,\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    update.ref match {\n+      case Seq(namePart) if resolver(namePart, col.name) => true\n+      case _ => false\n+    }\n+  }\n+\n+  protected def castIfNeeded(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35180160909b685a2b3ac070aafda2ddaabc91b4"}, "originalPosition": 153}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYwMDY0NjM3", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-560064637", "createdAt": "2020-12-30T14:16:31Z", "commit": {"oid": "35180160909b685a2b3ac070aafda2ddaabc91b4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDoxNjozMVrOIMuGxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDoxNjozMVrOIMuGxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIwOTIyMQ==", "bodyText": "Now we should have proper validation for writing nullable values to non-null columns.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550209221", "createdAt": "2020-12-30T14:16:31Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AnsiCast, AttributeReference, Cast, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import scala.collection.mutable\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(ref: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(a.key, a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.ref.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(col, update.expr, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.ref).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(ref = u.ref.tail))\n+    val updatedFieldExprs = applyUpdates(fieldExprs, newUpdates, resolver, namePrefix :+ col.name)\n+\n+    // construct a new struct with updated field expressions\n+    toNamedStruct(fields, updatedFieldExprs)\n+  }\n+\n+  private def toNamedStruct(fields: Seq[StructField], fieldExprs: Seq[Expression]): Expression = {\n+    val namedStructExprs = fields.zip(fieldExprs).flatMap { case (field, expr) =>\n+      Seq(Literal(field.name), expr)\n+    }\n+    CreateNamedStruct(namedStructExprs)\n+  }\n+\n+  private def hasExactMatch(\n+      updates: Seq[ColumnUpdate],\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    updates.exists(assignment => isExactMatch(assignment, col, resolver))\n+  }\n+\n+  private def isExactMatch(\n+      update: ColumnUpdate,\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    update.ref match {\n+      case Seq(namePart) if resolver(namePart, col.name) => true\n+      case _ => false\n+    }\n+  }\n+\n+  protected def castIfNeeded(\n+      tableAttr: NamedExpression,\n+      expr: Expression,\n+      resolver: Resolver): Expression = {\n+\n+    val storeAssignmentPolicy = conf.storeAssignmentPolicy\n+\n+    // run the type check and catch type errors\n+    storeAssignmentPolicy match {\n+      case StoreAssignmentPolicy.STRICT | StoreAssignmentPolicy.ANSI =>\n+        if (expr.nullable && !tableAttr.nullable) {\n+          throw new AnalysisException(\n+            s\"Cannot write nullable values to non-null column '${tableAttr.name}'\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35180160909b685a2b3ac070aafda2ddaabc91b4"}, "originalPosition": 165}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYwMDY0NzQ5", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-560064749", "createdAt": "2020-12-30T14:16:49Z", "commit": {"oid": "35180160909b685a2b3ac070aafda2ddaabc91b4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDoxNjo0OVrOIMuHMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDoxNjo0OVrOIMuHMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIwOTMzMQ==", "bodyText": "I made this implicit.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550209331", "createdAt": "2020-12-30T14:16:49Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AnsiCast, AttributeReference, Cast, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import scala.collection.mutable\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(ref: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(a.key, a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.ref.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(col, update.expr, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.ref).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(ref = u.ref.tail))\n+    val updatedFieldExprs = applyUpdates(fieldExprs, newUpdates, resolver, namePrefix :+ col.name)\n+\n+    // construct a new struct with updated field expressions\n+    toNamedStruct(fields, updatedFieldExprs)\n+  }\n+\n+  private def toNamedStruct(fields: Seq[StructField], fieldExprs: Seq[Expression]): Expression = {\n+    val namedStructExprs = fields.zip(fieldExprs).flatMap { case (field, expr) =>\n+      Seq(Literal(field.name), expr)\n+    }\n+    CreateNamedStruct(namedStructExprs)\n+  }\n+\n+  private def hasExactMatch(\n+      updates: Seq[ColumnUpdate],\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    updates.exists(assignment => isExactMatch(assignment, col, resolver))\n+  }\n+\n+  private def isExactMatch(\n+      update: ColumnUpdate,\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    update.ref match {\n+      case Seq(namePart) if resolver(namePart, col.name) => true\n+      case _ => false\n+    }\n+  }\n+\n+  protected def castIfNeeded(\n+      tableAttr: NamedExpression,\n+      expr: Expression,\n+      resolver: Resolver): Expression = {\n+\n+    val storeAssignmentPolicy = conf.storeAssignmentPolicy\n+\n+    // run the type check and catch type errors\n+    storeAssignmentPolicy match {\n+      case StoreAssignmentPolicy.STRICT | StoreAssignmentPolicy.ANSI =>\n+        if (expr.nullable && !tableAttr.nullable) {\n+          throw new AnalysisException(\n+            s\"Cannot write nullable values to non-null column '${tableAttr.name}'\")\n+        }\n+\n+        val errors = new mutable.ArrayBuffer[String]()\n+        val canWrite = DataType.canWrite(\n+          expr.dataType, tableAttr.dataType, byName = false, resolver, tableAttr.name,\n+          storeAssignmentPolicy, err => errors += err)\n+\n+        if (!canWrite) {\n+          throw new AnalysisException(s\"Cannot write incompatible data:\\n- ${errors.mkString(\"\\n- \")}\")\n+        }\n+\n+      case _ => // OK\n+    }\n+\n+    storeAssignmentPolicy match {\n+      case _ if tableAttr.dataType.sameType(expr.dataType) =>\n+        expr\n+      case StoreAssignmentPolicy.ANSI =>\n+        AnsiCast(expr, tableAttr.dataType, Option(conf.sessionLocalTimeZone))\n+      case _ =>\n+        Cast(expr, tableAttr.dataType, Option(conf.sessionLocalTimeZone))\n+    }\n+  }\n+\n+  implicit protected def asAssignmentReference(expr: Expression): Seq[String] = expr match {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35180160909b685a2b3ac070aafda2ddaabc91b4"}, "originalPosition": 190}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYwMDY1MDY0", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-560065064", "createdAt": "2020-12-30T14:17:41Z", "commit": {"oid": "35180160909b685a2b3ac070aafda2ddaabc91b4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDoxNzo0MVrOIMuILw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDoxNzo0MVrOIMuILw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIwOTU4Mw==", "bodyText": "I am going to address the last two TODOs before this PR is merged.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550209583", "createdAt": "2020-12-30T14:17:41Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+public abstract class TestMerge extends SparkRowLevelOperationsTestBase {\n+\n+  public TestMerge(String catalogName, String implementation, Map<String, String> config,\n+                   String fileFormat, boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS source\");\n+  }\n+\n+  // TODO: tests for reordering when operations succeed (both insert and update actions)\n+  // TODO: tests for action conditions\n+  // TODO: tests for writing nullable to not nullable, incompatible arrays, structs, atomic types", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35180160909b685a2b3ac070aafda2ddaabc91b4"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYwMDY2MzA3", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-560066307", "createdAt": "2020-12-30T14:20:41Z", "commit": {"oid": "35180160909b685a2b3ac070aafda2ddaabc91b4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDoyMDo0MVrOIMuLqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMFQxNDoyMDo0MVrOIMuLqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIxMDQ3Mw==", "bodyText": "Important: I am passing byName = false. We can pass byName = true if we wanted to throw an exception when a struct with fields (a, b) is assigned as a struct with fields (b, a). However, that would make it different compared to SQL INSERTs.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550210473", "createdAt": "2020-12-30T14:20:41Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AnsiCast, AttributeReference, Cast, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import scala.collection.mutable\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(ref: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(a.key, a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.ref.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(col, update.expr, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.ref).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(ref = u.ref.tail))\n+    val updatedFieldExprs = applyUpdates(fieldExprs, newUpdates, resolver, namePrefix :+ col.name)\n+\n+    // construct a new struct with updated field expressions\n+    toNamedStruct(fields, updatedFieldExprs)\n+  }\n+\n+  private def toNamedStruct(fields: Seq[StructField], fieldExprs: Seq[Expression]): Expression = {\n+    val namedStructExprs = fields.zip(fieldExprs).flatMap { case (field, expr) =>\n+      Seq(Literal(field.name), expr)\n+    }\n+    CreateNamedStruct(namedStructExprs)\n+  }\n+\n+  private def hasExactMatch(\n+      updates: Seq[ColumnUpdate],\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    updates.exists(assignment => isExactMatch(assignment, col, resolver))\n+  }\n+\n+  private def isExactMatch(\n+      update: ColumnUpdate,\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    update.ref match {\n+      case Seq(namePart) if resolver(namePart, col.name) => true\n+      case _ => false\n+    }\n+  }\n+\n+  protected def castIfNeeded(\n+      tableAttr: NamedExpression,\n+      expr: Expression,\n+      resolver: Resolver): Expression = {\n+\n+    val storeAssignmentPolicy = conf.storeAssignmentPolicy\n+\n+    // run the type check and catch type errors\n+    storeAssignmentPolicy match {\n+      case StoreAssignmentPolicy.STRICT | StoreAssignmentPolicy.ANSI =>\n+        if (expr.nullable && !tableAttr.nullable) {\n+          throw new AnalysisException(\n+            s\"Cannot write nullable values to non-null column '${tableAttr.name}'\")\n+        }\n+\n+        val errors = new mutable.ArrayBuffer[String]()\n+        val canWrite = DataType.canWrite(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35180160909b685a2b3ac070aafda2ddaabc91b4"}, "originalPosition": 169}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYwMzQ3MTky", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-560347192", "createdAt": "2020-12-31T13:09:22Z", "commit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMVQxMzowOToyMlrOIM-o9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMVQxMzowOToyMlrOIM-o9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDQ4MDExOA==", "bodyText": "I think we should set it to null instead of throwing an exception.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550480118", "createdAt": "2020-12-31T13:09:22Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))\n+        case d: DeleteAction => d\n+      }\n+\n+      val alignedNotMatchedActions = m.notMatchedActions.map {\n+        case i @ InsertAction(_, assignments) =>\n+          // check no nested columns are present\n+          val refs = assignments.map(_.key).map(asAssignmentReference)\n+          refs.foreach { ref =>\n+            if (ref.size > 1) {\n+              throw new AnalysisException(\n+                \"Nested fields are not supported inside INSERT clauses of MERGE operations: \" +\n+                s\"${ref.mkString(\"`\", \"`.`\", \"`\")}\")\n+            }\n+          }\n+\n+          val colNames = refs.map(_.head)\n+\n+          // check there are no duplicates\n+          val duplicateColNames = colNames.groupBy(identity).collect {\n+            case (name, matchingNames) if matchingNames.size > 1 => name\n+          }\n+\n+          if (duplicateColNames.nonEmpty) {\n+            throw new AnalysisException(\n+              s\"Duplicate column names inside INSERT clause: ${duplicateColNames.mkString(\", \")}\")\n+          }\n+\n+          // reorder assignments by the target table column order\n+          val assignmentMap = colNames.zip(assignments).toMap\n+          i.copy(assignments = alignInsertActionAssignments(m.targetTable, assignmentMap))\n+\n+        case _ =>\n+          throw new AnalysisException(\"Not matched actions can only contain INSERT\")\n+      }\n+\n+      m.copy(matchedActions = alignedMatchedActions, notMatchedActions = alignedNotMatchedActions)\n+  }\n+\n+  private def alignInsertActionAssignments(\n+      targetTable: LogicalPlan,\n+      assignmentMap: Map[String, Assignment]): Seq[Assignment] = {\n+\n+    val resolver = conf.resolver\n+\n+    targetTable.output.map { targetAttr =>\n+      val assignment = assignmentMap\n+        .find { case (name, _) => resolver(name, targetAttr.name) }\n+        .map { case (_, assignment) => assignment }\n+\n+      if (assignment.isEmpty) {\n+        throw new AnalysisException(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "originalPosition": 84}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYwMzQ3MzIx", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-560347321", "createdAt": "2020-12-31T13:10:05Z", "commit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMVQxMzoxMDowNVrOIM-pZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMVQxMzoxMDowNVrOIM-pZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDQ4MDIzMA==", "bodyText": "These TODOs require an actual implementation.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550480230", "createdAt": "2020-12-31T13:10:05Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+public abstract class TestMerge extends SparkRowLevelOperationsTestBase {\n+\n+  public TestMerge(String catalogName, String implementation, Map<String, String> config,\n+                   String fileFormat, boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS source\");\n+  }\n+\n+  // TODO: tests for reordering when operations succeed (both insert and update actions)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYxNDE4MTQw", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-561418140", "createdAt": "2021-01-04T23:28:05Z", "commit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMzoyODowNlrOIOEv_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMzoyODowNlrOIOEv_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYyODc5OQ==", "bodyText": "The equivalent match expression for notMatchedActions has a default case that throws \"Not matched actions can only contain INSERT\". Should there be a similar case here to reject any INSERT action?", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551628799", "createdAt": "2021-01-04T23:28:06Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))\n+        case d: DeleteAction => d", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYxNDI3NDk2", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-561427496", "createdAt": "2021-01-04T23:48:49Z", "commit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMzo0ODo0OVrOIOFPPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMzo0ODo0OVrOIOFPPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzNjc5OQ==", "bodyText": "If this happened, then two expressions in the left side of the assignment expression would match, right?\nINSERT (a.b, a.b) VALUES (2, 3)\nSince we know that the assignment references on the left are unique from an earlier check, we only have the second case, right?", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551636799", "createdAt": "2021-01-04T23:48:49Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AnsiCast, AttributeReference, Cast, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import scala.collection.mutable\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(ref: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(a.key, a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.ref.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(col, update.expr, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "originalPosition": 96}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYxNDI4Mjk1", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-561428295", "createdAt": "2021-01-04T23:51:13Z", "commit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMzo1MToxM1rOIOFRzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNFQyMzo1MToxM1rOIOFRzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzNzQ1NQ==", "bodyText": "Minor: I prefer not to add [ and ] unless they add value or form a literal.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551637455", "createdAt": "2021-01-04T23:51:13Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AnsiCast, AttributeReference, Cast, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import scala.collection.mutable\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(ref: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(a.key, a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.ref.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(col, update.expr, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.ref).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "originalPosition": 102}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYxNDMzMzkx", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-561433391", "createdAt": "2021-01-05T00:07:00Z", "commit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQwMDowNzowMFrOIOFjxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQwMDowNzowMFrOIOFjxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0MjA1Mg==", "bodyText": "I find the logic here hard to follow because applyUpdates and applyStructUpdates call one another to recursively traverse the schema. It is also strange that the current column is passed as NamedExpression, which will be created with an alias for each level.\nI think it would be easier to read and a little cleaner if you removed col and just traversed the schema in one method. It should be possible to build an expression equivalent to col from namePrefix and each field name.\nI don't think this is a blocker, though.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551642052", "createdAt": "2021-01-05T00:07:00Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AnsiCast, AttributeReference, Cast, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import scala.collection.mutable\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(ref: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(a.key, a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.ref.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(col, update.expr, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.ref).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "originalPosition": 119}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYxNDMzNjY1", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-561433665", "createdAt": "2021-01-05T00:07:46Z", "commit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQwMDowNzo0NlrOIOFklw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQwMDowNzo0NlrOIOFklw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0MjI2Mw==", "bodyText": "Updates with empty references will be removed by the filter in applyUpdates, right?", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551642263", "createdAt": "2021-01-05T00:07:46Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AnsiCast, AttributeReference, Cast, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import scala.collection.mutable\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(ref: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(a.key, a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.ref.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(col, update.expr, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.ref).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(ref = u.ref.tail))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "originalPosition": 120}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYxNDM1MzM4", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-561435338", "createdAt": "2021-01-05T00:13:10Z", "commit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQwMDoxMzoxMVrOIOFqUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQwMDoxMzoxMVrOIOFqUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0MzczMQ==", "bodyText": "What is the correct behavior here? Reject it?\nI think what would currently happen is it would fail if any existing field were accessed, but it would succeed if all fields were assigned. Is that correct?", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551643731", "createdAt": "2021-01-05T00:13:11Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+public abstract class TestMerge extends SparkRowLevelOperationsTestBase {\n+\n+  public TestMerge(String catalogName, String implementation, Map<String, String> config,\n+                   String fileFormat, boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS source\");\n+  }\n+\n+  // TODO: tests for reordering when operations succeed (both insert and update actions)\n+  // TODO: tests for modifying fields in a null struct", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYxNDM2NDE0", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-561436414", "createdAt": "2021-01-05T00:16:32Z", "commit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQwMDoxNjozMlrOIOFuWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQwMDoxNjozMlrOIOFuWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0NDc2MA==", "bodyText": "I don't think that this should have a WHEN MATCHED case, to ensure that it is failing for the NOT MATCHED THEN INSERT.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551644760", "createdAt": "2021-01-05T00:16:32Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+public abstract class TestMerge extends SparkRowLevelOperationsTestBase {\n+\n+  public TestMerge(String catalogName, String implementation, Map<String, String> config,\n+                   String fileFormat, boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS source\");\n+  }\n+\n+  // TODO: tests for reordering when operations succeed (both insert and update actions)\n+  // TODO: tests for modifying fields in a null struct\n+  // TODO: tests for subqueries in conditions\n+\n+  @Test\n+  public void testMergeWithNonExistingColumns() {\n+    createAndInitNestedColumnsTable();\n+    createOrReplaceView(\"source\", \"{ \\\"c1\\\": -100, \\\"c2\\\": -200 }\");\n+\n+    AssertHelpers.assertThrows(\"Should complain about the invalid top-level column\",\n+        AnalysisException.class, \"cannot resolve '`t.invalid_col`'\",\n+        () -> {\n+          sql(\"MERGE INTO %s t USING source s \" +\n+              \"ON t.id == s.c1 \" +\n+              \"WHEN MATCHED THEN \" +\n+              \"  UPDATE SET t.invalid_col = s.c2\", tableName);\n+        });\n+\n+    AssertHelpers.assertThrows(\"Should complain about the invalid nested column\",\n+        AnalysisException.class, \"No such struct field invalid_col\",\n+        () -> {\n+          sql(\"MERGE INTO %s t USING source s \" +\n+              \"ON t.id == s.c1 \" +\n+              \"WHEN MATCHED THEN \" +\n+              \"  UPDATE SET t.c.n2.invalid_col = s.c2\", tableName);\n+        });\n+\n+    AssertHelpers.assertThrows(\"Should complain about the invalid top-level column\",\n+        AnalysisException.class, \"cannot resolve '`invalid_col`'\",\n+        () -> {\n+          sql(\"MERGE INTO %s t USING source s \" +\n+              \"ON t.id == s.c1 \" +\n+              \"WHEN MATCHED THEN \" +\n+              \"  UPDATE SET t.c.n2.dn1 = s.c2 \" +\n+              \"WHEN NOT MATCHED THEN \" +\n+              \"  INSERT (id, invalid_col) VALUES (s.c1, null)\", tableName);\n+        });\n+  }\n+\n+  @Test\n+  public void testMergeWithInvalidColumnsInInsert() {\n+    createAndInitNestedColumnsTable();\n+    createOrReplaceView(\"source\", \"{ \\\"c1\\\": -100, \\\"c2\\\": -200 }\");\n+\n+    AssertHelpers.assertThrows(\"Should complain about the nested column\",\n+        AnalysisException.class, \"Nested fields are not supported inside INSERT clauses\",\n+        () -> {\n+          sql(\"MERGE INTO %s t USING source s \" +\n+              \"ON t.id == s.c1 \" +\n+              \"WHEN MATCHED THEN \" +\n+              \"  UPDATE SET t.c.n2.dn1 = s.c2 \" +\n+              \"WHEN NOT MATCHED THEN \" +\n+              \"  INSERT (id, c.n2) VALUES (s.c1, null)\", tableName);\n+        });\n+\n+    AssertHelpers.assertThrows(\"Should complain about duplicate columns\",\n+        AnalysisException.class, \"Duplicate column names inside INSERT clause\",\n+        () -> {\n+          sql(\"MERGE INTO %s t USING source s \" +\n+              \"ON t.id == s.c1 \" +\n+              \"WHEN MATCHED THEN \" +\n+              \"  UPDATE SET t.c.n2.dn1 = s.c2 \" +\n+              \"WHEN NOT MATCHED THEN \" +\n+              \"  INSERT (id, id) VALUES (s.c1, null)\", tableName);\n+        });\n+\n+    AssertHelpers.assertThrows(\"Should complain about missing columns\",\n+        AnalysisException.class, \"must provide values for all columns of the target table\",\n+        () -> {\n+          sql(\"MERGE INTO %s t USING source s \" +\n+              \"ON t.id == s.c1 \" +\n+              \"WHEN MATCHED THEN \" +\n+              \"  UPDATE SET t.c.n2.dn1 = s.c2 \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7c14b157748a2146f66ee8101b30413433834328"}, "originalPosition": 120}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7c14b157748a2146f66ee8101b30413433834328", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/7c14b157748a2146f66ee8101b30413433834328", "committedDate": "2020-12-31T13:06:48Z", "message": "More tests"}, "afterCommit": {"oid": "12b567796ccb6ef878d2c063d2802df42168907f", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/12b567796ccb6ef878d2c063d2802df42168907f", "committedDate": "2021-01-05T10:59:17Z", "message": "Spark: Add a rule to align updates in MERGE operations"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "12b567796ccb6ef878d2c063d2802df42168907f", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/12b567796ccb6ef878d2c063d2802df42168907f", "committedDate": "2021-01-05T10:59:17Z", "message": "Spark: Add a rule to align updates in MERGE operations"}, "afterCommit": {"oid": "8d48b6f6290b1f5a16cf1aea0d840ebb0ea36bcc", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/8d48b6f6290b1f5a16cf1aea0d840ebb0ea36bcc", "committedDate": "2021-01-05T11:01:44Z", "message": "Spark: Add a rule to align updates in MERGE operations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "58c7e78bb673e39fff0a71815073b8c07cef659a", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/58c7e78bb673e39fff0a71815073b8c07cef659a", "committedDate": "2021-01-05T11:02:43Z", "message": "Spark: Add a rule to align updates in MERGE operations"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8d48b6f6290b1f5a16cf1aea0d840ebb0ea36bcc", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/8d48b6f6290b1f5a16cf1aea0d840ebb0ea36bcc", "committedDate": "2021-01-05T11:01:44Z", "message": "Spark: Add a rule to align updates in MERGE operations"}, "afterCommit": {"oid": "58c7e78bb673e39fff0a71815073b8c07cef659a", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/58c7e78bb673e39fff0a71815073b8c07cef659a", "committedDate": "2021-01-05T11:02:43Z", "message": "Spark: Add a rule to align updates in MERGE operations"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYyMDQ1MzYx", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-562045361", "createdAt": "2021-01-05T18:51:41Z", "commit": {"oid": "58c7e78bb673e39fff0a71815073b8c07cef659a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQxODo1MTo0MVrOIOjItQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNVQxODo1MTo0MVrOIOjItQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjEyNjY0NQ==", "bodyText": "I think it would be much easier to read these tests if instead of using these methods, you just embedded the CREATE TABLE here, like this:\n  sql(\"CREATE TABLE %s (..., complex struct<c1 int, c2 int) ...\", tableName);\n  initTable(tableName);\nThat way, it is easy to see what the available columns are without needing to refer back to the base class. I found it easy to see the structure and data for source, but kept going back to see what the target table's schema was. That was a bit harder considering the method is changed in this test.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r552126645", "createdAt": "2021-01-05T18:51:41Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+public abstract class TestMerge extends SparkRowLevelOperationsTestBase {\n+\n+  public TestMerge(String catalogName, String implementation, Map<String, String> config,\n+                   String fileFormat, boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS source\");\n+  }\n+\n+  // TODO: tests for reordering when operations succeed (both insert and update actions)\n+  // TODO: tests for modifying fields in a null struct\n+  // TODO: tests for subqueries in conditions\n+\n+  @Test\n+  public void testMergeWithNonExistingColumns() {\n+    createAndInitNestedColumnsTable();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "58c7e78bb673e39fff0a71815073b8c07cef659a"}, "originalPosition": 54}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7e58d21b421157de078d96bf92e10f6d352802f5", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/7e58d21b421157de078d96bf92e10f6d352802f5", "committedDate": "2021-01-06T09:03:04Z", "message": "Some updates"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYzMDAwNzE3", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-563000717", "createdAt": "2021-01-06T19:57:07Z", "commit": {"oid": "7e58d21b421157de078d96bf92e10f6d352802f5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxOTo1NzowN1rOIPUWng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0wNlQxOTo1NzowN1rOIPUWng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkzMzAyMg==", "bodyText": "Nit: this looks like Runnable with different names. If we are just introducing it to pass a lambda, we could probably use Runnable instead.", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r552933022", "createdAt": "2021-01-06T19:57:07Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/SparkTestBase.java", "diffHunk": "@@ -132,4 +134,40 @@ protected void assertEquals(String context, List<Object[]> expectedRows, List<Ob\n   protected static String dbPath(String dbName) {\n     return metastore.getDatabasePath(dbName);\n   }\n+\n+  protected void withSQLConf(Map<String, String> conf, Action action) {\n+    SQLConf sqlConf = SQLConf.get();\n+\n+    Map<String, String> currentConfValues = Maps.newHashMap();\n+    conf.keySet().forEach(confKey -> {\n+      if (sqlConf.contains(confKey)) {\n+        String currentConfValue = sqlConf.getConfString(confKey);\n+        currentConfValues.put(confKey, currentConfValue);\n+      }\n+    });\n+\n+    conf.forEach((confKey, confValue) -> {\n+      if (SQLConf.staticConfKeys().contains(confKey)) {\n+        throw new RuntimeException(\"Cannot modify the value of a static config: \" + confKey);\n+      }\n+      sqlConf.setConfString(confKey, confValue);\n+    });\n+\n+    try {\n+      action.invoke();\n+    } finally {\n+      conf.forEach((confKey, confValue) -> {\n+        if (currentConfValues.containsKey(confKey)) {\n+          sqlConf.setConfString(confKey, currentConfValues.get(confKey));\n+        } else {\n+          sqlConf.unsetConf(confKey);\n+        }\n+      });\n+    }\n+  }\n+\n+  @FunctionalInterface\n+  protected interface Action {\n+    void invoke();\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7e58d21b421157de078d96bf92e10f6d352802f5"}, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYzMTUyMDc2", "url": "https://github.com/apache/iceberg/pull/1986#pullrequestreview-563152076", "createdAt": "2021-01-07T01:24:26Z", "commit": {"oid": "7e58d21b421157de078d96bf92e10f6d352802f5"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3329, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}