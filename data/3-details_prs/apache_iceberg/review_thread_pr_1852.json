{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI5NTExMjE5", "number": 1852, "reviewThreads": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxMjoyODo1NlrOE-obGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxNDowMToyMVrOE_Kc8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTA5NDY0OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxMjoyODo1NlrOH749Gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxNzoxNjo0OVrOH83YpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjU2MTE3OA==", "bodyText": "I feel like repartition should be optional and should be configured by the user. We can control it via table properties.\nWe currently plan to add the following properties:\nwrite.delete.isolation\nwrite.delete.mode\n\nThis config may be specific to Spark so we can call it engine.spark.write.delete.align-records or similar.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532561178", "createdAt": "2020-11-30T12:28:56Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()\n+    Project(scanPlan.output :+ fileNameExpr, scanPlan)\n+  }\n+\n+  private def buildWritePlan(\n+      remainingRowsPlan: LogicalPlan,\n+      output: Seq[AttributeReference]): LogicalPlan = {\n+\n+    // TODO: sort by _pos to keep the original ordering of rows\n+    // TODO: consider setting a file size limit\n+\n+    val fileNameCol = findOutputAttr(remainingRowsPlan, FILE_NAME_COL)\n+    val numShufflePartitions = SQLConf.get.numShufflePartitions\n+    val repartition = RepartitionByExpression(Seq(fileNameCol), remainingRowsPlan, numShufflePartitions)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU4NDAzNw==", "bodyText": "Sounds like a good follow-up.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533584037", "createdAt": "2020-12-01T17:16:49Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()\n+    Project(scanPlan.output :+ fileNameExpr, scanPlan)\n+  }\n+\n+  private def buildWritePlan(\n+      remainingRowsPlan: LogicalPlan,\n+      output: Seq[AttributeReference]): LogicalPlan = {\n+\n+    // TODO: sort by _pos to keep the original ordering of rows\n+    // TODO: consider setting a file size limit\n+\n+    val fileNameCol = findOutputAttr(remainingRowsPlan, FILE_NAME_COL)\n+    val numShufflePartitions = SQLConf.get.numShufflePartitions\n+    val repartition = RepartitionByExpression(Seq(fileNameCol), remainingRowsPlan, numShufflePartitions)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjU2MTE3OA=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTUyMzEzOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeleteFromTablePredicateCheck.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoxNTowMVrOH7892g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMzo0OTowNlrOH8tWlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYyNjkwNg==", "bodyText": "I'm a little confused why we want to fail this for all requests, not just those which might follow the Iceberg Delete pathway. Could you elaborate a little more? It looks to me like this will fail all DeleteFromTable operation regardless of whether an Iceberg table is involved.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532626906", "createdAt": "2020-11-30T14:15:01Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeleteFromTablePredicateCheck.scala", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Expression, InSubquery, Not}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, LogicalPlan}\n+\n+object DeleteFromTablePredicateCheck extends (LogicalPlan => Unit) {\n+\n+  override def apply(plan: LogicalPlan): Unit = {\n+    plan foreach {\n+      case DeleteFromTable(_, Some(condition)) if hasNullAwarePredicateWithinNot(condition) =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQxOTY2OA==", "bodyText": "Currently, Spark will fail all delete requests with subqueries so it felt OK. I do see the concern, though. I don't mind adding an extra check if we think it is worth it.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533419668", "createdAt": "2020-12-01T13:49:06Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeleteFromTablePredicateCheck.scala", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Expression, InSubquery, Not}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, LogicalPlan}\n+\n+object DeleteFromTablePredicateCheck extends (LogicalPlan => Unit) {\n+\n+  override def apply(plan: LogicalPlan): Unit = {\n+    plan foreach {\n+      case DeleteFromTable(_, Some(condition)) if hasNullAwarePredicateWithinNot(condition) =>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYyNjkwNg=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTU0MDAzOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeleteFromTablePredicateCheck.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoxODo0NVrOH79IDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMzo0OTo0OFrOH8tYcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYyOTUxNg==", "bodyText": "do we have to worry about \"In\" as well? Or only InSubquery?", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532629516", "createdAt": "2020-11-30T14:18:45Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeleteFromTablePredicateCheck.scala", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Expression, InSubquery, Not}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, LogicalPlan}\n+\n+object DeleteFromTablePredicateCheck extends (LogicalPlan => Unit) {\n+\n+  override def apply(plan: LogicalPlan): Unit = {\n+    plan foreach {\n+      case DeleteFromTable(_, Some(condition)) if hasNullAwarePredicateWithinNot(condition) =>\n+        // this limitation is present since SPARK-25154 fix is not yet available\n+        // we use Not(EqualsNullSafe(cond, true)) when deciding which records to keep\n+        // such conditions are rewritten by Spark as an existential join and currently Spark\n+        // does not handle correctly NOT IN subqueries nested into other expressions\n+        failAnalysis(\"Null-aware predicate sub-queries are not currently supported in DELETE\")\n+\n+      case _ => // OK\n+    }\n+  }\n+\n+  private def hasNullAwarePredicateWithinNot(cond: Expression): Boolean = {\n+    cond.find {\n+      case Not(expr) if expr.find(_.isInstanceOf[InSubquery]).isDefined => true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQyMDE0NQ==", "bodyText": "Just InSubquery since it is related to how subqueries are handled.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533420145", "createdAt": "2020-12-01T13:49:48Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/DeleteFromTablePredicateCheck.scala", "diffHunk": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Expression, InSubquery, Not}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, LogicalPlan}\n+\n+object DeleteFromTablePredicateCheck extends (LogicalPlan => Unit) {\n+\n+  override def apply(plan: LogicalPlan): Unit = {\n+    plan foreach {\n+      case DeleteFromTable(_, Some(condition)) if hasNullAwarePredicateWithinNot(condition) =>\n+        // this limitation is present since SPARK-25154 fix is not yet available\n+        // we use Not(EqualsNullSafe(cond, true)) when deciding which records to keep\n+        // such conditions are rewritten by Spark as an existential join and currently Spark\n+        // does not handle correctly NOT IN subqueries nested into other expressions\n+        failAnalysis(\"Null-aware predicate sub-queries are not currently supported in DELETE\")\n+\n+      case _ => // OK\n+    }\n+  }\n+\n+  private def hasNullAwarePredicateWithinNot(cond: Expression): Boolean = {\n+    cond.find {\n+      case Not(expr) if expr.find(_.isInstanceOf[InSubquery]).isDefined => true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYyOTUxNg=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTU3MzExOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeConditionsInRowLevelOperations.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoyNTo0M1rOH79bww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMzo1MDoxNlrOH8tZqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNDU2Mw==", "bodyText": "Maybe just me, but I would rename these 'cons' and 'table\" to be consistent with the naming directly above.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532634563", "createdAt": "2020-11-30T14:25:43Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeConditionsInRowLevelOperations.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, Filter, LocalRelation, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+\n+// we have to optimize expressions used in delete/update before we can rewrite row-level operations\n+// otherwise, we will have to deal with redundant casts and will not detect noop deletes\n+// it is a temp solution since we cannot inject rewrite of row-level ops after operator optimizations\n+object OptimizeConditionsInRowLevelOperations extends Rule[LogicalPlan] {\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case d @ DeleteFromTable(table, cond) if !SubqueryExpression.hasSubquery(cond.getOrElse(Literal.TrueLiteral)) =>\n+      val optimizedCond = optimizeCondition(cond.getOrElse(Literal.TrueLiteral), table)\n+      d.copy(condition = Some(optimizedCond))\n+  }\n+\n+  private def optimizeCondition(condition: Expression, targetTable: LogicalPlan): Expression = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQyMDQ1OQ==", "bodyText": "Agree, let me fix.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533420459", "createdAt": "2020-12-01T13:50:16Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeConditionsInRowLevelOperations.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, Filter, LocalRelation, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+\n+// we have to optimize expressions used in delete/update before we can rewrite row-level operations\n+// otherwise, we will have to deal with redundant casts and will not detect noop deletes\n+// it is a temp solution since we cannot inject rewrite of row-level ops after operator optimizations\n+object OptimizeConditionsInRowLevelOperations extends Rule[LogicalPlan] {\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case d @ DeleteFromTable(table, cond) if !SubqueryExpression.hasSubquery(cond.getOrElse(Literal.TrueLiteral)) =>\n+      val optimizedCond = optimizeCondition(cond.getOrElse(Literal.TrueLiteral), table)\n+      d.copy(condition = Some(optimizedCond))\n+  }\n+\n+  private def optimizeCondition(condition: Expression, targetTable: LogicalPlan): Expression = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNDU2Mw=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTU4NzQ4OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeConditionsInRowLevelOperations.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoyODo1OVrOH79klQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMzo1MjozOVrOH8tgTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNjgyMQ==", "bodyText": "Is this exhaustive? Shouldn't we also have the possibility of resolving into other relations as well?", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532636821", "createdAt": "2020-11-30T14:28:59Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeConditionsInRowLevelOperations.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, Filter, LocalRelation, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+\n+// we have to optimize expressions used in delete/update before we can rewrite row-level operations\n+// otherwise, we will have to deal with redundant casts and will not detect noop deletes\n+// it is a temp solution since we cannot inject rewrite of row-level ops after operator optimizations\n+object OptimizeConditionsInRowLevelOperations extends Rule[LogicalPlan] {\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case d @ DeleteFromTable(table, cond) if !SubqueryExpression.hasSubquery(cond.getOrElse(Literal.TrueLiteral)) =>\n+      val optimizedCond = optimizeCondition(cond.getOrElse(Literal.TrueLiteral), table)\n+      d.copy(condition = Some(optimizedCond))\n+  }\n+\n+  private def optimizeCondition(condition: Expression, targetTable: LogicalPlan): Expression = {\n+    val optimizer = SparkSession.active.sessionState.optimizer\n+    optimizer.execute(Filter(condition, targetTable)) match {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQyMjE1Ng==", "bodyText": "I think we better add a branch for other nodes and return the original expression. Good point.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533422156", "createdAt": "2020-12-01T13:52:39Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeConditionsInRowLevelOperations.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, Filter, LocalRelation, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+\n+// we have to optimize expressions used in delete/update before we can rewrite row-level operations\n+// otherwise, we will have to deal with redundant casts and will not detect noop deletes\n+// it is a temp solution since we cannot inject rewrite of row-level ops after operator optimizations\n+object OptimizeConditionsInRowLevelOperations extends Rule[LogicalPlan] {\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case d @ DeleteFromTable(table, cond) if !SubqueryExpression.hasSubquery(cond.getOrElse(Literal.TrueLiteral)) =>\n+      val optimizedCond = optimizeCondition(cond.getOrElse(Literal.TrueLiteral), table)\n+      d.copy(condition = Some(optimizedCond))\n+  }\n+\n+  private def optimizeCondition(condition: Expression, targetTable: LogicalPlan): Expression = {\n+    val optimizer = SparkSession.active.sessionState.optimizer\n+    optimizer.execute(Filter(condition, targetTable)) match {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNjgyMQ=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTYwOTQwOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeConditionsInRowLevelOperations.scala", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDozMzo1MlrOH79yOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMzo1MzoyM1rOH8tiTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY0MDMxNQ==", "bodyText": "The first and last case make sense here to me, If we get a Filter out of our optimization we want the new condition. If we end up with DataSourcecv2ScanRelation this is essentially a \"delete everything\" request and we pass back true.\nBut if we end up with a LocalRelation why do we want to delete nothing? Is this basically saying that we ended up trying to delete but the condition applied in the delete just results in not actually effecting a DSV2 table?", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532640315", "createdAt": "2020-11-30T14:33:52Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeConditionsInRowLevelOperations.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, Filter, LocalRelation, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+\n+// we have to optimize expressions used in delete/update before we can rewrite row-level operations\n+// otherwise, we will have to deal with redundant casts and will not detect noop deletes\n+// it is a temp solution since we cannot inject rewrite of row-level ops after operator optimizations\n+object OptimizeConditionsInRowLevelOperations extends Rule[LogicalPlan] {\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case d @ DeleteFromTable(table, cond) if !SubqueryExpression.hasSubquery(cond.getOrElse(Literal.TrueLiteral)) =>\n+      val optimizedCond = optimizeCondition(cond.getOrElse(Literal.TrueLiteral), table)\n+      d.copy(condition = Some(optimizedCond))\n+  }\n+\n+  private def optimizeCondition(condition: Expression, targetTable: LogicalPlan): Expression = {\n+    val optimizer = SparkSession.active.sessionState.optimizer\n+    optimizer.execute(Filter(condition, targetTable)) match {\n+      case Filter(optimizedCondition, _) => optimizedCondition", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjg1NTA1OQ==", "bodyText": "I think the LocalRelation would happen if 0 records matched. Spark would replace the scan + filter with an empty LocalRelation. That means that 0 records match the delete filter, so we can replace the whole delete filter with false to match 0 rows.\nSimilarly, if the filter is removed then it must have matched everything, so the filter should be true.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532855059", "createdAt": "2020-11-30T19:45:20Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeConditionsInRowLevelOperations.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, Filter, LocalRelation, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+\n+// we have to optimize expressions used in delete/update before we can rewrite row-level operations\n+// otherwise, we will have to deal with redundant casts and will not detect noop deletes\n+// it is a temp solution since we cannot inject rewrite of row-level ops after operator optimizations\n+object OptimizeConditionsInRowLevelOperations extends Rule[LogicalPlan] {\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case d @ DeleteFromTable(table, cond) if !SubqueryExpression.hasSubquery(cond.getOrElse(Literal.TrueLiteral)) =>\n+      val optimizedCond = optimizeCondition(cond.getOrElse(Literal.TrueLiteral), table)\n+      d.copy(condition = Some(optimizedCond))\n+  }\n+\n+  private def optimizeCondition(condition: Expression, targetTable: LogicalPlan): Expression = {\n+    val optimizer = SparkSession.active.sessionState.optimizer\n+    optimizer.execute(Filter(condition, targetTable)) match {\n+      case Filter(optimizedCondition, _) => optimizedCondition", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY0MDMxNQ=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkxNTE0MQ==", "bodyText": "ah so the opposite I thought. Ok Sounds good.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532915141", "createdAt": "2020-11-30T21:28:29Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeConditionsInRowLevelOperations.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, Filter, LocalRelation, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+\n+// we have to optimize expressions used in delete/update before we can rewrite row-level operations\n+// otherwise, we will have to deal with redundant casts and will not detect noop deletes\n+// it is a temp solution since we cannot inject rewrite of row-level ops after operator optimizations\n+object OptimizeConditionsInRowLevelOperations extends Rule[LogicalPlan] {\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case d @ DeleteFromTable(table, cond) if !SubqueryExpression.hasSubquery(cond.getOrElse(Literal.TrueLiteral)) =>\n+      val optimizedCond = optimizeCondition(cond.getOrElse(Literal.TrueLiteral), table)\n+      d.copy(condition = Some(optimizedCond))\n+  }\n+\n+  private def optimizeCondition(condition: Expression, targetTable: LogicalPlan): Expression = {\n+    val optimizer = SparkSession.active.sessionState.optimizer\n+    optimizer.execute(Filter(condition, targetTable)) match {\n+      case Filter(optimizedCondition, _) => optimizedCondition", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY0MDMxNQ=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQyMjY3MQ==", "bodyText": "@rdblue is correct.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533422671", "createdAt": "2020-12-01T13:53:23Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeConditionsInRowLevelOperations.scala", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{DeleteFromTable, Filter, LocalRelation, LogicalPlan}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+\n+// we have to optimize expressions used in delete/update before we can rewrite row-level operations\n+// otherwise, we will have to deal with redundant casts and will not detect noop deletes\n+// it is a temp solution since we cannot inject rewrite of row-level ops after operator optimizations\n+object OptimizeConditionsInRowLevelOperations extends Rule[LogicalPlan] {\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan transform {\n+    case d @ DeleteFromTable(table, cond) if !SubqueryExpression.hasSubquery(cond.getOrElse(Literal.TrueLiteral)) =>\n+      val optimizedCond = optimizeCondition(cond.getOrElse(Literal.TrueLiteral), table)\n+      d.copy(condition = Some(optimizedCond))\n+  }\n+\n+  private def optimizeCondition(condition: Expression, targetTable: LogicalPlan): Expression = {\n+    val optimizer = SparkSession.active.sessionState.optimizer\n+    optimizer.execute(Filter(condition, targetTable)) match {\n+      case Filter(optimizedCondition, _) => optimizedCondition", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY0MDMxNQ=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTY5OTU1OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDo1MjozMlrOH7-pgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDo1MjozMlrOH7-pgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1NDQ2NQ==", "bodyText": "This \"d\" is lonely", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532654465", "createdAt": "2020-11-30T14:52:32Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTcyNzYzOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDo1ODoxNVrOH7-6uQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMzo1NjoyMFrOH8tqVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1ODg3Mw==", "bodyText": "maybe this should be \"mergeWrite\"?", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532658873", "createdAt": "2020-11-30T14:58:15Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQyNDcyNQ==", "bodyText": "Let me update.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533424725", "createdAt": "2020-12-01T13:56:20Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1ODg3Mw=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTcyODYyOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDo1ODoyN1rOH7-7Wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxNzoxNjoyMVrOH83Xag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1OTAzNA==", "bodyText": "and this remainingWrite", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532659034", "createdAt": "2020-11-30T14:58:27Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQyNDc1OQ==", "bodyText": "I kind of like to have Plan in the name.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533424759", "createdAt": "2020-12-01T13:56:24Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1OTAzNA=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU4MzcyMg==", "bodyText": "I'm good with either writePlan or remainingWritePlan. I think remaining is implied since there is only one write.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533583722", "createdAt": "2020-12-01T17:16:21Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY1OTAzNA=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTc2MjY5OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNTowNTozOVrOH7_P_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMzo1Nzo1MlrOH8tusA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY2NDMxOA==", "bodyText": "This could potentially be much larger than the row data we are moving around, Maybe not important now but we may want to just switch this to a hash of the file name? or something small?", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532664318", "createdAt": "2020-11-30T15:05:39Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQyNTg0MA==", "bodyText": "Sure, we can revisit this later.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533425840", "createdAt": "2020-12-01T13:57:52Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY2NDMxOA=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTc3MzgxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNTowNzo1NFrOH7_WoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxMzoyOToyM1rOH8sjaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY2NjAxNw==", "bodyText": "What filters won't we be able to translate?", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532666017", "createdAt": "2020-11-30T15:07:54Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()\n+    Project(scanPlan.output :+ fileNameExpr, scanPlan)\n+  }\n+\n+  private def buildWritePlan(\n+      remainingRowsPlan: LogicalPlan,\n+      output: Seq[AttributeReference]): LogicalPlan = {\n+\n+    // TODO: sort by _pos to keep the original ordering of rows\n+    // TODO: consider setting a file size limit\n+\n+    val fileNameCol = findOutputAttr(remainingRowsPlan, FILE_NAME_COL)\n+    val numShufflePartitions = SQLConf.get.numShufflePartitions\n+    val repartition = RepartitionByExpression(Seq(fileNameCol), remainingRowsPlan, numShufflePartitions)\n+    val sort = Sort(Seq(SortOrder(fileNameCol, Ascending)), global = false, repartition)\n+    Project(output, sort)\n+  }\n+\n+  private def isDeleteWhereCase(relation: DataSourceV2Relation, cond: Expression): Boolean = {\n+    relation.table match {\n+      case t: ExtendedSupportsDelete if !SubqueryExpression.hasSubquery(cond) =>\n+        val predicates = splitConjunctivePredicates(cond)\n+        val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, relation.output)\n+        val dataSourceFilters = toDataSourceFilters(normalizedPredicates)\n+        val allPredicatesTranslated = normalizedPredicates.size == dataSourceFilters.length", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkxODEyMw==", "bodyText": "A surprising number. We see predicates with casts fairly often, but it could also be predicates with UDF calls or other transforms. Just about anything that isn't a comparison, or that isn't simplified to a comparison.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532918123", "createdAt": "2020-11-30T21:34:34Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()\n+    Project(scanPlan.output :+ fileNameExpr, scanPlan)\n+  }\n+\n+  private def buildWritePlan(\n+      remainingRowsPlan: LogicalPlan,\n+      output: Seq[AttributeReference]): LogicalPlan = {\n+\n+    // TODO: sort by _pos to keep the original ordering of rows\n+    // TODO: consider setting a file size limit\n+\n+    val fileNameCol = findOutputAttr(remainingRowsPlan, FILE_NAME_COL)\n+    val numShufflePartitions = SQLConf.get.numShufflePartitions\n+    val repartition = RepartitionByExpression(Seq(fileNameCol), remainingRowsPlan, numShufflePartitions)\n+    val sort = Sort(Seq(SortOrder(fileNameCol, Ascending)), global = false, repartition)\n+    Project(output, sort)\n+  }\n+\n+  private def isDeleteWhereCase(relation: DataSourceV2Relation, cond: Expression): Boolean = {\n+    relation.table match {\n+      case t: ExtendedSupportsDelete if !SubqueryExpression.hasSubquery(cond) =>\n+        val predicates = splitConjunctivePredicates(cond)\n+        val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, relation.output)\n+        val dataSourceFilters = toDataSourceFilters(normalizedPredicates)\n+        val allPredicatesTranslated = normalizedPredicates.size == dataSourceFilters.length", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY2NjAxNw=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 116}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQwNjU3MQ==", "bodyText": "Yeah, this logic is present in all places where the translation is done.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533406571", "createdAt": "2020-12-01T13:29:23Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()\n+    Project(scanPlan.output :+ fileNameExpr, scanPlan)\n+  }\n+\n+  private def buildWritePlan(\n+      remainingRowsPlan: LogicalPlan,\n+      output: Seq[AttributeReference]): LogicalPlan = {\n+\n+    // TODO: sort by _pos to keep the original ordering of rows\n+    // TODO: consider setting a file size limit\n+\n+    val fileNameCol = findOutputAttr(remainingRowsPlan, FILE_NAME_COL)\n+    val numShufflePartitions = SQLConf.get.numShufflePartitions\n+    val repartition = RepartitionByExpression(Seq(fileNameCol), remainingRowsPlan, numShufflePartitions)\n+    val sort = Sort(Seq(SortOrder(fileNameCol, Ascending)), global = false, repartition)\n+    Project(output, sort)\n+  }\n+\n+  private def isDeleteWhereCase(relation: DataSourceV2Relation, cond: Expression): Boolean = {\n+    relation.table match {\n+      case t: ExtendedSupportsDelete if !SubqueryExpression.hasSubquery(cond) =>\n+        val predicates = splitConjunctivePredicates(cond)\n+        val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, relation.output)\n+        val dataSourceFilters = toDataSourceFilters(normalizedPredicates)\n+        val allPredicatesTranslated = normalizedPredicates.size == dataSourceFilters.length", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY2NjAxNw=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTc4NjcxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedBatchScanExec.scala", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNToxMDozOFrOH7_ejQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQxNDowNjoxM1rOH9cehA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY2ODA0NQ==", "bodyText": "Maybe NonCachingBatchScanExec?", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532668045", "createdAt": "2020-11-30T15:10:38Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedBatchScanExec.scala", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference\n+import org.apache.spark.sql.catalyst.plans.QueryPlan\n+import org.apache.spark.sql.connector.read.{InputPartition, PartitionReaderFactory, Scan}\n+\n+// The only reason we need this class and cannot reuse BatchScanExec is because\n+// BatchScanExec caches input partitions and we cannot apply file filtering before execution\n+// Spark calls supportsColumnar during physical planning which, in turn, triggers split planning\n+// We must ensure the result is not cached so that we can push down file filters later\n+// The only difference compared to BatchScanExec is that we are using def instead of lazy val for splits\n+case class ExtendedBatchScanExec(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQyOTgyNQ==", "bodyText": "I tend to name things with Extended prefix where we don't plan to have separate classes in the future.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533429825", "createdAt": "2020-12-01T14:03:23Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedBatchScanExec.scala", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference\n+import org.apache.spark.sql.catalyst.plans.QueryPlan\n+import org.apache.spark.sql.connector.read.{InputPartition, PartitionReaderFactory, Scan}\n+\n+// The only reason we need this class and cannot reuse BatchScanExec is because\n+// BatchScanExec caches input partitions and we cannot apply file filtering before execution\n+// Spark calls supportsColumnar during physical planning which, in turn, triggers split planning\n+// We must ensure the result is not cached so that we can push down file filters later\n+// The only difference compared to BatchScanExec is that we are using def instead of lazy val for splits\n+case class ExtendedBatchScanExec(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY2ODA0NQ=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQzMDA2NA==", "bodyText": "Ideally, we should not have this node at all.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533430064", "createdAt": "2020-12-01T14:03:45Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedBatchScanExec.scala", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference\n+import org.apache.spark.sql.catalyst.plans.QueryPlan\n+import org.apache.spark.sql.connector.read.{InputPartition, PartitionReaderFactory, Scan}\n+\n+// The only reason we need this class and cannot reuse BatchScanExec is because\n+// BatchScanExec caches input partitions and we cannot apply file filtering before execution\n+// Spark calls supportsColumnar during physical planning which, in turn, triggers split planning\n+// We must ensure the result is not cached so that we can push down file filters later\n+// The only difference compared to BatchScanExec is that we are using def instead of lazy val for splits\n+case class ExtendedBatchScanExec(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY2ODA0NQ=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU4NTI3OQ==", "bodyText": "Should we try to get this fixed in Spark 3.1?", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533585279", "createdAt": "2020-12-01T17:18:37Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedBatchScanExec.scala", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference\n+import org.apache.spark.sql.catalyst.plans.QueryPlan\n+import org.apache.spark.sql.connector.read.{InputPartition, PartitionReaderFactory, Scan}\n+\n+// The only reason we need this class and cannot reuse BatchScanExec is because\n+// BatchScanExec caches input partitions and we cannot apply file filtering before execution\n+// Spark calls supportsColumnar during physical planning which, in turn, triggers split planning\n+// We must ensure the result is not cached so that we can push down file filters later\n+// The only difference compared to BatchScanExec is that we are using def instead of lazy val for splits\n+case class ExtendedBatchScanExec(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY2ODA0NQ=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDE5MTc0OA==", "bodyText": "I'd love too but we will have to justify it quite a bit. Do you think it is worth trying?", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r534191748", "createdAt": "2020-12-02T14:06:13Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedBatchScanExec.scala", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference\n+import org.apache.spark.sql.catalyst.plans.QueryPlan\n+import org.apache.spark.sql.connector.read.{InputPartition, PartitionReaderFactory, Scan}\n+\n+// The only reason we need this class and cannot reuse BatchScanExec is because\n+// BatchScanExec caches input partitions and we cannot apply file filtering before execution\n+// Spark calls supportsColumnar during physical planning which, in turn, triggers split planning\n+// We must ensure the result is not cached so that we can push down file filters later\n+// The only difference compared to BatchScanExec is that we are using def instead of lazy val for splits\n+case class ExtendedBatchScanExec(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY2ODA0NQ=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MTc5NTU3OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedDataSourceV2Implicits.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNToxMjoyN1rOH7_j5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxNDowOTo0MlrOH8uQAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY2OTQxMg==", "bodyText": "Currently this is just \"supports row level merge and delete\" so maybe we should detail that instead of \"row level operations\" which I feel like is a bit more vague.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532669412", "createdAt": "2020-11-30T15:12:27Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedDataSourceV2Implicits.scala", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.SupportsMerge\n+\n+// must be merged with DataSourceV2Implicits in Spark\n+object ExtendedDataSourceV2Implicits {\n+  implicit class TableHelper(table: Table) {\n+    def asMergeable: SupportsMerge = {\n+      table match {\n+        case support: SupportsMerge =>\n+          support\n+        case _ =>\n+          throw new AnalysisException(s\"Table does not support row level operations: ${table.name}\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQzNDM2OA==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533434368", "createdAt": "2020-12-01T14:09:42Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedDataSourceV2Implicits.scala", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.SupportsMerge\n+\n+// must be merged with DataSourceV2Implicits in Spark\n+object ExtendedDataSourceV2Implicits {\n+  implicit class TableHelper(table: Table) {\n+    def asMergeable: SupportsMerge = {\n+      table match {\n+        case support: SupportsMerge =>\n+          support\n+        case _ =>\n+          throw new AnalysisException(s\"Table does not support row level operations: ${table.name}\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjY2OTQxMg=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzMxOTg5OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMToxMzoxM1rOH8ODZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxNDowMTo1OVrOH8t6hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkwNjg1NQ==", "bodyText": "What about isMetadataDelete? I think that's more clear why we would not rewrite the plan. All of these plans are technically DELETE FROM ... WHERE.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r532906855", "createdAt": "2020-11-30T21:13:13Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()\n+    Project(scanPlan.output :+ fileNameExpr, scanPlan)\n+  }\n+\n+  private def buildWritePlan(\n+      remainingRowsPlan: LogicalPlan,\n+      output: Seq[AttributeReference]): LogicalPlan = {\n+\n+    // TODO: sort by _pos to keep the original ordering of rows\n+    // TODO: consider setting a file size limit\n+\n+    val fileNameCol = findOutputAttr(remainingRowsPlan, FILE_NAME_COL)\n+    val numShufflePartitions = SQLConf.get.numShufflePartitions\n+    val repartition = RepartitionByExpression(Seq(fileNameCol), remainingRowsPlan, numShufflePartitions)\n+    val sort = Sort(Seq(SortOrder(fileNameCol, Ascending)), global = false, repartition)\n+    Project(output, sort)\n+  }\n+\n+  private def isDeleteWhereCase(relation: DataSourceV2Relation, cond: Expression): Boolean = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQyODg3MQ==", "bodyText": "Agree.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533428871", "createdAt": "2020-12-01T14:01:59Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()\n+    Project(scanPlan.output :+ fileNameExpr, scanPlan)\n+  }\n+\n+  private def buildWritePlan(\n+      remainingRowsPlan: LogicalPlan,\n+      output: Seq[AttributeReference]): LogicalPlan = {\n+\n+    // TODO: sort by _pos to keep the original ordering of rows\n+    // TODO: consider setting a file size limit\n+\n+    val fileNameCol = findOutputAttr(remainingRowsPlan, FILE_NAME_COL)\n+    val numShufflePartitions = SQLConf.get.numShufflePartitions\n+    val repartition = RepartitionByExpression(Seq(fileNameCol), remainingRowsPlan, numShufflePartitions)\n+    val sort = Sort(Seq(SortOrder(fileNameCol, Ascending)), global = false, repartition)\n+    Project(output, sort)\n+  }\n+\n+  private def isDeleteWhereCase(relation: DataSourceV2Relation, cond: Expression): Boolean = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkwNjg1NQ=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NjY2OTkyOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxNDowMToyMVrOH8t4sg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxNjozNDoxMFrOH81f-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQyODQwMg==", "bodyText": "I want to bring attention to these TODOs. It would require a bit of effort for the first one but the second one is something we can do now. The main question is whether we want to allow any extra size overhead before closing files. For example, there may be a file with 1.1 GB of data and our target size can be 1GB. Having a hard limit would mean we will cut 1.1 GB file into two.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533428402", "createdAt": "2020-12-01T14:01:21Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()\n+    Project(scanPlan.output :+ fileNameExpr, scanPlan)\n+  }\n+\n+  private def buildWritePlan(\n+      remainingRowsPlan: LogicalPlan,\n+      output: Seq[AttributeReference]): LogicalPlan = {\n+\n+    // TODO: sort by _pos to keep the original ordering of rows\n+    // TODO: consider setting a file size limit", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQzNzA2Mw==", "bodyText": "The problem with such limits is that we never know what would be better. There may be an opposite situation when we had 1 GB file and added extra 100 MB from another file and wrote together. If we want to really solve it, we have to adapt our writers so that we can close the current file if _file column changes.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533437063", "createdAt": "2020-12-01T14:13:32Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()\n+    Project(scanPlan.output :+ fileNameExpr, scanPlan)\n+  }\n+\n+  private def buildWritePlan(\n+      remainingRowsPlan: LogicalPlan,\n+      output: Seq[AttributeReference]): LogicalPlan = {\n+\n+    // TODO: sort by _pos to keep the original ordering of rows\n+    // TODO: consider setting a file size limit", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQyODQwMg=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU1MzE0NA==", "bodyText": "The problem with a soft limit is that you have a hard limit somewhere. You could have 1.2 GB of data and the soft limit would make you cut at 1.1 GB. I think the issue you can't avoid is not knowing how much more data there is coming from the writer. I would not spend time worrying about more complicated logic here. We can still tune the files after the fact with rewrites.\nThis problem also has less of an impact if we get the _pos changes done, so I would focus effort there. Keeping the original order will help keep files the same size, and we can always group just the rows from one input file into an output file.", "url": "https://github.com/apache/iceberg/pull/1852#discussion_r533553144", "createdAt": "2020-12-01T16:34:10Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{sources, AnalysisException}\n+import org.apache.spark.sql.catalyst.expressions.{Alias, Ascending, Attribute, AttributeReference, EqualNullSafe, Expression, InputFileName, Literal, Not, PredicateHelper, SortOrder, SubqueryExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DeleteFromTable, DynamicFileFilter, Filter, LogicalPlan, Project, RepartitionByExpression, ReplaceData, Sort}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.catalog.ExtendedSupportsDelete\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.DataSourceStrategy\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation, PushDownUtils}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{BooleanType, StructType}\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+// TODO: should be part of early scan push down after the delete condition is optimized\n+object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging {\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  private val FILE_NAME_COL = \"_file\"\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    // don't rewrite deletes that can be answered by passing filters to deleteWhere in SupportsDelete\n+    case d @ DeleteFromTable(r: DataSourceV2Relation, Some(cond)) if isDeleteWhereCase(r, cond) =>\n+      d\n+\n+    // rewrite all operations that require reading the table to delete records\n+    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+      // TODO: do a switch based on whether we get BatchWrite or DeltaBatchWrite\n+      val writeInfo = newWriteInfo(r.schema)\n+      val mergeBuilder = r.table.asMergeable.newMergeBuilder(writeInfo)\n+\n+      val scanPlan = buildScanPlan(r.table, r.output, mergeBuilder, cond)\n+\n+      val remainingRowFilter = Not(EqualNullSafe(cond, Literal(true, BooleanType)))\n+      val remainingRowsPlan = Filter(remainingRowFilter, scanPlan)\n+\n+      val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+      val writePlan = buildWritePlan(remainingRowsPlan, r.output)\n+      ReplaceData(r, batchWrite, writePlan)\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      cond: Expression): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+\n+    val predicates = splitConjunctivePredicates(cond)\n+    val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, output)\n+    PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n+\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    val scanPlan = scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+\n+    // include file name so that we can group data back\n+    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()\n+    Project(scanPlan.output :+ fileNameExpr, scanPlan)\n+  }\n+\n+  private def buildWritePlan(\n+      remainingRowsPlan: LogicalPlan,\n+      output: Seq[AttributeReference]): LogicalPlan = {\n+\n+    // TODO: sort by _pos to keep the original ordering of rows\n+    // TODO: consider setting a file size limit", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzQyODQwMg=="}, "originalCommit": {"oid": "c77c6130e859ac671edde947d3c24452bcb4826a"}, "originalPosition": 101}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3197, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}