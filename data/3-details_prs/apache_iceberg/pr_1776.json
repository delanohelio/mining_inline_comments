{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIyMDEwNzQ0", "number": 1776, "title": "Spark: Refactor Spark writes into separate classes (V2)", "bodyText": "This PR refactors our Spark writes into separate classes so that we can add more writes with other commit functions.", "createdAt": "2020-11-16T22:39:21Z", "url": "https://github.com/apache/iceberg/pull/1776", "merged": true, "mergeCommit": {"oid": "8e2952c5809a8b2e82093a5665439570798798ce"}, "closed": true, "closedAt": "2020-11-17T16:35:55Z", "author": {"login": "aokolnychyi"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABddM6FygFqTUzMTg0NzU3MQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABddSmm5ABqjQwMDM2Mjg1NTg=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMxODQ3NTcx", "url": "https://github.com/apache/iceberg/pull/1776#pullrequestreview-531847571", "createdAt": "2020-11-16T22:43:20Z", "commit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0MzoyMFrOH0Y7aA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0MzoyMFrOH0Y7aA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDY5NjQyNA==", "bodyText": "SparkWrite contains only common things now.", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524696424", "createdAt": "2020-11-16T22:43:20Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -72,52 +75,67 @@\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n \n-class SparkBatchWrite implements BatchWrite {\n-  private static final Logger LOG = LoggerFactory.getLogger(SparkBatchWrite.class);\n+class SparkWrite {\n+  private static final Logger LOG = LoggerFactory.getLogger(SparkWrite.class);\n \n   private final Table table;\n+  private final String queryId;\n   private final FileFormat format;\n   private final Broadcast<FileIO> io;\n   private final Broadcast<EncryptionManager> encryptionManager;\n-  private final boolean overwriteDynamic;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMxODQ3OTI3", "url": "https://github.com/apache/iceberg/pull/1776#pullrequestreview-531847927", "createdAt": "2020-11-16T22:44:02Z", "commit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0NDowMlrOH0Y-Dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0NDowMlrOH0Y-Dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDY5NzEwMg==", "bodyText": "This logic moved to inner classes.", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524697102", "createdAt": "2020-11-16T22:44:02Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -170,45 +177,7 @@ protected void commitOperation(SnapshotUpdate<?> operation, int numFiles, String\n     LOG.info(\"Committed in {} ms\", duration);\n   }\n \n-  private void append(WriterCommitMessage[] messages) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "originalPosition": 148}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMxODQ4MTc1", "url": "https://github.com/apache/iceberg/pull/1776#pullrequestreview-531848175", "createdAt": "2020-11-16T22:44:30Z", "commit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0NDozMFrOH0ZAPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0NDozMFrOH0ZAPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDY5NzY2Mg==", "bodyText": "This logic moved to base batch and streaming write classes.", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524697662", "createdAt": "2020-11-16T22:44:30Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -236,9 +201,171 @@ protected Table table() {\n     return ImmutableList.of();\n   }\n \n-  @Override\n-  public String toString() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "originalPosition": 209}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMxODUwMzI4", "url": "https://github.com/apache/iceberg/pull/1776#pullrequestreview-531850328", "createdAt": "2020-11-16T22:48:46Z", "commit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0ODo0N1rOH0ZRGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0ODo0N1rOH0ZRGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDcwMTk3OA==", "bodyText": "In the future, Spark may have a write abstraction so we will make this class extend Write.", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524701978", "createdAt": "2020-11-16T22:48:47Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -72,52 +75,67 @@\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n \n-class SparkBatchWrite implements BatchWrite {\n-  private static final Logger LOG = LoggerFactory.getLogger(SparkBatchWrite.class);\n+class SparkWrite {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMxOTIzMzAz", "url": "https://github.com/apache/iceberg/pull/1776#pullrequestreview-531923303", "createdAt": "2020-11-17T00:12:18Z", "commit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMDoxMjoxOFrOH0ewuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMDoxMjoxOFrOH0ewuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc5MTk5Mg==", "bodyText": "Since this searches through old snapshots, what about find instead of get here? That signals that the operation is doing more than just fetching a pre-computed value.", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524791992", "createdAt": "2020-11-17T00:12:18Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -236,9 +201,171 @@ protected Table table() {\n     return ImmutableList.of();\n   }\n \n-  @Override\n-  public String toString() {\n-    return String.format(\"IcebergWrite(table=%s, format=%s)\", table, format);\n+  private abstract class BaseBatchWrite implements BatchWrite {\n+    @Override\n+    public DataWriterFactory createBatchWriterFactory(PhysicalWriteInfo info) {\n+      return createWriterFactory();\n+    }\n+\n+    @Override\n+    public void abort(WriterCommitMessage[] messages) {\n+      SparkWrite.this.abort(messages);\n+    }\n+\n+    @Override\n+    public String toString() {\n+      return String.format(\"IcebergBatchWrite(table=%s, format=%s)\", table, format);\n+    }\n+  }\n+\n+  private class BatchAppend extends BaseBatchWrite {\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+      AppendFiles append = table.newAppend();\n+\n+      int numFiles = 0;\n+      for (DataFile file : files(messages)) {\n+        numFiles += 1;\n+        append.appendFile(file);\n+      }\n+\n+      commitOperation(append, String.format(\"append with %d new data files\", numFiles));\n+    }\n+  }\n+\n+  private class DynamicOverwrite extends BaseBatchWrite {\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+      ReplacePartitions dynamicOverwrite = table.newReplacePartitions();\n+\n+      int numFiles = 0;\n+      for (DataFile file : files(messages)) {\n+        numFiles += 1;\n+        dynamicOverwrite.addFile(file);\n+      }\n+\n+      commitOperation(dynamicOverwrite, String.format(\"dynamic partition overwrite with %d new data files\", numFiles));\n+    }\n+  }\n+\n+  private class OverwriteByFilter extends BaseBatchWrite {\n+    private final Expression overwriteExpr;\n+\n+    private OverwriteByFilter(Expression overwriteExpr) {\n+      this.overwriteExpr = overwriteExpr;\n+    }\n+\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+      OverwriteFiles overwriteFiles = table.newOverwrite();\n+      overwriteFiles.overwriteByRowFilter(overwriteExpr);\n+\n+      int numFiles = 0;\n+      for (DataFile file : files(messages)) {\n+        numFiles += 1;\n+        overwriteFiles.addFile(file);\n+      }\n+\n+      String commitMsg = String.format(\"overwrite by filter %s with %d new data files\", overwriteExpr, numFiles);\n+      commitOperation(overwriteFiles, commitMsg);\n+    }\n+  }\n+\n+  private abstract class BaseStreamingWrite implements StreamingWrite {\n+    private static final String QUERY_ID_PROPERTY = \"spark.sql.streaming.queryId\";\n+    private static final String EPOCH_ID_PROPERTY = \"spark.sql.streaming.epochId\";\n+\n+    protected abstract String mode();\n+\n+    @Override\n+    public StreamingDataWriterFactory createStreamingWriterFactory(PhysicalWriteInfo info) {\n+      return createWriterFactory();\n+    }\n+\n+    @Override\n+    public final void commit(long epochId, WriterCommitMessage[] messages) {\n+      LOG.info(\"Committing epoch {} for query {} in {} mode\", epochId, queryId, mode());\n+\n+      table.refresh();\n+      Long lastCommittedEpochId = getLastCommittedEpochId();\n+      if (lastCommittedEpochId != null && epochId <= lastCommittedEpochId) {\n+        LOG.info(\"Skipping epoch {} for query {} as it was already committed\", epochId, queryId);\n+        return;\n+      }\n+\n+      doCommit(epochId, messages);\n+    }\n+\n+    protected abstract void doCommit(long epochId, WriterCommitMessage[] messages);\n+\n+    protected <T> void commit(SnapshotUpdate<T> snapshotUpdate, long epochId, String description) {\n+      snapshotUpdate.set(QUERY_ID_PROPERTY, queryId);\n+      snapshotUpdate.set(EPOCH_ID_PROPERTY, Long.toString(epochId));\n+      commitOperation(snapshotUpdate, description);\n+    }\n+\n+    private Long getLastCommittedEpochId() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "originalPosition": 314}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/2afa35f5c098f9588f22b7559d5d3f2dc7986eb6", "committedDate": "2020-11-16T22:37:13Z", "message": "Spark: Refactor Spark writes into separate classes"}, "afterCommit": {"oid": "e55227374fcab7a3286d053d507849fb92e0b7e7", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/e55227374fcab7a3286d053d507849fb92e0b7e7", "committedDate": "2020-11-17T05:17:11Z", "message": "Spark: Refactor Spark writes into separate classes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ebfad1be45f4df24a539fa9b4a743fe39e0e4318", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/ebfad1be45f4df24a539fa9b4a743fe39e0e4318", "committedDate": "2020-11-17T05:21:20Z", "message": "Spark: Refactor Spark writes into separate classes"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e55227374fcab7a3286d053d507849fb92e0b7e7", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/e55227374fcab7a3286d053d507849fb92e0b7e7", "committedDate": "2020-11-17T05:17:11Z", "message": "Spark: Refactor Spark writes into separate classes"}, "afterCommit": {"oid": "ebfad1be45f4df24a539fa9b4a743fe39e0e4318", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/ebfad1be45f4df24a539fa9b4a743fe39e0e4318", "committedDate": "2020-11-17T05:21:20Z", "message": "Spark: Refactor Spark writes into separate classes"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3766, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}