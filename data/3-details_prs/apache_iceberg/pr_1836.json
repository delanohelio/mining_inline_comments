{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI4MTM5NjMw", "number": 1836, "title": "Core: Add data and delete writers in FileAppenderFactory.", "bodyText": "This is an PR which is divided from this : #1818.", "createdAt": "2020-11-26T15:16:36Z", "url": "https://github.com/apache/iceberg/pull/1836", "merged": true, "mergeCommit": {"oid": "4383ad4960feb0f22b85dd26a463d7776b64c077"}, "closed": true, "closedAt": "2020-12-02T04:11:45Z", "author": {"login": "openinx"}, "timelineItems": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdgUe5wAH2gAyNTI4MTM5NjMwOjYwOTA0OWUxZTlhMDVkM2ZhNzZiYWNhM2ZkZDEzYjlmMGExZGNlOTk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdiElasgH2gAyNTI4MTM5NjMwOmU2YTgxNjkwMDBjYmI3NDBlM2U3YTE5YWU4ZTg0M2Y2MDVlNjQyZWQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "609049e1e9a05d3fa76baca3fdd13b9f0a1dce99", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/609049e1e9a05d3fa76baca3fdd13b9f0a1dce99", "committedDate": "2020-11-26T15:14:40Z", "message": "Core: Add data and delete writers in FileAppenderFactory."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c1a124772f4e48fd38ea8b81a316464712057b09", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/c1a124772f4e48fd38ea8b81a316464712057b09", "committedDate": "2020-11-27T05:34:28Z", "message": "Minor fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c", "committedDate": "2020-11-27T08:10:15Z", "message": "Fix broken unit tests."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM5ODE5MzEx", "url": "https://github.com/apache/iceberg/pull/1836#pullrequestreview-539819311", "createdAt": "2020-11-27T09:18:32Z", "commit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwOToxODozMlrOH62oLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwOToxODozMlrOH62oLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTQ3NDQ3OA==", "bodyText": "@rdblue   Here I use the parquet writer which is built from <path, pos, row> schema, rather than the writer built from row schema.\nI tried to hide all the <path, pos, row> write logic inside the Parquet#buildPositionWriter, but failed to make it work.  I passed a function to this builder by creating a  parquet writer built by rowSchema, and construct a PositionDeleteStructWriter with a path string writer,  a pos long writer and the  row struct writer,   it does not work because the currentPath for here has been messed up.   Still figuring out how to make this work in a graceful way...", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r531474478", "createdAt": "2020-11-27T09:18:32Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkAppenderFactory.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.io.UncheckedIOException;\n+import java.util.Map;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.deletes.EqualityDeleteWriter;\n+import org.apache.iceberg.deletes.PositionDeleteWriter;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkOrcWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.io.DataWriter;\n+import org.apache.iceberg.io.DeleteSchemaUtil;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileAppenderFactory;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class FlinkAppenderFactory implements FileAppenderFactory<RowData>, Serializable {\n+  private final Schema schema;\n+  private final RowType flinkSchema;\n+  private final Map<String, String> props;\n+  private final PartitionSpec spec;\n+  private final int[] equalityFieldIds;\n+  private final Schema eqDeleteRowSchema;\n+  private final Schema posDeleteRowSchema;\n+\n+  private RowType eqDeleteFlinkSchema = null;\n+  private RowType posDeleteFlinkSchema = null;\n+\n+  public FlinkAppenderFactory(Schema schema, RowType flinkSchema, Map<String, String> props, PartitionSpec spec) {\n+    this(schema, flinkSchema, props, spec, null, schema, null);\n+  }\n+\n+  public FlinkAppenderFactory(Schema schema, RowType flinkSchema, Map<String, String> props,\n+                              PartitionSpec spec, int[] equalityFieldIds,\n+                              Schema eqDeleteRowSchema, Schema posDeleteRowSchema) {\n+    this.schema = schema;\n+    this.flinkSchema = flinkSchema;\n+    this.props = props;\n+    this.spec = spec;\n+    this.equalityFieldIds = equalityFieldIds;\n+    this.eqDeleteRowSchema = eqDeleteRowSchema;\n+    this.posDeleteRowSchema = posDeleteRowSchema;\n+  }\n+\n+  private RowType lazyEqDeleteFlinkSchema() {\n+    if (eqDeleteFlinkSchema == null) {\n+      Preconditions.checkNotNull(eqDeleteRowSchema, \"Equality delete row schema shouldn't be null\");\n+      this.eqDeleteFlinkSchema = FlinkSchemaUtil.convert(eqDeleteRowSchema);\n+    }\n+    return eqDeleteFlinkSchema;\n+  }\n+\n+  private RowType lazyPosDeleteFlinkSchema() {\n+    if (posDeleteFlinkSchema == null) {\n+      Preconditions.checkNotNull(posDeleteRowSchema, \"Pos-delete row schema shouldn't be null\");\n+      this.posDeleteFlinkSchema = FlinkSchemaUtil.convert(posDeleteRowSchema);\n+    }\n+    return this.posDeleteFlinkSchema;\n+  }\n+\n+  @Override\n+  public FileAppender<RowData> newAppender(OutputFile outputFile, FileFormat format) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.write(outputFile)\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(flinkSchema))\n+              .setAll(props)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        case ORC:\n+          return ORC.write(outputFile)\n+              .createWriterFunc((iSchema, typDesc) -> FlinkOrcWriter.buildWriter(flinkSchema, iSchema))\n+              .setAll(props)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        case PARQUET:\n+          return Parquet.write(outputFile)\n+              .createWriterFunc(msgType -> FlinkParquetWriters.buildWriter(flinkSchema, msgType))\n+              .setAll(props)\n+              .metricsConfig(metricsConfig)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public DataWriter<RowData> newDataWriter(EncryptedOutputFile file, FileFormat format, StructLike partition) {\n+    return new DataWriter<>(\n+        newAppender(file.encryptingOutputFile(), format), format,\n+        file.encryptingOutputFile().location(), spec, partition, file.keyMetadata());\n+  }\n+\n+  @Override\n+  public EqualityDeleteWriter<RowData> newEqDeleteWriter(EncryptedOutputFile outputFile, FileFormat format,\n+                                                         StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(lazyEqDeleteFlinkSchema()))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        case PARQUET:\n+          return Parquet.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(msgType -> FlinkParquetWriters.buildWriter(lazyEqDeleteFlinkSchema(), msgType))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .metricsConfig(metricsConfig)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public PositionDeleteWriter<RowData> newPosDeleteWriter(EncryptedOutputFile outputFile, FileFormat format,\n+                                                          StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(lazyPosDeleteFlinkSchema()))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .rowSchema(posDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .buildPositionWriter();\n+\n+        case PARQUET:\n+          RowType flinkPosDeleteSchema = FlinkSchemaUtil.convert(DeleteSchemaUtil.posDeleteSchema(posDeleteRowSchema));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 198}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxMzI4OTkx", "url": "https://github.com/apache/iceberg/pull/1836#pullrequestreview-541328991", "createdAt": "2020-11-30T21:59:26Z", "commit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMTo1OToyN1rOH8PhjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMTo1OToyN1rOH8PhjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzMDk1Nw==", "bodyText": "Could this be the default implementation of newDataWriter? It looks the same across all of the implementations.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532930957", "createdAt": "2020-11-30T21:59:27Z", "author": {"login": "rdblue"}, "path": "data/src/main/java/org/apache/iceberg/data/GenericAppenderFactory.java", "diffHunk": "@@ -95,4 +115,88 @@ public GenericAppenderFactory setAll(Map<String, String> properties) {\n       throw new UncheckedIOException(e);\n     }\n   }\n+\n+  @Override\n+  public org.apache.iceberg.io.DataWriter<Record> newDataWriter(EncryptedOutputFile file, FileFormat format,\n+                                                                StructLike partition) {\n+    return new org.apache.iceberg.io.DataWriter<>(\n+        newAppender(file.encryptingOutputFile(), format), format,\n+        file.encryptingOutputFile().location(), spec, partition, file.keyMetadata());\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxMzI5NTQ2", "url": "https://github.com/apache/iceberg/pull/1836#pullrequestreview-541329546", "createdAt": "2020-11-30T22:00:16Z", "commit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjowMDoxN1rOH8PjQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjowMDoxN1rOH8PjQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzMTM5Mw==", "bodyText": "I think it should be \"unsupported\" rather than \"unknown\" because ORC is known, but not supported.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532931393", "createdAt": "2020-11-30T22:00:17Z", "author": {"login": "rdblue"}, "path": "data/src/main/java/org/apache/iceberg/data/GenericAppenderFactory.java", "diffHunk": "@@ -95,4 +115,88 @@ public GenericAppenderFactory setAll(Map<String, String> properties) {\n       throw new UncheckedIOException(e);\n     }\n   }\n+\n+  @Override\n+  public org.apache.iceberg.io.DataWriter<Record> newDataWriter(EncryptedOutputFile file, FileFormat format,\n+                                                                StructLike partition) {\n+    return new org.apache.iceberg.io.DataWriter<>(\n+        newAppender(file.encryptingOutputFile(), format), format,\n+        file.encryptingOutputFile().location(), spec, partition, file.keyMetadata());\n+  }\n+\n+  @Override\n+  public EqualityDeleteWriter<Record> newEqDeleteWriter(EncryptedOutputFile file, FileFormat format,\n+                                                        StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(config);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(file.encryptingOutputFile())\n+              .createWriterFunc(DataWriter::create)\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(config)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(file.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        case PARQUET:\n+          return Parquet.writeDeletes(file.encryptingOutputFile())\n+              .createWriterFunc(GenericParquetWriter::buildWriter)\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(config)\n+              .metricsConfig(metricsConfig)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(file.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public PositionDeleteWriter<Record> newPosDeleteWriter(EncryptedOutputFile file, FileFormat format,\n+                                                         StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(config);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(file.encryptingOutputFile())\n+              .createWriterFunc(DataWriter::create)\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(config)\n+              .rowSchema(posDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(file.keyMetadata())\n+              .buildPositionWriter();\n+\n+        case PARQUET:\n+          return Parquet.writeDeletes(file.encryptingOutputFile())\n+              .createWriterFunc(GenericParquetWriter::buildWriter)\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(config)\n+              .metricsConfig(metricsConfig)\n+              .rowSchema(posDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(file.keyMetadata())\n+              .buildPositionWriter();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 126}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxMzM0MjEy", "url": "https://github.com/apache/iceberg/pull/1836#pullrequestreview-541334212", "createdAt": "2020-11-30T22:07:44Z", "commit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjowNzo0NFrOH8Pxhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjowNzo0NFrOH8Pxhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzNTA0Nw==", "bodyText": "Why not set eqDeleteRowSchema to null since equalityFieldIds is null?", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532935047", "createdAt": "2020-11-30T22:07:44Z", "author": {"login": "rdblue"}, "path": "data/src/main/java/org/apache/iceberg/data/GenericAppenderFactory.java", "diffHunk": "@@ -42,10 +47,25 @@\n public class GenericAppenderFactory implements FileAppenderFactory<Record> {\n \n   private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final int[] equalityFieldIds;\n+  private final Schema eqDeleteRowSchema;\n+  private final Schema posDeleteRowSchema;\n   private final Map<String, String> config = Maps.newHashMap();\n \n-  public GenericAppenderFactory(Schema schema) {\n+  public GenericAppenderFactory(Schema schema, PartitionSpec spec) {\n+    this(schema, spec, null, schema, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 29}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxMzM0ODgw", "url": "https://github.com/apache/iceberg/pull/1836#pullrequestreview-541334880", "createdAt": "2020-11-30T22:08:49Z", "commit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjowODo1MFrOH8PzaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjowODo1MFrOH8PzaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzNTUyOQ==", "bodyText": "Could we keep the constructor this was using before so we don't need to change any tests that only use newAppender? There are 4 files just here that don't appear like they need to change just to add the spec that won't be used.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532935529", "createdAt": "2020-11-30T22:08:50Z", "author": {"login": "rdblue"}, "path": "data/src/test/java/org/apache/iceberg/data/GenericAppenderHelper.java", "diffHunk": "@@ -73,13 +74,12 @@ public DataFile writeFile(StructLike partition, List<Record> records) throws IOE\n     Preconditions.checkNotNull(table, \"table not set\");\n     File file = tmp.newFile();\n     Assert.assertTrue(file.delete());\n-    return appendToLocalFile(table, file, fileFormat, partition, records);\n+    return appendToLocalFile(table, file, fileFormat, partition, records, table.spec());\n   }\n \n-  private static DataFile appendToLocalFile(\n-      Table table, File file, FileFormat format, StructLike partition, List<Record> records)\n-      throws IOException {\n-    FileAppender<Record> appender = new GenericAppenderFactory(table.schema()).newAppender(\n+  private static DataFile appendToLocalFile(Table table, File file, FileFormat format, StructLike partition,\n+                                            List<Record> records, PartitionSpec spec) throws IOException {\n+    FileAppender<Record> appender = new GenericAppenderFactory(table.schema(), spec).newAppender(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 22}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxMzQ5NTU2", "url": "https://github.com/apache/iceberg/pull/1836#pullrequestreview-541349556", "createdAt": "2020-11-30T22:34:10Z", "commit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjozNDoxMVrOH8QiJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjozNDoxMVrOH8QiJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk0NzQ5NA==", "bodyText": "I'd rather not add this to the build method. Nothing distinguishes it from other options, so I think we should add transformations as configuration methods, like we do for equalityFieldIds.\nI'm also thinking that it would be good to have a more light-weight way to add these transforms. Rather than an additional accessor that has two abstract methods, why not just register functions? It could look like this:\n  Avro.writeDeletes(outFile)\n      ...\n      .transformPaths(StringData::fromString)\n      .buildPositionWriter();\nThat way it's easier to use a method reference rather than creating a class. And nothing actually needs to transform pos yet, so we can just leave that out.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532947494", "createdAt": "2020-11-30T22:34:11Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -379,42 +378,40 @@ public DeleteWriteBuilder equalityFieldIds(int... fieldIds) {\n           appenderBuilder.build(), FileFormat.PARQUET, location, spec, partition, keyMetadata, equalityFieldIds);\n     }\n \n-\n-    public <T> PositionDeleteWriter<T> buildPositionWriter() throws IOException {\n+    public <T> PositionDeleteWriter<T> buildPositionWriter(ParquetValueWriters.PathPosAccessor<?, ?> accessor)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 30}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxMzUzOTQw", "url": "https://github.com/apache/iceberg/pull/1836#pullrequestreview-541353940", "createdAt": "2020-11-30T22:42:15Z", "commit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjo0MjoxNVrOH8QwtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjo0MjoxNVrOH8QwtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk1MTIyMQ==", "bodyText": "I think this could just be table.schema(). No need to go through the spec to get the table schema.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532951221", "createdAt": "2020-11-30T22:42:15Z", "author": {"login": "rdblue"}, "path": "core/src/test/java/org/apache/iceberg/io/TestAppenderFactory.java", "diffHunk": "@@ -0,0 +1,265 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.deletes.EqualityDeleteWriter;\n+import org.apache.iceberg.deletes.PositionDelete;\n+import org.apache.iceberg.deletes.PositionDeleteWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestAppenderFactory<T> extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+\n+  private final FileFormat format;\n+  private final boolean partitioned;\n+\n+  private PartitionKey partition = null;\n+\n+  @Parameterized.Parameters(name = \"FileFormat={0}, Partitioned={1}\")\n+  public static Object[] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", false},\n+        new Object[] {\"avro\", true},\n+        new Object[] {\"parquet\", false},\n+        new Object[] {\"parquet\", true}\n+    };\n+  }\n+\n+\n+  public TestAppenderFactory(String fileFormat, boolean partitioned) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void setupTable() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    Assert.assertTrue(tableDir.delete()); // created by table create\n+\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+\n+    if (partitioned) {\n+      this.table = create(SCHEMA, SPEC);\n+    } else {\n+      this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n+    }\n+    this.partition = createPartitionKey();\n+\n+    table.updateProperties()\n+        .defaultFormat(format)\n+        .commit();\n+  }\n+\n+  protected abstract FileAppenderFactory<T> createAppenderFactory(List<Integer> equalityFieldIds,\n+                                                                  Schema eqDeleteSchema,\n+                                                                  Schema posDeleteRowSchema);\n+\n+  protected abstract T createRow(Integer id, String data);\n+\n+  protected abstract StructLikeSet expectedRowSet(Iterable<T> records) throws IOException;\n+\n+  protected abstract StructLikeSet actualRowSet(String... columns) throws IOException;\n+\n+  private OutputFileFactory createFileFactory() {\n+    return new OutputFileFactory(table.spec(), format, table.locationProvider(), table.io(),\n+        table.encryption(), 1, 1);\n+  }\n+\n+  private PartitionKey createPartitionKey() {\n+    if (table.spec().isUnpartitioned()) {\n+      return null;\n+    }\n+\n+    Record record = GenericRecord.create(table.spec().schema()).copy(ImmutableMap.of(\"data\", \"aaa\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 112}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxMzU0ODky", "url": "https://github.com/apache/iceberg/pull/1836#pullrequestreview-541354892", "createdAt": "2020-11-30T22:44:06Z", "commit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjo0NDowNlrOH8Qzyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjo0NDowNlrOH8Qzyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk1MjAxMQ==", "bodyText": "If the data here is bbb instead of ccc on purpose, then could you add a comment that this is testing that just id is used for comparison?", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532952011", "createdAt": "2020-11-30T22:44:06Z", "author": {"login": "rdblue"}, "path": "core/src/test/java/org/apache/iceberg/io/TestAppenderFactory.java", "diffHunk": "@@ -0,0 +1,265 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.deletes.EqualityDeleteWriter;\n+import org.apache.iceberg.deletes.PositionDelete;\n+import org.apache.iceberg.deletes.PositionDeleteWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestAppenderFactory<T> extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+\n+  private final FileFormat format;\n+  private final boolean partitioned;\n+\n+  private PartitionKey partition = null;\n+\n+  @Parameterized.Parameters(name = \"FileFormat={0}, Partitioned={1}\")\n+  public static Object[] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", false},\n+        new Object[] {\"avro\", true},\n+        new Object[] {\"parquet\", false},\n+        new Object[] {\"parquet\", true}\n+    };\n+  }\n+\n+\n+  public TestAppenderFactory(String fileFormat, boolean partitioned) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void setupTable() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    Assert.assertTrue(tableDir.delete()); // created by table create\n+\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+\n+    if (partitioned) {\n+      this.table = create(SCHEMA, SPEC);\n+    } else {\n+      this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n+    }\n+    this.partition = createPartitionKey();\n+\n+    table.updateProperties()\n+        .defaultFormat(format)\n+        .commit();\n+  }\n+\n+  protected abstract FileAppenderFactory<T> createAppenderFactory(List<Integer> equalityFieldIds,\n+                                                                  Schema eqDeleteSchema,\n+                                                                  Schema posDeleteRowSchema);\n+\n+  protected abstract T createRow(Integer id, String data);\n+\n+  protected abstract StructLikeSet expectedRowSet(Iterable<T> records) throws IOException;\n+\n+  protected abstract StructLikeSet actualRowSet(String... columns) throws IOException;\n+\n+  private OutputFileFactory createFileFactory() {\n+    return new OutputFileFactory(table.spec(), format, table.locationProvider(), table.io(),\n+        table.encryption(), 1, 1);\n+  }\n+\n+  private PartitionKey createPartitionKey() {\n+    if (table.spec().isUnpartitioned()) {\n+      return null;\n+    }\n+\n+    Record record = GenericRecord.create(table.spec().schema()).copy(ImmutableMap.of(\"data\", \"aaa\"));\n+\n+    PartitionKey partitionKey = new PartitionKey(table.spec(), table.schema());\n+    partitionKey.partition(record);\n+\n+    return partitionKey;\n+  }\n+\n+  private List<T> testRowSet() {\n+    return Lists.newArrayList(\n+        createRow(1, \"aaa\"),\n+        createRow(2, \"bbb\"),\n+        createRow(3, \"ccc\"),\n+        createRow(4, \"ddd\"),\n+        createRow(5, \"eee\")\n+    );\n+  }\n+\n+  private DataFile prepareDataFile(List<T> rowSet, FileAppenderFactory<T> appenderFactory,\n+                                   OutputFileFactory outputFileFactory) throws IOException {\n+    DataWriter<T> writer = appenderFactory.newDataWriter(outputFileFactory.newOutputFile(), format, partition);\n+    try (DataWriter<T> closeableWriter = writer) {\n+      for (T row : rowSet) {\n+        closeableWriter.add(row);\n+      }\n+    }\n+\n+    return writer.toDataFile();\n+  }\n+\n+  @Test\n+  public void testDataWriter() throws IOException {\n+    FileAppenderFactory<T> appenderFactory = createAppenderFactory(null, null, null);\n+    OutputFileFactory outputFileFactory = createFileFactory();\n+\n+    List<T> rowSet = testRowSet();\n+    DataFile dataFile = prepareDataFile(rowSet, appenderFactory, outputFileFactory);\n+\n+    table.newRowDelta()\n+        .addRows(dataFile)\n+        .commit();\n+\n+    Assert.assertEquals(\"Should have the expected records.\", expectedRowSet(rowSet), actualRowSet(\"*\"));\n+  }\n+\n+  @Test\n+  public void testEqDeleteWriter() throws IOException {\n+    List<Integer> equalityFieldIds = Lists.newArrayList(table.schema().findField(\"id\").fieldId());\n+    FileAppenderFactory<T> appenderFactory = createAppenderFactory(equalityFieldIds,\n+        table.schema().select(\"id\"), null);\n+    OutputFileFactory outputFileFactory = createFileFactory();\n+\n+    List<T> rowSet = testRowSet();\n+    DataFile dataFile = prepareDataFile(rowSet, appenderFactory, outputFileFactory);\n+\n+    table.newRowDelta()\n+        .addRows(dataFile)\n+        .commit();\n+\n+    List<T> deletes = Lists.newArrayList(\n+        createRow(1, \"aaa\"),\n+        createRow(3, \"bbb\"),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 173}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxMzU2Mjgw", "url": "https://github.com/apache/iceberg/pull/1836#pullrequestreview-541356280", "createdAt": "2020-11-30T22:46:44Z", "commit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjo0Njo0NFrOH8Q4aw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjo0Njo0NFrOH8Q4aw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk1MzE5NQ==", "bodyText": "Instead of reading from the table, I would rather see a test that the equality delete file contains the expected row data. In this case, it should not contain the data column. I would like to see that checked. And it would be good to add a case where the whole original row is written to the file.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532953195", "createdAt": "2020-11-30T22:46:44Z", "author": {"login": "rdblue"}, "path": "core/src/test/java/org/apache/iceberg/io/TestAppenderFactory.java", "diffHunk": "@@ -0,0 +1,265 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.deletes.EqualityDeleteWriter;\n+import org.apache.iceberg.deletes.PositionDelete;\n+import org.apache.iceberg.deletes.PositionDeleteWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestAppenderFactory<T> extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+\n+  private final FileFormat format;\n+  private final boolean partitioned;\n+\n+  private PartitionKey partition = null;\n+\n+  @Parameterized.Parameters(name = \"FileFormat={0}, Partitioned={1}\")\n+  public static Object[] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", false},\n+        new Object[] {\"avro\", true},\n+        new Object[] {\"parquet\", false},\n+        new Object[] {\"parquet\", true}\n+    };\n+  }\n+\n+\n+  public TestAppenderFactory(String fileFormat, boolean partitioned) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void setupTable() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    Assert.assertTrue(tableDir.delete()); // created by table create\n+\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+\n+    if (partitioned) {\n+      this.table = create(SCHEMA, SPEC);\n+    } else {\n+      this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n+    }\n+    this.partition = createPartitionKey();\n+\n+    table.updateProperties()\n+        .defaultFormat(format)\n+        .commit();\n+  }\n+\n+  protected abstract FileAppenderFactory<T> createAppenderFactory(List<Integer> equalityFieldIds,\n+                                                                  Schema eqDeleteSchema,\n+                                                                  Schema posDeleteRowSchema);\n+\n+  protected abstract T createRow(Integer id, String data);\n+\n+  protected abstract StructLikeSet expectedRowSet(Iterable<T> records) throws IOException;\n+\n+  protected abstract StructLikeSet actualRowSet(String... columns) throws IOException;\n+\n+  private OutputFileFactory createFileFactory() {\n+    return new OutputFileFactory(table.spec(), format, table.locationProvider(), table.io(),\n+        table.encryption(), 1, 1);\n+  }\n+\n+  private PartitionKey createPartitionKey() {\n+    if (table.spec().isUnpartitioned()) {\n+      return null;\n+    }\n+\n+    Record record = GenericRecord.create(table.spec().schema()).copy(ImmutableMap.of(\"data\", \"aaa\"));\n+\n+    PartitionKey partitionKey = new PartitionKey(table.spec(), table.schema());\n+    partitionKey.partition(record);\n+\n+    return partitionKey;\n+  }\n+\n+  private List<T> testRowSet() {\n+    return Lists.newArrayList(\n+        createRow(1, \"aaa\"),\n+        createRow(2, \"bbb\"),\n+        createRow(3, \"ccc\"),\n+        createRow(4, \"ddd\"),\n+        createRow(5, \"eee\")\n+    );\n+  }\n+\n+  private DataFile prepareDataFile(List<T> rowSet, FileAppenderFactory<T> appenderFactory,\n+                                   OutputFileFactory outputFileFactory) throws IOException {\n+    DataWriter<T> writer = appenderFactory.newDataWriter(outputFileFactory.newOutputFile(), format, partition);\n+    try (DataWriter<T> closeableWriter = writer) {\n+      for (T row : rowSet) {\n+        closeableWriter.add(row);\n+      }\n+    }\n+\n+    return writer.toDataFile();\n+  }\n+\n+  @Test\n+  public void testDataWriter() throws IOException {\n+    FileAppenderFactory<T> appenderFactory = createAppenderFactory(null, null, null);\n+    OutputFileFactory outputFileFactory = createFileFactory();\n+\n+    List<T> rowSet = testRowSet();\n+    DataFile dataFile = prepareDataFile(rowSet, appenderFactory, outputFileFactory);\n+\n+    table.newRowDelta()\n+        .addRows(dataFile)\n+        .commit();\n+\n+    Assert.assertEquals(\"Should have the expected records.\", expectedRowSet(rowSet), actualRowSet(\"*\"));\n+  }\n+\n+  @Test\n+  public void testEqDeleteWriter() throws IOException {\n+    List<Integer> equalityFieldIds = Lists.newArrayList(table.schema().findField(\"id\").fieldId());\n+    FileAppenderFactory<T> appenderFactory = createAppenderFactory(equalityFieldIds,\n+        table.schema().select(\"id\"), null);\n+    OutputFileFactory outputFileFactory = createFileFactory();\n+\n+    List<T> rowSet = testRowSet();\n+    DataFile dataFile = prepareDataFile(rowSet, appenderFactory, outputFileFactory);\n+\n+    table.newRowDelta()\n+        .addRows(dataFile)\n+        .commit();\n+\n+    List<T> deletes = Lists.newArrayList(\n+        createRow(1, \"aaa\"),\n+        createRow(3, \"bbb\"),\n+        createRow(5, \"ccc\")\n+    );\n+    EqualityDeleteWriter<T> eqDeleteWriter =\n+        appenderFactory.newEqDeleteWriter(outputFileFactory.newOutputFile(), format, partition);\n+    try (EqualityDeleteWriter<T> closeableWriter = eqDeleteWriter) {\n+      closeableWriter.deleteAll(deletes);\n+    }\n+\n+    table.newRowDelta()\n+        .addDeletes(eqDeleteWriter.toDeleteFile())\n+        .commit();\n+\n+    List<T> expected = Lists.newArrayList(\n+        createRow(2, \"bbb\"),\n+        createRow(4, \"ddd\")\n+    );\n+    Assert.assertEquals(\"Should have the expected records\", expectedRowSet(expected), actualRowSet(\"*\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 190}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxMzU2NzMy", "url": "https://github.com/apache/iceberg/pull/1836#pullrequestreview-541356732", "createdAt": "2020-11-30T22:47:35Z", "commit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjo0NzozNlrOH8Q54A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjo0NzozNlrOH8Q54A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk1MzU2OA==", "bodyText": "Similar to above, I think this should check that only the path and position columns are written to the file and that they are the expected values. The test below should check that the row column is present and set correctly for each row.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532953568", "createdAt": "2020-11-30T22:47:36Z", "author": {"login": "rdblue"}, "path": "core/src/test/java/org/apache/iceberg/io/TestAppenderFactory.java", "diffHunk": "@@ -0,0 +1,265 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.deletes.EqualityDeleteWriter;\n+import org.apache.iceberg.deletes.PositionDelete;\n+import org.apache.iceberg.deletes.PositionDeleteWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestAppenderFactory<T> extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+\n+  private final FileFormat format;\n+  private final boolean partitioned;\n+\n+  private PartitionKey partition = null;\n+\n+  @Parameterized.Parameters(name = \"FileFormat={0}, Partitioned={1}\")\n+  public static Object[] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", false},\n+        new Object[] {\"avro\", true},\n+        new Object[] {\"parquet\", false},\n+        new Object[] {\"parquet\", true}\n+    };\n+  }\n+\n+\n+  public TestAppenderFactory(String fileFormat, boolean partitioned) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void setupTable() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    Assert.assertTrue(tableDir.delete()); // created by table create\n+\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+\n+    if (partitioned) {\n+      this.table = create(SCHEMA, SPEC);\n+    } else {\n+      this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n+    }\n+    this.partition = createPartitionKey();\n+\n+    table.updateProperties()\n+        .defaultFormat(format)\n+        .commit();\n+  }\n+\n+  protected abstract FileAppenderFactory<T> createAppenderFactory(List<Integer> equalityFieldIds,\n+                                                                  Schema eqDeleteSchema,\n+                                                                  Schema posDeleteRowSchema);\n+\n+  protected abstract T createRow(Integer id, String data);\n+\n+  protected abstract StructLikeSet expectedRowSet(Iterable<T> records) throws IOException;\n+\n+  protected abstract StructLikeSet actualRowSet(String... columns) throws IOException;\n+\n+  private OutputFileFactory createFileFactory() {\n+    return new OutputFileFactory(table.spec(), format, table.locationProvider(), table.io(),\n+        table.encryption(), 1, 1);\n+  }\n+\n+  private PartitionKey createPartitionKey() {\n+    if (table.spec().isUnpartitioned()) {\n+      return null;\n+    }\n+\n+    Record record = GenericRecord.create(table.spec().schema()).copy(ImmutableMap.of(\"data\", \"aaa\"));\n+\n+    PartitionKey partitionKey = new PartitionKey(table.spec(), table.schema());\n+    partitionKey.partition(record);\n+\n+    return partitionKey;\n+  }\n+\n+  private List<T> testRowSet() {\n+    return Lists.newArrayList(\n+        createRow(1, \"aaa\"),\n+        createRow(2, \"bbb\"),\n+        createRow(3, \"ccc\"),\n+        createRow(4, \"ddd\"),\n+        createRow(5, \"eee\")\n+    );\n+  }\n+\n+  private DataFile prepareDataFile(List<T> rowSet, FileAppenderFactory<T> appenderFactory,\n+                                   OutputFileFactory outputFileFactory) throws IOException {\n+    DataWriter<T> writer = appenderFactory.newDataWriter(outputFileFactory.newOutputFile(), format, partition);\n+    try (DataWriter<T> closeableWriter = writer) {\n+      for (T row : rowSet) {\n+        closeableWriter.add(row);\n+      }\n+    }\n+\n+    return writer.toDataFile();\n+  }\n+\n+  @Test\n+  public void testDataWriter() throws IOException {\n+    FileAppenderFactory<T> appenderFactory = createAppenderFactory(null, null, null);\n+    OutputFileFactory outputFileFactory = createFileFactory();\n+\n+    List<T> rowSet = testRowSet();\n+    DataFile dataFile = prepareDataFile(rowSet, appenderFactory, outputFileFactory);\n+\n+    table.newRowDelta()\n+        .addRows(dataFile)\n+        .commit();\n+\n+    Assert.assertEquals(\"Should have the expected records.\", expectedRowSet(rowSet), actualRowSet(\"*\"));\n+  }\n+\n+  @Test\n+  public void testEqDeleteWriter() throws IOException {\n+    List<Integer> equalityFieldIds = Lists.newArrayList(table.schema().findField(\"id\").fieldId());\n+    FileAppenderFactory<T> appenderFactory = createAppenderFactory(equalityFieldIds,\n+        table.schema().select(\"id\"), null);\n+    OutputFileFactory outputFileFactory = createFileFactory();\n+\n+    List<T> rowSet = testRowSet();\n+    DataFile dataFile = prepareDataFile(rowSet, appenderFactory, outputFileFactory);\n+\n+    table.newRowDelta()\n+        .addRows(dataFile)\n+        .commit();\n+\n+    List<T> deletes = Lists.newArrayList(\n+        createRow(1, \"aaa\"),\n+        createRow(3, \"bbb\"),\n+        createRow(5, \"ccc\")\n+    );\n+    EqualityDeleteWriter<T> eqDeleteWriter =\n+        appenderFactory.newEqDeleteWriter(outputFileFactory.newOutputFile(), format, partition);\n+    try (EqualityDeleteWriter<T> closeableWriter = eqDeleteWriter) {\n+      closeableWriter.deleteAll(deletes);\n+    }\n+\n+    table.newRowDelta()\n+        .addDeletes(eqDeleteWriter.toDeleteFile())\n+        .commit();\n+\n+    List<T> expected = Lists.newArrayList(\n+        createRow(2, \"bbb\"),\n+        createRow(4, \"ddd\")\n+    );\n+    Assert.assertEquals(\"Should have the expected records\", expectedRowSet(expected), actualRowSet(\"*\"));\n+  }\n+\n+  @Test\n+  public void testPosDeleteWriter() throws IOException {\n+    // Initialize FileAppenderFactory without pos-delete row schema.\n+    FileAppenderFactory<T> appenderFactory = createAppenderFactory(null, null, null);\n+    OutputFileFactory outputFileFactory = createFileFactory();\n+\n+    List<T> rowSet = testRowSet();\n+    DataFile dataFile = prepareDataFile(rowSet, appenderFactory, outputFileFactory);\n+\n+    List<Pair<CharSequence, Long>> deletes = Lists.newArrayList(\n+        Pair.of(dataFile.path(), 0L),\n+        Pair.of(dataFile.path(), 2L),\n+        Pair.of(dataFile.path(), 4L)\n+    );\n+\n+    PositionDeleteWriter<T> eqDeleteWriter =\n+        appenderFactory.newPosDeleteWriter(outputFileFactory.newOutputFile(), format, partition);\n+    try (PositionDeleteWriter<T> closeableWriter = eqDeleteWriter) {\n+      for (Pair<CharSequence, Long> delete : deletes) {\n+        closeableWriter.delete(delete.first(), delete.second());\n+      }\n+    }\n+\n+    table.newRowDelta()\n+        .addRows(dataFile)\n+        .addDeletes(eqDeleteWriter.toDeleteFile())\n+        .validateDataFilesExist(eqDeleteWriter.referencedDataFiles())\n+        .validateDeletedFiles()\n+        .commit();\n+\n+    List<T> expected = Lists.newArrayList(\n+        createRow(2, \"bbb\"),\n+        createRow(4, \"ddd\")\n+    );\n+    Assert.assertEquals(\"Should have the expected records\", expectedRowSet(expected), actualRowSet(\"*\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 227}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8006b63bb4f2539d4afc7e0c4ede4d56b2295979", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/8006b63bb4f2539d4afc7e0c4ede4d56b2295979", "committedDate": "2020-12-01T06:57:42Z", "message": "Address comment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b26898d4746294bdec0d0bfce18060d864171c61", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/b26898d4746294bdec0d0bfce18060d864171c61", "committedDate": "2020-12-01T07:44:27Z", "message": "Minor changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8e423fb34d02da49e78a9750b6204f6ce23cead2", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/8e423fb34d02da49e78a9750b6204f6ce23cead2", "committedDate": "2020-12-01T07:59:30Z", "message": "Align the exception messages"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxNTg3Njkz", "url": "https://github.com/apache/iceberg/pull/1836#pullrequestreview-541587693", "createdAt": "2020-12-01T08:06:40Z", "commit": {"oid": "8e423fb34d02da49e78a9750b6204f6ce23cead2"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwODowNjo0MFrOH8cRig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwODowNjo0MFrOH8cRig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEzOTg1MA==", "bodyText": "Here I use the constructor that has spec argument because I believe we will use the DataWriter to append records once we switch to the RollingFileWriter. https://github.com/apache/iceberg/pull/1818/files#diff-fc9a9fd84d24c607fd85e053b08a559f56dd2dd2a46f1341c528e7a0269f873cR263.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r533139850", "createdAt": "2020-12-01T08:06:40Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -98,7 +98,7 @@ public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n         task, schema, schema, nameMapping, io.value(), encryptionManager.value(), caseSensitive);\n \n     StructType structType = SparkSchemaUtil.convert(schema);\n-    SparkAppenderFactory appenderFactory = new SparkAppenderFactory(properties, schema, structType);\n+    SparkAppenderFactory appenderFactory = new SparkAppenderFactory(properties, schema, structType, spec);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8e423fb34d02da49e78a9750b6204f6ce23cead2"}, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4b2b04bd45bafa5ea794fd35366e819afc45b574", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/4b2b04bd45bafa5ea794fd35366e819afc45b574", "committedDate": "2020-12-01T08:16:49Z", "message": "Make the pathPosSchema to be private"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyMzE2NTY5", "url": "https://github.com/apache/iceberg/pull/1836#pullrequestreview-542316569", "createdAt": "2020-12-01T21:36:58Z", "commit": {"oid": "4b2b04bd45bafa5ea794fd35366e819afc45b574"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQyMTozNjo1OVrOH9Au8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQyMTozNjo1OVrOH9Au8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzczNzIwMw==", "bodyText": "Nit: I think it would be better to use Function.identity().", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r533737203", "createdAt": "2020-12-01T21:36:59Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -281,6 +280,7 @@ public static DeleteWriteBuilder writeDeletes(OutputFile file) {\n     private StructLike partition = null;\n     private EncryptionKeyMetadata keyMetadata = null;\n     private int[] equalityFieldIds = null;\n+    private Function<CharSequence, ?> pathTransformFunc = t -> t;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4b2b04bd45bafa5ea794fd35366e819afc45b574"}, "originalPosition": 28}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyMzE3MTcy", "url": "https://github.com/apache/iceberg/pull/1836#pullrequestreview-542317172", "createdAt": "2020-12-01T21:37:56Z", "commit": {"oid": "4b2b04bd45bafa5ea794fd35366e819afc45b574"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQyMTozNzo1NlrOH9Awtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQyMTozNzo1NlrOH9Awtw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzczNzY1NQ==", "bodyText": "Does this line need to change? I'm fine removing the empty line, but I think throws can still fit on the previous line.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r533737655", "createdAt": "2020-12-01T21:37:56Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -379,37 +384,31 @@ public DeleteWriteBuilder equalityFieldIds(int... fieldIds) {\n           appenderBuilder.build(), FileFormat.PARQUET, location, spec, partition, keyMetadata, equalityFieldIds);\n     }\n \n-\n-    public <T> PositionDeleteWriter<T> buildPositionWriter() throws IOException {\n+    public <T> PositionDeleteWriter<T> buildPositionWriter()\n+        throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4b2b04bd45bafa5ea794fd35366e819afc45b574"}, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyMzE4MDM1", "url": "https://github.com/apache/iceberg/pull/1836#pullrequestreview-542318035", "createdAt": "2020-12-01T21:39:16Z", "commit": {"oid": "4b2b04bd45bafa5ea794fd35366e819afc45b574"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQyMTozOToxNlrOH9AzaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQyMTozOToxNlrOH9AzaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzczODM0NQ==", "bodyText": "Shouldn't this pass pathTransformFunc as well?", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r533738345", "createdAt": "2020-12-01T21:39:16Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -379,37 +384,31 @@ public DeleteWriteBuilder equalityFieldIds(int... fieldIds) {\n           appenderBuilder.build(), FileFormat.PARQUET, location, spec, partition, keyMetadata, equalityFieldIds);\n     }\n \n-\n-    public <T> PositionDeleteWriter<T> buildPositionWriter() throws IOException {\n+    public <T> PositionDeleteWriter<T> buildPositionWriter()\n+        throws IOException {\n       Preconditions.checkState(equalityFieldIds == null, \"Cannot create position delete file using delete field ids\");\n \n       meta(\"delete-type\", \"position\");\n \n       if (rowSchema != null && createWriterFunc != null) {\n         // the appender uses the row schema wrapped with position fields\n-        appenderBuilder.schema(new org.apache.iceberg.Schema(\n-            MetadataColumns.DELETE_FILE_PATH,\n-            MetadataColumns.DELETE_FILE_POS,\n-            NestedField.optional(\n-                MetadataColumns.DELETE_FILE_ROW_FIELD_ID, \"row\", rowSchema.asStruct(),\n-                MetadataColumns.DELETE_FILE_ROW_DOC)));\n+        appenderBuilder.schema(DeleteSchemaUtil.posDeleteSchema(rowSchema));\n \n         appenderBuilder.createWriterFunc(parquetSchema -> {\n           ParquetValueWriter<?> writer = createWriterFunc.apply(parquetSchema);\n           if (writer instanceof StructWriter) {\n-            return new PositionDeleteStructWriter<T>((StructWriter<?>) writer);\n+            return new PositionDeleteStructWriter<T>((StructWriter<?>) writer, pathTransformFunc);\n           } else {\n             throw new UnsupportedOperationException(\"Cannot wrap writer for position deletes: \" + writer.getClass());\n           }\n         });\n \n       } else {\n-        appenderBuilder.schema(new org.apache.iceberg.Schema(\n-            MetadataColumns.DELETE_FILE_PATH,\n-            MetadataColumns.DELETE_FILE_POS));\n+        appenderBuilder.schema(DeleteSchemaUtil.pathPosSchema());\n \n         appenderBuilder.createWriterFunc(parquetSchema ->\n-            new PositionDeleteStructWriter<T>((StructWriter<?>) GenericParquetWriter.buildWriter(parquetSchema)));\n+            new PositionDeleteStructWriter<T>((StructWriter<?>) GenericParquetWriter.buildWriter(parquetSchema),\n+                t -> t));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4b2b04bd45bafa5ea794fd35366e819afc45b574"}, "originalPosition": 85}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyMzIwMjc0", "url": "https://github.com/apache/iceberg/pull/1836#pullrequestreview-542320274", "createdAt": "2020-12-01T21:42:38Z", "commit": {"oid": "4b2b04bd45bafa5ea794fd35366e819afc45b574"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e6a8169000cbb740e3e7a19ae8e843f605e642ed", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/e6a8169000cbb740e3e7a19ae8e843f605e642ed", "committedDate": "2020-12-02T01:51:09Z", "message": "Address nit issues."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3433, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}