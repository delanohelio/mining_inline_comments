{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM5OTc1NDIy", "number": 1936, "title": "Flink: Support inferring parallelism for batch read.", "bodyText": "When using flink to query the iceberg table, the parallelism is the default parallelism of flink, but the number of datafiles on  iceberg table is different. The user do not know how much parallelism should be used, and setting a too large parallelism will cause  resource waste, setting the parallelism too small will cause the query to be slow, so we can add  parallelism infer.\nThe function is enabled by default. the parallelism is equal to the number of read splits. Of course, the user can manually turn off the infer function. In order to prevent too many datafiles from causing excessive parallelism, we also set a max infer parallelism.  When the infer parallelism exceeds the setting, use the max  parallelism.\nIn addition, we also need to compare with the limit in the select query statement to get a more appropriate parallelism in the case of limit pushdown, for example we have a sql  select * from table limit 1, and finally we infer the parallelism is 10, but we  only one parallel  is needed , besause we only need one data .", "createdAt": "2020-12-15T04:56:27Z", "url": "https://github.com/apache/iceberg/pull/1936", "merged": true, "mergeCommit": {"oid": "543a6cdd0538e505c4ab3fa0f0876a5a22bd7704"}, "closed": true, "closedAt": "2021-01-22T10:17:40Z", "author": {"login": "zhangjun0x01"}, "timelineItems": {"totalCount": 27, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdm-WEzgBqjQxMjM2NjU0MjE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdymX7xgFqTU3NDEyNDEyOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fa6e2365224f2ad38212a029dbb4f3135d9f187f", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/fa6e2365224f2ad38212a029dbb4f3135d9f187f", "committedDate": "2020-12-15T04:46:33Z", "message": "add parallelism optimize for IcebergTableSource"}, "afterCommit": {"oid": "d1b1b50dfd2f1d11bea431ca281e2c1864407ad2", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/d1b1b50dfd2f1d11bea431ca281e2c1864407ad2", "committedDate": "2020-12-17T07:24:02Z", "message": "add parallelism optimize for IcebergTableSource"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d1b1b50dfd2f1d11bea431ca281e2c1864407ad2", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/d1b1b50dfd2f1d11bea431ca281e2c1864407ad2", "committedDate": "2020-12-17T07:24:02Z", "message": "add parallelism optimize for IcebergTableSource"}, "afterCommit": {"oid": "5efeef410f78f99fab909907128ff7ec070b7869", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/5efeef410f78f99fab909907128ff7ec070b7869", "committedDate": "2020-12-18T03:00:48Z", "message": "add parallelism optimize for IcebergTableSource"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5efeef410f78f99fab909907128ff7ec070b7869", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/5efeef410f78f99fab909907128ff7ec070b7869", "committedDate": "2020-12-18T03:00:48Z", "message": "add parallelism optimize for IcebergTableSource"}, "afterCommit": {"oid": "adb96ec63828743c191fd66baaf66aab055f0636", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/adb96ec63828743c191fd66baaf66aab055f0636", "committedDate": "2020-12-18T03:09:27Z", "message": "add parallelism optimize for IcebergTableSource"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "adb96ec63828743c191fd66baaf66aab055f0636", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/adb96ec63828743c191fd66baaf66aab055f0636", "committedDate": "2020-12-18T03:09:27Z", "message": "add parallelism optimize for IcebergTableSource"}, "afterCommit": {"oid": "9cfe082290d3eaa30b5cb533ef00fba232520234", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/9cfe082290d3eaa30b5cb533ef00fba232520234", "committedDate": "2020-12-18T08:20:10Z", "message": "add parallelism optimize for IcebergTableSource"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9cfe082290d3eaa30b5cb533ef00fba232520234", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/9cfe082290d3eaa30b5cb533ef00fba232520234", "committedDate": "2020-12-18T08:20:10Z", "message": "add parallelism optimize for IcebergTableSource"}, "afterCommit": {"oid": "af2df6f04785246a1b2c0677019fedd267360925", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/af2df6f04785246a1b2c0677019fedd267360925", "committedDate": "2020-12-21T05:58:14Z", "message": "generate data by sql"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "af2df6f04785246a1b2c0677019fedd267360925", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/af2df6f04785246a1b2c0677019fedd267360925", "committedDate": "2020-12-21T05:58:14Z", "message": "generate data by sql"}, "afterCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413", "committedDate": "2021-01-11T01:34:19Z", "message": "generate data by sql"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTY1MjI4MTg1", "url": "https://github.com/apache/iceberg/pull/1936#pullrequestreview-565228185", "createdAt": "2021-01-11T10:31:17Z", "commit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMDozMToxN1rOIRPgJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xMVQxMTozOTo1M1rOIRRy6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk1MDY5Mw==", "bodyText": "The defaultValue(true) says deprecated now.  Let's change it to:\n  public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM =\n      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\").booleanType().defaultValue(true)\n          .withDescription(\"If is false, parallelism of source are set by config.\\n\" +\n              \"If is true, source parallelism is inferred according to splits number.\\n\");\nThe similar thing in TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554950693", "createdAt": "2021-01-11T10:31:17Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+\n+public class FlinkTableOptions {\n+\n+  private FlinkTableOptions() {\n+  }\n+\n+  public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM =\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\").defaultValue(true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk1NDA4Mg==", "bodyText": "How about moving those lines into a separate method ?", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554954082", "createdAt": "2021-01-11T10:37:06Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -195,7 +205,29 @@ public FlinkInputFormat buildFormat() {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        return env.createInput(format, rowTypeInfo);\n+        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk3ODk5MA==", "bodyText": "Nit: use UncheckedIOException here.\n            throw new UncheckedIOException(\"Failed to create iceberg input splits for table: \" + table, e);", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554978990", "createdAt": "2021-01-11T11:21:44Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -195,7 +205,29 @@ public FlinkInputFormat buildFormat() {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        return env.createInput(format, rowTypeInfo);\n+        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+        if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+          int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+          if (max < 1) {\n+            throw new IllegalConfigurationException(\n+                FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+          }\n+\n+          int splitNum = 0;\n+          try {\n+            FlinkInputSplit[] splits = format.createInputSplits(0);\n+            splitNum = splits.length;\n+          } catch (IOException e) {\n+            throw new RuntimeException(\"get input split  error.\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk3OTM0NA==", "bodyText": "nit: Preconditions.checkState ?", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554979344", "createdAt": "2021-01-11T11:22:29Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -195,7 +205,29 @@ public FlinkInputFormat buildFormat() {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        return env.createInput(format, rowTypeInfo);\n+        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+        if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+          int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+          if (max < 1) {\n+            throw new IllegalConfigurationException(\n+                FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+          }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk4MDU3Mw==", "bodyText": "It may be overflow when casting the long limit to integer  ?  I'd like to use (int) Math.min(parallelism, limit).", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554980573", "createdAt": "2021-01-11T11:25:10Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -195,7 +205,29 @@ public FlinkInputFormat buildFormat() {\n       Preconditions.checkNotNull(env, \"StreamExecutionEnvironment should not be null\");\n       FlinkInputFormat format = buildFormat();\n       if (isBounded(context)) {\n-        return env.createInput(format, rowTypeInfo);\n+        int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+        if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+          int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+          if (max < 1) {\n+            throw new IllegalConfigurationException(\n+                FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+          }\n+\n+          int splitNum = 0;\n+          try {\n+            FlinkInputSplit[] splits = format.createInputSplits(0);\n+            splitNum = splits.length;\n+          } catch (IOException e) {\n+            throw new RuntimeException(\"get input split  error.\", e);\n+          }\n+\n+          parallelism = Math.min(splitNum, max);\n+        }\n+\n+        parallelism = limit > 0 ? Math.min(parallelism, (int) limit) : parallelism;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk4MzQyNA==", "bodyText": "Is there another way to assert the parallelism as expected value ?  Here we're using flink's planner to get the ExecNode ,  I'm concerning that we're using flink's Internal codes which would be a big trouble when upgrading the flink version.  Pls see this PR #1956", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554983424", "createdAt": "2021-01-11T11:30:30Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -103,4 +124,45 @@ public void testLimitPushDown() {\n     Assert.assertEquals(\"should have 1 record\", 1, mixedResult.size());\n     Assert.assertArrayEquals(\"Should produce the expected records\", mixedResult.get(0), new Object[] {1, \"a\"});\n   }\n+\n+  @Test\n+  public void testParallelismOptimize() {\n+    sql(\"INSERT INTO %s  VALUES (1,'hello')\", TABLE_NAME);\n+    sql(\"INSERT INTO %s  VALUES (2,'iceberg')\", TABLE_NAME);\n+\n+    TableEnvironment tenv = getTableEnv();\n+\n+    // empty table ,parallelism at least 1\n+    Table tableEmpty = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));\n+    testParallelismSettingTranslateAndAssert(1, tableEmpty, tenv);\n+\n+    // make sure to generate 2 CombinedScanTasks\n+    org.apache.iceberg.Table table = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, TABLE_NAME));\n+    Stream<FileScanTask> stream = StreamSupport.stream(table.newScan().planFiles().spliterator(), false);\n+    Optional<FileScanTask> fileScanTaskOptional =  stream.max(Comparator.comparing(FileScanTask::length));\n+    Assert.assertTrue(fileScanTaskOptional.isPresent());\n+    long maxFileLen = fileScanTaskOptional.get().length();\n+    sql(\"ALTER TABLE %s SET ('read.split.open-file-cost'='1', 'read.split.target-size'='%s')\", TABLE_NAME, maxFileLen);\n+\n+    // 2 splits ,the parallelism is  2\n+    Table tableSelect = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));\n+    testParallelismSettingTranslateAndAssert(2, tableSelect, tenv);\n+\n+    // 2 splits  and limit is 1 ,the parallelism is  1\n+    Table tableLimit = tenv.sqlQuery(String.format(\"SELECT * FROM %s LIMIT 1\", TABLE_NAME));\n+    testParallelismSettingTranslateAndAssert(1, tableLimit, tenv);\n+  }\n+\n+  private void testParallelismSettingTranslateAndAssert(int expected, Table table, TableEnvironment tEnv) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk4NDYzNw==", "bodyText": "nit:  testParallelismOptimize -> testInferedParallelism", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554984637", "createdAt": "2021-01-11T11:32:59Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -103,4 +124,45 @@ public void testLimitPushDown() {\n     Assert.assertEquals(\"should have 1 record\", 1, mixedResult.size());\n     Assert.assertArrayEquals(\"Should produce the expected records\", mixedResult.get(0), new Object[] {1, \"a\"});\n   }\n+\n+  @Test\n+  public void testParallelismOptimize() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NDk4ODI2NQ==", "bodyText": "nit: how about introducing a small method:\n private Table sqlQuery(String sql, Object... args) {\n    return getTableEnv().sqlQuery(String.format(sql, args));\n  }", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r554988265", "createdAt": "2021-01-11T11:39:53Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -103,4 +124,45 @@ public void testLimitPushDown() {\n     Assert.assertEquals(\"should have 1 record\", 1, mixedResult.size());\n     Assert.assertArrayEquals(\"Should produce the expected records\", mixedResult.get(0), new Object[] {1, \"a\"});\n   }\n+\n+  @Test\n+  public void testParallelismOptimize() {\n+    sql(\"INSERT INTO %s  VALUES (1,'hello')\", TABLE_NAME);\n+    sql(\"INSERT INTO %s  VALUES (2,'iceberg')\", TABLE_NAME);\n+\n+    TableEnvironment tenv = getTableEnv();\n+\n+    // empty table ,parallelism at least 1\n+    Table tableEmpty = tenv.sqlQuery(String.format(\"SELECT * FROM %s\", TABLE_NAME));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413"}, "originalPosition": 63}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/6820d4e965c6b97a83bc5a97ae2c22bb3f2e5413", "committedDate": "2021-01-11T01:34:19Z", "message": "generate data by sql"}, "afterCommit": {"oid": "21ba46c286c0304bdcda28f717269114d1702d79", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/21ba46c286c0304bdcda28f717269114d1702d79", "committedDate": "2021-01-12T02:13:06Z", "message": "fix some issues"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "21ba46c286c0304bdcda28f717269114d1702d79", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/21ba46c286c0304bdcda28f717269114d1702d79", "committedDate": "2021-01-12T02:13:06Z", "message": "fix some issues"}, "afterCommit": {"oid": "5d3ca11b065155a0ced27e2712821d671751843d", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/5d3ca11b065155a0ced27e2712821d671751843d", "committedDate": "2021-01-14T08:25:28Z", "message": "merge from master"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5d3ca11b065155a0ced27e2712821d671751843d", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/5d3ca11b065155a0ced27e2712821d671751843d", "committedDate": "2021-01-14T08:25:28Z", "message": "merge from master"}, "afterCommit": {"oid": "4769b00c7df53e6d3beb305caf147781dfd771a2", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/4769b00c7df53e6d3beb305caf147781dfd771a2", "committedDate": "2021-01-16T11:34:11Z", "message": "add assert context"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4769b00c7df53e6d3beb305caf147781dfd771a2", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/4769b00c7df53e6d3beb305caf147781dfd771a2", "committedDate": "2021-01-16T11:34:11Z", "message": "add assert context"}, "afterCommit": {"oid": "b646fb3358ad4ba017a79d1c9ecc8c933dbf56a4", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/b646fb3358ad4ba017a79d1c9ecc8c933dbf56a4", "committedDate": "2021-01-16T14:12:32Z", "message": "add assert context"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "52a3468994549cc5f7c916be58f87fed720744a1", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/52a3468994549cc5f7c916be58f87fed720744a1", "committedDate": "2021-01-18T05:36:55Z", "message": "get the parallelism simply"}, "afterCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f", "committedDate": "2021-01-19T07:16:42Z", "message": "fix conflict"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTcxOTEyOTc3", "url": "https://github.com/apache/iceberg/pull/1936#pullrequestreview-571912977", "createdAt": "2021-01-20T06:49:06Z", "commit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNjo0OTowNlrOIWvUgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMFQwNzo0NjowNlrOIWwwpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcxNDg4MA==", "bodyText": "Nit:  I'd like to change this builder chain like the following ( That's more easy to read the change):\n  @Override\n  public DataStream<RowData> getDataStream(StreamExecutionEnvironment execEnv) {\n    return FlinkSource.forRowData()\n        .env(execEnv)\n        .tableLoader(loader)\n        .project(getProjectedSchema())\n        .limit(limit)\n        .filters(filters)\n        .flinkConf(readableConfig)\n        .properties(properties)\n        .build();\n  }", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560714880", "createdAt": "2021-01-20T06:49:06Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "diffHunk": "@@ -79,13 +84,13 @@ public boolean isBounded() {\n \n   @Override\n   public TableSource<RowData> projectFields(int[] fields) {\n-    return new IcebergTableSource(loader, schema, properties, fields, isLimitPushDown, limit, filters);\n+    return new IcebergTableSource(loader, schema, properties, fields, isLimitPushDown, limit, filters, readableConfig);\n   }\n \n   @Override\n   public DataStream<RowData> getDataStream(StreamExecutionEnvironment execEnv) {\n     return FlinkSource.forRowData().env(execEnv).tableLoader(loader).project(getProjectedSchema()).limit(limit)\n-        .filters(filters).properties(properties).build();\n+        .filters(filters).flinkConf(readableConfig).properties(properties).build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcxODg2OA==", "bodyText": "Nit: it's more clear to make each option definition into a separate line:\n      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\")\n          .booleanType()\n          .defaultValue(true)\n          .withDescription(\"If is false, parallelism of source are set by config.\\n\" +\n              \"If is true, source parallelism is inferred according to splits number.\\n\");", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560718868", "createdAt": "2021-01-20T06:59:39Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+\n+public class FlinkTableOptions {\n+\n+  private FlinkTableOptions() {\n+  }\n+\n+  public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM =\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\").booleanType().defaultValue(true)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcxODk0Mw==", "bodyText": "ditto", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560718943", "createdAt": "2021-01-20T06:59:47Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkTableOptions.java", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+\n+public class FlinkTableOptions {\n+\n+  private FlinkTableOptions() {\n+  }\n+\n+  public static final ConfigOption<Boolean> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM =\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism\").booleanType().defaultValue(true)\n+          .withDescription(\"If is false, parallelism of source are set by config.\\n\" +\n+              \"If is true, source parallelism is inferred according to splits number.\\n\");\n+\n+  public static final ConfigOption<Integer> TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX =\n+      ConfigOptions.key(\"table.exec.iceberg.infer-source-parallelism.max\").intType().defaultValue(100)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyNDA3NA==", "bodyText": "In this comment, I think I did not describe the things  clearly.   I mean  we could move the inferParallelism into a separate method, don't have to contains the DataStream constructing or chaining methods.\nprivate int inferParallelism(FlinkInputFormat format, ScanContext context) {\n   // ....\n}", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560724074", "createdAt": "2021-01-20T07:13:30Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -197,7 +206,7 @@ public FlinkInputFormat buildFormat() {\n       TypeInformation<RowData> typeInfo = RowDataTypeInfo.of(FlinkSchemaUtil.convert(context.project()));\n \n       if (!context.isStreaming()) {\n-        return env.createInput(format, typeInfo);\n+        return createInputDataStream(format, context, typeInfo);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyNTY1NQ==", "bodyText": "Nit:  I'd like to make this code more readable:\n      if (context.limit() > 0) {\n        int limit = context.limit() >= Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) context.limit();\n        parallelism = Math.min(parallelism, limit);\n      }\n\n     // parallelism must be positive.\n      parallelism = Math.max(1, parallelism);", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560725655", "createdAt": "2021-01-20T07:17:23Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -208,6 +217,30 @@ public FlinkInputFormat buildFormat() {\n             .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format));\n       }\n     }\n+\n+    private DataStream<RowData> createInputDataStream(FlinkInputFormat format, ScanContext context,\n+                                                      TypeInformation<RowData> typeInfo) {\n+      int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+        Preconditions.checkState(max >= 1,\n+            FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+        int splitNum = 0;\n+        try {\n+          FlinkInputSplit[] splits = format.createInputSplits(0);\n+          splitNum = splits.length;\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(\"Failed to create iceberg input splits for table: \" + table, e);\n+        }\n+\n+        parallelism = Math.min(splitNum, max);\n+      }\n+\n+      int limitInt = context.limit() > Integer.MAX_VALUE ? Integer.MAX_VALUE : (int) context.limit();\n+      parallelism = limitInt > 0 ? Math.min(parallelism, limitInt) : parallelism;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyNzkzNQ==", "bodyText": "For those users that write flink batch jobs in Java API ,  they will always pass a flink's Configuration, right ?   So how about defining this as  org.apache.flink.configuration.Configuraiton  ?", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560727935", "createdAt": "2021-01-20T07:22:25Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -70,6 +73,7 @@ public static Builder forRowData() {\n     private Table table;\n     private TableLoader tableLoader;\n     private TableSchema projectedSchema;\n+    private ReadableConfig flinkConf;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyODg3Nw==", "bodyText": "Nit: use maxInterParallelism pls.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560728877", "createdAt": "2021-01-20T07:24:53Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -208,6 +217,30 @@ public FlinkInputFormat buildFormat() {\n             .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format));\n       }\n     }\n+\n+    private DataStream<RowData> createInputDataStream(FlinkInputFormat format, ScanContext context,\n+                                                      TypeInformation<RowData> typeInfo) {\n+      int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDcyOTcxNg==", "bodyText": "Nit:  this assignment is redundant ( from intellij).", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560729716", "createdAt": "2021-01-20T07:26:52Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -208,6 +217,30 @@ public FlinkInputFormat buildFormat() {\n             .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format));\n       }\n     }\n+\n+    private DataStream<RowData> createInputDataStream(FlinkInputFormat format, ScanContext context,\n+                                                      TypeInformation<RowData> typeInfo) {\n+      int parallelism = flinkConf.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int max = flinkConf.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);\n+        Preconditions.checkState(max >= 1,\n+            FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX.key() + \" cannot be less than 1\");\n+        int splitNum = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA==", "bodyText": "Shouldn't the inferParallelism only affect the batch job (See FlinkSource#Builder#build)?  So there's no reason that providing unit test in streaming  mode ?\nIn my mind,  Providing unit tests to check whether the inferParallelism() is returning the expected parallelism value is enough for this changes.   Seems like The ITCase is validating the behavior of DataStreamSource#setParallelism ,  we could think it's always correct because it's a basic API in flink.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560737410", "createdAt": "2021-01-20T07:43:59Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -685,4 +782,60 @@ public void testSqlParseError() {\n     AssertHelpers.assertThrows(\"The NaN is not supported by flink now. \",\n         NumberFormatException.class, () -> sql(sqlParseErrorLTE));\n   }\n+\n+  /**\n+   * The sql can be executed in both streaming and batch mode, in order to get the parallelism, we convert the flink\n+   * Table to flink DataStream, so we only use streaming mode here.\n+   *\n+   * @throws TableNotExistException table not exist exception\n+   */\n+  @Test\n+  public void testInferedParallelism() throws TableNotExistException {\n+    Assume.assumeTrue(\"The execute mode should  be streaming mode\", isStreamingJob);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 567}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczODQ2OQ==", "bodyText": "In this way, we don't have to change so many codes in this class. Maybe we could just add unit tests in TestFlinkScan.java", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r560738469", "createdAt": "2021-01-20T07:46:06Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -685,4 +782,60 @@ public void testSqlParseError() {\n     AssertHelpers.assertThrows(\"The NaN is not supported by flink now. \",\n         NumberFormatException.class, () -> sql(sqlParseErrorLTE));\n   }\n+\n+  /**\n+   * The sql can be executed in both streaming and batch mode, in order to get the parallelism, we convert the flink\n+   * Table to flink DataStream, so we only use streaming mode here.\n+   *\n+   * @throws TableNotExistException table not exist exception\n+   */\n+  @Test\n+  public void testInferedParallelism() throws TableNotExistException {\n+    Assume.assumeTrue(\"The execute mode should  be streaming mode\", isStreamingJob);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MDczNzQxMA=="}, "originalCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f"}, "originalPosition": 567}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/a8ef6a3d0347c084e0cb128a5ac94e6a78b8959f", "committedDate": "2021-01-19T07:16:42Z", "message": "fix conflict"}, "afterCommit": {"oid": "cba8621ea0891c9abcdae9ab34821b15e953d0d8", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/cba8621ea0891c9abcdae9ab34821b15e953d0d8", "committedDate": "2021-01-21T06:38:55Z", "message": "add test case in TestFlinkScanSql"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cba8621ea0891c9abcdae9ab34821b15e953d0d8", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/cba8621ea0891c9abcdae9ab34821b15e953d0d8", "committedDate": "2021-01-21T06:38:55Z", "message": "add test case in TestFlinkScanSql"}, "afterCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/47ae11a8a83dee8adebc3a66301a21d3652f0092", "committedDate": "2021-01-21T07:19:18Z", "message": "add test case in TestFlinkScanSql"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTczOTA1NDE1", "url": "https://github.com/apache/iceberg/pull/1936#pullrequestreview-573905415", "createdAt": "2021-01-22T02:23:29Z", "commit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwMjoyMzoyOVrOIYSPUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0yMlQwNDowOTozMVrOIYUtCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjMzNTU2OA==", "bodyText": "Nit:  maxInterParallelism -> maxInferParallelism,  seems like it's a typo ?", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562335568", "createdAt": "2021-01-22T02:23:29Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -208,6 +218,33 @@ public FlinkInputFormat buildFormat() {\n             .transform(readerOperatorName, typeInfo, StreamingReaderOperator.factory(format));\n       }\n     }\n+\n+    int inferParallelism(FlinkInputFormat format, ScanContext context) {\n+      int parallelism = readableConfig.get(ExecutionConfigOptions.TABLE_EXEC_RESOURCE_DEFAULT_PARALLELISM);\n+      if (readableConfig.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM)) {\n+        int maxInterParallelism = readableConfig.get(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM_MAX);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjMzODI3NQ==", "bodyText": "Nit:  Let's move this line to line60,  so that the assignment order of IcebergTableSource constructor could align with these definitions.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562338275", "createdAt": "2021-01-22T02:27:20Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "diffHunk": "@@ -51,25 +52,29 @@\n   private final TableLoader loader;\n   private final TableSchema schema;\n   private final Map<String, String> properties;\n+  private final ReadableConfig readableConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM0MDA2MA==", "bodyText": "Nit:  maybe we'd better also align the orders as above commented.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562340060", "createdAt": "2021-01-22T02:29:55Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "diffHunk": "@@ -79,13 +84,20 @@ public boolean isBounded() {\n \n   @Override\n   public TableSource<RowData> projectFields(int[] fields) {\n-    return new IcebergTableSource(loader, schema, properties, fields, isLimitPushDown, limit, filters);\n+    return new IcebergTableSource(loader, schema, properties, fields, isLimitPushDown, limit, filters, readableConfig);\n   }\n \n   @Override\n   public DataStream<RowData> getDataStream(StreamExecutionEnvironment execEnv) {\n-    return FlinkSource.forRowData().env(execEnv).tableLoader(loader).project(getProjectedSchema()).limit(limit)\n-        .filters(filters).properties(properties).build();\n+    return FlinkSource.forRowData()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3MjA1Ng==", "bodyText": "We can disable the table.exec.iceberg.infer-source-parallelism  for all the batch tests by default, then we don't have to change all cases from this file.   Actually,  we have wrote many unit tests which depends on the parallelism, for example  this PR #2064.  Using the inter-parallelism for batch unit tests will introduce extra complexity and instability,  so I recommend to disable the infer parallelism in our batch unit tests by default:\ndiff --git a/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java b/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java\nindex 5b8e58cf..ab3d56ea 100644\n--- a/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java\n+++ b/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java\n@@ -62,10 +62,17 @@ public abstract class FlinkTestBase extends AbstractTestBase {\n     if (tEnv == null) {\n       synchronized (this) {\n         if (tEnv == null) {\n-          this.tEnv = TableEnvironment.create(EnvironmentSettings\n+          EnvironmentSettings settings = EnvironmentSettings\n               .newInstance()\n               .useBlinkPlanner()\n-              .inBatchMode().build());\n+              .inBatchMode()\n+              .build();\n+\n+          TableEnvironment env = TableEnvironment.create(settings);\n+          env.getConfig().getConfiguration()\n+              .set(FlinkTableOptions.TABLE_EXEC_ICEBERG_INFER_SOURCE_PARALLELISM, false);\n+\n+          tEnv = env;\n         }\n       }\n     }", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562372056", "createdAt": "2021-01-22T03:54:04Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -137,7 +136,10 @@ public void testFilterPushDownEqual() {\n     Assert.assertEquals(\"Should have 1 record\", 1, result.size());\n     Assert.assertArrayEquals(\"Should produce the expected record\", expectRecord, result.get(0));\n \n-    Assert.assertEquals(\"Should create only one scan\", 1, scanEventCount);\n+    // Because we add infer parallelism, all data files will be scanned first.\n+    // Flink will call FlinkInputFormat#createInputSplits method to scan the data files,\n+    // plus the operation to get the execution plan, so there are three scan event.\n+    Assert.assertEquals(\"Should create 3 scans\", 3, scanEventCount);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3MzIxNA==", "bodyText": "Nit:  inter parallelism should be at least 1.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562373214", "createdAt": "2021-01-22T03:58:11Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java", "diffHunk": "@@ -106,6 +109,51 @@ public void testResiduals() throws Exception {\n     assertRecords(runWithFilter(filter, \"where dt='2020-03-20' and id=123\"), expectedRecords, SCHEMA);\n   }\n \n+  @Test\n+  public void testInferedParallelism() throws IOException {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+\n+    TableLoader tableLoader = TableLoader.fromHadoopTable(table.location());\n+    FlinkInputFormat flinkInputFormat = FlinkSource.forRowData().tableLoader(tableLoader).table(table).buildFormat();\n+    ScanContext scanContext = ScanContext.builder().build();\n+\n+    // Empty table ,parallelism at least 1", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3MzgxMw==", "bodyText": "Should we provide a new Configuration()  for this variable ?  Otherwise,  it will just throw NPE if people forget to provide a flinkConf in FlinkSource#Builder because we don't check the nullable in interParallelism.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562373813", "createdAt": "2021-01-22T04:00:29Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -70,6 +73,7 @@ public static Builder forRowData() {\n     private Table table;\n     private TableLoader tableLoader;\n     private TableSchema projectedSchema;\n+    private ReadableConfig readableConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3NTE4Mw==", "bodyText": "Those random generated records will be located in partition 2020-03-21 ?   I guess it's not.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562375183", "createdAt": "2021-01-22T04:06:32Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java", "diffHunk": "@@ -106,6 +109,51 @@ public void testResiduals() throws Exception {\n     assertRecords(runWithFilter(filter, \"where dt='2020-03-20' and id=123\"), expectedRecords, SCHEMA);\n   }\n \n+  @Test\n+  public void testInferedParallelism() throws IOException {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+\n+    TableLoader tableLoader = TableLoader.fromHadoopTable(table.location());\n+    FlinkInputFormat flinkInputFormat = FlinkSource.forRowData().tableLoader(tableLoader).table(table).buildFormat();\n+    ScanContext scanContext = ScanContext.builder().build();\n+\n+    // Empty table ,parallelism at least 1\n+    int parallelism = FlinkSource.forRowData()\n+        .flinkConf(new Configuration())\n+        .inferParallelism(flinkInputFormat, scanContext);\n+    Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism);\n+\n+    List<Record> writeRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n+    writeRecords.get(0).set(1, 123L);\n+    writeRecords.get(0).set(2, \"2020-03-20\");\n+    writeRecords.get(1).set(1, 456L);\n+    writeRecords.get(1).set(2, \"2020-03-20\");\n+\n+    GenericAppenderHelper helper = new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER);\n+\n+    DataFile dataFile1 = helper.writeFile(TestHelpers.Row.of(\"2020-03-20\", 0), writeRecords);\n+    DataFile dataFile2 = helper.writeFile(TestHelpers.Row.of(\"2020-03-21\", 0),\n+        RandomGenericData.generate(SCHEMA, 2, 0L));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU2MjM3NTk0NA==", "bodyText": "I think there're other test cases that we don't cover, it's good to cover those tests.\n\ntable.exec.iceberg.infer-source-parallelism=false;\ntable.exec.iceberg.infer-source-parallelism.max <= numberOfSplits;\ntable.exec.iceberg.infer-source-parallelism.max > numberOfSplits;\ntable.exec.iceberg.infer-source-parallelism.max > limit;\ntable.exec.iceberg.infer-source-parallelism.max <= limit;\n\nDivide those cases into small method if necessary.", "url": "https://github.com/apache/iceberg/pull/1936#discussion_r562375944", "createdAt": "2021-01-22T04:09:31Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScanSql.java", "diffHunk": "@@ -106,6 +109,51 @@ public void testResiduals() throws Exception {\n     assertRecords(runWithFilter(filter, \"where dt='2020-03-20' and id=123\"), expectedRecords, SCHEMA);\n   }\n \n+  @Test\n+  public void testInferedParallelism() throws IOException {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+\n+    TableLoader tableLoader = TableLoader.fromHadoopTable(table.location());\n+    FlinkInputFormat flinkInputFormat = FlinkSource.forRowData().tableLoader(tableLoader).table(table).buildFormat();\n+    ScanContext scanContext = ScanContext.builder().build();\n+\n+    // Empty table ,parallelism at least 1\n+    int parallelism = FlinkSource.forRowData()\n+        .flinkConf(new Configuration())\n+        .inferParallelism(flinkInputFormat, scanContext);\n+    Assert.assertEquals(\"Should produce the expected parallelism.\", 1, parallelism);\n+\n+    List<Record> writeRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n+    writeRecords.get(0).set(1, 123L);\n+    writeRecords.get(0).set(2, \"2020-03-20\");\n+    writeRecords.get(1).set(1, 456L);\n+    writeRecords.get(1).set(2, \"2020-03-20\");\n+\n+    GenericAppenderHelper helper = new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER);\n+\n+    DataFile dataFile1 = helper.writeFile(TestHelpers.Row.of(\"2020-03-20\", 0), writeRecords);\n+    DataFile dataFile2 = helper.writeFile(TestHelpers.Row.of(\"2020-03-21\", 0),\n+        RandomGenericData.generate(SCHEMA, 2, 0L));\n+    helper.appendToTable(dataFile1, dataFile2);\n+\n+    // Make sure to generate 2 CombinedScanTasks\n+    long maxFileLen = Math.max(dataFile1.fileSizeInBytes(), dataFile2.fileSizeInBytes());\n+    executeSQL(String\n+        .format(\"ALTER TABLE t SET ('read.split.open-file-cost'='1', 'read.split.target-size'='%s')\", maxFileLen));\n+\n+    // 2 splits ,the parallelism is  2\n+    parallelism = FlinkSource.forRowData()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092"}, "originalPosition": 55}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ef7083111d0b0a3eba31c73f0eb1438b050633ad", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/ef7083111d0b0a3eba31c73f0eb1438b050633ad", "committedDate": "2021-01-22T06:02:02Z", "message": "add parallelism optimize for IcebergTableSource"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a58e0dd7fdd19097f6e4941f10c360592bb7e19e", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/a58e0dd7fdd19097f6e4941f10c360592bb7e19e", "committedDate": "2021-01-22T06:02:02Z", "message": "generate data by sql"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "92a6983540b0bb2e9cfa38bc327564885d2aedb1", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/92a6983540b0bb2e9cfa38bc327564885d2aedb1", "committedDate": "2021-01-22T06:02:02Z", "message": "fix some issues"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2abc15e811cb9ef161ff4d4da00800a052a04e4f", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/2abc15e811cb9ef161ff4d4da00800a052a04e4f", "committedDate": "2021-01-22T06:02:02Z", "message": "merge from master"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dd92b66becd8f3966c52c6dc3a3335ffd434842a", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/dd92b66becd8f3966c52c6dc3a3335ffd434842a", "committedDate": "2021-01-22T06:02:02Z", "message": "add assert context"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f73d8c62173abb632c5f7eee900b7439146e6f52", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/f73d8c62173abb632c5f7eee900b7439146e6f52", "committedDate": "2021-01-22T06:02:02Z", "message": "get the parallelism simply"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e31f3193233fe8ac539764aa83634b5aac6dc4e4", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/e31f3193233fe8ac539764aa83634b5aac6dc4e4", "committedDate": "2021-01-22T06:02:02Z", "message": "fix conflict"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8d925daaeef5b06bb12c9c0db2331294c5f926b3", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/8d925daaeef5b06bb12c9c0db2331294c5f926b3", "committedDate": "2021-01-22T06:02:02Z", "message": "add test case in TestFlinkScanSql"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5f656f8e4f277d88f64f8f6b3074f5e76e2fb014", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/5f656f8e4f277d88f64f8f6b3074f5e76e2fb014", "committedDate": "2021-01-22T07:51:51Z", "message": "fix some issues , add test case"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "47ae11a8a83dee8adebc3a66301a21d3652f0092", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/47ae11a8a83dee8adebc3a66301a21d3652f0092", "committedDate": "2021-01-21T07:19:18Z", "message": "add test case in TestFlinkScanSql"}, "afterCommit": {"oid": "5f656f8e4f277d88f64f8f6b3074f5e76e2fb014", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/5f656f8e4f277d88f64f8f6b3074f5e76e2fb014", "committedDate": "2021-01-22T07:51:51Z", "message": "fix some issues , add test case"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTc0MTI0MTI4", "url": "https://github.com/apache/iceberg/pull/1936#pullrequestreview-574124128", "createdAt": "2021-01-22T10:15:59Z", "commit": {"oid": "5f656f8e4f277d88f64f8f6b3074f5e76e2fb014"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3264, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}