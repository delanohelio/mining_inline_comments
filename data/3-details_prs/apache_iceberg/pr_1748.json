{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE4NDU1OTAz", "number": 1748, "title": "Hive read via HiveCatalog documentation", "bodyText": "", "createdAt": "2020-11-10T12:31:09Z", "url": "https://github.com/apache/iceberg/pull/1748", "merged": true, "mergeCommit": {"oid": "953a7fdd7f79ef86c7ad7d063efdbe32901da50e"}, "closed": true, "closedAt": "2020-11-20T01:10:37Z", "author": {"login": "massdosage"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdbIe3ugH2gAyNTE4NDU1OTAzOjYyNTIyNmVlNmZiNmQxNjdlMTViMGNhZTNkNWRiZWRjMmZiNTBlMWM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdeBZOKgH2gAyNTE4NDU1OTAzOjZjMjczMWU1NTM5YTI2NWI2ZGU3MWI3NjkzNjg0N2M1ZjdjMjM5MmE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "625226ee6fb6d167e15b0cae3d5dbedc2fb50e1c", "author": {"user": {"login": "massdosage", "name": "Adrian Woodhead"}}, "url": "https://github.com/apache/iceberg/commit/625226ee6fb6d167e15b0cae3d5dbedc2fb50e1c", "committedDate": "2020-11-10T12:26:09Z", "message": "initial commit to get comments on changes needed"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI3MTU3MTM2", "url": "https://github.com/apache/iceberg/pull/1748#pullrequestreview-527157136", "createdAt": "2020-11-10T12:34:57Z", "commit": {"oid": "625226ee6fb6d167e15b0cae3d5dbedc2fb50e1c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxMjozNDo1N1rOHwagyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQxMjozNDo1N1rOHwagyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyODA3NA==", "bodyText": "@pvary What does one need to do in order to get the table set up properly for the Hive read path in this case? What I have tried to do so far is this, first create an Iceberg table using the HiveCatalog like so:\n PartitionSpec spec = PartitionSpec.unpartitioned();\n Schema schema = new Schema(optional(1, \"id\", Types.LongType.get()), optional(2, \"name\", Types.StringType.get()));\n SparkSession spark = SparkSession.builder().appName(\"IcebergTest\").getOrCreate();\n Configuration hadoopConfiguration = spark.sparkContext().hadoopConfiguration();\n Catalog catalog = new HiveCatalog(hadoopConfiguration);\n TableIdentifier tableId = TableIdentifier.of(\"test\", \"iceberg_table_from_hive_catalog\");\n catalog.createTable(tableId, schema, spec);\n\nThe table created in Hive by the above has DDL like so:\nCREATE EXTERNAL TABLE `iceberg_table_from_hive_catalog`(\n  `id` bigint COMMENT '', \n  `name` string COMMENT '')\nROW FORMAT SERDE \n  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' \nSTORED AS INPUTFORMAT \n  'org.apache.hadoop.mapred.FileInputFormat' \nOUTPUTFORMAT \n  'org.apache.hadoop.mapred.FileOutputFormat'\nLOCATION\n  's3://REDACTED/iceberg_table_from_hive_catalog'\nTBLPROPERTIES (\n  'metadata_location'='s3://REDACTED/iceberg_table_from_hive_catalog/metadata/00000-7addbbf2-1836-4973-86af-0511ae7577fb.metadata.json', \n  'table_type'='ICEBERG', \n  'transient_lastDdlTime'='1605007216')\n\nWhich is obviously incorrect as the StorageHandler hasn't been set etc.. I know you worked on a PR that set this all up properly as long as some config/setup was performed at table creation time. Can you please let me know what I need to do and I'll then document it accordingly once I test it working?", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r520528074", "createdAt": "2020-11-10T12:34:57Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,19 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`.\n+TODO: what do we need to set up when we create this table programatically for everything to be registered correctly for read usage in Hive?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "625226ee6fb6d167e15b0cae3d5dbedc2fb50e1c"}, "originalPosition": 9}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2294af096ebd109efd902a4107081d4f85abe524", "author": {"user": {"login": "massdosage", "name": "Adrian Woodhead"}}, "url": "https://github.com/apache/iceberg/commit/2294af096ebd109efd902a4107081d4f85abe524", "committedDate": "2020-11-12T13:21:08Z", "message": "added details on two different methods for creating the table"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0ce135ca4ceb3cd683893575b61c3cdd8d193f5e", "author": {"user": {"login": "massdosage", "name": "Adrian Woodhead"}}, "url": "https://github.com/apache/iceberg/commit/0ce135ca4ceb3cd683893575b61c3cdd8d193f5e", "committedDate": "2020-11-12T13:21:54Z", "message": "fix indentation level"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5MzQ0Njky", "url": "https://github.com/apache/iceberg/pull/1748#pullrequestreview-529344692", "createdAt": "2020-11-12T17:54:09Z", "commit": {"oid": "0ce135ca4ceb3cd683893575b61c3cdd8d193f5e"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxNzo1NDowOVrOHyGzvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQxNzo1NDo1NFrOHyG1yA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjMwMjM5Ng==", "bodyText": "This seems to me contradictory to the following lines. Is this TODO still valid?", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r522302396", "createdAt": "2020-11-12T17:54:09Z", "author": {"login": "rdsr"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,38 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive.\n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`. In order for Iceberg to correctly set up the Hive table for querying some configuration values need to be set, the two options for this are described below - you can use either or the other depending on your use case.\n+\n+##### Hive Configuration\n+The value `iceberg.engine.hive.enabled` needs to be set to `true` and added to the Hive configuration file on the classpath of the application creating the table. This can be done by modifying the relevant `hive-site.xml`. Alternatively this can done programatically like so:\n+```java\n+Configuration hadoopConfiguration = spark.sparkContext().hadoopConfiguration();\n+hadoopConfiguration.set(ConfigProperties.ENGINE_HIVE_ENABLED, \"true\"); //iceberg.engine.hive.enabled=true\n+HiveCatalog catalog = new HiveCatalog(hadoopConfiguration);\n+...\n+catalog.createTable(tableId, schema, spec);\n+```\n+\n+##### Table Property Configuration\n+The property `engine.hive.enabled` needs to be set to `true` and added to the table properties when creating the Iceberg table. This can be done like so:\n+```java\n+    Map<String, String> tableProperties = new HashMap<String, String>();\n+    tableProperties.put(TableProperties.ENGINE_HIVE_ENABLED, \"true\"); ////engine.hive.enabled=true\n+    catalog.createTable(tableId, schema, spec, tableProperties);\n+```\n+\n+#### Query the Iceberg table via Hive\n+TODO: tables created by the above can't just be read \"as is\", need to document steps needed in order to be able to query them here.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ce135ca4ceb3cd683893575b61c3cdd8d193f5e"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjMwMjkyMA==", "bodyText": "I assume we can query existing Iceberg tables also through Hive by setting the iceberg.engine.hive.enabled flag? Do you think it worth calling that out?", "url": "https://github.com/apache/iceberg/pull/1748#discussion_r522302920", "createdAt": "2020-11-12T17:54:54Z", "author": {"login": "rdsr"}, "path": "site/docs/hive.md", "diffHunk": "@@ -50,6 +50,38 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_a;\n ```\n \n+#### Using Hive Catalog\n+Iceberg tables created using `HiveCatalog` are automatically registered with Hive.\n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HiveCatalog`. For the purposes of this documentation we will assume that the table is called `table_b` and that the table location is `s3://some_path/table_b`. In order for Iceberg to correctly set up the Hive table for querying some configuration values need to be set, the two options for this are described below - you can use either or the other depending on your use case.\n+\n+##### Hive Configuration\n+The value `iceberg.engine.hive.enabled` needs to be set to `true` and added to the Hive configuration file on the classpath of the application creating the table. This can be done by modifying the relevant `hive-site.xml`. Alternatively this can done programatically like so:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0ce135ca4ceb3cd683893575b61c3cdd8d193f5e"}, "originalPosition": 11}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6c2731e5539a265b6de71b76936847c5f7c2392a", "author": {"user": {"login": "massdosage", "name": "Adrian Woodhead"}}, "url": "https://github.com/apache/iceberg/commit/6c2731e5539a265b6de71b76936847c5f7c2392a", "committedDate": "2020-11-19T11:52:25Z", "message": "added property for querying HiveCatalog table"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3734, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}