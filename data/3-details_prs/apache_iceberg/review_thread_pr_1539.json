{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk1OTAzNTM1", "number": 1539, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwNDoyOTozOFrOEpT1xA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNjo1MToyNVrOEpjtmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNzUyMTMyOnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwNDoyOTozOFrOHa6Hng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwNTo0MDoyNlrOHa7L1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzI0Ng==", "bodyText": "Can we remove these lines then?", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497977246", "createdAt": "2020-10-01T04:29:38Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -130,6 +130,24 @@ public void close() throws CatalogException {\n     }\n   }\n \n+  // Going to save this change for another PR as this PR is already really big.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f8f2a5827054008833c514d920d5fbd82ff5187"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3OTk2NA==", "bodyText": "Yeah. All of the classes that show up kind of wonky, I'm going to just treat the same in this PR (let them show up as (new String[2]{\"hello\", \"world\"}).toString like the JUnit runner wants them to), and then I'll open an issue to follow up on them in a separate PR.\nSo this PR can be a straight addition of just names to the tests.", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497979964", "createdAt": "2020-10-01T04:42:01Z", "author": {"login": "kbendick"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -130,6 +130,24 @@ public void close() throws CatalogException {\n     }\n   }\n \n+  // Going to save this change for another PR as this PR is already really big.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzI0Ng=="}, "originalCommit": {"oid": "3f8f2a5827054008833c514d920d5fbd82ff5187"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4MjAyMg==", "bodyText": "I have removed this and will open an issue to track all of the places where the test parameters show up as string array references or other wonky toString values.", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497982022", "createdAt": "2020-10-01T04:50:14Z", "author": {"login": "kbendick"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -130,6 +130,24 @@ public void close() throws CatalogException {\n     }\n   }\n \n+  // Going to save this change for another PR as this PR is already really big.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzI0Ng=="}, "originalCommit": {"oid": "3f8f2a5827054008833c514d920d5fbd82ff5187"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4NDg0Nw==", "bodyText": "Ahhh I see your comment about reusing namespace. That makes the most sense to me as well. Given that I'll have to change the non-test code, I'll do i in a separate PR. I'll create an issue for it now and get to it sometime this week.", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497984847", "createdAt": "2020-10-01T05:02:10Z", "author": {"login": "kbendick"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -130,6 +130,24 @@ public void close() throws CatalogException {\n     }\n   }\n \n+  // Going to save this change for another PR as this PR is already really big.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzI0Ng=="}, "originalCommit": {"oid": "3f8f2a5827054008833c514d920d5fbd82ff5187"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5NDcwOA==", "bodyText": "I've removed those lines and created an issue to update baseNamespace to use the Namespace class for its human-readable toString implementation and its preexisting convenient constructors.\nHere's the issue: #1541. I'll pick it up sometime this week if nobody else does.", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497994708", "createdAt": "2020-10-01T05:40:26Z", "author": {"login": "kbendick"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -130,6 +130,24 @@ public void close() throws CatalogException {\n     }\n   }\n \n+  // Going to save this change for another PR as this PR is already really big.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzI0Ng=="}, "originalCommit": {"oid": "3f8f2a5827054008833c514d920d5fbd82ff5187"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNzUyMzk2OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwNDozMToxMlrOHa6JKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMDoxMTo0NlrOHbZS7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzY0MQ==", "bodyText": "I don't think that metadata columns have been implemented for vectorized Parquet yet? Does it pass if you uncomment it?", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497977641", "createdAt": "2020-10-01T04:31:12Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java", "diffHunk": "@@ -105,12 +105,10 @@\n     }\n   }\n \n-  @Parameterized.Parameters\n-  public static Object[][] parameters() {\n-    return new Object[][] {\n-        new Object[] { false },\n-        // new Object[] { true }\n-    };\n+  @Parameterized.Parameters(name =  \"vectorized = {0}\")\n+  // Note - Does not currently run with the `true` parameter. Why is that?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3f8f2a5827054008833c514d920d5fbd82ff5187"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4MDg4OA==", "bodyText": "Nope. None of the three tests pass with true.\nHere's a stack trace from testReadRowNumbersWithFilter:\nActual should be an InternalRow: schema\njava.lang.AssertionError: Actual should be an InternalRow: schema\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.assertTrue(Assert.java:41)\n\tat org.apache.iceberg.spark.data.TestHelpers.assertEquals(TestHelpers.java:611)\n\tat org.apache.iceberg.spark.data.TestHelpers.assertEquals(TestHelpers.java:600)\n\tat org.apache.iceberg.spark.data.TestSparkParquetReadMetadataColumns.readAndValidate(TestSparkParquetReadMetadataColumns.java:212)\n\tat org.apache.iceberg.spark.data.TestSparkParquetReadMetadataColumns.testReadRowNumbersWithFilter(TestSparkParquetReadMetadataColumns.java:165)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n...", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497980888", "createdAt": "2020-10-01T04:45:38Z", "author": {"login": "kbendick"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java", "diffHunk": "@@ -105,12 +105,10 @@\n     }\n   }\n \n-  @Parameterized.Parameters\n-  public static Object[][] parameters() {\n-    return new Object[][] {\n-        new Object[] { false },\n-        // new Object[] { true }\n-    };\n+  @Parameterized.Parameters(name =  \"vectorized = {0}\")\n+  // Note - Does not currently run with the `true` parameter. Why is that?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzY0MQ=="}, "originalCommit": {"oid": "3f8f2a5827054008833c514d920d5fbd82ff5187"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4MzI4OA==", "bodyText": "I can open an issue for this if you'd like. I know there's been a lot of discussion around the time and cost of reading tables with very large numbers of metadata files, and possibly this would help?", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497983288", "createdAt": "2020-10-01T04:55:21Z", "author": {"login": "kbendick"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java", "diffHunk": "@@ -105,12 +105,10 @@\n     }\n   }\n \n-  @Parameterized.Parameters\n-  public static Object[][] parameters() {\n-    return new Object[][] {\n-        new Object[] { false },\n-        // new Object[] { true }\n-    };\n+  @Parameterized.Parameters(name =  \"vectorized = {0}\")\n+  // Note - Does not currently run with the `true` parameter. Why is that?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzY0MQ=="}, "originalCommit": {"oid": "3f8f2a5827054008833c514d920d5fbd82ff5187"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4NTU3MA==", "bodyText": "I opened an issue to add support: #1540\nPossibly we can remove the discussion of true and just add a link to the issue? Is this even something we care about implementing (I believe likely yes if we allow metadata files to reach the GB size). Especially given that we had a discussion today in the meeting and later on slack about distributed query planning for metadata and file pruning started by @aokolnychyi. However, I would defer to your judgement about whether or not to keep the issue open.", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497985570", "createdAt": "2020-10-01T05:05:03Z", "author": {"login": "kbendick"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java", "diffHunk": "@@ -105,12 +105,10 @@\n     }\n   }\n \n-  @Parameterized.Parameters\n-  public static Object[][] parameters() {\n-    return new Object[][] {\n-        new Object[] { false },\n-        // new Object[] { true }\n-    };\n+  @Parameterized.Parameters(name =  \"vectorized = {0}\")\n+  // Note - Does not currently run with the `true` parameter. Why is that?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzY0MQ=="}, "originalCommit": {"oid": "3f8f2a5827054008833c514d920d5fbd82ff5187"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODM4OTYxMw==", "bodyText": "Yes, we should point to the issue. Thanks for updating it.\nTo clarify, this is for vectorized data reads with the _pos metadata column to get row position, not for reading metadata files. I think that this is being implemented in #1356.", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498389613", "createdAt": "2020-10-01T16:58:23Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java", "diffHunk": "@@ -105,12 +105,10 @@\n     }\n   }\n \n-  @Parameterized.Parameters\n-  public static Object[][] parameters() {\n-    return new Object[][] {\n-        new Object[] { false },\n-        // new Object[] { true }\n-    };\n+  @Parameterized.Parameters(name =  \"vectorized = {0}\")\n+  // Note - Does not currently run with the `true` parameter. Why is that?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzY0MQ=="}, "originalCommit": {"oid": "3f8f2a5827054008833c514d920d5fbd82ff5187"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ4ODA0Ng==", "bodyText": "Thanks for the clarification. I changed the issue title to Add in support for vectorized parquet reads with filters on row position in metadata. How does that sound? I also updated the comment.", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498488046", "createdAt": "2020-10-01T20:11:46Z", "author": {"login": "kbendick"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java", "diffHunk": "@@ -105,12 +105,10 @@\n     }\n   }\n \n-  @Parameterized.Parameters\n-  public static Object[][] parameters() {\n-    return new Object[][] {\n-        new Object[] { false },\n-        // new Object[] { true }\n-    };\n+  @Parameterized.Parameters(name =  \"vectorized = {0}\")\n+  // Note - Does not currently run with the `true` parameter. Why is that?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk3NzY0MQ=="}, "originalCommit": {"oid": "3f8f2a5827054008833c514d920d5fbd82ff5187"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNzU2NTE1OnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/TestIcebergInputFormats.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwNDo1Njo1NlrOHa6glw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwNjowMDo1MVrOHa7ihg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4MzYzOQ==", "bodyText": "@rdblue should I remove this one and instead grab it in a follow up PR? Its just part of the test suites, but the template parameter T could be grabbed via java reflection - and it is different in some of the tests.", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497983639", "createdAt": "2020-10-01T04:56:56Z", "author": {"login": "kbendick"}, "path": "mr/src/test/java/org/apache/iceberg/mr/TestIcebergInputFormats.java", "diffHunk": "@@ -408,6 +408,11 @@ public String name() {\n         public TestInputFormat<T> create(Configuration conf) {\n           return function.apply(conf);\n         }\n+\n+        @Override\n+        public String toString() {\n+          return String.format(\"Test%s<T>\", name());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0042199763aa5a6dd0ff59119e705562829be901"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk5ODIwNg==", "bodyText": "I'll leave the toString method and then open an issue for getting the proper name of the template parameter, given that we test with multiple types substituted for T. It can be obtained via java reflection and since this is limited to the test files, there's no real overhead outside of potentially test time (but this isn't called so often as to be a concern).", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r497998206", "createdAt": "2020-10-01T05:52:52Z", "author": {"login": "kbendick"}, "path": "mr/src/test/java/org/apache/iceberg/mr/TestIcebergInputFormats.java", "diffHunk": "@@ -408,6 +408,11 @@ public String name() {\n         public TestInputFormat<T> create(Configuration conf) {\n           return function.apply(conf);\n         }\n+\n+        @Override\n+        public String toString() {\n+          return String.format(\"Test%s<T>\", name());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4MzYzOQ=="}, "originalCommit": {"oid": "0042199763aa5a6dd0ff59119e705562829be901"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODAwMDUxOA==", "bodyText": "Ok. I've decided to leave it in as stands and I've created an issue to see if I can follow up on it so that we have the different types used in the parameter groups properly displayed in the tests: #1542", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498000518", "createdAt": "2020-10-01T06:00:51Z", "author": {"login": "kbendick"}, "path": "mr/src/test/java/org/apache/iceberg/mr/TestIcebergInputFormats.java", "diffHunk": "@@ -408,6 +408,11 @@ public String name() {\n         public TestInputFormat<T> create(Configuration conf) {\n           return function.apply(conf);\n         }\n+\n+        @Override\n+        public String toString() {\n+          return String.format(\"Test%s<T>\", name());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzk4MzYzOQ=="}, "originalCommit": {"oid": "0042199763aa5a6dd0ff59119e705562829be901"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNzczNDY4OnYy", "diffSide": "RIGHT", "path": "data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwNjoyMzo0NlrOHa8A2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMDowMDowOFrOHbY-Rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODAwODI4Mw==", "bodyText": "I checked on this, and ORC-611 is closed and the fix versions are 1.6.4 and 1.7.0.\nI'm going to see if I can get these tests to pass or open a ticket about possibly upgrading our ORC version if not.", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498008283", "createdAt": "2020-10-01T06:23:46Z", "author": {"login": "kbendick"}, "path": "data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java", "diffHunk": "@@ -222,52 +222,48 @@ public void createParquetInputFile(List<Record> records) throws IOException {\n   private final Object readValue;\n   private final Object skipValue;\n \n-  @Parameterized.Parameters\n+  @Parameterized.Parameters(name = \"format = {0} column = {1} readValue = {2} skipValue = {3}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-        new Object[] { \"parquet\", \"boolean\", false, true },\n-        new Object[] { \"parquet\", \"int\", 5, 55 },\n-        new Object[] { \"parquet\", \"long\", 5_000_000_049L, 5_000L },\n-        new Object[] { \"parquet\", \"float\", 1.97f, 2.11f },\n-        new Object[] { \"parquet\", \"double\", 2.11d, 1.97d },\n-        new Object[] { \"parquet\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n-        new Object[] { \"parquet\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n-        new Object[] { \"parquet\", \"timestamp\",\n-            \"2018-06-29T10:02:34.000000\",\n-            \"2018-06-29T15:02:34.000000\" },\n-        new Object[] { \"parquet\", \"timestamptz\",\n-            \"2018-06-29T10:02:34.000000+00:00\",\n-            \"2018-06-29T10:02:34.000000-07:00\" },\n-        new Object[] { \"parquet\", \"string\", \"tapir\", \"monthly\" },\n-        // new Object[] { \"parquet\", \"uuid\", uuid, UUID.randomUUID() }, // not supported yet\n-        new Object[] { \"parquet\", \"fixed\", \"abcd\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3 } },\n-        new Object[] { \"parquet\", \"binary\", \"xyz\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3, 4, 5 } },\n-        new Object[] { \"parquet\", \"int_decimal\", \"77.77\", \"12.34\" },\n-        new Object[] { \"parquet\", \"long_decimal\", \"88.88\", \"12.34\" },\n-        new Object[] { \"parquet\", \"fixed_decimal\", \"99.99\", \"12.34\" },\n+        { \"parquet\", \"boolean\", false, true },\n+        { \"parquet\", \"int\", 5, 55 },\n+        { \"parquet\", \"long\", 5_000_000_049L, 5_000L },\n+        { \"parquet\", \"float\", 1.97f, 2.11f },\n+        { \"parquet\", \"double\", 2.11d, 1.97d },\n+        { \"parquet\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n+        { \"parquet\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n+        { \"parquet\", \"timestamp\", \"2018-06-29T10:02:34.000000\", \"2018-06-29T15:02:34.000000\" },\n+        { \"parquet\", \"timestamptz\", \"2018-06-29T10:02:34.000000+00:00\", \"2018-06-29T10:02:34.000000-07:00\" },\n+        { \"parquet\", \"string\", \"tapir\", \"monthly\" },\n+        // { \"parquet\", \"uuid\", uuid, UUID.randomUUID() }, // not supported yet\n+        { \"parquet\", \"fixed\", \"abcd\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3 } },\n+        { \"parquet\", \"binary\", \"xyz\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3, 4, 5 } },\n+        { \"parquet\", \"int_decimal\", \"77.77\", \"12.34\" },\n+        { \"parquet\", \"long_decimal\", \"88.88\", \"12.34\" },\n+        { \"parquet\", \"fixed_decimal\", \"99.99\", \"12.34\" },\n \n-        new Object[] { \"orc\", \"boolean\", false, true },\n-        new Object[] { \"orc\", \"int\", 5, 55 },\n-        new Object[] { \"orc\", \"long\", 5_000_000_049L, 5_000L },\n-        new Object[] { \"orc\", \"float\", 1.97f, 2.11f },\n-        new Object[] { \"orc\", \"double\", 2.11d, 1.97d },\n-        new Object[] { \"orc\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n-        new Object[] { \"orc\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n+        { \"orc\", \"boolean\", false, true },\n+        { \"orc\", \"int\", 5, 55 },\n+        { \"orc\", \"long\", 5_000_000_049L, 5_000L },\n+        { \"orc\", \"float\", 1.97f, 2.11f },\n+        { \"orc\", \"double\", 2.11d, 1.97d },\n+        { \"orc\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n+        { \"orc\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n         // Temporarily disable filters on Timestamp columns due to ORC-611\n-        // new Object[] { \"orc\", \"timestamp\",\n+        // { \"orc\", \"timestamp\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fcec58cc5a7351c2fc72978ca5cfef4e0aa4fbe7"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODM2NTQ5NA==", "bodyText": "I tried updating but it did not affect the tests. There were several orc related files on the compile time path, but I'm not sure what ran during the tests.\nIt looks like the issue ORC-611 was closed, but it still exists in https://issues.apache.org/jira/browse/HIVE-23036", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498365494", "createdAt": "2020-10-01T16:16:19Z", "author": {"login": "kbendick"}, "path": "data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java", "diffHunk": "@@ -222,52 +222,48 @@ public void createParquetInputFile(List<Record> records) throws IOException {\n   private final Object readValue;\n   private final Object skipValue;\n \n-  @Parameterized.Parameters\n+  @Parameterized.Parameters(name = \"format = {0} column = {1} readValue = {2} skipValue = {3}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-        new Object[] { \"parquet\", \"boolean\", false, true },\n-        new Object[] { \"parquet\", \"int\", 5, 55 },\n-        new Object[] { \"parquet\", \"long\", 5_000_000_049L, 5_000L },\n-        new Object[] { \"parquet\", \"float\", 1.97f, 2.11f },\n-        new Object[] { \"parquet\", \"double\", 2.11d, 1.97d },\n-        new Object[] { \"parquet\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n-        new Object[] { \"parquet\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n-        new Object[] { \"parquet\", \"timestamp\",\n-            \"2018-06-29T10:02:34.000000\",\n-            \"2018-06-29T15:02:34.000000\" },\n-        new Object[] { \"parquet\", \"timestamptz\",\n-            \"2018-06-29T10:02:34.000000+00:00\",\n-            \"2018-06-29T10:02:34.000000-07:00\" },\n-        new Object[] { \"parquet\", \"string\", \"tapir\", \"monthly\" },\n-        // new Object[] { \"parquet\", \"uuid\", uuid, UUID.randomUUID() }, // not supported yet\n-        new Object[] { \"parquet\", \"fixed\", \"abcd\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3 } },\n-        new Object[] { \"parquet\", \"binary\", \"xyz\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3, 4, 5 } },\n-        new Object[] { \"parquet\", \"int_decimal\", \"77.77\", \"12.34\" },\n-        new Object[] { \"parquet\", \"long_decimal\", \"88.88\", \"12.34\" },\n-        new Object[] { \"parquet\", \"fixed_decimal\", \"99.99\", \"12.34\" },\n+        { \"parquet\", \"boolean\", false, true },\n+        { \"parquet\", \"int\", 5, 55 },\n+        { \"parquet\", \"long\", 5_000_000_049L, 5_000L },\n+        { \"parquet\", \"float\", 1.97f, 2.11f },\n+        { \"parquet\", \"double\", 2.11d, 1.97d },\n+        { \"parquet\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n+        { \"parquet\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n+        { \"parquet\", \"timestamp\", \"2018-06-29T10:02:34.000000\", \"2018-06-29T15:02:34.000000\" },\n+        { \"parquet\", \"timestamptz\", \"2018-06-29T10:02:34.000000+00:00\", \"2018-06-29T10:02:34.000000-07:00\" },\n+        { \"parquet\", \"string\", \"tapir\", \"monthly\" },\n+        // { \"parquet\", \"uuid\", uuid, UUID.randomUUID() }, // not supported yet\n+        { \"parquet\", \"fixed\", \"abcd\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3 } },\n+        { \"parquet\", \"binary\", \"xyz\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3, 4, 5 } },\n+        { \"parquet\", \"int_decimal\", \"77.77\", \"12.34\" },\n+        { \"parquet\", \"long_decimal\", \"88.88\", \"12.34\" },\n+        { \"parquet\", \"fixed_decimal\", \"99.99\", \"12.34\" },\n \n-        new Object[] { \"orc\", \"boolean\", false, true },\n-        new Object[] { \"orc\", \"int\", 5, 55 },\n-        new Object[] { \"orc\", \"long\", 5_000_000_049L, 5_000L },\n-        new Object[] { \"orc\", \"float\", 1.97f, 2.11f },\n-        new Object[] { \"orc\", \"double\", 2.11d, 1.97d },\n-        new Object[] { \"orc\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n-        new Object[] { \"orc\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n+        { \"orc\", \"boolean\", false, true },\n+        { \"orc\", \"int\", 5, 55 },\n+        { \"orc\", \"long\", 5_000_000_049L, 5_000L },\n+        { \"orc\", \"float\", 1.97f, 2.11f },\n+        { \"orc\", \"double\", 2.11d, 1.97d },\n+        { \"orc\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n+        { \"orc\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n         // Temporarily disable filters on Timestamp columns due to ORC-611\n-        // new Object[] { \"orc\", \"timestamp\",\n+        // { \"orc\", \"timestamp\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODAwODI4Mw=="}, "originalCommit": {"oid": "fcec58cc5a7351c2fc72978ca5cfef4e0aa4fbe7"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODM3NjU1NA==", "bodyText": "I'm going to also try 1.7.0, but I'm wondering if we aren't fully excluding some ORC dependencies from other dependencies.", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498376554", "createdAt": "2020-10-01T16:34:52Z", "author": {"login": "kbendick"}, "path": "data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java", "diffHunk": "@@ -222,52 +222,48 @@ public void createParquetInputFile(List<Record> records) throws IOException {\n   private final Object readValue;\n   private final Object skipValue;\n \n-  @Parameterized.Parameters\n+  @Parameterized.Parameters(name = \"format = {0} column = {1} readValue = {2} skipValue = {3}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-        new Object[] { \"parquet\", \"boolean\", false, true },\n-        new Object[] { \"parquet\", \"int\", 5, 55 },\n-        new Object[] { \"parquet\", \"long\", 5_000_000_049L, 5_000L },\n-        new Object[] { \"parquet\", \"float\", 1.97f, 2.11f },\n-        new Object[] { \"parquet\", \"double\", 2.11d, 1.97d },\n-        new Object[] { \"parquet\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n-        new Object[] { \"parquet\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n-        new Object[] { \"parquet\", \"timestamp\",\n-            \"2018-06-29T10:02:34.000000\",\n-            \"2018-06-29T15:02:34.000000\" },\n-        new Object[] { \"parquet\", \"timestamptz\",\n-            \"2018-06-29T10:02:34.000000+00:00\",\n-            \"2018-06-29T10:02:34.000000-07:00\" },\n-        new Object[] { \"parquet\", \"string\", \"tapir\", \"monthly\" },\n-        // new Object[] { \"parquet\", \"uuid\", uuid, UUID.randomUUID() }, // not supported yet\n-        new Object[] { \"parquet\", \"fixed\", \"abcd\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3 } },\n-        new Object[] { \"parquet\", \"binary\", \"xyz\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3, 4, 5 } },\n-        new Object[] { \"parquet\", \"int_decimal\", \"77.77\", \"12.34\" },\n-        new Object[] { \"parquet\", \"long_decimal\", \"88.88\", \"12.34\" },\n-        new Object[] { \"parquet\", \"fixed_decimal\", \"99.99\", \"12.34\" },\n+        { \"parquet\", \"boolean\", false, true },\n+        { \"parquet\", \"int\", 5, 55 },\n+        { \"parquet\", \"long\", 5_000_000_049L, 5_000L },\n+        { \"parquet\", \"float\", 1.97f, 2.11f },\n+        { \"parquet\", \"double\", 2.11d, 1.97d },\n+        { \"parquet\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n+        { \"parquet\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n+        { \"parquet\", \"timestamp\", \"2018-06-29T10:02:34.000000\", \"2018-06-29T15:02:34.000000\" },\n+        { \"parquet\", \"timestamptz\", \"2018-06-29T10:02:34.000000+00:00\", \"2018-06-29T10:02:34.000000-07:00\" },\n+        { \"parquet\", \"string\", \"tapir\", \"monthly\" },\n+        // { \"parquet\", \"uuid\", uuid, UUID.randomUUID() }, // not supported yet\n+        { \"parquet\", \"fixed\", \"abcd\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3 } },\n+        { \"parquet\", \"binary\", \"xyz\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3, 4, 5 } },\n+        { \"parquet\", \"int_decimal\", \"77.77\", \"12.34\" },\n+        { \"parquet\", \"long_decimal\", \"88.88\", \"12.34\" },\n+        { \"parquet\", \"fixed_decimal\", \"99.99\", \"12.34\" },\n \n-        new Object[] { \"orc\", \"boolean\", false, true },\n-        new Object[] { \"orc\", \"int\", 5, 55 },\n-        new Object[] { \"orc\", \"long\", 5_000_000_049L, 5_000L },\n-        new Object[] { \"orc\", \"float\", 1.97f, 2.11f },\n-        new Object[] { \"orc\", \"double\", 2.11d, 1.97d },\n-        new Object[] { \"orc\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n-        new Object[] { \"orc\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n+        { \"orc\", \"boolean\", false, true },\n+        { \"orc\", \"int\", 5, 55 },\n+        { \"orc\", \"long\", 5_000_000_049L, 5_000L },\n+        { \"orc\", \"float\", 1.97f, 2.11f },\n+        { \"orc\", \"double\", 2.11d, 1.97d },\n+        { \"orc\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n+        { \"orc\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n         // Temporarily disable filters on Timestamp columns due to ORC-611\n-        // new Object[] { \"orc\", \"timestamp\",\n+        // { \"orc\", \"timestamp\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODAwODI4Mw=="}, "originalCommit": {"oid": "fcec58cc5a7351c2fc72978ca5cfef4e0aa4fbe7"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ4Mjc1OQ==", "bodyText": "Turns out 1.7.0 is not published to maven. I am keeping these tests commented out with a mention of the corresponding still open HIVE ticket, as it indicates where in the OrcInputFormat for Hive is the issue. Might be of use to us.", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498482759", "createdAt": "2020-10-01T20:00:08Z", "author": {"login": "kbendick"}, "path": "data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java", "diffHunk": "@@ -222,52 +222,48 @@ public void createParquetInputFile(List<Record> records) throws IOException {\n   private final Object readValue;\n   private final Object skipValue;\n \n-  @Parameterized.Parameters\n+  @Parameterized.Parameters(name = \"format = {0} column = {1} readValue = {2} skipValue = {3}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-        new Object[] { \"parquet\", \"boolean\", false, true },\n-        new Object[] { \"parquet\", \"int\", 5, 55 },\n-        new Object[] { \"parquet\", \"long\", 5_000_000_049L, 5_000L },\n-        new Object[] { \"parquet\", \"float\", 1.97f, 2.11f },\n-        new Object[] { \"parquet\", \"double\", 2.11d, 1.97d },\n-        new Object[] { \"parquet\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n-        new Object[] { \"parquet\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n-        new Object[] { \"parquet\", \"timestamp\",\n-            \"2018-06-29T10:02:34.000000\",\n-            \"2018-06-29T15:02:34.000000\" },\n-        new Object[] { \"parquet\", \"timestamptz\",\n-            \"2018-06-29T10:02:34.000000+00:00\",\n-            \"2018-06-29T10:02:34.000000-07:00\" },\n-        new Object[] { \"parquet\", \"string\", \"tapir\", \"monthly\" },\n-        // new Object[] { \"parquet\", \"uuid\", uuid, UUID.randomUUID() }, // not supported yet\n-        new Object[] { \"parquet\", \"fixed\", \"abcd\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3 } },\n-        new Object[] { \"parquet\", \"binary\", \"xyz\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3, 4, 5 } },\n-        new Object[] { \"parquet\", \"int_decimal\", \"77.77\", \"12.34\" },\n-        new Object[] { \"parquet\", \"long_decimal\", \"88.88\", \"12.34\" },\n-        new Object[] { \"parquet\", \"fixed_decimal\", \"99.99\", \"12.34\" },\n+        { \"parquet\", \"boolean\", false, true },\n+        { \"parquet\", \"int\", 5, 55 },\n+        { \"parquet\", \"long\", 5_000_000_049L, 5_000L },\n+        { \"parquet\", \"float\", 1.97f, 2.11f },\n+        { \"parquet\", \"double\", 2.11d, 1.97d },\n+        { \"parquet\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n+        { \"parquet\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n+        { \"parquet\", \"timestamp\", \"2018-06-29T10:02:34.000000\", \"2018-06-29T15:02:34.000000\" },\n+        { \"parquet\", \"timestamptz\", \"2018-06-29T10:02:34.000000+00:00\", \"2018-06-29T10:02:34.000000-07:00\" },\n+        { \"parquet\", \"string\", \"tapir\", \"monthly\" },\n+        // { \"parquet\", \"uuid\", uuid, UUID.randomUUID() }, // not supported yet\n+        { \"parquet\", \"fixed\", \"abcd\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3 } },\n+        { \"parquet\", \"binary\", \"xyz\".getBytes(StandardCharsets.UTF_8), new byte[] { 0, 1, 2, 3, 4, 5 } },\n+        { \"parquet\", \"int_decimal\", \"77.77\", \"12.34\" },\n+        { \"parquet\", \"long_decimal\", \"88.88\", \"12.34\" },\n+        { \"parquet\", \"fixed_decimal\", \"99.99\", \"12.34\" },\n \n-        new Object[] { \"orc\", \"boolean\", false, true },\n-        new Object[] { \"orc\", \"int\", 5, 55 },\n-        new Object[] { \"orc\", \"long\", 5_000_000_049L, 5_000L },\n-        new Object[] { \"orc\", \"float\", 1.97f, 2.11f },\n-        new Object[] { \"orc\", \"double\", 2.11d, 1.97d },\n-        new Object[] { \"orc\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n-        new Object[] { \"orc\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n+        { \"orc\", \"boolean\", false, true },\n+        { \"orc\", \"int\", 5, 55 },\n+        { \"orc\", \"long\", 5_000_000_049L, 5_000L },\n+        { \"orc\", \"float\", 1.97f, 2.11f },\n+        { \"orc\", \"double\", 2.11d, 1.97d },\n+        { \"orc\", \"date\", \"2018-06-29\", \"2018-05-03\" },\n+        { \"orc\", \"time\", \"10:02:34.000000\", \"10:02:34.000001\" },\n         // Temporarily disable filters on Timestamp columns due to ORC-611\n-        // new Object[] { \"orc\", \"timestamp\",\n+        // { \"orc\", \"timestamp\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODAwODI4Mw=="}, "originalCommit": {"oid": "fcec58cc5a7351c2fc72978ca5cfef4e0aa4fbe7"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyMDEyMTg3OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNjo1MToyNVrOHbTC-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQyMDowMTowN1rOHbZADA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODM4NTY1OQ==", "bodyText": "Can we remove this since we don't need to change this file?", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498385659", "createdAt": "2020-10-01T16:51:25Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -130,6 +130,7 @@ public void close() throws CatalogException {\n     }\n   }\n \n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4cedb50bacdcf858506bf7defbe9682e1d6c0b4"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQ4MzIxMg==", "bodyText": "Removed!", "url": "https://github.com/apache/iceberg/pull/1539#discussion_r498483212", "createdAt": "2020-10-01T20:01:07Z", "author": {"login": "kbendick"}, "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java", "diffHunk": "@@ -130,6 +130,7 @@ public void close() throws CatalogException {\n     }\n   }\n \n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODM4NTY1OQ=="}, "originalCommit": {"oid": "d4cedb50bacdcf858506bf7defbe9682e1d6c0b4"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3565, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}