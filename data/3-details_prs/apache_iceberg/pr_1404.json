{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc2MzA5NDM3", "number": 1404, "title": "Flink: Add flink job id to state backend for handling flink job redeployment", "bodyText": "This patch will address the comment from the discussion.  FYI @stevenzwu @rdblue @JingsongLi .", "createdAt": "2020-08-31T13:45:38Z", "url": "https://github.com/apache/iceberg/pull/1404", "merged": true, "mergeCommit": {"oid": "88ec6d0f144c89a77286c6de5b96dcf52f2c9dc5"}, "closed": true, "closedAt": "2020-09-16T20:05:56Z", "author": {"login": "openinx"}, "timelineItems": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdETRsHgFqTQ3ODY0MTczNg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdHb4gsABqjM3NDk0ODc2Mjg=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4NjQxNzM2", "url": "https://github.com/apache/iceberg/pull/1404#pullrequestreview-478641736", "createdAt": "2020-08-31T14:00:26Z", "commit": {"oid": "1cde142db5bdf9f24b1f577c77878756e9787041"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxNDowMDoyN1rOHJ6EaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxNDowMDoyN1rOHJ6EaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDE1MDYzMw==", "bodyText": "Assume the case:   the flink job bootstrap first and write few records,  after the first snapshotState(1) finished, its job crashed, then it started to restore from the first snapshot.  Because we don't commit any iceberg transaction, so the maxCommittedCheckpointId will be -1,  finally the job won't recover successfully.\nSo in theory, this Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID) should be incorrect here.   For the snapshot expiration case,  we may need to find other ways to deal with.", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r480150633", "createdAt": "2020-08-31T14:00:27Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -108,18 +115,26 @@ public void initializeState(StateInitializationContext context) throws Exception\n     this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n \n     this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    this.jobIdState = context.getOperatorStateStore().getListState(JOB_ID_DESCRIPTOR);\n     if (context.isRestored()) {\n-      this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n-      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n-      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n-      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n-      // the iceberg table.\n-      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cde142db5bdf9f24b1f577c77878756e9787041"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4NzMwMzg4", "url": "https://github.com/apache/iceberg/pull/1404#pullrequestreview-478730388", "createdAt": "2020-08-31T15:44:40Z", "commit": {"oid": "1cde142db5bdf9f24b1f577c77878756e9787041"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxNTo0NDo0MFrOHJ-Q8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxNTo0NDo0MFrOHJ-Q8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDIxOTM3Ng==", "bodyText": "nit: move this inside the if section to make the if-else more symmetric.", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r480219376", "createdAt": "2020-08-31T15:44:40Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -108,18 +115,26 @@ public void initializeState(StateInitializationContext context) throws Exception\n     this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n \n     this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    this.jobIdState = context.getOperatorStateStore().getListState(JOB_ID_DESCRIPTOR);\n     if (context.isRestored()) {\n-      this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n-      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n-      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n-      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n-      // the iceberg table.\n-      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n-          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n-\n-      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n-      // Only keep the uncommitted data files in the cache.\n-      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+      String oldFlinkJobId = jobIdState.get().iterator().next();\n+      Preconditions.checkState(oldFlinkJobId != null && oldFlinkJobId.length() > 0,\n+          \"Flink job id parsed from checkpoint snapshot shouldn't be null or empty\");\n+      long oldMaxCommittedCheckpointId = getMaxCommittedCheckpointId(table, oldFlinkJobId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cde142db5bdf9f24b1f577c77878756e9787041"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4NzQ0MTE2", "url": "https://github.com/apache/iceberg/pull/1404#pullrequestreview-478744116", "createdAt": "2020-08-31T16:02:53Z", "commit": {"oid": "1cde142db5bdf9f24b1f577c77878756e9787041"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxNjowMjo1M1rOHJ-7eQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxNjowMjo1M1rOHJ-7eQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDIzMDI2NQ==", "bodyText": "I am wondering if it is better to consolidate all committer checkpoint states into a single structure, e.g. Pojo class or Avro record which are Flink supported state types for schema evolution.", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r480230265", "createdAt": "2020-08-31T16:02:53Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -88,6 +88,13 @@\n   private transient Table table;\n   private transient long maxCommittedCheckpointId;\n \n+  // There're two cases that we restore from flink checkpoints: the first case is restoring from snapshot created by the\n+  // same flink job; another case is restoring from snapshot created by another different job. For the second case, we\n+  // need to maintain the old flink job's id in flink state backend to find the max-committed-checkpoint-id when\n+  // traversing iceberg table's snapshots.\n+  private static final ListStateDescriptor<String> JOB_ID_DESCRIPTOR = new ListStateDescriptor<>(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cde142db5bdf9f24b1f577c77878756e9787041"}, "originalPosition": 8}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4NzY2NTAw", "url": "https://github.com/apache/iceberg/pull/1404#pullrequestreview-478766500", "createdAt": "2020-08-31T16:34:19Z", "commit": {"oid": "1cde142db5bdf9f24b1f577c77878756e9787041"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxNjozNDoxOVrOHKACJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0zMVQxNjozNDoxOVrOHKACJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDI0ODM1Nw==", "bodyText": "what is the reason making this a static method now?", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r480248357", "createdAt": "2020-08-31T16:34:19Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -151,13 +169,16 @@ public void notifyCheckpointComplete(long checkpointId) throws Exception {\n     // For step#4, we don't need to commit iceberg table again because in step#3 we've committed all the files,\n     // Besides, we need to maintain the max-committed-checkpoint-id to be increasing.\n     if (checkpointId > maxCommittedCheckpointId) {\n-      commitUpToCheckpoint(checkpointId);\n+      commitUpToCheckpoint(table, dataFilesPerCheckpoint, flinkJobId, checkpointId);\n       this.maxCommittedCheckpointId = checkpointId;\n     }\n   }\n \n-  private void commitUpToCheckpoint(long checkpointId) {\n-    NavigableMap<Long, List<DataFile>> pendingFileMap = dataFilesPerCheckpoint.headMap(checkpointId, true);\n+  private static void commitUpToCheckpoint(Table table,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1cde142db5bdf9f24b1f577c77878756e9787041"}, "originalPosition": 74}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg1NTU5ODE5", "url": "https://github.com/apache/iceberg/pull/1404#pullrequestreview-485559819", "createdAt": "2020-09-10T04:38:02Z", "commit": {"oid": "4582f1585aa5ac82477bc11f9728ecd2b6e89a8d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwNDozODowMlrOHPio2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwNDozODowMlrOHPio2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA1ODIwMg==", "bodyText": "nit: since we are using guava, we can use Strings.isNullOrEmpty.", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r486058202", "createdAt": "2020-09-10T04:38:02Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -108,18 +115,25 @@ public void initializeState(StateInitializationContext context) throws Exception\n     this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n \n     this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    this.jobIdState = context.getOperatorStateStore().getListState(JOB_ID_DESCRIPTOR);\n     if (context.isRestored()) {\n       this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n-      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n-      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n-      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n-      // the iceberg table.\n-      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n-          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n-\n-      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n-      // Only keep the uncommitted data files in the cache.\n-      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+\n+      String oldFlinkJobId = jobIdState.get().iterator().next();\n+      Preconditions.checkState(oldFlinkJobId != null && oldFlinkJobId.length() > 0,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4582f1585aa5ac82477bc11f9728ecd2b6e89a8d"}, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg1NTYwMDIz", "url": "https://github.com/apache/iceberg/pull/1404#pullrequestreview-485560023", "createdAt": "2020-09-10T04:38:41Z", "commit": {"oid": "4582f1585aa5ac82477bc11f9728ecd2b6e89a8d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwNDozODo0MVrOHPipkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwNDozODo0MVrOHPipkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA1ODM4NA==", "bodyText": "nit: oldFlinkJobId -> restoredFlinkJobId, since it may be the same job id for failure recovery case.", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r486058384", "createdAt": "2020-09-10T04:38:41Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -108,18 +115,25 @@ public void initializeState(StateInitializationContext context) throws Exception\n     this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n \n     this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    this.jobIdState = context.getOperatorStateStore().getListState(JOB_ID_DESCRIPTOR);\n     if (context.isRestored()) {\n       this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n-      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n-      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n-      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n-      // the iceberg table.\n-      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n-          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n-\n-      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n-      // Only keep the uncommitted data files in the cache.\n-      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+\n+      String oldFlinkJobId = jobIdState.get().iterator().next();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4582f1585aa5ac82477bc11f9728ecd2b6e89a8d"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg1NTYxNjQ4", "url": "https://github.com/apache/iceberg/pull/1404#pullrequestreview-485561648", "createdAt": "2020-09-10T04:44:15Z", "commit": {"oid": "4582f1585aa5ac82477bc11f9728ecd2b6e89a8d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwNDo0NDoxNVrOHPivJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQwNDo0NDoxNVrOHPivJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjA1OTgxMw==", "bodyText": "nit: maybe a simpler version? we can also remove the line 120 above\nlong maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, restoredFlinkJobId);", "url": "https://github.com/apache/iceberg/pull/1404#discussion_r486059813", "createdAt": "2020-09-10T04:44:15Z", "author": {"login": "stevenzwu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -108,18 +115,25 @@ public void initializeState(StateInitializationContext context) throws Exception\n     this.maxCommittedCheckpointId = INITIAL_CHECKPOINT_ID;\n \n     this.checkpointsState = context.getOperatorStateStore().getListState(STATE_DESCRIPTOR);\n+    this.jobIdState = context.getOperatorStateStore().getListState(JOB_ID_DESCRIPTOR);\n     if (context.isRestored()) {\n       this.maxCommittedCheckpointId = getMaxCommittedCheckpointId(table, flinkJobId);\n-      // In the restoring path, it should have one valid snapshot for current flink job at least, so the max committed\n-      // checkpoint id should be positive. If it's not positive, that means someone might have removed or expired the\n-      // iceberg snapshot, in that case we should throw an exception in case of committing duplicated data files into\n-      // the iceberg table.\n-      Preconditions.checkState(maxCommittedCheckpointId != INITIAL_CHECKPOINT_ID,\n-          \"There should be an existing iceberg snapshot for current flink job: %s\", flinkJobId);\n-\n-      SortedMap<Long, List<DataFile>> restoredDataFiles = checkpointsState.get().iterator().next();\n-      // Only keep the uncommitted data files in the cache.\n-      this.dataFilesPerCheckpoint.putAll(restoredDataFiles.tailMap(maxCommittedCheckpointId + 1));\n+\n+      String oldFlinkJobId = jobIdState.get().iterator().next();\n+      Preconditions.checkState(oldFlinkJobId != null && oldFlinkJobId.length() > 0,\n+          \"Flink job id parsed from checkpoint snapshot shouldn't be null or empty\");\n+\n+      long oldMaxCommittedCheckpointId = flinkJobId.equals(oldFlinkJobId) ?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4582f1585aa5ac82477bc11f9728ecd2b6e89a8d"}, "originalPosition": 36}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9d4897ee56570767336ca1ebcf43f6b3d9c71bc9", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/9d4897ee56570767336ca1ebcf43f6b3d9c71bc9", "committedDate": "2020-09-10T07:34:21Z", "message": "Flink: Add flink job id to state backend for handling flink job redeployment."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c82d04a43e3fbc6b95e364ce10e68be2c582b58c", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/c82d04a43e3fbc6b95e364ce10e68be2c582b58c", "committedDate": "2020-09-10T07:34:21Z", "message": "Minior fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1d6d8f00e18e2006a71b2c7c175761964314b8b7", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/1d6d8f00e18e2006a71b2c7c175761964314b8b7", "committedDate": "2020-09-10T07:34:21Z", "message": "Make the max-committed-checkpoint-id parsing more clear"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "69a1089b918a50193bccf7d231ce4c443b8670cb", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/69a1089b918a50193bccf7d231ce4c443b8670cb", "committedDate": "2020-09-10T07:34:21Z", "message": "Address comment from stevenzwu"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b0614f63ecf8ba964e0a61429638af6dbff3c0cf", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/b0614f63ecf8ba964e0a61429638af6dbff3c0cf", "committedDate": "2020-09-10T07:39:50Z", "message": "Rebase to master and fix the compile issues"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8a850f3944f656476fdcb3273bb60c0d6372f93b", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/8a850f3944f656476fdcb3273bb60c0d6372f93b", "committedDate": "2020-09-10T07:15:11Z", "message": "Address comment from stevenzwu"}, "afterCommit": {"oid": "b0614f63ecf8ba964e0a61429638af6dbff3c0cf", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/b0614f63ecf8ba964e0a61429638af6dbff3c0cf", "committedDate": "2020-09-10T07:39:50Z", "message": "Rebase to master and fix the compile issues"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4144, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}