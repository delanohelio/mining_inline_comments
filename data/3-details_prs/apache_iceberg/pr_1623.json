{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA1MjMzMDg0", "number": 1623, "title": "Flink:  Add a rewrite datafile action for flink", "bodyText": "this pr is for #1610\nAdd a rewrite datafile action for Flink, the PR contains the content of #1624. PR #1624 extracts some common functions to iceberg core. I will do the rebase after PR #1624 is merged.", "createdAt": "2020-10-17T07:22:54Z", "url": "https://github.com/apache/iceberg/pull/1623", "merged": true, "mergeCommit": {"oid": "f9f5bcfe66be2c808a4f14f59c195f92c31de8fa"}, "closed": true, "closedAt": "2020-11-04T11:34:11Z", "author": {"login": "zhangjun0x01"}, "timelineItems": {"totalCount": 45, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdTsD7zgFqTUxMTEyMzkwNg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdZL53RAFqTUyMzI2NjU3OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExMTIzOTA2", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-511123906", "createdAt": "2020-10-18T09:06:07Z", "commit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOFQwOTowNjowN1rOHjkE8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOFQwOToxODo0OFrOHjkTGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzA1MzI5Nw==", "bodyText": "Hi ZhangJun, Thanks for your work. Seems the class only used in Flink and the logic here already exists in Spark part. I think it's better to split this pr into 2 parts, one is refactor current rewrite code and the other is 'add a rewrite action for flink'.", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507053297", "createdAt": "2020-10-18T09:06:07Z", "author": {"login": "simonsssu"}, "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT, R> extends BaseSnapshotUpdateAction<ThisT, R> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzA1MzU3OQ==", "bodyText": "Useless blank line.", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507053579", "createdAt": "2020-10-18T09:07:06Z", "author": {"login": "simonsssu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    taskWriterFactory = new RowDataTaskWriterFactory(table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        table.io(),\n+        table.encryption(),\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive,\n+        encryptionManager,\n+        taskWriterFactory);\n+\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream()\n+        .flatMap(Collection::stream)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager,\n+                      TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzA1NjkyMg==", "bodyText": "Can we use a try-with-resource here?  I think it will simply the code to handle exception.", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507056922", "createdAt": "2020-10-18T09:18:48Z", "author": {"login": "simonsssu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    taskWriterFactory = new RowDataTaskWriterFactory(table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        table.io(),\n+        table.encryption(),\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive,\n+        encryptionManager,\n+        taskWriterFactory);\n+\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream()\n+        .flatMap(Collection::stream)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager,\n+                      TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+\n+    }\n+\n+\n+    @Override\n+    public List<DataFile> map(CombinedScanTask task) throws Exception {\n+      // Initialize the task writer.\n+      this.writer = taskWriterFactory.create();\n+      RowDataIterator iterator =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "originalPosition": 151}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExNDg5MDYx", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-511489061", "createdAt": "2020-10-19T07:44:15Z", "commit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwNzo0NDoxNlrOHkBl2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwNzo0NDoxNlrOHkBl2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzUzNjg1OA==", "bodyText": "Could we just return  the Lists.newArrayList(writer.complete()) ?", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507536858", "createdAt": "2020-10-19T07:44:16Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    taskWriterFactory = new RowDataTaskWriterFactory(table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        table.io(),\n+        table.encryption(),\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive,\n+        encryptionManager,\n+        taskWriterFactory);\n+\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream()\n+        .flatMap(Collection::stream)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager,\n+                      TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+\n+    }\n+\n+\n+    @Override\n+    public List<DataFile> map(CombinedScanTask task) throws Exception {\n+      // Initialize the task writer.\n+      this.writer = taskWriterFactory.create();\n+      RowDataIterator iterator =\n+          new RowDataIterator(task, io, encryptionManager, schema, schema,\n+              nameMapping, caseSensitive);\n+      try {\n+        while (iterator.hasNext()) {\n+          RowData rowData = iterator.next();\n+          writer.write(rowData);\n+        }\n+        iterator.close();\n+        writer.close();\n+        return Lists.newArrayList(writer.complete()).stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "originalPosition": 161}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExNDkxMzgx", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-511491381", "createdAt": "2020-10-19T07:47:22Z", "commit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwNzo0NzoyMlrOHkBtow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwNzo0NzoyMlrOHkBtow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzUzODg1MQ==", "bodyText": "The following writer.complete() will close firstly and then return the completed data file list, pls see the javadoc in TaskWriter:\n  /**\n   * Close the writer and get the completed data files.\n   *\n   * @return the completed data files of this task writer.\n   */\n  DataFile[] complete() throws IOException;\nSo here we don't have to call writer.close() now.", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507538851", "createdAt": "2020-10-19T07:47:22Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    taskWriterFactory = new RowDataTaskWriterFactory(table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        table.io(),\n+        table.encryption(),\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive,\n+        encryptionManager,\n+        taskWriterFactory);\n+\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream()\n+        .flatMap(Collection::stream)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager,\n+                      TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+\n+    }\n+\n+\n+    @Override\n+    public List<DataFile> map(CombinedScanTask task) throws Exception {\n+      // Initialize the task writer.\n+      this.writer = taskWriterFactory.create();\n+      RowDataIterator iterator =\n+          new RowDataIterator(task, io, encryptionManager, schema, schema,\n+              nameMapping, caseSensitive);\n+      try {\n+        while (iterator.hasNext()) {\n+          RowData rowData = iterator.next();\n+          writer.write(rowData);\n+        }\n+        iterator.close();\n+        writer.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "originalPosition": 160}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExNDkyNzc3", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-511492777", "createdAt": "2020-10-19T07:49:14Z", "commit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwNzo0OToxNVrOHkBx-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwNzo0OToxNVrOHkBx-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzUzOTk2Mg==", "bodyText": "nit:  pls don't left a new empty line here.", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507539962", "createdAt": "2020-10-19T07:49:15Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    taskWriterFactory = new RowDataTaskWriterFactory(table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        table.io(),\n+        table.encryption(),\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive,\n+        encryptionManager,\n+        taskWriterFactory);\n+\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream()\n+        .flatMap(Collection::stream)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager,\n+                      TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "originalPosition": 143}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExNDk1Mjg5", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-511495289", "createdAt": "2020-10-19T07:52:27Z", "commit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwNzo1MjoyN1rOHkB5pg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwNzo1MjoyN1rOHkB5pg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0MTkyNg==", "bodyText": "Those variables spec, properties, locations are never used in this classes , do we need them here ?", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507541926", "createdAt": "2020-10-19T07:52:27Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "originalPosition": 58}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExNDk1NjYy", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-511495662", "createdAt": "2020-10-19T07:52:56Z", "commit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwNzo1Mjo1NlrOHkB6rQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwNzo1Mjo1NlrOHkB6rQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0MjE4OQ==", "bodyText": "The same thing for table.", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507542189", "createdAt": "2020-10-19T07:52:56Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "originalPosition": 66}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExNDk3MjMz", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-511497233", "createdAt": "2020-10-19T07:54:54Z", "commit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwNzo1NDo1NFrOHkB_RQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwNzo1NDo1NFrOHkB_RQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0MzM2NQ==", "bodyText": "Maybe it's time to move all the properties parser to a TablePropsUtil now.", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507543365", "createdAt": "2020-10-19T07:54:54Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "originalPosition": 83}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExNTAyNzIw", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-511502720", "createdAt": "2020-10-19T08:01:55Z", "commit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwODowMTo1NVrOHkCP9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwODowMTo1NVrOHkCP9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0NzYzOQ==", "bodyText": "The targetFileSizeBytes will always be unlimited ?  Pls set a configuration keys for it,  here is the discussion we had for spark compaction.", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507547639", "createdAt": "2020-10-19T08:01:55Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    taskWriterFactory = new RowDataTaskWriterFactory(table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        table.io(),\n+        table.encryption(),\n+        Long.MAX_VALUE,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "originalPosition": 94}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExNTI2MDAw", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-511526000", "createdAt": "2020-10-19T08:30:40Z", "commit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwODozMDo0MFrOHkDVZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwODozMDo0MFrOHkDVZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU2NTQxNA==", "bodyText": "nit: Do we need to switch to a new line here ?", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507565414", "createdAt": "2020-10-19T08:30:40Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    taskWriterFactory = new RowDataTaskWriterFactory(table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        table.io(),\n+        table.encryption(),\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive,\n+        encryptionManager,\n+        taskWriterFactory);\n+\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream()\n+        .flatMap(Collection::stream)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager,\n+                      TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+\n+    }\n+\n+\n+    @Override\n+    public List<DataFile> map(CombinedScanTask task) throws Exception {\n+      // Initialize the task writer.\n+      this.writer = taskWriterFactory.create();\n+      RowDataIterator iterator =\n+          new RowDataIterator(task, io, encryptionManager, schema, schema,\n+              nameMapping, caseSensitive);\n+      try {\n+        while (iterator.hasNext()) {\n+          RowData rowData = iterator.next();\n+          writer.write(rowData);\n+        }\n+        iterator.close();\n+        writer.close();\n+        return Lists.newArrayList(writer.complete()).stream()\n+            .collect(Collectors.toList());\n+      } catch (Throwable originalThrowable) {\n+        try {\n+          LOG.error(\"Aborting task\", originalThrowable);\n+\n+          LOG.error(\"Aborting commit for  (subTaskId {}, attemptId {})\",\n+              subTaskId, attemptId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "originalPosition": 168}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExNTI2NjAz", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-511526603", "createdAt": "2020-10-19T08:31:22Z", "commit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwODozMToyMlrOHkDXSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwODozMToyMlrOHkDXSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU2NTg5OQ==", "bodyText": "nit:  iterator is always not-null ?", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507565899", "createdAt": "2020-10-19T08:31:22Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final Map<String, String> properties;\n+  private final FileFormat format;\n+  private final LocationProvider locations;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final Table table;\n+\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         FileIO io, EncryptionManager encryptionManager) {\n+    this.table = table;\n+    this.schema = table.schema();\n+    this.spec = spec;\n+    this.locations = table.locationProvider();\n+    this.properties = table.properties();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = table.properties().get(DEFAULT_NAME_MAPPING);\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    taskWriterFactory = new RowDataTaskWriterFactory(table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        table.io(),\n+        table.encryption(),\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive,\n+        encryptionManager,\n+        taskWriterFactory);\n+\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream()\n+        .flatMap(Collection::stream)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager,\n+                      TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+\n+    }\n+\n+\n+    @Override\n+    public List<DataFile> map(CombinedScanTask task) throws Exception {\n+      // Initialize the task writer.\n+      this.writer = taskWriterFactory.create();\n+      RowDataIterator iterator =\n+          new RowDataIterator(task, io, encryptionManager, schema, schema,\n+              nameMapping, caseSensitive);\n+      try {\n+        while (iterator.hasNext()) {\n+          RowData rowData = iterator.next();\n+          writer.write(rowData);\n+        }\n+        iterator.close();\n+        writer.close();\n+        return Lists.newArrayList(writer.complete()).stream()\n+            .collect(Collectors.toList());\n+      } catch (Throwable originalThrowable) {\n+        try {\n+          LOG.error(\"Aborting task\", originalThrowable);\n+\n+          LOG.error(\"Aborting commit for  (subTaskId {}, attemptId {})\",\n+              subTaskId, attemptId);\n+          if (iterator != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "originalPosition": 169}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExNTI5MjUw", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-511529250", "createdAt": "2020-10-19T08:34:27Z", "commit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwODozNDoyN1rOHkDfKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwODozNDoyN1rOHkDfKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU2NzkxNQ==", "bodyText": "Does this class need to be serializable ?  I checked the code, seems it won't need to be encoded or decoded for network transferring ?", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507567915", "createdAt": "2020-10-19T08:34:27Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTExNTM1NTQw", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-511535540", "createdAt": "2020-10-19T08:41:56Z", "commit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwODo0MTo1NlrOHkDybQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwODo0MTo1NlrOHkDybQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU3Mjg0NQ==", "bodyText": "I think here we'd better to throw the exception to upper layer ?  How about throwing the UncheckedIOException(e) ?", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r507572845", "createdAt": "2020-10-19T08:41:56Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.api.java.ExecutionEnvironment;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.source.RowDataRewriter;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction extends\n+    RewriteDataFilesActionBase<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private ExecutionEnvironment env = null;\n+  private boolean caseSensitive;\n+\n+  public RewriteDataFilesAction(ExecutionEnvironment env, Table table) {\n+    super(table);\n+    this.env = env;\n+    caseSensitive = Boolean.parseBoolean(table.properties().getOrDefault(\"case-sensitive\", \"false\"));\n+  }\n+\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = getFilteredGroupedTasks();\n+\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    List<DataFile> currentDataFiles = getCurrentDataFiles(filteredGroupedTasks);\n+\n+    List<CombinedScanTask> combinedScanTasks = getCombinedScanTasks(filteredGroupedTasks);\n+\n+    DataSet<CombinedScanTask> dataSet = env.fromCollection(combinedScanTasks);\n+    env.setParallelism(combinedScanTasks.size());\n+\n+    RowDataRewriter rowDataRewriter =\n+        new RowDataRewriter(getTable(), getTable().spec(), caseSensitive, getTable().io(), getEncryptionManager());\n+\n+    List<DataFile> addedDataFiles = null;\n+    try {\n+      addedDataFiles = rowDataRewriter.rewriteDataForTasks(dataSet);\n+    } catch (Exception e) {\n+      LOG.error(\"rewrite data files error\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6"}, "originalPosition": 74}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0124e3e236f6163551064c43a42a205be2cbd0f6", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/0124e3e236f6163551064c43a42a205be2cbd0f6", "committedDate": "2020-10-17T08:21:09Z", "message": "extract some common functions to iceberg-core"}, "afterCommit": {"oid": "4f5c6db70a2e4571cf295caa7f1ed0f6fbadf9ff", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/4f5c6db70a2e4571cf295caa7f1ed0f6fbadf9ff", "committedDate": "2020-10-19T10:37:32Z", "message": "Add a rewrite datafile action for flink"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4f5c6db70a2e4571cf295caa7f1ed0f6fbadf9ff", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/4f5c6db70a2e4571cf295caa7f1ed0f6fbadf9ff", "committedDate": "2020-10-19T10:37:32Z", "message": "Add a rewrite datafile action for flink"}, "afterCommit": {"oid": "29d587c26c83215d22c89908e6c4d48e5fc03229", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/29d587c26c83215d22c89908e6c4d48e5fc03229", "committedDate": "2020-10-19T11:02:42Z", "message": "Add a rewrite datafile action for flink"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "29d587c26c83215d22c89908e6c4d48e5fc03229", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/29d587c26c83215d22c89908e6c4d48e5fc03229", "committedDate": "2020-10-19T11:02:42Z", "message": "Add a rewrite datafile action for flink"}, "afterCommit": {"oid": "43083687475865c9d07c2a78287946fc43964ff3", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/43083687475865c9d07c2a78287946fc43964ff3", "committedDate": "2020-10-19T11:21:55Z", "message": "Add a rewrite datafile action for flink\n\nrm PartitionSpec"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEyMzQ1Mjc0", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-512345274", "createdAt": "2020-10-20T05:39:43Z", "commit": {"oid": "43083687475865c9d07c2a78287946fc43964ff3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwNTozOTo0M1rOHkrZnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwNTozOTo0M1rOHkrZnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODIyMTg1NQ==", "bodyText": "Do we really need this class ? Why not use org.apache.iceberg.util.PropertyUtil instead ?", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r508221855", "createdAt": "2020-10-20T05:39:43Z", "author": {"login": "simonsssu"}, "path": "flink/src/main/java/org/apache/iceberg/flink/TablePropertiesUtil.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43083687475865c9d07c2a78287946fc43964ff3"}, "originalPosition": 1}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "43083687475865c9d07c2a78287946fc43964ff3", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/43083687475865c9d07c2a78287946fc43964ff3", "committedDate": "2020-10-19T11:21:55Z", "message": "Add a rewrite datafile action for flink\n\nrm PartitionSpec"}, "afterCommit": {"oid": "d68fd2ee0a8fd2e5a7e2501fc99e79d77eb73e91", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/d68fd2ee0a8fd2e5a7e2501fc99e79d77eb73e91", "committedDate": "2020-10-22T03:20:20Z", "message": "RewriteDataFilesAction for flink"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d68fd2ee0a8fd2e5a7e2501fc99e79d77eb73e91", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/d68fd2ee0a8fd2e5a7e2501fc99e79d77eb73e91", "committedDate": "2020-10-22T03:20:20Z", "message": "RewriteDataFilesAction for flink"}, "afterCommit": {"oid": "4d0e1db3e2685e9c8f5cb9fb300f20a847df2955", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/4d0e1db3e2685e9c8f5cb9fb300f20a847df2955", "committedDate": "2020-10-22T03:57:13Z", "message": "RewriteDataFilesAction for flink"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4d0e1db3e2685e9c8f5cb9fb300f20a847df2955", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/4d0e1db3e2685e9c8f5cb9fb300f20a847df2955", "committedDate": "2020-10-22T03:57:13Z", "message": "RewriteDataFilesAction for flink"}, "afterCommit": {"oid": "f150a6c77b763e5fbfd51928c1dc679a948fc7c0", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/f150a6c77b763e5fbfd51928c1dc679a948fc7c0", "committedDate": "2020-10-26T10:51:38Z", "message": "RewriteDataFilesAction for flink"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f150a6c77b763e5fbfd51928c1dc679a948fc7c0", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/f150a6c77b763e5fbfd51928c1dc679a948fc7c0", "committedDate": "2020-10-26T10:51:38Z", "message": "RewriteDataFilesAction for flink"}, "afterCommit": {"oid": "f640e6e3646ed4062ee4457594511002b6d08969", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/f640e6e3646ed4062ee4457594511002b6d08969", "committedDate": "2020-10-28T06:08:26Z", "message": "RewriteDataFilesAction for flink"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f640e6e3646ed4062ee4457594511002b6d08969", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/f640e6e3646ed4062ee4457594511002b6d08969", "committedDate": "2020-10-28T06:08:26Z", "message": "RewriteDataFilesAction for flink"}, "afterCommit": {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/cf65fed93538f5625e9c22b823c92b97b28b34a1", "committedDate": "2020-10-28T06:20:33Z", "message": "RewriteDataFilesAction for flink"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIwMzUzODg2", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-520353886", "createdAt": "2020-10-30T03:25:11Z", "commit": {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1"}, "state": "COMMENTED", "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQwMzoyNToxMlrOHq7Wmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQwNzoyNToxMFrOHrDuuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDc3NDY4Mg==", "bodyText": "why setFileIO ?  That's quite confusing,  it's more likely we are getting fileIO while we name it as setFileIO ?  I don't think we need to change this.", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514774682", "createdAt": "2020-10-30T03:25:12Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -268,7 +272,7 @@ private void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<Data\n     }\n   }\n \n-  protected abstract FileIO fileIO();\n+  protected abstract FileIO setFileIO();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDg5NjczMw==", "bodyText": "I'm thinking that we may need to move this class into org.apache.iceberg.flink.actions as we flink & spark both have the same class name Actions under the package org.apache.iceberg.actions,  then how to identify the class if someone want to execute the actions in the same project for both flink and spark ?", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514896733", "createdAt": "2020-10-30T06:35:22Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/actions/Actions.java", "diffHunk": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDg5NzI3OA==", "bodyText": "nit: don't have to wrap the line here ..", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514897278", "createdAt": "2020-10-30T06:37:19Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.api.java.ExecutionEnvironment;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.source.RowDataRewriter;\n+import org.apache.iceberg.io.FileIO;\n+\n+public class RewriteDataFilesAction extends", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDg5ODgyMw==", "bodyText": "nit: could we keep the same order between the constructor's argument assignment and fields definition ?  for example:\nprivate final int a;\nprivate final int b;\n\npublic Construct(int a, int b){\n    this.a = a;\n    this.b = b;\n}", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514898823", "createdAt": "2020-10-30T06:42:53Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final FileFormat format;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, boolean caseSensitive, FileIO io, EncryptionManager encryptionManager) {\n+    this.schema = table.schema();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = PropertyUtil.propertyAsString(table.properties(), DEFAULT_NAME_MAPPING, null);\n+    String formatString = PropertyUtil.propertyAsString(table.properties(), TableProperties.DEFAULT_FILE_FORMAT,\n+        TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDg5OTkwNw==", "bodyText": "nit:  code format ?", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514899907", "createdAt": "2020-10-30T06:46:37Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/util/PropertyUtil.java", "diffHunk": "@@ -52,4 +52,13 @@ public static long propertyAsLong(Map<String, String> properties,\n     }\n     return defaultValue;\n   }\n+\n+  public static String propertyAsString(Map<String, String> properties,\n+                                    String property, String defaultValue) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwMDQ1Ng==", "bodyText": "nit: could we separate this format parser into a separate code block by left a new empty line ? That makes the code more clear.", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514900456", "createdAt": "2020-10-30T06:48:28Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final FileFormat format;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, boolean caseSensitive, FileIO io, EncryptionManager encryptionManager) {\n+    this.schema = table.schema();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = PropertyUtil.propertyAsString(table.properties(), DEFAULT_NAME_MAPPING, null);\n+    String formatString = PropertyUtil.propertyAsString(table.properties(), TableProperties.DEFAULT_FILE_FORMAT,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1"}, "originalPosition": 68}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwMTY5OQ==", "bodyText": "we don't need to expose this class to public ? As the apache iceberg is a lib,  so we're tried to avoid to expose unnecessary internal classes or interfaces to public, so that users won't abuse them.", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514901699", "createdAt": "2020-10-30T06:52:39Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwMjU4Mg==", "bodyText": "I think we've forgot to close the RowDataIterator if encountered any exception when iterate the records.  Pls handle this carefully because if close failed then we won't want to close it again in the catch block.", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514902582", "createdAt": "2020-10-30T06:55:37Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final FileFormat format;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, boolean caseSensitive, FileIO io, EncryptionManager encryptionManager) {\n+    this.schema = table.schema();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = PropertyUtil.propertyAsString(table.properties(), DEFAULT_NAME_MAPPING, null);\n+    String formatString = PropertyUtil.propertyAsString(table.properties(), TableProperties.DEFAULT_FILE_FORMAT,\n+        TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    this.taskWriterFactory = new RowDataTaskWriterFactory(\n+        table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        io,\n+        encryptionManager,\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive, encryptionManager, taskWriterFactory);\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream().flatMap(Collection::stream).collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager, TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+    }\n+\n+    @Override\n+    public List<DataFile> map(CombinedScanTask task) throws Exception {\n+      // Initialize the task writer.\n+      this.writer = taskWriterFactory.create();\n+      RowDataIterator iterator =\n+          new RowDataIterator(task, io, encryptionManager, schema, schema, nameMapping, caseSensitive);\n+      try {\n+        while (iterator.hasNext()) {\n+          RowData rowData = iterator.next();\n+          writer.write(rowData);\n+        }\n+        iterator.close();\n+        return Lists.newArrayList(writer.complete());\n+      } catch (Throwable originalThrowable) {\n+        try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwMzA4OA==", "bodyText": "What's the reason that we need to supress the inner exception when it's different with the originalThrowable ?", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514903088", "createdAt": "2020-10-30T06:57:25Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final FileFormat format;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, boolean caseSensitive, FileIO io, EncryptionManager encryptionManager) {\n+    this.schema = table.schema();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = PropertyUtil.propertyAsString(table.properties(), DEFAULT_NAME_MAPPING, null);\n+    String formatString = PropertyUtil.propertyAsString(table.properties(), TableProperties.DEFAULT_FILE_FORMAT,\n+        TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    this.taskWriterFactory = new RowDataTaskWriterFactory(\n+        table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        io,\n+        encryptionManager,\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive, encryptionManager, taskWriterFactory);\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream().flatMap(Collection::stream).collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;\n+    private transient int subTaskId;\n+    private transient int attemptId;\n+\n+    private final Schema schema;\n+    private final String nameMapping;\n+    private final FileIO io;\n+    private final boolean caseSensitive;\n+    private final EncryptionManager encryptionManager;\n+    private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+    public RewriteMap(Schema schema, String nameMapping, FileIO io, boolean caseSensitive,\n+                      EncryptionManager encryptionManager, TaskWriterFactory<RowData> taskWriterFactory) {\n+      this.schema = schema;\n+      this.nameMapping = nameMapping;\n+      this.io = io;\n+      this.caseSensitive = caseSensitive;\n+      this.encryptionManager = encryptionManager;\n+      this.taskWriterFactory = taskWriterFactory;\n+    }\n+\n+    @Override\n+    public void open(Configuration parameters) {\n+      this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+      this.attemptId = getRuntimeContext().getAttemptNumber();\n+      // Initialize the task writer factory.\n+      this.taskWriterFactory.initialize(subTaskId, attemptId);\n+    }\n+\n+    @Override\n+    public List<DataFile> map(CombinedScanTask task) throws Exception {\n+      // Initialize the task writer.\n+      this.writer = taskWriterFactory.create();\n+      RowDataIterator iterator =\n+          new RowDataIterator(task, io, encryptionManager, schema, schema, nameMapping, caseSensitive);\n+      try {\n+        while (iterator.hasNext()) {\n+          RowData rowData = iterator.next();\n+          writer.write(rowData);\n+        }\n+        iterator.close();\n+        return Lists.newArrayList(writer.complete());\n+      } catch (Throwable originalThrowable) {\n+        try {\n+          LOG.error(\"Aborting commit for  (subTaskId {}, attemptId {})\", subTaskId, attemptId);\n+          writer.abort();\n+          LOG.error(\"Aborted commit for  (subTaskId {}, attemptId {})\", subTaskId, attemptId);\n+        } catch (Throwable inner) {\n+          if (originalThrowable != inner) {\n+            originalThrowable.addSuppressed(inner);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwNDA0OQ==", "bodyText": "nit:  could just use local variable ?  don't have to be a transient field.", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514904049", "createdAt": "2020-10-30T07:00:49Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final FileFormat format;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, boolean caseSensitive, FileIO io, EncryptionManager encryptionManager) {\n+    this.schema = table.schema();\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.caseSensitive = caseSensitive;\n+    this.nameMapping = PropertyUtil.propertyAsString(table.properties(), DEFAULT_NAME_MAPPING, null);\n+    String formatString = PropertyUtil.propertyAsString(table.properties(), TableProperties.DEFAULT_FILE_FORMAT,\n+        TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    this.taskWriterFactory = new RowDataTaskWriterFactory(\n+        table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        io,\n+        encryptionManager,\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(DataSet<CombinedScanTask> dataSet) throws Exception {\n+    RewriteMap map = new RewriteMap(schema, nameMapping, io, caseSensitive, encryptionManager, taskWriterFactory);\n+    DataSet<List<DataFile>> ds = dataSet.map(map);\n+    return ds.collect().stream().flatMap(Collection::stream).collect(Collectors.toList());\n+  }\n+\n+  public static class RewriteMap extends RichMapFunction<CombinedScanTask, List<DataFile>> {\n+\n+    private transient TaskWriter<RowData> writer;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwODU2Nw==", "bodyText": "I think we may need to refactor the SimpleDataUtil.assertTableRecords, because it will use Set to check the expected records, that means it will de-duplicate the same records automatically, and won't identify the case that have two <1, 'hello'> records.", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514908567", "createdAt": "2020-10-30T07:15:22Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.FlinkCatalogTestBase;\n+import org.apache.iceberg.flink.SimpleDataUtil;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestRewriteDataFilesAction extends FlinkCatalogTestBase {\n+\n+  private static final String TABLE_NAME_UNPARTIITONED = \"test_table_unpartitioned\";\n+  private static final String TABLE_NAME_PARTIITONED = \"test_table_partitioned\";\n+  private final FileFormat format;\n+  private Table icebergTableUnPartitioned;\n+  private Table icebergTablePartitioned;\n+\n+  public TestRewriteDataFilesAction(String catalogName, String[] baseNamespace, FileFormat format) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n+  }\n+\n+  @Parameterized.Parameters(name = \"catalogName={0}, baseNamespace={1}, format={2}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+        String catalogName = (String) catalogParams[0];\n+        String[] baseNamespace = (String[]) catalogParams[1];\n+        parameters.add(new Object[] {catalogName, baseNamespace, format});\n+      }\n+    }\n+    return parameters;\n+  }\n+\n+  @Before\n+  public void before() {\n+    super.before();\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n+    sql(\"CREATE TABLE %s (id int, data varchar) with ('write.format.default'='%s')\", TABLE_NAME_UNPARTIITONED,\n+        format.name());\n+    icebergTableUnPartitioned = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace,\n+        TABLE_NAME_UNPARTIITONED));\n+\n+    sql(\"CREATE TABLE %s (id int, data varchar)  PARTITIONED BY (data) with ('write.format.default'='%s')\",\n+        TABLE_NAME_PARTIITONED, format.name());\n+    icebergTablePartitioned = validationCatalog.loadTable(TableIdentifier.of(icebergNamespace,\n+        TABLE_NAME_PARTIITONED));\n+  }\n+\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME_UNPARTIITONED);\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME_PARTIITONED);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n+    super.clean();\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesEmptyTable() throws Exception {\n+    Assert.assertNull(\"Table must be empty\", icebergTableUnPartitioned.currentSnapshot());\n+    Actions.forTable(icebergTableUnPartitioned)\n+        .rewriteDataFiles()\n+        .execute();\n+    Assert.assertNull(\"Table must stay empty\", icebergTableUnPartitioned.currentSnapshot());\n+  }\n+\n+\n+  @Test\n+  public void testRewriteDataFilesUnpartitionedTable() throws Exception {\n+    sql(\"INSERT INTO %s SELECT 1, 'hello'\", TABLE_NAME_UNPARTIITONED);\n+    sql(\"INSERT INTO %s SELECT 2, 'world'\", TABLE_NAME_UNPARTIITONED);\n+\n+    icebergTableUnPartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = icebergTableUnPartitioned.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 2 data files before rewrite\", 2, dataFiles.size());\n+\n+    RewriteDataFilesActionResult result =\n+        Actions.forTable(icebergTableUnPartitioned)\n+            .rewriteDataFiles()\n+            .execute();\n+\n+    Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+    icebergTableUnPartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = icebergTableUnPartitioned.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 1 data files after rewrite\", 1, dataFiles1.size());\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(icebergTableUnPartitioned, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\")\n+    ));\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesPartitionedTable() throws Exception {\n+    sql(\"INSERT INTO %s SELECT 1, 'hello' \", TABLE_NAME_PARTIITONED);\n+    sql(\"INSERT INTO %s SELECT 2, 'hello' \", TABLE_NAME_PARTIITONED);\n+    sql(\"INSERT INTO %s SELECT 3, 'world' \", TABLE_NAME_PARTIITONED);\n+    sql(\"INSERT INTO %s SELECT 4, 'world' \", TABLE_NAME_PARTIITONED);\n+\n+    icebergTablePartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = icebergTablePartitioned.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 4 data files before rewrite\", 4, dataFiles.size());\n+\n+    RewriteDataFilesActionResult result =\n+        Actions.forTable(icebergTablePartitioned)\n+            .rewriteDataFiles()\n+            .execute();\n+\n+    Assert.assertEquals(\"Action should rewrite 4 data files\", 4, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 2 data file\", 2, result.addedDataFiles().size());\n+\n+    icebergTablePartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = icebergTablePartitioned.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 2 data files after rewrite\", 2, dataFiles1.size());\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(icebergTablePartitioned, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"hello\"),\n+        SimpleDataUtil.createRecord(3, \"world\"),\n+        SimpleDataUtil.createRecord(4, \"world\")\n+    ));\n+  }\n+\n+\n+  @Test\n+  public void testRewriteDataFilesWithFilter() throws Exception {\n+    sql(\"INSERT INTO %s SELECT 1, 'hello' \", TABLE_NAME_PARTIITONED);\n+    sql(\"INSERT INTO %s SELECT 1, 'hello' \", TABLE_NAME_PARTIITONED);\n+    sql(\"INSERT INTO %s SELECT 1, 'world' \", TABLE_NAME_PARTIITONED);\n+    sql(\"INSERT INTO %s SELECT 2, 'world' \", TABLE_NAME_PARTIITONED);\n+    sql(\"INSERT INTO %s SELECT 3, 'world' \", TABLE_NAME_PARTIITONED);\n+\n+    icebergTablePartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = icebergTablePartitioned.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 5 data files before rewrite\", 5, dataFiles.size());\n+\n+    RewriteDataFilesActionResult result =\n+        Actions.forTable(icebergTablePartitioned)\n+            .rewriteDataFiles()\n+            .filter(Expressions.equal(\"id\", 1))\n+            .filter(Expressions.startsWith(\"data\", \"he\"))\n+            .execute();\n+\n+    Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+    icebergTablePartitioned.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = icebergTablePartitioned.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 4 data files after rewrite\", 4, dataFiles1.size());\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(icebergTablePartitioned, Lists.newArrayList(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwODk1Nw==", "bodyText": "How about use protected ?", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514908957", "createdAt": "2020-10-30T07:16:39Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java", "diffHunk": "@@ -72,7 +72,7 @@ protected TableEnvironment getTableEnv() {\n     return tEnv;\n   }\n \n-  List<Object[]> sql(String query, Object... args) {\n+  public List<Object[]> sql(String query, Object... args) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkxMTkyOQ==", "bodyText": "I think we should also provide an unit test similar to the spark testRewriteLargeTableHasResiduals, that test the option ignoreResiduals.", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r514911929", "createdAt": "2020-10-30T07:25:10Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,211 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.FlinkCatalogTestBase;\n+import org.apache.iceberg.flink.SimpleDataUtil;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestRewriteDataFilesAction extends FlinkCatalogTestBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1"}, "originalPosition": 42}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cf65fed93538f5625e9c22b823c92b97b28b34a1", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/cf65fed93538f5625e9c22b823c92b97b28b34a1", "committedDate": "2020-10-28T06:20:33Z", "message": "RewriteDataFilesAction for flink"}, "afterCommit": {"oid": "7548af2e07bd829f92d45645e073d5c06a1dfc88", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/7548af2e07bd829f92d45645e073d5c06a1dfc88", "committedDate": "2020-10-31T03:23:25Z", "message": "RewriteDataFilesAction for flink"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7548af2e07bd829f92d45645e073d5c06a1dfc88", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/7548af2e07bd829f92d45645e073d5c06a1dfc88", "committedDate": "2020-10-31T03:23:25Z", "message": "RewriteDataFilesAction for flink"}, "afterCommit": {"oid": "1ac5493b1017473a78831567c7e299c3644b416b", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/1ac5493b1017473a78831567c7e299c3644b416b", "committedDate": "2020-10-31T03:23:25Z", "message": "RewriteDataFilesAction for flink"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMzEwMjQ0", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-521310244", "createdAt": "2020-11-02T03:17:19Z", "commit": {"oid": "6661548b9c3a368aeccb72d08f31bb75f96cf888"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6661548b9c3a368aeccb72d08f31bb75f96cf888", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/6661548b9c3a368aeccb72d08f31bb75f96cf888", "committedDate": "2020-10-31T09:19:21Z", "message": "fix checkstyle"}, "afterCommit": {"oid": "75143b1a306e0ec5ffd07c958f0bf5d01ec70dfd", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/75143b1a306e0ec5ffd07c958f0bf5d01ec70dfd", "committedDate": "2020-11-02T05:32:46Z", "message": " RewriteDataFilesAction for flink"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxNTU2MzE0", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-521556314", "createdAt": "2020-11-02T11:42:22Z", "commit": {"oid": "75143b1a306e0ec5ffd07c958f0bf5d01ec70dfd"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMTo0MjoyMlrOHsBBJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxMTo0MjoyMlrOHsBBJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTkxNjA2OA==", "bodyText": "nit: TABLE_NAME_UNPARTIITONED -> TABLE_NAME_UNPARTITIONED.\nTABLE_NAME_PARTIITONED has the similar issue.", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r515916068", "createdAt": "2020-11-02T11:42:22Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+\n+package org.apache.iceberg.flink.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.commons.lang3.StringUtils;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.actions.RewriteDataFilesActionResult;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.FlinkCatalogTestBase;\n+import org.apache.iceberg.flink.SimpleDataUtil;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.flink.SimpleDataUtil.RECORD;\n+\n+@RunWith(Parameterized.class)\n+public class TestRewriteDataFilesAction extends FlinkCatalogTestBase {\n+\n+  private static final String TABLE_NAME_UNPARTIITONED = \"test_table_unpartitioned\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "75143b1a306e0ec5ffd07c958f0bf5d01ec70dfd"}, "originalPosition": 53}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "75143b1a306e0ec5ffd07c958f0bf5d01ec70dfd", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/75143b1a306e0ec5ffd07c958f0bf5d01ec70dfd", "committedDate": "2020-11-02T05:32:46Z", "message": " RewriteDataFilesAction for flink"}, "afterCommit": {"oid": "e4c28953643c0b7f5f7f0a7336d0a7315bebeb12", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/e4c28953643c0b7f5f7f0a7336d0a7315bebeb12", "committedDate": "2020-11-03T01:10:31Z", "message": "fix Typo"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyMjAwOTIz", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-522200923", "createdAt": "2020-11-03T05:44:54Z", "commit": {"oid": "e4c28953643c0b7f5f7f0a7336d0a7315bebeb12"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyMjAyMjU3", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-522202257", "createdAt": "2020-11-03T05:50:07Z", "commit": {"oid": "e4c28953643c0b7f5f7f0a7336d0a7315bebeb12"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwNTo1MDowN1rOHsg8Sg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QwNTo1MDowN1rOHsg8Sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjQzOTExNA==", "bodyText": "I think setParallelism may not works.\nWhat we want is a SparkContext.parallelize, but this setParallelism just set parallelism, records still in a single node, downstream operators only have computation on one node.\nCan you check this?", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r516439114", "createdAt": "2020-11-03T05:50:07Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.actions;\n+\n+import java.util.List;\n+import org.apache.flink.api.java.DataSet;\n+import org.apache.flink.api.java.ExecutionEnvironment;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.actions.BaseRewriteDataFilesAction;\n+import org.apache.iceberg.flink.source.RowDataRewriter;\n+import org.apache.iceberg.io.FileIO;\n+\n+public class RewriteDataFilesAction extends BaseRewriteDataFilesAction<RewriteDataFilesAction> {\n+\n+  private ExecutionEnvironment env;\n+\n+  public RewriteDataFilesAction(ExecutionEnvironment env, Table table) {\n+    super(table);\n+    this.env = env;\n+  }\n+\n+  @Override\n+  protected FileIO fileIO() {\n+    return table().io();\n+  }\n+\n+  @Override\n+  protected List<DataFile> rewriteDataForTasks(List<CombinedScanTask> combinedScanTasks) {\n+    DataSet<CombinedScanTask> dataSet = env.fromCollection(combinedScanTasks).setParallelism(combinedScanTasks.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4c28953643c0b7f5f7f0a7336d0a7315bebeb12"}, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyNDEwMjI2", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-522410226", "createdAt": "2020-11-03T11:35:23Z", "commit": {"oid": "a408953c4a4d9a5e2ecca16aebf02d3c51fe2631"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxMTozNToyM1rOHsq4lA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxMTozNToyM1rOHsq4lA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjYwMjAwNA==", "bodyText": "If change to use datastream to submit this job,  then the argument dataSet should be renamed to dataStream ?", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r516602004", "createdAt": "2020-11-03T11:35:23Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.flink.api.common.functions.RichMapFunction;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamUtils;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.sink.RowDataTaskWriterFactory;\n+import org.apache.iceberg.flink.sink.TaskWriterFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class RowDataRewriter {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Schema schema;\n+  private final FileFormat format;\n+  private final String nameMapping;\n+  private final FileIO io;\n+  private final boolean caseSensitive;\n+  private final EncryptionManager encryptionManager;\n+  private final TaskWriterFactory<RowData> taskWriterFactory;\n+\n+  public RowDataRewriter(Table table, boolean caseSensitive, FileIO io, EncryptionManager encryptionManager) {\n+    this.schema = table.schema();\n+    this.caseSensitive = caseSensitive;\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.nameMapping = PropertyUtil.propertyAsString(table.properties(), DEFAULT_NAME_MAPPING, null);\n+\n+    String formatString = PropertyUtil.propertyAsString(table.properties(), TableProperties.DEFAULT_FILE_FORMAT,\n+        TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    this.format = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    RowType flinkSchema = FlinkSchemaUtil.convert(table.schema());\n+    this.taskWriterFactory = new RowDataTaskWriterFactory(\n+        table.schema(),\n+        flinkSchema,\n+        table.spec(),\n+        table.locationProvider(),\n+        io,\n+        encryptionManager,\n+        Long.MAX_VALUE,\n+        format,\n+        table.properties());\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(DataStream<CombinedScanTask> dataSet, int parallelism) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a408953c4a4d9a5e2ecca16aebf02d3c51fe2631"}, "originalPosition": 86}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a408953c4a4d9a5e2ecca16aebf02d3c51fe2631", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/a408953c4a4d9a5e2ecca16aebf02d3c51fe2631", "committedDate": "2020-11-03T10:31:13Z", "message": "set parallelism for map"}, "afterCommit": {"oid": "5a0dcdf7001807ccd83be3d4caa49f8b5c077a64", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/5a0dcdf7001807ccd83be3d4caa49f8b5c077a64", "committedDate": "2020-11-04T03:12:30Z", "message": "add max Parallelism for RewriteDataFilesAction"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMTQ0NDc4", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-523144478", "createdAt": "2020-11-04T08:43:52Z", "commit": {"oid": "5a0dcdf7001807ccd83be3d4caa49f8b5c077a64"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODo0Mzo1M1rOHtOELA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwODo0Mzo1M1rOHtOELA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzE3ODQxMg==", "bodyText": "Do we need to pass the whole List<CombinedScanTask> here ?  I think we only need the numOfScanTasks ?", "url": "https://github.com/apache/iceberg/pull/1623#discussion_r517178412", "createdAt": "2020-11-04T08:43:53Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -268,6 +268,23 @@ private void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<Data\n     }\n   }\n \n+  protected int getParallelism(int parallelism, List<CombinedScanTask> combinedScanTasks) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5a0dcdf7001807ccd83be3d4caa49f8b5c077a64"}, "originalPosition": 4}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d02e7420e06bd706b085c0d3266ce6327e35de05", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/d02e7420e06bd706b085c0d3266ce6327e35de05", "committedDate": "2020-11-04T10:31:09Z", "message": "RewriteDataFilesAction for flink"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0ffd2609ff1f69236f8f197e69b0973d294623f1", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/0ffd2609ff1f69236f8f197e69b0973d294623f1", "committedDate": "2020-11-04T10:31:09Z", "message": "RewriteDataFilesAction for flink"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b87a3eb76dce132c6ff3842090155133a78c7797", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/b87a3eb76dce132c6ff3842090155133a78c7797", "committedDate": "2020-11-04T10:31:09Z", "message": "fix unit test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1d19c039b6fd3b1462c8372912f9f552c79dc32d", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/1d19c039b6fd3b1462c8372912f9f552c79dc32d", "committedDate": "2020-11-04T10:31:09Z", "message": "fix checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "607885e05518803d0c12d54d411423b9eceb270c", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/607885e05518803d0c12d54d411423b9eceb270c", "committedDate": "2020-11-04T10:31:09Z", "message": " RewriteDataFilesAction for flink"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "634e9c4612093a63facb3831d333b43789fcdd92", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/634e9c4612093a63facb3831d333b43789fcdd92", "committedDate": "2020-11-04T10:31:09Z", "message": "fix Typo"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b011aa525ec3c2509ab0ced2848f9c69fc810fd", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/6b011aa525ec3c2509ab0ced2848f9c69fc810fd", "committedDate": "2020-11-04T10:31:09Z", "message": "use DataStream instead of dataset"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "72c5a98524e84478cc0abc01773ff20aa572ab61", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/72c5a98524e84478cc0abc01773ff20aa572ab61", "committedDate": "2020-11-04T10:31:09Z", "message": "set parallelism for map"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aa78d406bb49ba638e94132db2e9de109dbb1596", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/aa78d406bb49ba638e94132db2e9de109dbb1596", "committedDate": "2020-11-04T10:31:09Z", "message": "add max Parallelism for RewriteDataFilesAction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "054ca38f36a9f85ff6cfce9f21c27ca8757a3593", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/054ca38f36a9f85ff6cfce9f21c27ca8757a3593", "committedDate": "2020-11-04T10:31:09Z", "message": "move maxParallelism to flink RewriteDataFilesAction"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5a0dcdf7001807ccd83be3d4caa49f8b5c077a64", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/5a0dcdf7001807ccd83be3d4caa49f8b5c077a64", "committedDate": "2020-11-04T03:12:30Z", "message": "add max Parallelism for RewriteDataFilesAction"}, "afterCommit": {"oid": "054ca38f36a9f85ff6cfce9f21c27ca8757a3593", "author": {"user": null}, "url": "https://github.com/apache/iceberg/commit/054ca38f36a9f85ff6cfce9f21c27ca8757a3593", "committedDate": "2020-11-04T10:31:09Z", "message": "move maxParallelism to flink RewriteDataFilesAction"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzMjY2NTc4", "url": "https://github.com/apache/iceberg/pull/1623#pullrequestreview-523266578", "createdAt": "2020-11-04T11:17:30Z", "commit": {"oid": "054ca38f36a9f85ff6cfce9f21c27ca8757a3593"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3978, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}