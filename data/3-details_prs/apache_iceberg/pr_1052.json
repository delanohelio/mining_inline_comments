{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIxMjU0MTMw", "number": 1052, "title": "Fix RemoveOrphanFilesAction when file_path is not a qualified path", "bodyText": "If we don't use qualified path (file:/temp/test_db) to create or save into (Hadoop) table, then the file_path queried out is not a qualified path, for example:\n  private Dataset<Row> buildValidDataFileDF() {\n    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n    return spark.read().format(\"iceberg\")\n        .load(allDataFilesMetadataTable)\n        .select(\"file_path\");\n  }\nThe result here could be:\n+-----------------------------------------------------------------------------------+\n|file_path                                                                          |\n+-----------------------------------------------------------------------------------+\n|tmp/iceberg_test2/data/00000-172-2805f207-2c0d-4717-acc2-fed60430afeb-00000.parquet|\n|tmp/iceberg_test2/data/00001-173-bd5e807d-e96f-49de-b84e-2c254c0777bb-00000.parquet|\n|tmp/iceberg_test2/data/00002-174-fb7f84f0-d2ed-4e53-b5b9-6ef2f7da8a73-00000.parquet|\n+-----------------------------------------------------------------------------------+\n\nBut the code here file.getPath().toString() in RemoveOrphanFilesAction#listDirRecursively returns qualified path:\n     for (FileStatus file : fs.listStatus(path, HiddenPathFilter.get())) {\n        if (file.isDirectory()) {\n          subDirs.add(file.getPath().toString());\n        } else if (file.isFile() && predicate.test(file)) {\n          matchingFiles.add(file.getPath().toString());\n        }\n      }\nSo the join condition equalTo may not correctly get the orphan files and delete the file mistakenly.\nSo here propose to fix the join condition to contains. Another solution is to change the relative path to qualified one in everywhere.", "createdAt": "2020-05-21T09:39:53Z", "url": "https://github.com/apache/iceberg/pull/1052", "merged": true, "mergeCommit": {"oid": "b3932e13de0e3bb2445f68dc4b3dfb34b911d0fb"}, "closed": true, "closedAt": "2020-05-22T15:46:39Z", "author": {"login": "jerryshao"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcjZetWAH2gAyNDIxMjU0MTMwOmY2ZmQ5MmY0YzAyYzU1ZGE5MjQ1Mzc2ZmY0YTk0MWNlNzljOWZlNzQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcjrQX5AH2gAyNDIxMjU0MTMwOmNlY2YzODgyMWI2ZmVlN2ZkMDFjMWZmYjJlMGQyNzY1NTNjZWYwNDk=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "f6fd92f4c02c55da9245376ff4a941ce79c9fe74", "author": {"user": {"login": "jerryshao", "name": "Saisai Shao"}}, "url": "https://github.com/apache/iceberg/commit/f6fd92f4c02c55da9245376ff4a941ce79c9fe74", "committedDate": "2020-05-21T08:34:36Z", "message": "Fix RemoveOrphanFilesAction when file_path is not a qualified path"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eaca67080c3db7f5473b7d1d9970115fa25ebf11", "author": {"user": {"login": "jerryshao", "name": "Saisai Shao"}}, "url": "https://github.com/apache/iceberg/commit/eaca67080c3db7f5473b7d1d9970115fa25ebf11", "committedDate": "2020-05-21T09:24:08Z", "message": "add UT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7f19edcf8a8d8e56aa20c0584461b802496bcc0a", "author": {"user": {"login": "jerryshao", "name": "Saisai Shao"}}, "url": "https://github.com/apache/iceberg/commit/7f19edcf8a8d8e56aa20c0584461b802496bcc0a", "committedDate": "2020-05-21T09:39:05Z", "message": "Simplify UT"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE2Mzk0OTUx", "url": "https://github.com/apache/iceberg/pull/1052#pullrequestreview-416394951", "createdAt": "2020-05-21T18:37:25Z", "commit": {"oid": "7f19edcf8a8d8e56aa20c0584461b802496bcc0a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxODozNzoyNlrOGY-ScA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQxODozNzoyNlrOGY-ScA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODgzOTUzNg==", "bodyText": "Isn't this going to cause Spark to use a nested loop join (full join) because there is no way to partition the data for this expression?\nTo fix it, what about using just the file name as well? File names should be unique because we embed the write UUID, partition, and task ID. And if we add both checks, filename could be used to distribute the data without many collisions and contains could be used for final correctness.\nColumn nameEqual = filename(actualFileDF.col(\"file_path\")).equals(filename(validFileDF.col(\"file_path\")));\nColumn actualContains = actualFileDF.col(\"file_path\").contains(validFileDF.col(\"file_path\"));\nColumn joinCond = nameEqual.and(actualContains);\nFYI @aokolnychyi.", "url": "https://github.com/apache/iceberg/pull/1052#discussion_r428839536", "createdAt": "2020-05-21T18:37:26Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RemoveOrphanFilesAction.java", "diffHunk": "@@ -141,7 +141,7 @@ public RemoveOrphanFilesAction deleteWith(Consumer<String> newDeleteFunc) {\n     Dataset<Row> validFileDF = validDataFileDF.union(validMetadataFileDF);\n     Dataset<Row> actualFileDF = buildActualFileDF();\n \n-    Column joinCond = validFileDF.col(\"file_path\").equalTo(actualFileDF.col(\"file_path\"));\n+    Column joinCond = actualFileDF.col(\"file_path\").contains(validFileDF.col(\"file_path\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7f19edcf8a8d8e56aa20c0584461b802496bcc0a"}, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cecf38821b6fee7fd01c1ffb2e0d276553cef049", "author": {"user": {"login": "jerryshao", "name": "Saisai Shao"}}, "url": "https://github.com/apache/iceberg/commit/cecf38821b6fee7fd01c1ffb2e0d276553cef049", "committedDate": "2020-05-22T05:17:14Z", "message": "Address the comments"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4441, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}