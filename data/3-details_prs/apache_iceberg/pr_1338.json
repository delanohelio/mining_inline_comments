{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY3NjU1MDEw", "number": 1338, "title": "Add file stats range optimizations for DeleteFileIndex", "bodyText": "This adds two optimizations for DeleteFileIndex:\n\nFor position deletes, if the data file path is not in the range of values for a delete file's file_path column, ignore the delete file\nFor equality deletes, if the data file and delete file value ranges for any equality column do not overlap, ignore the delete file", "createdAt": "2020-08-13T22:02:24Z", "url": "https://github.com/apache/iceberg/pull/1338", "merged": true, "mergeCommit": {"oid": "c8d9c85c29ad9426fbaa46687fdc07b1ce1b8f71"}, "closed": true, "closedAt": "2020-08-18T21:51:49Z", "author": {"login": "rdblue"}, "timelineItems": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc-nZSHAH2gAyNDY3NjU1MDEwOjg1MTI4YmIwNjZhOWQ5NWU0ZTg0N2VhZDkzN2Y4YmE3MDQ5NDlkY2U=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdAOOBGgFqTQ2OTg1NTkwNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "85128bb066a9d95e4e847ead937f8ba704949dce", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/85128bb066a9d95e4e847ead937f8ba704949dce", "committedDate": "2020-08-13T22:03:18Z", "message": "Add file stats range optimizations for DeleteFileIndex."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e9d0806aec9c9e0b85b992cf50ca65219448b6b8", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/e9d0806aec9c9e0b85b992cf50ca65219448b6b8", "committedDate": "2020-08-13T21:59:49Z", "message": "Add file stats range optimizations for DeleteFileIndex."}, "afterCommit": {"oid": "85128bb066a9d95e4e847ead937f8ba704949dce", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/85128bb066a9d95e4e847ead937f8ba704949dce", "committedDate": "2020-08-13T22:03:18Z", "message": "Add file stats range optimizations for DeleteFileIndex."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY3MjY4MTg3", "url": "https://github.com/apache/iceberg/pull/1338#pullrequestreview-467268187", "createdAt": "2020-08-14T02:24:22Z", "commit": {"oid": "85128bb066a9d95e4e847ead937f8ba704949dce"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwMjoyNDoyMlrOHAlZzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwMjoyNDoyMlrOHAlZzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDM3NDg2Mw==", "bodyText": "Q:    Is it possible that the dataLowers.get(id) or dataUppers.get(id) could be null ?  Assume the case,  we have a table with two columns. (a, b):\n\ntxn1:  insert a (1,2), then insert file1 will have (1,2);\ntxn2:  delete a (1,2) with equality fields (a,b),  then delete file2 will have (1,2) ;\ntxn3:  add an optional column c in schema;\ntxn4:  insert a (1,2,5) then insert file3 will have (1,2,5);\ntxn5:  delete a (1,2,5) with equality fields (a,b,c), then delete file4 will have (1,2,5).\n\nFinally, the  data file1 will check to decide wether is overlap with file4.  but delete file4 have equalityFieldSet (a,b,c) and file1 don't have the column c.  So seems the dataLowers.get(id)  could be null.  The following Comparator will throw NPE I guess.", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r470374863", "createdAt": "2020-08-14T02:24:22Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +100,101 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, schema))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        // check that the delete file can contain the data file's file_path\n+        Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+        Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+        if (lowers == null || uppers == null) {\n+          return true;\n+        }\n+\n+        Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+        int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+        ByteBuffer lower = lowers.get(pathId);\n+        if (lower != null &&\n+            Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {\n+          return false;\n+        }\n+\n+        ByteBuffer upper = uppers.get(pathId);\n+        if (upper != null &&\n+            Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, upper)) > 0) {\n+          return false;\n+        }\n+\n+        break;\n+\n+      case EQUALITY_DELETES:\n+        if (dataFile.lowerBounds() == null || dataFile.upperBounds() == null ||\n+            deleteFile.lowerBounds() == null || deleteFile.upperBounds() == null) {\n+          return true;\n+        }\n+\n+        Map<Integer, ByteBuffer> dataLowers = dataFile.lowerBounds();\n+        Map<Integer, ByteBuffer> dataUppers = dataFile.upperBounds();\n+        Map<Integer, ByteBuffer> deleteLowers = deleteFile.lowerBounds();\n+        Map<Integer, ByteBuffer> deleteUppers = deleteFile.upperBounds();\n+\n+        for (int id : deleteFile.equalityFieldIds()) {\n+          Type type = schema.findType(id);\n+          if (!type.isPrimitiveType()) {\n+            return true;\n+          }\n+\n+          if (!rangesOverlap(type.asPrimitiveType(),\n+              dataLowers.get(id), dataUppers.get(id), deleteLowers.get(id), deleteUppers.get(id))) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85128bb066a9d95e4e847ead937f8ba704949dce"}, "originalPosition": 108}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY3MzMzMzk1", "url": "https://github.com/apache/iceberg/pull/1338#pullrequestreview-467333395", "createdAt": "2020-08-14T06:24:06Z", "commit": {"oid": "85128bb066a9d95e4e847ead937f8ba704949dce"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNjoyNDowNlrOHAo84g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNjoyNDowNlrOHAo84g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQzMjk5NA==", "bodyText": "How about the case:  data file1 have a range ['3', '5'],  delete file2 have a range ['2', '8'] ?   I think it should also be overlap but here we will return false ?\nIMO, we should return comparator.compare(deleteLower, dataUpper) <= 0 && comparator.compare(deleteUpper, dataLower) >=0  .    That should handle all overlap cases.", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r470432994", "createdAt": "2020-08-14T06:24:06Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +100,101 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, schema))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        // check that the delete file can contain the data file's file_path\n+        Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+        Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+        if (lowers == null || uppers == null) {\n+          return true;\n+        }\n+\n+        Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+        int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+        ByteBuffer lower = lowers.get(pathId);\n+        if (lower != null &&\n+            Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {\n+          return false;\n+        }\n+\n+        ByteBuffer upper = uppers.get(pathId);\n+        if (upper != null &&\n+            Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, upper)) > 0) {\n+          return false;\n+        }\n+\n+        break;\n+\n+      case EQUALITY_DELETES:\n+        if (dataFile.lowerBounds() == null || dataFile.upperBounds() == null ||\n+            deleteFile.lowerBounds() == null || deleteFile.upperBounds() == null) {\n+          return true;\n+        }\n+\n+        Map<Integer, ByteBuffer> dataLowers = dataFile.lowerBounds();\n+        Map<Integer, ByteBuffer> dataUppers = dataFile.upperBounds();\n+        Map<Integer, ByteBuffer> deleteLowers = deleteFile.lowerBounds();\n+        Map<Integer, ByteBuffer> deleteUppers = deleteFile.upperBounds();\n+\n+        for (int id : deleteFile.equalityFieldIds()) {\n+          Type type = schema.findType(id);\n+          if (!type.isPrimitiveType()) {\n+            return true;\n+          }\n+\n+          if (!rangesOverlap(type.asPrimitiveType(),\n+              dataLowers.get(id), dataUppers.get(id), deleteLowers.get(id), deleteUppers.get(id))) {\n+            return false;\n+          }\n+        }\n+        break;\n+    }\n+\n+    return true;\n+  }\n+\n+  private static <T> boolean rangesOverlap(Type.PrimitiveType type,\n+                                           ByteBuffer dataLower, ByteBuffer dataUpper,\n+                                           ByteBuffer deleteLower, ByteBuffer deleteUpper) {\n+    Comparator<T> comparator = Comparators.forType(type);\n+    T low = Conversions.fromByteBuffer(type, dataLower);\n+    T high = Conversions.fromByteBuffer(type, dataUpper);\n+\n+    if (contains(comparator, low, high, Conversions.fromByteBuffer(type, deleteLower))) {\n+      return true;\n+    }\n+\n+    if (contains(comparator, low, high, Conversions.fromByteBuffer(type, deleteUpper))) {\n+      return true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85128bb066a9d95e4e847ead937f8ba704949dce"}, "originalPosition": 130}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "95ccd5e272c9457c6b9780a02b8a45f57dd6a877", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/95ccd5e272c9457c6b9780a02b8a45f57dd6a877", "committedDate": "2020-08-17T22:12:49Z", "message": "Fix range overlap check and add null check."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b4e177b7c8fbd979b9afe0a00e8adde3cb6d9280", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/b4e177b7c8fbd979b9afe0a00e8adde3cb6d9280", "committedDate": "2020-08-17T22:12:49Z", "message": "Add tests for DataFileIndex stats filters."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "721a484a2c908e2e0f58022a3e78f4b5334e1928", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/721a484a2c908e2e0f58022a3e78f4b5334e1928", "committedDate": "2020-08-17T22:16:50Z", "message": "Minor fixes for delete files."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c6d8b7fcce666607c54a2d25abcf752ee9cb8719", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/c6d8b7fcce666607c54a2d25abcf752ee9cb8719", "committedDate": "2020-08-17T22:17:29Z", "message": "Fix FilterIterator with reused values."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY4ODcxNDM3", "url": "https://github.com/apache/iceberg/pull/1338#pullrequestreview-468871437", "createdAt": "2020-08-17T22:23:01Z", "commit": {"oid": "c6d8b7fcce666607c54a2d25abcf752ee9cb8719"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QyMjoyMzowMlrOHB81GA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QyMjoyMzowMlrOHB81GA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgwNzI1Ng==", "bodyText": "This was needed to ensure the stats columns are projected for data files when there are delete files, even if the stats columns were not requested by the caller.", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r471807256", "createdAt": "2020-08-17T22:23:02Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/ManifestGroup.java", "diffHunk": "@@ -163,6 +166,9 @@ ManifestGroup planWith(ExecutorService newExecutorService) {\n     DeleteFileIndex deleteFiles = deleteIndexBuilder.build();\n \n     boolean dropStats = ManifestReader.dropStats(dataFilter, columns);\n+    if (!deleteFiles.isEmpty()) {\n+      select(Streams.concat(columns.stream(), ManifestReader.STATS_COLUMNS.stream()).collect(Collectors.toList()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6d8b7fcce666607c54a2d25abcf752ee9cb8719"}, "originalPosition": 22}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY4ODcyMDA5", "url": "https://github.com/apache/iceberg/pull/1338#pullrequestreview-468872009", "createdAt": "2020-08-17T22:24:27Z", "commit": {"oid": "c6d8b7fcce666607c54a2d25abcf752ee9cb8719"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QyMjoyNDoyN1rOHB83DQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QyMjoyNDoyN1rOHB83DQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgwNzc1Nw==", "bodyText": "This fixes the filter with reused containers, like GenericRecord. The advance call in next would replace values in a reused row, which would in effect return the next matching row.", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r471807757", "createdAt": "2020-08-17T22:24:27Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/util/FilterIterator.java", "diffHunk": "@@ -34,21 +34,21 @@\n public abstract class FilterIterator<T> implements CloseableIterator<T> {\n   private final Iterator<T> items;\n   private boolean closed;\n-  private boolean hasNext;\n+  private boolean nextReady;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6d8b7fcce666607c54a2d25abcf752ee9cb8719"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY4ODcyNDk4", "url": "https://github.com/apache/iceberg/pull/1338#pullrequestreview-468872498", "createdAt": "2020-08-17T22:25:37Z", "commit": {"oid": "c6d8b7fcce666607c54a2d25abcf752ee9cb8719"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QyMjoyNTozN1rOHB85Dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xN1QyMjoyNTozN1rOHB85Dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgwODI3MQ==", "bodyText": "Checking for createWriterFunc here is needed because forTable sets the row schema. So rows can be included in position deletes when the writer func is added and when there is a row schema.", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r471808271", "createdAt": "2020-08-17T22:25:37Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -383,10 +383,7 @@ public DeleteWriteBuilder equalityFieldIds(int... fieldIds) {\n \n       meta(\"delete-type\", \"position\");\n \n-      if (rowSchema != null) {\n-        Preconditions.checkState(createWriterFunc != null,\n-            \"Cannot create delete file with deletes rows unless createWriterFunc is set\");\n-\n+      if (rowSchema != null && createWriterFunc != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6d8b7fcce666607c54a2d25abcf752ee9cb8719"}, "originalPosition": 33}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b32e087a782e34fa1660e045b057a2f448997a12", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/b32e087a782e34fa1660e045b057a2f448997a12", "committedDate": "2020-08-17T22:27:26Z", "message": "Fix checkstyle issues."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5MzE3NzEx", "url": "https://github.com/apache/iceberg/pull/1338#pullrequestreview-469317711", "createdAt": "2020-08-18T11:51:56Z", "commit": {"oid": "b32e087a782e34fa1660e045b057a2f448997a12"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMTo1MTo1NlrOHCP4Tw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMjo1NToxNFrOHCSjKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjExOTM3NQ==", "bodyText": "Nice catch.", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472119375", "createdAt": "2020-08-18T11:51:56Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -175,6 +175,7 @@ public PartitionData copy() {\n     this.keyMetadata = toCopy.keyMetadata == null ? null : Arrays.copyOf(toCopy.keyMetadata, toCopy.keyMetadata.length);\n     this.splitOffsets = toCopy.splitOffsets == null ? null :\n         Arrays.copyOf(toCopy.splitOffsets, toCopy.splitOffsets.length);\n+    this.equalityIds = toCopy.equalityIds != null ? Arrays.copyOf(toCopy.equalityIds, toCopy.equalityIds.length) : null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b32e087a782e34fa1660e045b057a2f448997a12"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjEzNzI5Nw==", "bodyText": "Why change this ? IMO, if rowSchema is not null and createWriteFunc is null, we should throw exception, rather than going to the delete files without row path ?", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472137297", "createdAt": "2020-08-18T12:24:52Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/avro/Avro.java", "diffHunk": "@@ -300,10 +300,7 @@ public DeleteWriteBuilder equalityFieldIds(int... fieldIds) {\n \n       meta(\"delete-type\", \"position\");\n \n-      if (rowSchema != null) {\n-        Preconditions.checkState(createWriterFunc != null,\n-            \"Cannot create delete file with deletes rows unless createWriterFunc is set\");\n-\n+      if (rowSchema != null && createWriterFunc != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b32e087a782e34fa1660e045b057a2f448997a12"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjE2MzExMw==", "bodyText": "Q:  What's the case that nullValueCount or valueCount will be null ?  Does that mean there's no null item in this file ?\nIt's safe enough to return false here (Because the canContainEqDeletesForFile won't skip the files to join , while if return true in this method then it possible that we will skip files to join),  but once return true we must ensure that it's surely the right case.", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472163113", "createdAt": "2020-08-18T12:55:14Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +104,157 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, specsById.get(file.specId()).schema()))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        return canContainPosDeletesForFile(dataFile, deleteFile);\n+\n+      case EQUALITY_DELETES:\n+        return canContainEqDeletesForFile(dataFile, deleteFile, schema);\n+    }\n+\n+    return true;\n+  }\n+\n+  private static boolean canContainPosDeletesForFile(DataFile dataFile, DeleteFile deleteFile) {\n+    // check that the delete file can contain the data file's file_path\n+    Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+    if (lowers == null || uppers == null) {\n+      return true;\n+    }\n+\n+    Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+    int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+    ByteBuffer lower = lowers.get(pathId);\n+    if (lower != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {\n+      return false;\n     }\n+\n+    ByteBuffer upper = uppers.get(pathId);\n+    if (upper != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, upper)) > 0) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private static boolean canContainEqDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    if (dataFile.lowerBounds() == null || dataFile.upperBounds() == null ||\n+        deleteFile.lowerBounds() == null || deleteFile.upperBounds() == null) {\n+      return true;\n+    }\n+\n+    Map<Integer, ByteBuffer> dataLowers = dataFile.lowerBounds();\n+    Map<Integer, ByteBuffer> dataUppers = dataFile.upperBounds();\n+    Map<Integer, ByteBuffer> deleteLowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> deleteUppers = deleteFile.upperBounds();\n+\n+    Map<Integer, Long> dataNullCounts = dataFile.nullValueCounts();\n+    Map<Integer, Long> dataValueCounts = dataFile.valueCounts();\n+    Map<Integer, Long> deleteNullCounts = deleteFile.nullValueCounts();\n+    Map<Integer, Long> deleteValueCounts = deleteFile.valueCounts();\n+\n+    for (int id : deleteFile.equalityFieldIds()) {\n+      Types.NestedField field = schema.findField(id);\n+      if (!field.type().isPrimitiveType()) {\n+        return true;\n+      }\n+\n+      if (allNull(dataNullCounts, dataValueCounts, field) && allNonNull(deleteNullCounts, field)) {\n+        return false;\n+      }\n+\n+      if (allNull(deleteNullCounts, deleteValueCounts, field) && allNonNull(dataNullCounts, field)) {\n+        return false;\n+      }\n+\n+      ByteBuffer dataLower = dataLowers.get(id);\n+      ByteBuffer dataUpper = dataUppers.get(id);\n+      ByteBuffer deleteLower = deleteLowers.get(id);\n+      ByteBuffer deleteUpper = deleteUppers.get(id);\n+      if (dataLower == null || dataUpper == null || deleteLower == null || deleteUpper == null) {\n+        return true;\n+      }\n+\n+      if (!rangesOverlap(field.type().asPrimitiveType(), dataLower, dataUpper, deleteLower, deleteUpper)) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  private static <T> boolean rangesOverlap(Type.PrimitiveType type,\n+                                           ByteBuffer dataLowerBuf, ByteBuffer dataUpperBuf,\n+                                           ByteBuffer deleteLowerBuf, ByteBuffer deleteUpperBuf) {\n+    Comparator<T> comparator = Comparators.forType(type);\n+    T dataLower = Conversions.fromByteBuffer(type, dataLowerBuf);\n+    T dataUpper = Conversions.fromByteBuffer(type, dataUpperBuf);\n+    T deleteLower = Conversions.fromByteBuffer(type, deleteLowerBuf);\n+    T deleteUpper = Conversions.fromByteBuffer(type, deleteUpperBuf);\n+\n+    return comparator.compare(deleteLower, dataUpper) <= 0 && comparator.compare(dataLower, deleteUpper) <= 0;\n+  }\n+\n+  private static boolean allNonNull(Map<Integer, Long> nullValueCounts, Types.NestedField field) {\n+    if (field.isRequired()) {\n+      return true;\n+    }\n+\n+    if (nullValueCounts == null) {\n+      return false;\n+    }\n+\n+    Long nullValueCount = nullValueCounts.get(field.fieldId());\n+    if (nullValueCount == null) {\n+      return false;\n+    }\n+\n+    return nullValueCount <= 0;\n+  }\n+\n+  private static boolean allNull(Map<Integer, Long> nullValueCounts, Map<Integer, Long> valueCounts,\n+                                 Types.NestedField field) {\n+    if (field.isRequired()) {\n+      return false;\n+    }\n+\n+    if (nullValueCounts == null || valueCounts == null) {\n+      return false;\n+    }\n+\n+    Long nullValueCount = nullValueCounts.get(field.fieldId());\n+    Long valueCount = valueCounts.get(field.fieldId());\n+    if (nullValueCount == null || valueCount == null) {\n+      return true;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b32e087a782e34fa1660e045b057a2f448997a12"}, "originalPosition": 199}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NTc2MjA3", "url": "https://github.com/apache/iceberg/pull/1338#pullrequestreview-469576207", "createdAt": "2020-08-18T15:33:42Z", "commit": {"oid": "b32e087a782e34fa1660e045b057a2f448997a12"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNTozMzo0M1rOHCaN5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNTozMzo0M1rOHCaN5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI4ODc0MA==", "bodyText": "nit: why expected size is 1?", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472288740", "createdAt": "2020-08-18T15:33:43Z", "author": {"login": "aokolnychyi"}, "path": "api/src/main/java/org/apache/iceberg/data/Record.java", "diffHunk": "@@ -35,4 +36,26 @@\n   Record copy();\n \n   Record copy(Map<String, Object> overwriteValues);\n+\n+  default Record copy(String field, Object value) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);\n+    overwriteValues.put(field, value);\n+    return copy(overwriteValues);\n+  }\n+\n+  default Record copy(String field1, Object value1, String field2, Object value2) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b32e087a782e34fa1660e045b057a2f448997a12"}, "originalPosition": 20}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NTc2MzUy", "url": "https://github.com/apache/iceberg/pull/1338#pullrequestreview-469576352", "createdAt": "2020-08-18T15:33:52Z", "commit": {"oid": "b32e087a782e34fa1660e045b057a2f448997a12"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNTozMzo1MlrOHCaOVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNTozMzo1MlrOHCaOVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI4ODg1NQ==", "bodyText": "nit: same here", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472288855", "createdAt": "2020-08-18T15:33:52Z", "author": {"login": "aokolnychyi"}, "path": "api/src/main/java/org/apache/iceberg/data/Record.java", "diffHunk": "@@ -35,4 +36,26 @@\n   Record copy();\n \n   Record copy(Map<String, Object> overwriteValues);\n+\n+  default Record copy(String field, Object value) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);\n+    overwriteValues.put(field, value);\n+    return copy(overwriteValues);\n+  }\n+\n+  default Record copy(String field1, Object value1, String field2, Object value2) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);\n+    overwriteValues.put(field1, value1);\n+    overwriteValues.put(field2, value2);\n+    return copy(overwriteValues);\n+  }\n+\n+  default Record copy(String field1, Object value1, String field2, Object value2, String field3, Object value3) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b32e087a782e34fa1660e045b057a2f448997a12"}, "originalPosition": 27}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NTg2MDY0", "url": "https://github.com/apache/iceberg/pull/1338#pullrequestreview-469586064", "createdAt": "2020-08-18T15:44:46Z", "commit": {"oid": "b32e087a782e34fa1660e045b057a2f448997a12"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNTo0NDo0NlrOHCar3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNTo0NDo0NlrOHCar3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI5NjQxNA==", "bodyText": "nit: will it fit on one line if we define a comparator as a separate variable?", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472296414", "createdAt": "2020-08-18T15:44:46Z", "author": {"login": "aokolnychyi"}, "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +104,157 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, specsById.get(file.specId()).schema()))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        return canContainPosDeletesForFile(dataFile, deleteFile);\n+\n+      case EQUALITY_DELETES:\n+        return canContainEqDeletesForFile(dataFile, deleteFile, schema);\n+    }\n+\n+    return true;\n+  }\n+\n+  private static boolean canContainPosDeletesForFile(DataFile dataFile, DeleteFile deleteFile) {\n+    // check that the delete file can contain the data file's file_path\n+    Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+    if (lowers == null || uppers == null) {\n+      return true;\n+    }\n+\n+    Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+    int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+    ByteBuffer lower = lowers.get(pathId);\n+    if (lower != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b32e087a782e34fa1660e045b057a2f448997a12"}, "originalPosition": 97}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NTg2MTE3", "url": "https://github.com/apache/iceberg/pull/1338#pullrequestreview-469586117", "createdAt": "2020-08-18T15:44:49Z", "commit": {"oid": "b32e087a782e34fa1660e045b057a2f448997a12"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNTo0NDo1MFrOHCar-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNTo0NDo1MFrOHCar-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI5NjQ0Mw==", "bodyText": "same here", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472296443", "createdAt": "2020-08-18T15:44:50Z", "author": {"login": "aokolnychyi"}, "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +104,157 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, specsById.get(file.specId()).schema()))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        return canContainPosDeletesForFile(dataFile, deleteFile);\n+\n+      case EQUALITY_DELETES:\n+        return canContainEqDeletesForFile(dataFile, deleteFile, schema);\n+    }\n+\n+    return true;\n+  }\n+\n+  private static boolean canContainPosDeletesForFile(DataFile dataFile, DeleteFile deleteFile) {\n+    // check that the delete file can contain the data file's file_path\n+    Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+    if (lowers == null || uppers == null) {\n+      return true;\n+    }\n+\n+    Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+    int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+    ByteBuffer lower = lowers.get(pathId);\n+    if (lower != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {\n+      return false;\n     }\n+\n+    ByteBuffer upper = uppers.get(pathId);\n+    if (upper != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, upper)) > 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b32e087a782e34fa1660e045b057a2f448997a12"}, "originalPosition": 103}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NTkwMDM0", "url": "https://github.com/apache/iceberg/pull/1338#pullrequestreview-469590034", "createdAt": "2020-08-18T15:49:12Z", "commit": {"oid": "b32e087a782e34fa1660e045b057a2f448997a12"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNTo0OToxMlrOHCa3zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNTo0OToxMlrOHCa3zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI5OTQ3MA==", "bodyText": "A few comments would be helpful here for devs who will work on this later", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472299470", "createdAt": "2020-08-18T15:49:12Z", "author": {"login": "aokolnychyi"}, "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +104,157 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, specsById.get(file.specId()).schema()))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        return canContainPosDeletesForFile(dataFile, deleteFile);\n+\n+      case EQUALITY_DELETES:\n+        return canContainEqDeletesForFile(dataFile, deleteFile, schema);\n+    }\n+\n+    return true;\n+  }\n+\n+  private static boolean canContainPosDeletesForFile(DataFile dataFile, DeleteFile deleteFile) {\n+    // check that the delete file can contain the data file's file_path\n+    Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+    if (lowers == null || uppers == null) {\n+      return true;\n+    }\n+\n+    Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+    int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+    ByteBuffer lower = lowers.get(pathId);\n+    if (lower != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {\n+      return false;\n     }\n+\n+    ByteBuffer upper = uppers.get(pathId);\n+    if (upper != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, upper)) > 0) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private static boolean canContainEqDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    if (dataFile.lowerBounds() == null || dataFile.upperBounds() == null ||\n+        deleteFile.lowerBounds() == null || deleteFile.upperBounds() == null) {\n+      return true;\n+    }\n+\n+    Map<Integer, ByteBuffer> dataLowers = dataFile.lowerBounds();\n+    Map<Integer, ByteBuffer> dataUppers = dataFile.upperBounds();\n+    Map<Integer, ByteBuffer> deleteLowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> deleteUppers = deleteFile.upperBounds();\n+\n+    Map<Integer, Long> dataNullCounts = dataFile.nullValueCounts();\n+    Map<Integer, Long> dataValueCounts = dataFile.valueCounts();\n+    Map<Integer, Long> deleteNullCounts = deleteFile.nullValueCounts();\n+    Map<Integer, Long> deleteValueCounts = deleteFile.valueCounts();\n+\n+    for (int id : deleteFile.equalityFieldIds()) {\n+      Types.NestedField field = schema.findField(id);\n+      if (!field.type().isPrimitiveType()) {\n+        return true;\n+      }\n+\n+      if (allNull(dataNullCounts, dataValueCounts, field) && allNonNull(deleteNullCounts, field)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b32e087a782e34fa1660e045b057a2f448997a12"}, "originalPosition": 133}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "591f7808d80d91fd975a6d59d1cbdd06c8e6250b", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/591f7808d80d91fd975a6d59d1cbdd06c8e6250b", "committedDate": "2020-08-18T18:35:04Z", "message": "Fix checkstyle and review comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "52db59f151be59f4a9c9260f11e505c5903f7a9d", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/52db59f151be59f4a9c9260f11e505c5903f7a9d", "committedDate": "2020-08-18T18:36:41Z", "message": "Fix allNull in DeleteFileIndex when counts are unknown."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "690da6ad40d01aacbd864218f7335922616eee6b", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/690da6ad40d01aacbd864218f7335922616eee6b", "committedDate": "2020-08-18T18:47:52Z", "message": "Add comments to canContain for equality, update null handling."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3b994df9140e4f3dc5a78faa11da5d9f41e61ac2", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/3b994df9140e4f3dc5a78faa11da5d9f41e61ac2", "committedDate": "2020-08-18T18:50:46Z", "message": "Add a test for null values without value range overlap."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8c0b01a643dc69e1ee5a704bfbd6785343192dbd", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/8c0b01a643dc69e1ee5a704bfbd6785343192dbd", "committedDate": "2020-08-18T18:55:36Z", "message": "Apply equality count checks even if value ranges are missing."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5ODU1OTA1", "url": "https://github.com/apache/iceberg/pull/1338#pullrequestreview-469855905", "createdAt": "2020-08-18T21:51:13Z", "commit": {"oid": "8c0b01a643dc69e1ee5a704bfbd6785343192dbd"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4041, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}