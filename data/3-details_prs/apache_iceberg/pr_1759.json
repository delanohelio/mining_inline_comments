{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIwMTUzNjg2", "number": 1759, "title": "Spark: Implement RollbackToSnapshotProcedure", "bodyText": "This PR implements a procedure to rollback to a given snapshot id.\nResovles #1592.", "createdAt": "2020-11-12T20:57:12Z", "url": "https://github.com/apache/iceberg/pull/1759", "merged": true, "mergeCommit": {"oid": "e1839c3b6caedf8221d08984a0260ef18893747b"}, "closed": true, "closedAt": "2020-11-13T18:59:05Z", "author": {"login": "aokolnychyi"}, "timelineItems": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdb49WUgH2gAyNTIwMTUzNjg2OjJlY2E0NDg0ZDRkZWYyMzcxOWM4ZmE5MGRkYWRhYWIzZjhiNjkyMTI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdcL5L8AFqTUzMDMyNzc4Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/2eca4484d4def23719c8fa90ddadaab3f8b69212", "committedDate": "2020-11-12T20:54:53Z", "message": "Spark: Implement RollbackToSnapshotProcedure"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NDkxOTU4", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529491958", "createdAt": "2020-11-12T20:58:28Z", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMDo1ODoyOFrOHyOHSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMDo1ODoyOFrOHyOHSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyMjA4OA==", "bodyText": "Initially, I added a single test suite for all operations that manage snapshots. We may have a suite per procedure now.", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522422088", "createdAt": "2020-11-12T20:58:28Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Test;\n+\n+public class TestManageSnapshotsProcedures extends SparkExtensionsTestBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NDkyOTEy", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529492912", "createdAt": "2020-11-12T20:59:50Z", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMDo1OTo1MVrOHyOKOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMDo1OTo1MVrOHyOKOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyMjg0MQ==", "bodyText": "Our table catalog is always case sensitive right now.", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522422841", "createdAt": "2020-11-12T20:59:51Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/BaseCatalog.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.Locale;\n+import org.apache.iceberg.spark.procedures.SparkProcedure;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.SupportsNamespaces;\n+import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureCatalog;\n+\n+abstract class BaseCatalog implements StagingTableCatalog, ProcedureCatalog, SupportsNamespaces {\n+\n+  @Override\n+  public Procedure loadProcedure(Identifier ident) throws NoSuchProcedureException {\n+    String[] namespace = ident.namespace();\n+    String name = ident.name();\n+\n+    // namespace resolution is case sensitive to match how we resolve namespaces for tables right now\n+    if (namespace.length == 1 && namespace[0].equals(\"system\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NDkzMDEz", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529493013", "createdAt": "2020-11-12T21:00:01Z", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTowMDowMVrOHyOKkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTowMDowMVrOHyOKkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyMjkyOA==", "bodyText": "This is debatable.", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522422928", "createdAt": "2020-11-12T21:00:01Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/BaseCatalog.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.Locale;\n+import org.apache.iceberg.spark.procedures.SparkProcedure;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.SupportsNamespaces;\n+import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureCatalog;\n+\n+abstract class BaseCatalog implements StagingTableCatalog, ProcedureCatalog, SupportsNamespaces {\n+\n+  @Override\n+  public Procedure loadProcedure(Identifier ident) throws NoSuchProcedureException {\n+    String[] namespace = ident.namespace();\n+    String name = ident.name();\n+\n+    // namespace resolution is case sensitive to match how we resolve namespaces for tables right now\n+    if (namespace.length == 1 && namespace[0].equals(\"system\")) {\n+      try {\n+        // procedure resolution is case insensitive to match the existing Spark behavior for functions\n+        // SimpleFunctionRegistry normalizes function names but leaves namespace resolution to the caller\n+        SparkProcedure procedure = SparkProcedure.valueOf(name.toUpperCase(Locale.ROOT));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NDk0ODgy", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529494882", "createdAt": "2020-11-12T21:02:45Z", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTowMjo0NVrOHyOQIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTowMjo0NVrOHyOQIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyNDM1NQ==", "bodyText": "This place requires discussion. Right now, I allow calling procedures like this:\nCALL cat.system.rollback_to_snapshot_id('`name.space`', '`table.name`', snapshot_id)\n\nWhere the namespace and table names are strings but they may be quoted.", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522424355", "createdAt": "2020-11-12T21:02:45Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.function.Function;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n+import org.apache.spark.sql.execution.CacheManager;\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation;\n+import scala.Option;\n+import scala.collection.Seq;\n+\n+abstract class BaseProcedure implements Procedure {\n+  private final SparkSession spark;\n+  private final TableCatalog catalog;\n+\n+  protected BaseProcedure(TableCatalog catalog) {\n+    this.spark = SparkSession.active();\n+    this.catalog = catalog;\n+  }\n+\n+  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n+    Preconditions.checkArgument(!namespace.isEmpty(), \"Namespace cannot be empty\");\n+    Preconditions.checkArgument(!tableName.isEmpty(), \"Table name cannot be empty\");\n+\n+    Identifier ident = toIdentifier(namespace, tableName);\n+    SparkTable sparkTable = loadSparkTable(ident);\n+    org.apache.iceberg.Table icebergTable = sparkTable.table();\n+\n+    T result = func.apply(icebergTable);\n+\n+    refreshSparkCache(ident, sparkTable);\n+\n+    return result;\n+  }\n+\n+  // we have to parse both namespace and name as they may be quoted\n+  protected Identifier toIdentifier(String namespaceAsString, String name) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "originalPosition": 64}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NDk2MTI1", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529496125", "createdAt": "2020-11-12T21:04:42Z", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTowNDo0MlrOHyOUCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTowNDo0MlrOHyOUCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQyNTM1NA==", "bodyText": "Here is the test for quoted identifiers.", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522425354", "createdAt": "2020-11-12T21:04:42Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Test;\n+\n+public class TestManageSnapshotsProcedures extends SparkExtensionsTestBase {\n+\n+  public TestManageSnapshotsProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotUsingPositionalArgs() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot('%s', '%s', %dL)\",\n+        catalogName, tableIdent.namespace(), tableIdent.name(), firstSnapshot.snapshotId());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"Rollback must be successful\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotUsingNamedArgs() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot(snapshot_id => %dL, namespace => '%s', table => '%s')\",\n+        catalogName, firstSnapshot.snapshotId(), tableIdent.namespace(), tableIdent.name());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"Rollback must be successful\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotRefreshesRelationCache() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    Dataset<Row> query = spark.sql(\"SELECT * FROM \" + tableName + \" WHERE id = 1\");\n+    query.createOrReplaceTempView(\"tmp\");\n+\n+    spark.sql(\"CACHE TABLE tmp\");\n+\n+    assertEquals(\"View should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM tmp\"));\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot(namespace => '%s', table => '%s', snapshot_id => %dL)\",\n+        catalogName, tableIdent.namespace(), tableIdent.name(), firstSnapshot.snapshotId());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"View cache must be invalidated\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM tmp\"));\n+\n+    sql(\"UNCACHE TABLE tmp\");\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotWithQuotedIdentifiers() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    StringBuilder quotedNamespaceBuilder = new StringBuilder();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "originalPosition": 166}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTEwMTA1", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529510105", "createdAt": "2020-11-12T21:26:47Z", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToyNjo0N1rOHyO_Cw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToyNjo0N1rOHyO_Cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzNjM2Mw==", "bodyText": "It looks like this line is the only reason for this test class. What about adding a way to pass additional Spark properties to the base class instead? Like protected Map<String, String> sparkConfig().", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522436363", "createdAt": "2020-11-12T21:26:47Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkExtensionsTestBase.java", "diffHunk": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.hive.TestHiveMetastore;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.internal.SQLConf;\n+import org.junit.BeforeClass;\n+\n+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.METASTOREURIS;\n+\n+public abstract class SparkExtensionsTestBase extends SparkCatalogTestBase {\n+\n+  public SparkExtensionsTestBase(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @BeforeClass\n+  public static void startMetastoreAndSpark() {\n+    SparkTestBase.metastore = new TestHiveMetastore();\n+    metastore.start();\n+    SparkTestBase.hiveConf = metastore.hiveConf();\n+\n+    SparkTestBase.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n+        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "originalPosition": 48}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTExNDU5", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529511459", "createdAt": "2020-11-12T21:28:54Z", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToyODo1NFrOHyPDNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMToyODo1NFrOHyPDNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzNzQzMA==", "bodyText": "Is %dL required or could it be %d? I think because of the casting that was added, it could be %d.", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522437430", "createdAt": "2020-11-12T21:28:54Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Test;\n+\n+public class TestManageSnapshotsProcedures extends SparkExtensionsTestBase {\n+\n+  public TestManageSnapshotsProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotUsingPositionalArgs() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot('%s', '%s', %dL)\",\n+        catalogName, tableIdent.namespace(), tableIdent.name(), firstSnapshot.snapshotId());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"Rollback must be successful\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotUsingNamedArgs() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot(snapshot_id => %dL, namespace => '%s', table => '%s')\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "originalPosition": 98}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTEzNzQx", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529513741", "createdAt": "2020-11-12T21:32:26Z", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTozMjoyNlrOHyPKSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTozMjoyNlrOHyPKSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQzOTI0Mg==", "bodyText": "There are a couple more cases:\n\nPositional arguments missing table name\nPositional arguments missing namespace", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522439242", "createdAt": "2020-11-12T21:32:26Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestManageSnapshotsProcedures.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Test;\n+\n+public class TestManageSnapshotsProcedures extends SparkExtensionsTestBase {\n+\n+  public TestManageSnapshotsProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotUsingPositionalArgs() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot('%s', '%s', %dL)\",\n+        catalogName, tableIdent.namespace(), tableIdent.name(), firstSnapshot.snapshotId());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"Rollback must be successful\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotUsingNamedArgs() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot(snapshot_id => %dL, namespace => '%s', table => '%s')\",\n+        catalogName, firstSnapshot.snapshotId(), tableIdent.namespace(), tableIdent.name());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"Rollback must be successful\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotRefreshesRelationCache() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    Dataset<Row> query = spark.sql(\"SELECT * FROM \" + tableName + \" WHERE id = 1\");\n+    query.createOrReplaceTempView(\"tmp\");\n+\n+    spark.sql(\"CACHE TABLE tmp\");\n+\n+    assertEquals(\"View should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM tmp\"));\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot(namespace => '%s', table => '%s', snapshot_id => %dL)\",\n+        catalogName, tableIdent.namespace(), tableIdent.name(), firstSnapshot.snapshotId());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"View cache must be invalidated\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM tmp\"));\n+\n+    sql(\"UNCACHE TABLE tmp\");\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotWithQuotedIdentifiers() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    StringBuilder quotedNamespaceBuilder = new StringBuilder();\n+    for (String level : tableIdent.namespace().levels()) {\n+      quotedNamespaceBuilder.append(\"`\");\n+      quotedNamespaceBuilder.append(level);\n+      quotedNamespaceBuilder.append(\"`\");\n+    }\n+    String quotedNamespace = quotedNamespaceBuilder.toString();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot('%s', '`%s`', %d)\",\n+        catalogName, quotedNamespace, tableIdent.name(), firstSnapshot.snapshotId());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"Rollback must be successful\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testRollbackToInvalidSnapshot() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+\n+    Namespace namespace = tableIdent.namespace();\n+    String tableName = tableIdent.name();\n+\n+    AssertHelpers.assertThrows(\"Should reject invalid snapshot id\",\n+        ValidationException.class, \"Cannot roll back to unknown snapshot id\",\n+        () -> sql(\"CALL %s.system.rollback_to_snapshot('%s', '%s', -1L)\", catalogName, namespace, tableName));\n+  }\n+\n+  @Test\n+  public void testInvalidRollbackToSnapshotCases() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "originalPosition": 200}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTE1NDI3", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529515427", "createdAt": "2020-11-12T21:35:07Z", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTozNTowN1rOHyPPaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTozNTowN1rOHyPPaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MDU1NQ==", "bodyText": "What about namespace is null?", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522440555", "createdAt": "2020-11-12T21:35:07Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.function.Function;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n+import org.apache.spark.sql.execution.CacheManager;\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation;\n+import scala.Option;\n+import scala.collection.Seq;\n+\n+abstract class BaseProcedure implements Procedure {\n+  private final SparkSession spark;\n+  private final TableCatalog catalog;\n+\n+  protected BaseProcedure(TableCatalog catalog) {\n+    this.spark = SparkSession.active();\n+    this.catalog = catalog;\n+  }\n+\n+  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n+    Preconditions.checkArgument(!namespace.isEmpty(), \"Namespace cannot be empty\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTE3MDAy", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529517002", "createdAt": "2020-11-12T21:37:45Z", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTozNzo0NVrOHyPUUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTozNzo0NVrOHyPUUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MTgwOA==", "bodyText": "Even in a private method, I don't think it is a good practice to use Scala Seq because of compatibility problems. It would be safer to pass this directly into a converter to get a List.", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522441808", "createdAt": "2020-11-12T21:37:45Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.function.Function;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n+import org.apache.spark.sql.execution.CacheManager;\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation;\n+import scala.Option;\n+import scala.collection.Seq;\n+\n+abstract class BaseProcedure implements Procedure {\n+  private final SparkSession spark;\n+  private final TableCatalog catalog;\n+\n+  protected BaseProcedure(TableCatalog catalog) {\n+    this.spark = SparkSession.active();\n+    this.catalog = catalog;\n+  }\n+\n+  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n+    Preconditions.checkArgument(!namespace.isEmpty(), \"Namespace cannot be empty\");\n+    Preconditions.checkArgument(!tableName.isEmpty(), \"Table name cannot be empty\");\n+\n+    Identifier ident = toIdentifier(namespace, tableName);\n+    SparkTable sparkTable = loadSparkTable(ident);\n+    org.apache.iceberg.Table icebergTable = sparkTable.table();\n+\n+    T result = func.apply(icebergTable);\n+\n+    refreshSparkCache(ident, sparkTable);\n+\n+    return result;\n+  }\n+\n+  // we have to parse both namespace and name as they may be quoted\n+  protected Identifier toIdentifier(String namespaceAsString, String name) {\n+    Seq<String> namespaceParts = parseMultipartIdentifier(namespaceAsString);\n+    String[] namespace = new String[namespaceParts.size()];\n+    namespaceParts.copyToArray(namespace);\n+\n+    Seq<String> nameParts = parseMultipartIdentifier(name);\n+    Preconditions.checkArgument(nameParts.size() == 1, \"Name must consist of one part: %s\", name);\n+\n+    return Identifier.of(namespace, nameParts.head());\n+  }\n+\n+  private Seq<String> parseMultipartIdentifier(String identifierAsString) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "originalPosition": 75}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTE3NTg4", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529517588", "createdAt": "2020-11-12T21:38:41Z", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTozODo0MVrOHyPWGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTozODo0MVrOHyPWGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MjI2NQ==", "bodyText": "Why not let the NoSuchTableException propagate?", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522442265", "createdAt": "2020-11-12T21:38:41Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.function.Function;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n+import org.apache.spark.sql.execution.CacheManager;\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation;\n+import scala.Option;\n+import scala.collection.Seq;\n+\n+abstract class BaseProcedure implements Procedure {\n+  private final SparkSession spark;\n+  private final TableCatalog catalog;\n+\n+  protected BaseProcedure(TableCatalog catalog) {\n+    this.spark = SparkSession.active();\n+    this.catalog = catalog;\n+  }\n+\n+  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n+    Preconditions.checkArgument(!namespace.isEmpty(), \"Namespace cannot be empty\");\n+    Preconditions.checkArgument(!tableName.isEmpty(), \"Table name cannot be empty\");\n+\n+    Identifier ident = toIdentifier(namespace, tableName);\n+    SparkTable sparkTable = loadSparkTable(ident);\n+    org.apache.iceberg.Table icebergTable = sparkTable.table();\n+\n+    T result = func.apply(icebergTable);\n+\n+    refreshSparkCache(ident, sparkTable);\n+\n+    return result;\n+  }\n+\n+  // we have to parse both namespace and name as they may be quoted\n+  protected Identifier toIdentifier(String namespaceAsString, String name) {\n+    Seq<String> namespaceParts = parseMultipartIdentifier(namespaceAsString);\n+    String[] namespace = new String[namespaceParts.size()];\n+    namespaceParts.copyToArray(namespace);\n+\n+    Seq<String> nameParts = parseMultipartIdentifier(name);\n+    Preconditions.checkArgument(nameParts.size() == 1, \"Name must consist of one part: %s\", name);\n+\n+    return Identifier.of(namespace, nameParts.head());\n+  }\n+\n+  private Seq<String> parseMultipartIdentifier(String identifierAsString) {\n+    try {\n+      ParserInterface parser = spark.sessionState().sqlParser();\n+      return parser.parseMultipartIdentifier(identifierAsString);\n+    } catch (ParseException e) {\n+      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+    }\n+  }\n+\n+  protected SparkTable loadSparkTable(Identifier ident) {\n+    try {\n+      Table table = catalog.loadTable(ident);\n+      ValidationException.check(table instanceof SparkTable, \"%s is not %s\", ident, SparkTable.class.getName());\n+      return (SparkTable) table;\n+    } catch (NoSuchTableException e) {\n+      throw new RuntimeException(String.format(\"Couldn't load table '%s' in catalog '%s'\", ident, catalog.name()), e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "originalPosition": 90}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTE4NDQ2", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529518446", "createdAt": "2020-11-12T21:40:08Z", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTo0MDowOFrOHyPY8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTo0MDowOFrOHyPY8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0Mjk5NQ==", "bodyText": "Why not static?", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522442995", "createdAt": "2020-11-12T21:40:08Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import org.apache.iceberg.Snapshot;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class RollbackToSnapshotProcedure extends BaseProcedure {\n+\n+  private final ProcedureParameter[] parameters = new ProcedureParameter[]{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTE4NTcy", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529518572", "createdAt": "2020-11-12T21:40:21Z", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTo0MDoyMVrOHyPZYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTo0MDoyMVrOHyPZYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MzEwNQ==", "bodyText": "Same here. This could be static right?", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522443105", "createdAt": "2020-11-12T21:40:21Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import org.apache.iceberg.Snapshot;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class RollbackToSnapshotProcedure extends BaseProcedure {\n+\n+  private final ProcedureParameter[] parameters = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"namespace\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"snapshot_id\", DataTypes.LongType)\n+  };\n+  private final StructField[] outputFields = new StructField[]{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTE5MzY3", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529519367", "createdAt": "2020-11-12T21:41:39Z", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTo0MTozOVrOHyPb0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTo0MTozOVrOHyPb0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0MzczMA==", "bodyText": "What about \"previous_snapshot_id\"? I think that's clear enough. Or at least as clear as \"previous_current\".", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522443730", "createdAt": "2020-11-12T21:41:39Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import org.apache.iceberg.Snapshot;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class RollbackToSnapshotProcedure extends BaseProcedure {\n+\n+  private final ProcedureParameter[] parameters = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"namespace\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"snapshot_id\", DataTypes.LongType)\n+  };\n+  private final StructField[] outputFields = new StructField[]{\n+      new StructField(\"previous_current_snapshot_id\", DataTypes.LongType, false, Metadata.empty()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTIwNzY1", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529520765", "createdAt": "2020-11-12T21:43:59Z", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTo0Mzo1OVrOHyPgDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTo0Mzo1OVrOHyPgDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0NDgxNA==", "bodyText": "It may be cleaner to use new StructType().addField(...).addField(...).", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522444814", "createdAt": "2020-11-12T21:43:59Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import org.apache.iceberg.Snapshot;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class RollbackToSnapshotProcedure extends BaseProcedure {\n+\n+  private final ProcedureParameter[] parameters = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"namespace\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"snapshot_id\", DataTypes.LongType)\n+  };\n+  private final StructField[] outputFields = new StructField[]{\n+      new StructField(\"previous_current_snapshot_id\", DataTypes.LongType, false, Metadata.empty()),\n+      new StructField(\"current_snapshot_id\", DataTypes.LongType, false, Metadata.empty())\n+  };\n+  private final StructType outputType = new StructType(outputFields);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTIyMjg3", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529522287", "createdAt": "2020-11-12T21:46:25Z", "commit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTo0NjoyNVrOHyPkiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMTo0NjoyNVrOHyPkiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ0NTk2MA==", "bodyText": "It seems strange that the builder passes catalog to build, rather than withTableCatalog.", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522445960", "createdAt": "2020-11-12T21:46:25Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SparkProcedure.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n+\n+public enum SparkProcedure {\n+  ROLLBACK_TO_SNAPSHOT(RollbackToSnapshotProcedure::new);\n+\n+  private final ProcedureBuilder builder;\n+\n+  SparkProcedure(ProcedureBuilder builder) {\n+    this.builder = builder;\n+  }\n+\n+  public Procedure build(TableCatalog catalog) {\n+    return builder.build(catalog);\n+  }\n+\n+  private interface ProcedureBuilder {\n+    Procedure build(TableCatalog catalog);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2eca4484d4def23719c8fa90ddadaab3f8b69212"}, "originalPosition": 39}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2b73b16c56b4e7432d33d58468e6f7657a137298", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/2b73b16c56b4e7432d33d58468e6f7657a137298", "committedDate": "2020-11-12T22:09:00Z", "message": "Some fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "cf1e33f69f6b57bd2fd3b5e4f8befd74ec135c20", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/cf1e33f69f6b57bd2fd3b5e4f8befd74ec135c20", "committedDate": "2020-11-12T22:10:14Z", "message": "Remove extra line"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTQzOTI3", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529543927", "createdAt": "2020-11-12T22:16:00Z", "commit": {"oid": "cf1e33f69f6b57bd2fd3b5e4f8befd74ec135c20"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMjoxNjowMFrOHyQnHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMjoxNjowMFrOHyQnHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2MzAwNQ==", "bodyText": "Should be PARAMETERS for checkstyle. Same with OUTPUT_TYPE.", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522463005", "createdAt": "2020-11-12T22:16:00Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import org.apache.iceberg.Snapshot;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class RollbackToSnapshotProcedure extends BaseProcedure {\n+\n+  private static final ProcedureParameter[] parameters = new ProcedureParameter[]{", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf1e33f69f6b57bd2fd3b5e4f8befd74ec135c20"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTI5NTQ1MDQ2", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-529545046", "createdAt": "2020-11-12T22:17:54Z", "commit": {"oid": "cf1e33f69f6b57bd2fd3b5e4f8befd74ec135c20"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMjoxNzo1NVrOHyQtjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMlQyMjoxNzo1NVrOHyQtjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjQ2NDY1Mg==", "bodyText": "Just to make sure: this works without an explicit catalog right? I should be able to CALL system.rollback_to_snapshot(...) and it uses the current catalog?", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r522464652", "createdAt": "2020-11-12T22:17:55Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Test;\n+\n+public class TestRollbackToSnapshotProcedure extends SparkExtensionsTestBase {\n+\n+  public TestRollbackToSnapshotProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotUsingPositionalArgs() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot('%s', '%s', %dL)\",\n+        catalogName, tableIdent.namespace(), tableIdent.name(), firstSnapshot.snapshotId());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"Rollback must be successful\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotUsingNamedArgs() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot(snapshot_id => %dL, namespace => '%s', table => '%s')\",\n+        catalogName, firstSnapshot.snapshotId(), tableIdent.namespace(), tableIdent.name());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"Rollback must be successful\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotRefreshesRelationCache() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    Dataset<Row> query = spark.sql(\"SELECT * FROM \" + tableName + \" WHERE id = 1\");\n+    query.createOrReplaceTempView(\"tmp\");\n+\n+    spark.sql(\"CACHE TABLE tmp\");\n+\n+    assertEquals(\"View should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM tmp\"));\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot(namespace => '%s', table => '%s', snapshot_id => %dL)\",\n+        catalogName, tableIdent.namespace(), tableIdent.name(), firstSnapshot.snapshotId());\n+\n+    assertEquals(\"Procedure output must match\",\n+        ImmutableList.of(row(secondSnapshot.snapshotId(), firstSnapshot.snapshotId())),\n+        output);\n+\n+    assertEquals(\"View cache must be invalidated\",\n+        ImmutableList.of(row(1L, \"a\")),\n+        sql(\"SELECT * FROM tmp\"));\n+\n+    sql(\"UNCACHE TABLE tmp\");\n+  }\n+\n+  @Test\n+  public void testRollbackToSnapshotWithQuotedIdentifiers() {\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot firstSnapshot = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+\n+    table.refresh();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+\n+    StringBuilder quotedNamespaceBuilder = new StringBuilder();\n+    for (String level : tableIdent.namespace().levels()) {\n+      quotedNamespaceBuilder.append(\"`\");\n+      quotedNamespaceBuilder.append(level);\n+      quotedNamespaceBuilder.append(\"`\");\n+    }\n+    String quotedNamespace = quotedNamespaceBuilder.toString();\n+\n+    List<Object[]> output = sql(\n+        \"CALL %s.system.rollback_to_snapshot('%s', '`%s`', %d)\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cf1e33f69f6b57bd2fd3b5e4f8befd74ec135c20"}, "originalPosition": 175}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "027076348afcae569967b3f4b5923e36d1f01fa4", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/027076348afcae569967b3f4b5923e36d1f01fa4", "committedDate": "2020-11-13T07:00:23Z", "message": "Minor updates"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMjAzMTMw", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-530203130", "createdAt": "2020-11-13T16:22:30Z", "commit": {"oid": "027076348afcae569967b3f4b5923e36d1f01fa4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxNjoyMjozMFrOHy066g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxNjoyMjozMFrOHy066g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzA1Nzg5OA==", "bodyText": "We may need to reason about concurrent operations on this table instance if caching is enabled. Here, there is a possibility there will be an operation in between we got previousSnapshot and before we committed so the output may not be precise. It may be even more important in other procedures later.\n@rdblue, thoughts?", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r523057898", "createdAt": "2020-11-13T16:22:30Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/RollbackToSnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.spark.procedures.SparkProcedures.ProcedureBuilder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * A procedure that rollbacks a table to a specific snapshot id.\n+ */\n+class RollbackToSnapshotProcedure extends BaseProcedure {\n+\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"namespace\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"snapshot_id\", DataTypes.LongType)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"previous_snapshot_id\", DataTypes.LongType, false, Metadata.empty()),\n+      new StructField(\"current_snapshot_id\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  public static ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<RollbackToSnapshotProcedure>() {\n+      @Override\n+      public RollbackToSnapshotProcedure doBuild() {\n+        return new RollbackToSnapshotProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  private RollbackToSnapshotProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String namespace = args.getString(0);\n+    String tableName = args.getString(1);\n+    long snapshotId = args.getLong(2);\n+\n+    return modifyIcebergTable(namespace, tableName, table -> {\n+      Snapshot previousSnapshot = table.currentSnapshot();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "027076348afcae569967b3f4b5923e36d1f01fa4"}, "originalPosition": 79}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwMzI3Nzg2", "url": "https://github.com/apache/iceberg/pull/1759#pullrequestreview-530327786", "createdAt": "2020-11-13T18:58:31Z", "commit": {"oid": "027076348afcae569967b3f4b5923e36d1f01fa4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxODo1ODozMVrOHy7TPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xM1QxODo1ODozMVrOHy7TPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE2MjQyOA==", "bodyText": "Why not make build abstract?", "url": "https://github.com/apache/iceberg/pull/1759#discussion_r523162428", "createdAt": "2020-11-13T18:58:31Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.function.Function;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.procedures.SparkProcedures.ProcedureBuilder;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.Table;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n+import org.apache.spark.sql.execution.CacheManager;\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation;\n+import scala.Option;\n+import scala.collection.Seq;\n+\n+abstract class BaseProcedure implements Procedure {\n+  private final SparkSession spark;\n+  private final TableCatalog tableCatalog;\n+\n+  protected BaseProcedure(TableCatalog tableCatalog) {\n+    this.spark = SparkSession.active();\n+    this.tableCatalog = tableCatalog;\n+  }\n+\n+  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n+    Preconditions.checkArgument(namespace != null && !namespace.isEmpty(), \"Namespace cannot be empty\");\n+    Preconditions.checkArgument(tableName != null && !tableName.isEmpty(), \"Table name cannot be empty\");\n+\n+    Identifier ident = toIdentifier(namespace, tableName);\n+    SparkTable sparkTable = loadSparkTable(ident);\n+    org.apache.iceberg.Table icebergTable = sparkTable.table();\n+\n+    T result = func.apply(icebergTable);\n+\n+    refreshSparkCache(ident, sparkTable);\n+\n+    return result;\n+  }\n+\n+  // we have to parse both namespace and name as they may be quoted\n+  protected Identifier toIdentifier(String namespaceAsString, String name) {\n+    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n+\n+    String[] nameParts = parseMultipartIdentifier(name);\n+    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n+\n+    return Identifier.of(namespaceParts, nameParts[0]);\n+  }\n+\n+  private String[] parseMultipartIdentifier(String identifierAsString) {\n+    try {\n+      ParserInterface parser = spark.sessionState().sqlParser();\n+      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n+      String[] nameParts = new String[namePartsSeq.size()];\n+      namePartsSeq.copyToArray(nameParts);\n+      return nameParts;\n+    } catch (ParseException e) {\n+      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+    }\n+  }\n+\n+  protected SparkTable loadSparkTable(Identifier ident) {\n+    try {\n+      Table table = tableCatalog.loadTable(ident);\n+      ValidationException.check(table instanceof SparkTable, \"%s is not %s\", ident, SparkTable.class.getName());\n+      return (SparkTable) table;\n+    } catch (NoSuchTableException e) {\n+      String errMsg = String.format(\"Couldn't load table '%s' in catalog '%s'\", ident, tableCatalog.name());\n+      throw new RuntimeException(errMsg, e);\n+    }\n+  }\n+\n+  protected void refreshSparkCache(Identifier ident, Table table) {\n+    CacheManager cacheManager = spark.sharedState().cacheManager();\n+    DataSourceV2Relation relation = DataSourceV2Relation.create(table, Option.apply(tableCatalog), Option.apply(ident));\n+    cacheManager.recacheByPlan(spark, relation);\n+  }\n+\n+  protected abstract static class Builder<T extends BaseProcedure> implements ProcedureBuilder {\n+    private TableCatalog tableCatalog;\n+\n+    @Override\n+    public Builder<T> withTableCatalog(TableCatalog newTableCatalog) {\n+      this.tableCatalog = newTableCatalog;\n+      return this;\n+    }\n+\n+    @Override\n+    public T build() {\n+      return doBuild();\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "027076348afcae569967b3f4b5923e36d1f01fa4"}, "originalPosition": 115}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3743, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}