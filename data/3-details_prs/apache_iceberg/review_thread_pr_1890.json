{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM0NTk4OTg2", "number": 1890, "reviewThreads": {"totalCount": 28, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzoxNjoyMlrOFCe7XA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxMzo0NjoxNVrOFC7qaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MTQ4MTg4OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzoxNjoyMlrOIBq2RA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzoyNzoyM1rOIBrj5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMTUwOA==", "bodyText": "Last time we talked, we thought about defaulting the catalog to the procedure catalog, not the default catalog.\nI thought we would simply take the first parsed name part and check if spark.sql.catalog._name_part_ is set. If yes, it means our identifier contains a catalog and we don't have to prepend the procedure catalog to name parts and should resolve the catalog and validate the resolved catalog matches tableCatalog. If there is no such catalog, we can just call tableCatalog.load with the constructed identifier.", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538621508", "createdAt": "2020-12-08T17:16:22Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMTc5NA==", "bodyText": "@RussellSpitzer @rdblue, thoughts?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538621794", "createdAt": "2020-12-08T17:16:37Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMTUwOA=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyNDQxMw==", "bodyText": "I did this based on some of the discussion me and Ryan were having\nWe know we have a catalog if there is a registered catalog with the name\n\n\n\n\n\n11:45\nIn Spark, the rules are:\n1. If the identifier is a single part, it is a catalog name. Fill in current catalog and current namespace\n2. If the first part of the identifier is a known catalog, it is a fully qualified name\n3. If the first part is not a known catalog, fill in the current catalog (not namespace because it is already there)\n11:46\nThe only difference here would be that we consider the current catalog to be the procedure catalog because this is in the procedure's context\n\nI thought this was reasonable but we could force the catalog as well", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538624413", "createdAt": "2020-12-08T17:19:07Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMTUwOA=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyNjIxNQ==", "bodyText": "Isn't that what Spark3Util.catalogAndIdentifier does? I think the only difference is that it uses Spark's catalogManager instead of simply checking table properties.", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538626215", "createdAt": "2020-12-08T17:20:49Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMTUwOA=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyNjU2Mg==", "bodyText": "I think I missed an update here.", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538626562", "createdAt": "2020-12-08T17:21:12Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMTUwOA=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzMzE4OQ==", "bodyText": "I added an argument to catalogAndIdentifier, the last parameter is the \"current catalog\" that's used", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538633189", "createdAt": "2020-12-08T17:27:23Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMTUwOA=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MTQ4NTQxOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": false, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzoxNjo1NFrOIBq4gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMjowNTo0NFrOIB4VqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMjA4Mw==", "bodyText": "We should validate the resolved catalog is tableCatalog to make sure we don't modify tables in other catalogs.", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538622083", "createdAt": "2020-12-08T17:16:54Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n     }\n+\n+    return catalogAndIdentifier.identifier();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyNTgwNA==", "bodyText": "I didn't want that to be a general requirement, But I could add it here. Ie if we want other procedures which operate on tables in other catalogs", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538625804", "createdAt": "2020-12-08T17:20:23Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n     }\n+\n+    return catalogAndIdentifier.identifier();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMjA4Mw=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzNzIzMw==", "bodyText": "Well, maybe the validation should not happen here. We could call it toCatalogAndIdentifier and validate above.", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538637233", "createdAt": "2020-12-08T17:30:56Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n     }\n+\n+    return catalogAndIdentifier.identifier();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMjA4Mw=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY4Nzk4OA==", "bodyText": "That's a good idea", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538687988", "createdAt": "2020-12-08T18:19:16Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n     }\n+\n+    return catalogAndIdentifier.identifier();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMjA4Mw=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2OTA0Mg==", "bodyText": "Looks like this validation is still here?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538769042", "createdAt": "2020-12-08T20:04:56Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n     }\n+\n+    return catalogAndIdentifier.identifier();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMjA4Mw=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwMjU3OQ==", "bodyText": "Ah yes I got very confused. I think we should just let this be for now and remove the check. I think instead \"loadSparkTable\" can have the check since it assumes that we are loading out of this catalog.", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538802579", "createdAt": "2020-12-08T20:58:06Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n     }\n+\n+    return catalogAndIdentifier.identifier();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMjA4Mw=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwNDUxMw==", "bodyText": "Sorry I flipped back again, i'm just going to not put the condition here. We'll get an error in \"loadSparkTable\" which won't have the type argument but should be clear enough?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538804513", "createdAt": "2020-12-08T21:01:24Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n     }\n+\n+    return catalogAndIdentifier.identifier();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMjA4Mw=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg0MjUzNg==", "bodyText": "The check definitely needs to happen somewhere, and the error should be specific: \"Cannot run stored procedure %s.%s: %s is not in catalog %s\", catalogName, procName, tableName, catalogName.\nI don't see a way for loadSparkTable to catch that the original identifier was actually for a different catalog because the catalog for this identifier is discarded.", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538842536", "createdAt": "2020-12-08T22:05:44Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n     }\n+\n+    return catalogAndIdentifier.identifier();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyMjA4Mw=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MTUwNzM5OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/ExpireSnapshotsProcedure.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzoyMDoyM1rOIBrHBg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxODoyOTowNFrOIBviJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyNTc5OA==", "bodyText": "I think we better call our variables tableIdent rather than tableName now. I think we can keep the parameter name as table.", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538625798", "createdAt": "2020-12-08T17:20:23Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/ExpireSnapshotsProcedure.java", "diffHunk": "@@ -77,12 +76,11 @@ public StructType outputType() {\n \n   @Override\n   public InternalRow[] call(InternalRow args) {\n-    String namespace = args.getString(0);\n-    String tableName = args.getString(1);\n-    Long olderThanMillis = args.isNullAt(2) ? null : DateTimeUtils.toMillis(args.getLong(2));\n-    Integer retainLastNum = args.isNullAt(3) ? null : args.getInt(3);\n+    String tableName = args.getString(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY5ODI3OA==", "bodyText": "sounds good, renames all around", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538698278", "createdAt": "2020-12-08T18:29:04Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/ExpireSnapshotsProcedure.java", "diffHunk": "@@ -77,12 +76,11 @@ public StructType outputType() {\n \n   @Override\n   public InternalRow[] call(InternalRow args) {\n-    String namespace = args.getString(0);\n-    String tableName = args.getString(1);\n-    Long olderThanMillis = args.isNullAt(2) ? null : DateTimeUtils.toMillis(args.getLong(2));\n-    Integer retainLastNum = args.isNullAt(3) ? null : args.getInt(3);\n+    String tableName = args.getString(0);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyNTc5OA=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MTUyMjMyOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzoyMjo0NFrOIBrQ2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxODoyOToxNVrOIBvi4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyODMxNA==", "bodyText": "nit: shall we call it identifierAsString to be consistent?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538628314", "createdAt": "2020-12-08T17:22:44Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -48,20 +46,18 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     this.tableCatalog = tableCatalog;\n   }\n \n-  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(namespace, tableName, true, func);\n+  protected <T> T modifyIcebergTable(String identifier, Function<org.apache.iceberg.Table, T> func) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY5ODQ2NA==", "bodyText": "changed", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538698464", "createdAt": "2020-12-08T18:29:15Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -48,20 +46,18 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     this.tableCatalog = tableCatalog;\n   }\n \n-  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(namespace, tableName, true, func);\n+  protected <T> T modifyIcebergTable(String identifier, Function<org.apache.iceberg.Table, T> func) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyODMxNA=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MTUyNzU1OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzoyMzozNFrOIBrUPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzoyOTowMVrOIBrqiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyOTE4MQ==", "bodyText": "Shall we do an import so that we can refer to CatalogAndIdentifier directly?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538629181", "createdAt": "2020-12-08T17:23:34Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzNDg5MQ==", "bodyText": "Can do", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538634891", "createdAt": "2020-12-08T17:29:01Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyOTE4MQ=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MTUyOTg2OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzoyMzo1OVrOIBrVsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzoyMzo1OVrOIBrVsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYyOTU1NQ==", "bodyText": "nit: identifier -> identifierAsString?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538629555", "createdAt": "2020-12-08T17:23:59Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,25 +71,15 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  protected Identifier toIdentifier(String identifier) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MTUzNDcwOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzoyNDo0MlrOIBrYtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzoyOTo1MVrOIBru1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzMDMyNQ==", "bodyText": "nit: formatting is off", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538630325", "createdAt": "2020-12-08T17:24:42Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -605,31 +605,41 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark, name, spark.sessionState().catalogManager().currentCatalog());\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n+                                                            CatalogPlugin fallBackCatalog) throws ParseException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzNTk5MQ==", "bodyText": "I wish I could teach IDEA this, every time I run reformat it swaps it back", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538635991", "createdAt": "2020-12-08T17:29:51Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -605,31 +605,41 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark, name, spark.sessionState().catalogManager().currentCatalog());\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n+                                                            CatalogPlugin fallBackCatalog) throws ParseException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzMDMyNQ=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MTU0MDE5OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzoyNTozMFrOIBrcQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzoyOToyN1rOIBrs0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzMTIzNA==", "bodyText": "Shall we call it defaultCatalog?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538631234", "createdAt": "2020-12-08T17:25:30Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -605,31 +605,41 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark, name, spark.sessionState().catalogManager().currentCatalog());\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n+                                                            CatalogPlugin fallBackCatalog) throws ParseException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzNTQ3NQ==", "bodyText": "I think that's a fine name as well", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538635475", "createdAt": "2020-12-08T17:29:27Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -605,31 +605,41 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark, name, spark.sessionState().catalogManager().currentCatalog());\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n+                                                            CatalogPlugin fallBackCatalog) throws ParseException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODYzMTIzNA=="}, "originalCommit": {"oid": "2d7ab929ffc3f7887a3726aa79bc3766f12852f3"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MjM2NTE5OnYy", "diffSide": "LEFT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxOTo1NTowMFrOIBzedQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMToyODoyMlrOIB2_GA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Mjg2OQ==", "bodyText": "Should this have a test for empty table name?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538762869", "createdAt": "2020-12-08T19:55:00Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java", "diffHunk": "@@ -171,18 +167,10 @@ public void testInvalidCherrypickSnapshotCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.cherrypick_snapshot('n', 't')\", catalogName));\n+        () -> sql(\"CALL %s.system.cherrypick_snapshot('t')\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n         AnalysisException.class, \"Wrong arg type for snapshot_id: cannot cast\",\n-        () -> sql(\"CALL %s.system.cherrypick_snapshot('n', 't', 2.2)\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject empty namespace\",\n-        IllegalArgumentException.class, \"Namespace cannot be empty\",\n-        () -> sql(\"CALL %s.system.cherrypick_snapshot('', 't', 1L)\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject empty table name\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc4MzY1Ng==", "bodyText": "+1", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538783656", "createdAt": "2020-12-08T20:28:52Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java", "diffHunk": "@@ -171,18 +167,10 @@ public void testInvalidCherrypickSnapshotCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.cherrypick_snapshot('n', 't')\", catalogName));\n+        () -> sql(\"CALL %s.system.cherrypick_snapshot('t')\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n         AnalysisException.class, \"Wrong arg type for snapshot_id: cannot cast\",\n-        () -> sql(\"CALL %s.system.cherrypick_snapshot('n', 't', 2.2)\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject empty namespace\",\n-        IllegalArgumentException.class, \"Namespace cannot be empty\",\n-        () -> sql(\"CALL %s.system.cherrypick_snapshot('', 't', 1L)\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject empty table name\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Mjg2OQ=="}, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgyMDM3Ng==", "bodyText": "added", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538820376", "createdAt": "2020-12-08T21:28:22Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java", "diffHunk": "@@ -171,18 +167,10 @@ public void testInvalidCherrypickSnapshotCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.cherrypick_snapshot('n', 't')\", catalogName));\n+        () -> sql(\"CALL %s.system.cherrypick_snapshot('t')\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n         AnalysisException.class, \"Wrong arg type for snapshot_id: cannot cast\",\n-        () -> sql(\"CALL %s.system.cherrypick_snapshot('n', 't', 2.2)\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject empty namespace\",\n-        IllegalArgumentException.class, \"Namespace cannot be empty\",\n-        () -> sql(\"CALL %s.system.cherrypick_snapshot('', 't', 1L)\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject empty table name\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Mjg2OQ=="}, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MjM3MzQ0OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxOTo1Njo1MFrOIBzjGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMTowNTowMFrOIB2JtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2NDA1OQ==", "bodyText": "Nit: whitespace change?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538764059", "createdAt": "2020-12-08T19:56:50Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java", "diffHunk": "@@ -184,8 +184,8 @@ public void testRollbackToTimestampWithQuotedIdentifiers() {\n     String quotedNamespace = quotedNamespaceBuilder.toString();\n \n     List<Object[]> output = sql(\n-        \"CALL %s.system.rollback_to_timestamp('%s', '`%s`', TIMESTAMP '%s')\",\n-        catalogName, quotedNamespace, tableIdent.name(), firstSnapshotTimestamp);\n+        \"CALL %s.system.rollback_to_timestamp('%s', TIMESTAMP '%s')\",\n+            catalogName, quotedNamespace + \".`\" + tableIdent.name() + \"`\", firstSnapshotTimestamp);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwNjcwOQ==", "bodyText": "got it, bad copy from the other rollback method", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538806709", "createdAt": "2020-12-08T21:05:00Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java", "diffHunk": "@@ -184,8 +184,8 @@ public void testRollbackToTimestampWithQuotedIdentifiers() {\n     String quotedNamespace = quotedNamespaceBuilder.toString();\n \n     List<Object[]> output = sql(\n-        \"CALL %s.system.rollback_to_timestamp('%s', '`%s`', TIMESTAMP '%s')\",\n-        catalogName, quotedNamespace, tableIdent.name(), firstSnapshotTimestamp);\n+        \"CALL %s.system.rollback_to_timestamp('%s', TIMESTAMP '%s')\",\n+            catalogName, quotedNamespace + \".`\" + tableIdent.name() + \"`\", firstSnapshotTimestamp);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2NDA1OQ=="}, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MjM5MTcxOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "isResolved": true, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMDowMTowOVrOIBztrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwOTozMToyOFrOICK8vA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Njc2NA==", "bodyText": "This namespace is associated with a catalog. I don't think this should use the catalog passed in with this namespace unless that catalog is the current catalog. I would do the following:\n\nIf the fallback catalog is the current catalog, use the current namespace\nIf the fallback catalog is not the current catalog, use its default namespace (catalog.defaultNamespace())\n\nThe catalog's default namespace is used when you switch to that catalog. The default namespace becomes the current namespace. Using the default fits with the idea that the fallback catalog is the current catalog for the context of the stored procedure.\nIt this is difficult to implement, then we can always go back to using the current catalog rather than the procedure catalog.", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538766764", "createdAt": "2020-12-08T20:01:09Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -605,31 +605,41 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark, name, spark.sessionState().catalogManager().currentCatalog());\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n+                                                          CatalogPlugin defaultCatalog) throws ParseException {\n     ParserInterface parser = spark.sessionState().sqlParser();\n     Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n     List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n-    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier, defaultCatalog);\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    return catalogAndIdentifier(spark, nameParts, spark.sessionState().catalogManager().currentCatalog());\n   }\n \n   /**\n    * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n    * Attempts to find the catalog and identifier a multipart identifier represents\n    * @param spark Spark session to use for resolution\n    * @param nameParts Multipart identifier representing a table\n+   * @param fallBackCatalog Catalog to use if none is specified\n    * @return The CatalogPlugin and Identifier for the table\n    */\n-  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts,\n+                                                          CatalogPlugin fallBackCatalog) {\n     Preconditions.checkArgument(!nameParts.isEmpty(),\n         \"Cannot determine catalog and Identifier from empty name parts\");\n     CatalogManager catalogManager = spark.sessionState().catalogManager();\n-    CatalogPlugin currentCatalog = catalogManager.currentCatalog();\n     String[] currentNamespace = catalogManager.currentNamespace();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2NzM1Mg==", "bodyText": "Yeah I was wondering a bit about this, I think that's a reasonable solve", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538767352", "createdAt": "2020-12-08T20:02:02Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -605,31 +605,41 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark, name, spark.sessionState().catalogManager().currentCatalog());\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n+                                                          CatalogPlugin defaultCatalog) throws ParseException {\n     ParserInterface parser = spark.sessionState().sqlParser();\n     Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n     List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n-    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier, defaultCatalog);\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    return catalogAndIdentifier(spark, nameParts, spark.sessionState().catalogManager().currentCatalog());\n   }\n \n   /**\n    * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n    * Attempts to find the catalog and identifier a multipart identifier represents\n    * @param spark Spark session to use for resolution\n    * @param nameParts Multipart identifier representing a table\n+   * @param fallBackCatalog Catalog to use if none is specified\n    * @return The CatalogPlugin and Identifier for the table\n    */\n-  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts,\n+                                                          CatalogPlugin fallBackCatalog) {\n     Preconditions.checkArgument(!nameParts.isEmpty(),\n         \"Cannot determine catalog and Identifier from empty name parts\");\n     CatalogManager catalogManager = spark.sessionState().catalogManager();\n-    CatalogPlugin currentCatalog = catalogManager.currentCatalog();\n     String[] currentNamespace = catalogManager.currentNamespace();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Njc2NA=="}, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc3ODc0MQ==", "bodyText": "Does this mean we can always call \"defaultCatalog.defaultNamespace\"? What is the difference between catalogManager.currentNamespace and the default catalog's implementation?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538778741", "createdAt": "2020-12-08T20:20:44Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -605,31 +605,41 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark, name, spark.sessionState().catalogManager().currentCatalog());\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n+                                                          CatalogPlugin defaultCatalog) throws ParseException {\n     ParserInterface parser = spark.sessionState().sqlParser();\n     Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n     List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n-    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier, defaultCatalog);\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    return catalogAndIdentifier(spark, nameParts, spark.sessionState().catalogManager().currentCatalog());\n   }\n \n   /**\n    * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n    * Attempts to find the catalog and identifier a multipart identifier represents\n    * @param spark Spark session to use for resolution\n    * @param nameParts Multipart identifier representing a table\n+   * @param fallBackCatalog Catalog to use if none is specified\n    * @return The CatalogPlugin and Identifier for the table\n    */\n-  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts,\n+                                                          CatalogPlugin fallBackCatalog) {\n     Preconditions.checkArgument(!nameParts.isEmpty(),\n         \"Cannot determine catalog and Identifier from empty name parts\");\n     CatalogManager catalogManager = spark.sessionState().catalogManager();\n-    CatalogPlugin currentCatalog = catalogManager.currentCatalog();\n     String[] currentNamespace = catalogManager.currentNamespace();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Njc2NA=="}, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc4MjE5MQ==", "bodyText": "def currentNamespace: Array[String] = synchronized {\n    _currentNamespace.getOrElse {\n      if (currentCatalog.name() == SESSION_CATALOG_NAME) {\n        Array(v1SessionCatalog.getCurrentDatabase)\n      } else {\n        currentCatalog.defaultNamespace()\n      }\n    }\n  }\nV1 always making things more difficult\nOh also _currentNamespace. ok np", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538782191", "createdAt": "2020-12-08T20:26:21Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -605,31 +605,41 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark, name, spark.sessionState().catalogManager().currentCatalog());\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n+                                                          CatalogPlugin defaultCatalog) throws ParseException {\n     ParserInterface parser = spark.sessionState().sqlParser();\n     Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n     List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n-    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier, defaultCatalog);\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    return catalogAndIdentifier(spark, nameParts, spark.sessionState().catalogManager().currentCatalog());\n   }\n \n   /**\n    * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n    * Attempts to find the catalog and identifier a multipart identifier represents\n    * @param spark Spark session to use for resolution\n    * @param nameParts Multipart identifier representing a table\n+   * @param fallBackCatalog Catalog to use if none is specified\n    * @return The CatalogPlugin and Identifier for the table\n    */\n-  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts,\n+                                                          CatalogPlugin fallBackCatalog) {\n     Preconditions.checkArgument(!nameParts.isEmpty(),\n         \"Cannot determine catalog and Identifier from empty name parts\");\n     CatalogManager catalogManager = spark.sessionState().catalogManager();\n-    CatalogPlugin currentCatalog = catalogManager.currentCatalog();\n     String[] currentNamespace = catalogManager.currentNamespace();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Njc2NA=="}, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc5MjU2MQ==", "bodyText": "The current namespace is whatever you've set with USE. The default namespace is the default when you change catalogs without specifying a namespace (USE CATALOG cat).", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538792561", "createdAt": "2020-12-08T20:44:19Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -605,31 +605,41 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark, name, spark.sessionState().catalogManager().currentCatalog());\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n+                                                          CatalogPlugin defaultCatalog) throws ParseException {\n     ParserInterface parser = spark.sessionState().sqlParser();\n     Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n     List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n-    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier, defaultCatalog);\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    return catalogAndIdentifier(spark, nameParts, spark.sessionState().catalogManager().currentCatalog());\n   }\n \n   /**\n    * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n    * Attempts to find the catalog and identifier a multipart identifier represents\n    * @param spark Spark session to use for resolution\n    * @param nameParts Multipart identifier representing a table\n+   * @param fallBackCatalog Catalog to use if none is specified\n    * @return The CatalogPlugin and Identifier for the table\n    */\n-  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts,\n+                                                          CatalogPlugin fallBackCatalog) {\n     Preconditions.checkArgument(!nameParts.isEmpty(),\n         \"Cannot determine catalog and Identifier from empty name parts\");\n     CatalogManager catalogManager = spark.sessionState().catalogManager();\n-    CatalogPlugin currentCatalog = catalogManager.currentCatalog();\n     String[] currentNamespace = catalogManager.currentNamespace();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Njc2NA=="}, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgxNjc4Mg==", "bodyText": "Ugh :(", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538816782", "createdAt": "2020-12-08T21:22:19Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -605,31 +605,41 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark, name, spark.sessionState().catalogManager().currentCatalog());\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n+                                                          CatalogPlugin defaultCatalog) throws ParseException {\n     ParserInterface parser = spark.sessionState().sqlParser();\n     Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n     List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n-    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier, defaultCatalog);\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    return catalogAndIdentifier(spark, nameParts, spark.sessionState().catalogManager().currentCatalog());\n   }\n \n   /**\n    * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n    * Attempts to find the catalog and identifier a multipart identifier represents\n    * @param spark Spark session to use for resolution\n    * @param nameParts Multipart identifier representing a table\n+   * @param fallBackCatalog Catalog to use if none is specified\n    * @return The CatalogPlugin and Identifier for the table\n    */\n-  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts,\n+                                                          CatalogPlugin fallBackCatalog) {\n     Preconditions.checkArgument(!nameParts.isEmpty(),\n         \"Cannot determine catalog and Identifier from empty name parts\");\n     CatalogManager catalogManager = spark.sessionState().catalogManager();\n-    CatalogPlugin currentCatalog = catalogManager.currentCatalog();\n     String[] currentNamespace = catalogManager.currentNamespace();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Njc2NA=="}, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTE0NzQ1Mg==", "bodyText": "The current logic seems to be as discussed here so I am resolving this thread.", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539147452", "createdAt": "2020-12-09T09:31:28Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -605,31 +605,41 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark, name, spark.sessionState().catalogManager().currentCatalog());\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n+                                                          CatalogPlugin defaultCatalog) throws ParseException {\n     ParserInterface parser = spark.sessionState().sqlParser();\n     Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n     List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n-    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier, defaultCatalog);\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    return catalogAndIdentifier(spark, nameParts, spark.sessionState().catalogManager().currentCatalog());\n   }\n \n   /**\n    * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n    * Attempts to find the catalog and identifier a multipart identifier represents\n    * @param spark Spark session to use for resolution\n    * @param nameParts Multipart identifier representing a table\n+   * @param fallBackCatalog Catalog to use if none is specified\n    * @return The CatalogPlugin and Identifier for the table\n    */\n-  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts,\n+                                                          CatalogPlugin fallBackCatalog) {\n     Preconditions.checkArgument(!nameParts.isEmpty(),\n         \"Cannot determine catalog and Identifier from empty name parts\");\n     CatalogManager catalogManager = spark.sessionState().catalogManager();\n-    CatalogPlugin currentCatalog = catalogManager.currentCatalog();\n     String[] currentNamespace = catalogManager.currentNamespace();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Njc2NA=="}, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MjQwMDA5OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMDowMzowNVrOIBzyag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMDo1MjoyMlrOIB1kyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Nzk3OA==", "bodyText": "It would be nice to follow this recommendation from the other review: https://github.com/apache/iceberg/pull/1525/files#r538758721\nThe description could be the argument name.", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538767978", "createdAt": "2020-12-08T20:03:05Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -74,26 +71,20 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     return result;\n   }\n \n-  // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  private Identifier toIdentifier(String identifierAsString) {\n+    CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc5MDIyOA==", "bodyText": "This will require a bit more refactoring since our callers are usually via modifyIcebergTable/withIcebergTable", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538790228", "createdAt": "2020-12-08T20:40:22Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -74,26 +71,20 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     return result;\n   }\n \n-  // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  private Identifier toIdentifier(String identifierAsString) {\n+    CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Nzk3OA=="}, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc5NzI1OQ==", "bodyText": "Moved the \"toIdentifier\" calls into the Procedures and now the modify/with functions take identifiers directly", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538797259", "createdAt": "2020-12-08T20:52:22Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -74,26 +71,20 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     return result;\n   }\n \n-  // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  private Identifier toIdentifier(String identifierAsString) {\n+    CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2Nzk3OA=="}, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MjQwMTA3OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMDowMzoxOVrOIBzzBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMDowMzoxOVrOIBzzBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2ODEzMg==", "bodyText": "Nit: missing newline between control flow.", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538768132", "createdAt": "2020-12-08T20:03:19Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -74,26 +71,20 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     return result;\n   }\n \n-  // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  private Identifier toIdentifier(String identifierAsString) {\n+    CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n+    }\n+    if (!catalogAndIdentifier.catalog().equals(tableCatalog)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MjQwMTc4OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMDowMzozNFrOIBzzdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMDowMzozNFrOIBzzdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc2ODI0NA==", "bodyText": "Use a precondition?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538768244", "createdAt": "2020-12-08T20:03:34Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -74,26 +71,20 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     return result;\n   }\n \n-  // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n-\n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+  private Identifier toIdentifier(String identifierAsString) {\n+    CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(\"Cannot parse identifier\", e);\n+    }\n+    if (!catalogAndIdentifier.catalog().equals(tableCatalog)) {\n+      throw new IllegalArgumentException(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MjUzMjc4OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMDozNToyOVrOIB0-XQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMDozNToyOVrOIB0-XQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc4NzQyMQ==", "bodyText": "nit: I think we call it fallBackCatalog here and defaultCatalog above", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538787421", "createdAt": "2020-12-08T20:35:29Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -605,31 +605,41 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n   }\n \n   public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark, name, spark.sessionState().catalogManager().currentCatalog());\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name,\n+                                                          CatalogPlugin defaultCatalog) throws ParseException {\n     ParserInterface parser = spark.sessionState().sqlParser();\n     Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);\n     List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);\n-    return catalogAndIdentifier(spark, javaMultiPartIdentifier);\n+    return catalogAndIdentifier(spark, javaMultiPartIdentifier, defaultCatalog);\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    return catalogAndIdentifier(spark, nameParts, spark.sessionState().catalogManager().currentCatalog());\n   }\n \n   /**\n    * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n    * Attempts to find the catalog and identifier a multipart identifier represents\n    * @param spark Spark session to use for resolution\n    * @param nameParts Multipart identifier representing a table\n+   * @param fallBackCatalog Catalog to use if none is specified", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MjU0NTU0OnYy", "diffSide": "LEFT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMDozODo1NFrOIB1Fug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMTo0NDoxMlrOIB3kNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc4OTMwNg==", "bodyText": "Should we keep one precondition for identifierAsString?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538789306", "createdAt": "2020-12-08T20:38:54Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -48,20 +47,18 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     this.tableCatalog = tableCatalog;\n   }\n \n-  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(namespace, tableName, true, func);\n+  protected <T> T modifyIcebergTable(String identifierAsString, Function<org.apache.iceberg.Table, T> func) {\n+    return execute(identifierAsString, true, func);\n   }\n \n-  protected <T> T withIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(namespace, tableName, false, func);\n+  protected <T> T withIcebergTable(String identifierAsString, Function<org.apache.iceberg.Table, T> func) {\n+    return execute(identifierAsString, false, func);\n   }\n \n-  private <T> T execute(String namespace, String tableName, boolean refreshSparkCache,\n+  private <T> T execute(String identifierAsString, boolean refreshSparkCache,\n                         Function<org.apache.iceberg.Table, T> func) {\n-    Preconditions.checkArgument(namespace != null && !namespace.isEmpty(), \"Namespace cannot be empty\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgyOTg3OQ==", "bodyText": "There is a precondition on the null/empty issue in \"toIdentifier\"", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538829879", "createdAt": "2020-12-08T21:44:12Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -48,20 +47,18 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     this.tableCatalog = tableCatalog;\n   }\n \n-  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(namespace, tableName, true, func);\n+  protected <T> T modifyIcebergTable(String identifierAsString, Function<org.apache.iceberg.Table, T> func) {\n+    return execute(identifierAsString, true, func);\n   }\n \n-  protected <T> T withIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(namespace, tableName, false, func);\n+  protected <T> T withIcebergTable(String identifierAsString, Function<org.apache.iceberg.Table, T> func) {\n+    return execute(identifierAsString, false, func);\n   }\n \n-  private <T> T execute(String namespace, String tableName, boolean refreshSparkCache,\n+  private <T> T execute(String identifierAsString, boolean refreshSparkCache,\n                         Function<org.apache.iceberg.Table, T> func) {\n-    Preconditions.checkArgument(namespace != null && !namespace.isEmpty(), \"Namespace cannot be empty\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc4OTMwNg=="}, "originalCommit": {"oid": "8d650b1fb2599ee913a88e1a5d99008b13589a71"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MzIzNzcyOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMzozNTo1OVrOIB7TBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQyMzozNTo1OVrOIB7TBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5MTAxNQ==", "bodyText": "This is really wordy. I'd probably simplify it to \"Cannot run procedure %s in catalog %s: %s is a table in catalog %s\".", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r538891015", "createdAt": "2020-12-08T23:35:59Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -79,10 +79,14 @@ protected Identifier toIdentifier(String identifierAsString, String argName) {\n     try {\n       catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n-      throw new IllegalArgumentException(String.format(\"Cannot parse identifier [%s] for argument %s\",\n+      throw new IllegalArgumentException(String.format(\"Cannot parse identifier '%s' for argument %s\",\n           identifierAsString, argName), e);\n     }\n \n+    Preconditions.checkArgument(catalogAndIdentifier.catalog().equals(tableCatalog), \"Cannot run procedure\" +\n+        \" in catalog '%s': Argument %s was set to '%s' which resolves to a table in a different catalog '%s'\",\n+        tableCatalog.name(), argName, identifierAsString, catalogAndIdentifier.catalog().name());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "094987f1c263aa1bdeaca422260a2f9fc41cea7b"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NDU3NTI5OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwNzo0NTowOVrOICGqPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwNzo0NTowOVrOICGqPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA3NzE4Mw==", "bodyText": "nit: with?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539077183", "createdAt": "2020-12-09T07:45:09Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java", "diffHunk": "@@ -171,18 +167,14 @@ public void testInvalidCherrypickSnapshotCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.cherrypick_snapshot('n', 't')\", catalogName));\n+        () -> sql(\"CALL %s.system.cherrypick_snapshot('t')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "094987f1c263aa1bdeaca422260a2f9fc41cea7b"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NDU3NzUxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwNzo0NTo0N1rOICGrhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwNzo0NTo0N1rOICGrhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA3NzUxMQ==", "bodyText": "nit: with?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539077511", "createdAt": "2020-12-09T07:45:47Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java", "diffHunk": "@@ -163,18 +163,28 @@ public void testInvalidExpireSnapshotsCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.expire_snapshots('n')\", catalogName));\n+        () -> sql(\"CALL %s.system.expire_snapshots()\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n-        RuntimeException.class, \"Couldn't parse identifier\",\n+        AnalysisException.class, \"Wrong arg type\",\n         () -> sql(\"CALL %s.system.expire_snapshots('n', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject empty namespace\",\n-        IllegalArgumentException.class, \"Namespace cannot be empty\",\n-        () -> sql(\"CALL %s.system.expire_snapshots('', 't')\", catalogName));\n+    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "094987f1c263aa1bdeaca422260a2f9fc41cea7b"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NDU4MjExOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwNzo0NzowMlrOICGuCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwNzo0NzowMlrOICGuCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA3ODE1Mg==", "bodyText": "nit: we can probably use our sql() helper method to get parameterization for free.", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539078152", "createdAt": "2020-12-09T07:47:02Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java", "diffHunk": "@@ -163,18 +163,28 @@ public void testInvalidExpireSnapshotsCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.expire_snapshots('n')\", catalogName));\n+        () -> sql(\"CALL %s.system.expire_snapshots()\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n-        RuntimeException.class, \"Couldn't parse identifier\",\n+        AnalysisException.class, \"Wrong arg type\",\n         () -> sql(\"CALL %s.system.expire_snapshots('n', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject empty namespace\",\n-        IllegalArgumentException.class, \"Namespace cannot be empty\",\n-        () -> sql(\"CALL %s.system.expire_snapshots('', 't')\", catalogName));\n+    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",\n+        IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n+        () -> sql(\"CALL %s.system.expire_snapshots('')\", catalogName));\n+  }\n \n-    AssertHelpers.assertThrows(\"Should reject empty table name\",\n-        IllegalArgumentException.class, \"Table name cannot be empty\",\n-        () -> sql(\"CALL %s.system.expire_snapshots('n', '')\", catalogName));\n+  @Test\n+  public void testResolvingTableInAnotherCatalog() throws IOException {\n+    String anotherCatalog = \"another_\" + catalogName;\n+    spark.conf().set(\"spark.sql.catalog.\" + anotherCatalog, SparkCatalog.class.getName());\n+    spark.conf().set(\"spark.sql.catalog.\" + anotherCatalog + \".type\", \"hadoop\");\n+    spark.conf().set(\"spark.sql.catalog.\" + anotherCatalog + \".warehouse\", \"file:\" + temp.newFolder().toString());\n+    spark.sql(String.format(\"CREATE TABLE %s.%s (id bigint NOT NULL, data string) USING iceberg\", anotherCatalog,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "094987f1c263aa1bdeaca422260a2f9fc41cea7b"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NDU4NDcxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwNzo0Nzo0MFrOICGvgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwNzo0Nzo0MFrOICGvgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA3ODUyOQ==", "bodyText": "nit: with?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539078529", "createdAt": "2020-12-09T07:47:40Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java", "diffHunk": "@@ -230,18 +222,14 @@ public void testInvalidRemoveOrphanFilesCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.remove_orphan_files('n')\", catalogName));\n+        () -> sql(\"CALL %s.system.remove_orphan_files()\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n-        RuntimeException.class, \"Couldn't parse identifier\",\n+        AnalysisException.class, \"Wrong arg type\",\n         () -> sql(\"CALL %s.system.remove_orphan_files('n', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject empty namespace\",\n-        IllegalArgumentException.class, \"Namespace cannot be empty\",\n-        () -> sql(\"CALL %s.system.remove_orphan_files('', 't')\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject empty table name\",\n-        IllegalArgumentException.class, \"Table name cannot be empty\",\n-        () -> sql(\"CALL %s.system.remove_orphan_files('n', '')\", catalogName));\n+    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "094987f1c263aa1bdeaca422260a2f9fc41cea7b"}, "originalPosition": 116}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NDU4NTUxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwNzo0Nzo1NVrOICGv9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwNzo0Nzo1NVrOICGv9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA3ODY0NQ==", "bodyText": "nit: with?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539078645", "createdAt": "2020-12-09T07:47:55Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java", "diffHunk": "@@ -162,22 +157,18 @@ public void testInvalidRewriteManifestsCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.rewrite_manifests('n')\", catalogName));\n+        () -> sql(\"CALL %s.system.rewrite_manifests()\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n-        RuntimeException.class, \"Couldn't parse identifier\",\n+        AnalysisException.class, \"Wrong arg type\",\n         () -> sql(\"CALL %s.system.rewrite_manifests('n', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject empty namespace\",\n-        IllegalArgumentException.class, \"Namespace cannot be empty\",\n-        () -> sql(\"CALL %s.system.rewrite_manifests('', 't')\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject empty table name\",\n-        IllegalArgumentException.class, \"Table name cannot be empty\",\n-        () -> sql(\"CALL %s.system.rewrite_manifests('n', '')\", catalogName));\n-\n     AssertHelpers.assertThrows(\"Should reject duplicate arg names name\",\n         AnalysisException.class, \"Duplicate procedure argument: table\",\n         () -> sql(\"CALL %s.system.rewrite_manifests(table => 't', tAbLe => 't')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "094987f1c263aa1bdeaca422260a2f9fc41cea7b"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NDU4NjQxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwNzo0ODoxMlrOICGwgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwNzo0ODoxMlrOICGwgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA3ODc4NQ==", "bodyText": "nit: with?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539078785", "createdAt": "2020-12-09T07:48:12Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java", "diffHunk": "@@ -243,30 +239,22 @@ public void testInvalidRollbackToSnapshotCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.rollback_to_snapshot('n', 't')\", catalogName));\n+        () -> sql(\"CALL %s.system.rollback_to_snapshot('t')\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.rollback_to_snapshot('n', 1L)\", catalogName));\n+        () -> sql(\"CALL %s.system.rollback_to_snapshot(1L)\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.rollback_to_snapshot(namespace => 'n', snapshot_id => 1L)\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n-        AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.rollback_to_snapshot(table => 't', snapshot_id => 1L)\", catalogName));\n+        () -> sql(\"CALL %s.system.rollback_to_snapshot(table => 't')\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n         AnalysisException.class, \"Wrong arg type for snapshot_id: cannot cast\",\n-        () -> sql(\"CALL %s.system.rollback_to_snapshot('n', 't', 2.2)\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject empty namespace\",\n-        IllegalArgumentException.class, \"Namespace cannot be empty\",\n-        () -> sql(\"CALL %s.system.rollback_to_snapshot('', 't', 1L)\", catalogName));\n+        () -> sql(\"CALL %s.system.rollback_to_snapshot('t', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject empty table name\",\n-        IllegalArgumentException.class, \"Table name cannot be empty\",\n-        () -> sql(\"CALL %s.system.rollback_to_snapshot('n', '', 1L)\", catalogName));\n+    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "094987f1c263aa1bdeaca422260a2f9fc41cea7b"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NDg4NTAzOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwODo1NzozM1rOICJcHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwODo1NzozM1rOICJcHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTEyMjcxNw==", "bodyText": "nit: can fit on one line now", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539122717", "createdAt": "2020-12-09T08:57:33Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -48,20 +48,17 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     this.tableCatalog = tableCatalog;\n   }\n \n-  protected <T> T modifyIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(namespace, tableName, true, func);\n+  protected <T> T modifyIcebergTable(Identifier ident, Function<org.apache.iceberg.Table, T> func) {\n+    return execute(ident, true, func);\n   }\n \n-  protected <T> T withIcebergTable(String namespace, String tableName, Function<org.apache.iceberg.Table, T> func) {\n-    return execute(namespace, tableName, false, func);\n+  protected <T> T withIcebergTable(Identifier ident, Function<org.apache.iceberg.Table, T> func) {\n+    return execute(ident, false, func);\n   }\n \n-  private <T> T execute(String namespace, String tableName, boolean refreshSparkCache,\n+  private <T> T execute(Identifier ident, boolean refreshSparkCache,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "094987f1c263aa1bdeaca422260a2f9fc41cea7b"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NTAzODcwOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwOToyOToyMVrOICK2nw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwOToyOToyMVrOICK2nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTE0NTg4Nw==", "bodyText": "I like Ryan's suggestion in another PR to add a variant of catalogAndIdentifier that does not throw a checked parse exception. Explicit handling of parse exceptions makes other places more complicated.\nIf we follow that idea and add this method to Spark3Util:\n\n  public static CatalogAndIdentifier catalogAndIdentifier(String description, SparkSession spark,\n                                                          String name, CatalogPlugin defaultCatalog) {\n    try {\n      return catalogAndIdentifier(spark, name, defaultCatalog);\n    } catch (ParseException e) {\n      throw new IllegalArgumentException(\"Cannot parse \" + description + \": \" + name, e);\n    }\n  }\n\nThis place can look like this:\n  protected Identifier toIdentifier(String identifierAsString, String argName) {\n    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n        \"Cannot handle an empty identifier for argument %s\", argName);\n\n    CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n        \"identifier for arg \" + argName, spark, identifierAsString, tableCatalog);\n\n    CatalogPlugin catalog = catalogAndIdentifier.catalog();\n    Identifier identifier = catalogAndIdentifier.identifier();\n\n    Preconditions.checkArgument(\n        catalog.equals(tableCatalog),\n        \"Cannot run procedure in catalog %s: %s is a table in catalog %s\",\n        tableCatalog.name(), identifierAsString, catalog);\n\n    return identifier;\n  }", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539145887", "createdAt": "2020-12-09T09:29:21Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -74,26 +71,23 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n     return result;\n   }\n \n-  // we have to parse both namespace and name as they may be quoted\n-  protected Identifier toIdentifier(String namespaceAsString, String name) {\n-    String[] namespaceParts = parseMultipartIdentifier(namespaceAsString);\n+  protected Identifier toIdentifier(String identifierAsString, String argName) {\n+    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n+        \"Cannot handle an empty identifier for argument %s\", argName);\n \n-    String[] nameParts = parseMultipartIdentifier(name);\n-    Preconditions.checkArgument(nameParts.length == 1, \"Name must consist of one part: %s\", name);\n-\n-    return Identifier.of(namespaceParts, nameParts[0]);\n-  }\n-\n-  private String[] parseMultipartIdentifier(String identifierAsString) {\n+    CatalogAndIdentifier catalogAndIdentifier;\n     try {\n-      ParserInterface parser = spark.sessionState().sqlParser();\n-      Seq<String> namePartsSeq = parser.parseMultipartIdentifier(identifierAsString);\n-      String[] nameParts = new String[namePartsSeq.size()];\n-      namePartsSeq.copyToArray(nameParts);\n-      return nameParts;\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n     } catch (ParseException e) {\n-      throw new RuntimeException(\"Couldn't parse identifier: \" + identifierAsString, e);\n+      throw new IllegalArgumentException(String.format(\"Cannot parse identifier '%s' for argument %s\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "094987f1c263aa1bdeaca422260a2f9fc41cea7b"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NTExMDQ3OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwOTo0Mzo1N1rOICLhKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwOTo0Mzo1N1rOICLhKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTE1Njc3Nw==", "bodyText": "extra space", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539156777", "createdAt": "2020-12-09T09:43:57Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java", "diffHunk": "@@ -184,8 +184,8 @@ public void testRollbackToTimestampWithQuotedIdentifiers() {\n     String quotedNamespace = quotedNamespaceBuilder.toString();\n \n     List<Object[]> output = sql(\n-        \"CALL %s.system.rollback_to_timestamp('%s', '`%s`', TIMESTAMP '%s')\",\n-        catalogName, quotedNamespace, tableIdent.name(), firstSnapshotTimestamp);\n+        \"CALL %s.system.rollback_to_timestamp('%s', TIMESTAMP '%s')\",\n+         catalogName, quotedNamespace + \".`\" + tableIdent.name() + \"`\", firstSnapshotTimestamp);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "094987f1c263aa1bdeaca422260a2f9fc41cea7b"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NTExNDA3OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwOTo0NDo0MVrOICLjUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwOTo0NDo0MVrOICLjUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTE1NzMyOQ==", "bodyText": "nit: with?", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539157329", "createdAt": "2020-12-09T09:44:41Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java", "diffHunk": "@@ -196,30 +196,26 @@ public void testInvalidRollbackToSnapshotCases() {\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.set_current_snapshot('n', 't')\", catalogName));\n+        () -> sql(\"CALL %s.system.set_current_snapshot('t')\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.set_current_snapshot('n', 1L)\", catalogName));\n+        () -> sql(\"CALL %s.system.set_current_snapshot(1L)\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.set_current_snapshot(namespace => 'n', snapshot_id => 1L)\", catalogName));\n+        () -> sql(\"CALL %s.system.set_current_snapshot(snapshot_id => 1L)\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n         AnalysisException.class, \"Missing required parameters\",\n-        () -> sql(\"CALL %s.system.set_current_snapshot(table => 't', snapshot_id => 1L)\", catalogName));\n+        () -> sql(\"CALL %s.system.set_current_snapshot(table => 't')\", catalogName));\n \n     AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n         AnalysisException.class, \"Wrong arg type for snapshot_id: cannot cast\",\n-        () -> sql(\"CALL %s.system.set_current_snapshot('n', 't', 2.2)\", catalogName));\n+        () -> sql(\"CALL %s.system.set_current_snapshot('t', 2.2)\", catalogName));\n \n-    AssertHelpers.assertThrows(\"Should reject empty namespace\",\n-        IllegalArgumentException.class, \"Namespace cannot be empty\",\n-        () -> sql(\"CALL %s.system.set_current_snapshot('', 't', 1L)\", catalogName));\n-\n-    AssertHelpers.assertThrows(\"Should reject empty table name\",\n-        IllegalArgumentException.class, \"Table name cannot be empty\",\n-        () -> sql(\"CALL %s.system.set_current_snapshot('n', '', 1L)\", catalogName));\n+    AssertHelpers.assertThrows(\"Should reject calls without empty table identifier\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "094987f1c263aa1bdeaca422260a2f9fc41cea7b"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NjE4OTg0OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxMzo0NjoxNVrOICVXjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxMzo0NjoxNVrOICVXjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTMxODE1Ng==", "bodyText": "I missed to call catalog.name() on the third arg.", "url": "https://github.com/apache/iceberg/pull/1890#discussion_r539318156", "createdAt": "2020-12-09T13:46:15Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -75,19 +73,18 @@ protected Identifier toIdentifier(String identifierAsString, String argName) {\n     Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n         \"Cannot handle an empty identifier for argument %s\", argName);\n \n-    CatalogAndIdentifier catalogAndIdentifier;\n-    try {\n-      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifierAsString, tableCatalog);\n-    } catch (ParseException e) {\n-      throw new IllegalArgumentException(String.format(\"Cannot parse identifier '%s' for argument %s\",\n-          identifierAsString, argName), e);\n-    }\n+    CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n+        \"identifier for arg \" + argName, spark, identifierAsString, tableCatalog);\n+\n+    CatalogPlugin catalog = catalogAndIdentifier.catalog();\n+    Identifier identifier = catalogAndIdentifier.identifier();\n \n-    Preconditions.checkArgument(catalogAndIdentifier.catalog().equals(tableCatalog), \"Cannot run procedure\" +\n-        \" in catalog '%s': Argument %s was set to '%s' which resolves to a table in a different catalog '%s'\",\n-        tableCatalog.name(), argName, identifierAsString, catalogAndIdentifier.catalog().name());\n+    Preconditions.checkArgument(\n+        catalog.equals(tableCatalog),\n+        \"Cannot run procedure in catalog %s: %s is a table in catalog %s\",\n+        tableCatalog.name(), identifierAsString, catalog);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2343e9569a98cb7ea019ce5a7d23249f78920f8b"}, "originalPosition": 43}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3280, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}