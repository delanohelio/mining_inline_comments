{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQzMzUwNDMx", "number": 1161, "title": "Fix errors when use removeOrphanFiles By HadoopCatalog", "bodyText": "I fixed the problem by doing some debugging and modifying some of the code.\nIsuues url:1160", "createdAt": "2020-07-02T08:10:45Z", "url": "https://github.com/apache/iceberg/pull/1161", "merged": true, "mergeCommit": {"oid": "92ae3d38bd348b2cc7058ccdc83bd618d0233474"}, "closed": true, "closedAt": "2020-07-05T20:38:19Z", "author": {"login": "zhangdove"}, "timelineItems": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcw6OfTAH2gAyNDQzMzUwNDMxOjY2MDg2NzVmNDI5ZTUwZjc4NjA1YzdiNDFlM2U5MjJhZTI1NzhjMjU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcxzKl9AH2gAyNDQzMzUwNDMxOmU2NzZjYTc5ZDczY2MzOTZmMzcwMDgwMjk1NDk4OTg0YjFmMjQ3ZDM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "6608675f429e50f78605c7b41e3e922ae2578c25", "author": {"user": {"login": "zhangdove", "name": "zhangdove"}}, "url": "https://github.com/apache/iceberg/commit/6608675f429e50f78605c7b41e3e922ae2578c25", "committedDate": "2020-07-02T08:04:46Z", "message": "Fix errors when use removeOrphanFiles By HadoopCatalog"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxNDc5MjY4", "url": "https://github.com/apache/iceberg/pull/1161#pullrequestreview-441479268", "createdAt": "2020-07-02T08:52:03Z", "commit": {"oid": "6608675f429e50f78605c7b41e3e922ae2578c25"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwODo1MjowNFrOGsDo_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwODo1MjowNFrOGsDo_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg1MDE3Mg==", "bodyText": "Is possible to provide a unit test addressing your changes ?\nThanks.", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r448850172", "createdAt": "2020-07-02T08:52:04Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -30,8 +31,11 @@ protected String metadataTableName(MetadataTableType type) {\n     String tableName = table().toString();\n     if (tableName.contains(\"/\")) {\n       return tableName + \"#\" + type;\n-    } else if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\")) {\n-      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // HadoopCatalog tableName style is 'hadoop.ns.tb'\n+      return ((BaseTable) table()).operations().current().location() + \"#\" + type;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6608675f429e50f78605c7b41e3e922ae2578c25"}, "originalPosition": 16}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f2f4223dea0312ca6945dc6166de9b6df33bbaec", "author": {"user": {"login": "zhangdove", "name": "zhangdove"}}, "url": "https://github.com/apache/iceberg/commit/f2f4223dea0312ca6945dc6166de9b6df33bbaec", "committedDate": "2020-07-02T09:55:50Z", "message": "add a unit test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "21d859ba9752c96128a1543c2f14da626515733c", "author": {"user": {"login": "zhangdove", "name": "zhangdove"}}, "url": "https://github.com/apache/iceberg/commit/21d859ba9752c96128a1543c2f14da626515733c", "committedDate": "2020-07-02T10:06:45Z", "message": "remove some char"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec", "author": {"user": {"login": "zhangdove", "name": "zhangdove"}}, "url": "https://github.com/apache/iceberg/commit/527bb60c067a12463688a99a2a05d67c86fb92ec", "committedDate": "2020-07-02T10:26:18Z", "message": "repair code style"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTAzODg5", "url": "https://github.com/apache/iceberg/pull/1161#pullrequestreview-442503889", "createdAt": "2020-07-03T16:37:30Z", "commit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjozNzozMFrOGs0x1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjozNzozMFrOGs0x1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NTI1NA==", "bodyText": "I don't think this comment is correct because the solution is to convert to a path table reference.\nIf i understand correctly, the problem in Spark 2.4 is that IcebergSource uses the Hive catalog or it uses HadoopTables to load a path. The table you're passing in was loaded by HadoopCatalog, so it doesn't work because that catalog is not available. I agree that the solution is to convert a Hadoop table to a path reference, but then this comment should explain what's happening: a HadoopCatalog was used to load the table, so convert to the path.", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449655254", "createdAt": "2020-07-03T16:37:30Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -30,8 +31,11 @@ protected String metadataTableName(MetadataTableType type) {\n     String tableName = table().toString();\n     if (tableName.contains(\"/\")) {\n       return tableName + \"#\" + type;\n-    } else if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\")) {\n-      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // HadoopCatalog tableName style is 'hadoop.ns.tb'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 15}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTA0MDcy", "url": "https://github.com/apache/iceberg/pull/1161#pullrequestreview-442504072", "createdAt": "2020-07-03T16:38:05Z", "commit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjozODowNVrOGs0ybA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjozODowNVrOGs0ybA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NTQwNA==", "bodyText": "Why not use table().location() instead? There's no need to cast to BaseTable and access TableOperations.", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449655404", "createdAt": "2020-07-03T16:38:05Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -30,8 +31,11 @@ protected String metadataTableName(MetadataTableType type) {\n     String tableName = table().toString();\n     if (tableName.contains(\"/\")) {\n       return tableName + \"#\" + type;\n-    } else if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\")) {\n-      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // HadoopCatalog tableName style is 'hadoop.ns.tb'\n+      return ((BaseTable) table()).operations().current().location() + \"#\" + type;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 16}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTA0NTQy", "url": "https://github.com/apache/iceberg/pull/1161#pullrequestreview-442504542", "createdAt": "2020-07-03T16:39:34Z", "commit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjozOTozNFrOGs0z2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjozOTozNFrOGs0z2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NTc3MQ==", "bodyText": "Nit: Indentation is off. It should be 2 indents (4 spaces) from the indent of the line that is being continued, df.select(...).", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449655771", "createdAt": "2020-07-03T16:39:34Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java", "diffHunk": "@@ -542,4 +545,41 @@ public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException, Inte\n     Assert.assertEquals(\"Action should find 1 file\", invalidFiles, result);\n     Assert.assertTrue(\"Invalid file should be present\", fs.exists(new Path(invalidFiles.get(0))));\n   }\n+\n+  @Test\n+  public void testRemoveOrphanFilesWithHadoopCatalog() throws InterruptedException {\n+    HadoopCatalog catalog = new HadoopCatalog(new Configuration(), tableLocation);\n+    String namespaceName = \"testDb\";\n+    String tableName = \"testTb\";\n+\n+    Namespace namespace = Namespace.of(namespaceName);\n+    TableIdentifier tableIdentifier = TableIdentifier.of(namespace, tableName);\n+    Table table = catalog.createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap());\n+\n+    List<ThreeColumnRecord> records = Lists.newArrayList(\n+            new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n+    );\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+\n+    String tableFileSystemPath = tableLocation + \"/\" + namespaceName + \"/\" + tableName;\n+    df.select(\"c1\", \"c2\", \"c3\")\n+            .write()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTA1MTY2", "url": "https://github.com/apache/iceberg/pull/1161#pullrequestreview-442505166", "createdAt": "2020-07-03T16:41:41Z", "commit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjo0MTo0MVrOGs016g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjo0MTo0MVrOGs016g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NjI5OA==", "bodyText": "Can you add the comment that explains this line from the other test cases?", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449656298", "createdAt": "2020-07-03T16:41:41Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java", "diffHunk": "@@ -542,4 +545,41 @@ public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException, Inte\n     Assert.assertEquals(\"Action should find 1 file\", invalidFiles, result);\n     Assert.assertTrue(\"Invalid file should be present\", fs.exists(new Path(invalidFiles.get(0))));\n   }\n+\n+  @Test\n+  public void testRemoveOrphanFilesWithHadoopCatalog() throws InterruptedException {\n+    HadoopCatalog catalog = new HadoopCatalog(new Configuration(), tableLocation);\n+    String namespaceName = \"testDb\";\n+    String tableName = \"testTb\";\n+\n+    Namespace namespace = Namespace.of(namespaceName);\n+    TableIdentifier tableIdentifier = TableIdentifier.of(namespace, tableName);\n+    Table table = catalog.createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap());\n+\n+    List<ThreeColumnRecord> records = Lists.newArrayList(\n+            new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n+    );\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+\n+    String tableFileSystemPath = tableLocation + \"/\" + namespaceName + \"/\" + tableName;\n+    df.select(\"c1\", \"c2\", \"c3\")\n+            .write()\n+            .format(\"iceberg\")\n+            .mode(\"append\")\n+            .save(tableFileSystemPath);\n+\n+    df.write().mode(\"append\").parquet(tableFileSystemPath + \"/data\");\n+\n+    Thread.sleep(1000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTA1MzEz", "url": "https://github.com/apache/iceberg/pull/1161#pullrequestreview-442505313", "createdAt": "2020-07-03T16:42:08Z", "commit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjo0MjowOFrOGs02Wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjo0MjowOFrOGs02Wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NjQxMA==", "bodyText": "You can combine this with the previous line:\nList<String> deletedFiles = Actions.forTable(table)\n    .removeOrphanFiles()\n    .olderThan(timestamp)\n    .execute();", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449656410", "createdAt": "2020-07-03T16:42:08Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java", "diffHunk": "@@ -542,4 +545,41 @@ public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException, Inte\n     Assert.assertEquals(\"Action should find 1 file\", invalidFiles, result);\n     Assert.assertTrue(\"Invalid file should be present\", fs.exists(new Path(invalidFiles.get(0))));\n   }\n+\n+  @Test\n+  public void testRemoveOrphanFilesWithHadoopCatalog() throws InterruptedException {\n+    HadoopCatalog catalog = new HadoopCatalog(new Configuration(), tableLocation);\n+    String namespaceName = \"testDb\";\n+    String tableName = \"testTb\";\n+\n+    Namespace namespace = Namespace.of(namespaceName);\n+    TableIdentifier tableIdentifier = TableIdentifier.of(namespace, tableName);\n+    Table table = catalog.createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap());\n+\n+    List<ThreeColumnRecord> records = Lists.newArrayList(\n+            new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n+    );\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+\n+    String tableFileSystemPath = tableLocation + \"/\" + namespaceName + \"/\" + tableName;\n+    df.select(\"c1\", \"c2\", \"c3\")\n+            .write()\n+            .format(\"iceberg\")\n+            .mode(\"append\")\n+            .save(tableFileSystemPath);\n+\n+    df.write().mode(\"append\").parquet(tableFileSystemPath + \"/data\");\n+\n+    Thread.sleep(1000);\n+\n+    long timestamp = System.currentTimeMillis();\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    List<String> result = actions.removeOrphanFiles()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 45}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTA1NTUz", "url": "https://github.com/apache/iceberg/pull/1161#pullrequestreview-442505553", "createdAt": "2020-07-03T16:42:48Z", "commit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjo0Mjo0OFrOGs03Gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjo0Mjo0OFrOGs03Gw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NjYwMw==", "bodyText": "Minor: The other tests don't use a separate variable for this. Could this be embedded in the olderThan call?", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449656603", "createdAt": "2020-07-03T16:42:48Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java", "diffHunk": "@@ -542,4 +545,41 @@ public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException, Inte\n     Assert.assertEquals(\"Action should find 1 file\", invalidFiles, result);\n     Assert.assertTrue(\"Invalid file should be present\", fs.exists(new Path(invalidFiles.get(0))));\n   }\n+\n+  @Test\n+  public void testRemoveOrphanFilesWithHadoopCatalog() throws InterruptedException {\n+    HadoopCatalog catalog = new HadoopCatalog(new Configuration(), tableLocation);\n+    String namespaceName = \"testDb\";\n+    String tableName = \"testTb\";\n+\n+    Namespace namespace = Namespace.of(namespaceName);\n+    TableIdentifier tableIdentifier = TableIdentifier.of(namespace, tableName);\n+    Table table = catalog.createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap());\n+\n+    List<ThreeColumnRecord> records = Lists.newArrayList(\n+            new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n+    );\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+\n+    String tableFileSystemPath = tableLocation + \"/\" + namespaceName + \"/\" + tableName;\n+    df.select(\"c1\", \"c2\", \"c3\")\n+            .write()\n+            .format(\"iceberg\")\n+            .mode(\"append\")\n+            .save(tableFileSystemPath);\n+\n+    df.write().mode(\"append\").parquet(tableFileSystemPath + \"/data\");\n+\n+    Thread.sleep(1000);\n+\n+    long timestamp = System.currentTimeMillis();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTA1ODEy", "url": "https://github.com/apache/iceberg/pull/1161#pullrequestreview-442505812", "createdAt": "2020-07-03T16:43:39Z", "commit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjo0MzozOVrOGs036g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjo0MzozOVrOGs036g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NjgxMA==", "bodyText": "Does this work using table.location instead of building the path here?", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449656810", "createdAt": "2020-07-03T16:43:39Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java", "diffHunk": "@@ -542,4 +545,41 @@ public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException, Inte\n     Assert.assertEquals(\"Action should find 1 file\", invalidFiles, result);\n     Assert.assertTrue(\"Invalid file should be present\", fs.exists(new Path(invalidFiles.get(0))));\n   }\n+\n+  @Test\n+  public void testRemoveOrphanFilesWithHadoopCatalog() throws InterruptedException {\n+    HadoopCatalog catalog = new HadoopCatalog(new Configuration(), tableLocation);\n+    String namespaceName = \"testDb\";\n+    String tableName = \"testTb\";\n+\n+    Namespace namespace = Namespace.of(namespaceName);\n+    TableIdentifier tableIdentifier = TableIdentifier.of(namespace, tableName);\n+    Table table = catalog.createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap());\n+\n+    List<ThreeColumnRecord> records = Lists.newArrayList(\n+            new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n+    );\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+\n+    String tableFileSystemPath = tableLocation + \"/\" + namespaceName + \"/\" + tableName;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 30}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ccfbbf7527b7218fd8bb6a0c4d679248e751303d", "author": {"user": {"login": "zhangdove", "name": "zhangdove"}}, "url": "https://github.com/apache/iceberg/commit/ccfbbf7527b7218fd8bb6a0c4d679248e751303d", "committedDate": "2020-07-04T07:09:46Z", "message": "A few small changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "86569e3a95525e77ed681ffcf49107dcb0dbe3b9", "author": {"user": {"login": "zhangdove", "name": "zhangdove"}}, "url": "https://github.com/apache/iceberg/commit/86569e3a95525e77ed681ffcf49107dcb0dbe3b9", "committedDate": "2020-07-04T07:18:52Z", "message": "remove no used import"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNjMzNDIw", "url": "https://github.com/apache/iceberg/pull/1161#pullrequestreview-442633420", "createdAt": "2020-07-04T21:59:49Z", "commit": {"oid": "86569e3a95525e77ed681ffcf49107dcb0dbe3b9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNFQyMTo1OTo0OVrOGs-JFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNFQyMTo1OTo0OVrOGs-JFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTgwODY2Mg==", "bodyText": "Looks like this needs to be updated. There is no need to remove hadoop. if Hadoop tables don't use this code path.", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449808662", "createdAt": "2020-07-04T21:59:49Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -30,8 +30,11 @@ protected String metadataTableName(MetadataTableType type) {\n     String tableName = table().toString();\n     if (tableName.contains(\"/\")) {\n       return tableName + \"#\" + type;\n-    } else if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\")) {\n-      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // Load a path by HadoopCatalog or HadoopTables\n+      return table().location() + \"#\" + type;\n+    } else if (tableName.startsWith(\"hive.\")) {\n+      // HiveCatalog prepend a logical name which we need to drop for Spark 2.4\n       return tableName.replaceFirst(\"(hadoop\\\\.)|(hive\\\\.)\", \"\") + \".\" + type;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86569e3a95525e77ed681ffcf49107dcb0dbe3b9"}, "originalPosition": 11}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNjMzNTA2", "url": "https://github.com/apache/iceberg/pull/1161#pullrequestreview-442633506", "createdAt": "2020-07-04T22:03:06Z", "commit": {"oid": "86569e3a95525e77ed681ffcf49107dcb0dbe3b9"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNFQyMjowMzowNlrOGs-J4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNFQyMjowMzowNlrOGs-J4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTgwODg2NA==", "bodyText": "How about \"for HadoopCatalog tables, use the table location to load the metadata table because IcebergCatalog uses HiveCatalog when the table is identified by name\".", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449808864", "createdAt": "2020-07-04T22:03:06Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -30,8 +30,11 @@ protected String metadataTableName(MetadataTableType type) {\n     String tableName = table().toString();\n     if (tableName.contains(\"/\")) {\n       return tableName + \"#\" + type;\n-    } else if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\")) {\n-      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // Load a path by HadoopCatalog or HadoopTables", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86569e3a95525e77ed681ffcf49107dcb0dbe3b9"}, "originalPosition": 7}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e676ca79d73cc396f370080295498984b1f247d3", "author": {"user": {"login": "zhangdove", "name": "zhangdove"}}, "url": "https://github.com/apache/iceberg/commit/e676ca79d73cc396f370080295498984b1f247d3", "committedDate": "2020-07-05T02:25:06Z", "message": "update the comment and remove matching"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4595, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}