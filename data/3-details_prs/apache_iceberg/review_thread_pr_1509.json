{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkyODI1Njkx", "number": 1509, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxOToxODo1NFrOEnk0xA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMjoyMDozNFrOEoR8GQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5OTMzMjUyOnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/TypeToFlinkType.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxOToxODo1NFrOHYPt4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxOToxODo1NFrOHYPt4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NTM3Nw==", "bodyText": "Can you update the comment to state what Flink's default precision is?", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r495185377", "createdAt": "2020-09-25T19:18:54Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/TypeToFlinkType.java", "diffHunk": "@@ -100,8 +100,8 @@ public LogicalType primitive(Type.PrimitiveType primitive) {\n       case DATE:\n         return new DateType();\n       case TIME:\n-        // MICROS\n-        return new TimeType(6);\n+        // Flink only support TimeType with default precision now.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9924db4dbd80d94ed97987e44671fa55d591f772"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5OTMzNDkzOnYy", "diffSide": "RIGHT", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxOToxOTo1NlrOHYPvWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMTo1OToyOFrOHZRUAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NTc1NQ==", "bodyText": "Are these related changes, or should the time changes go into a separate PR?", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r495185755", "createdAt": "2020-09-25T19:19:56Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java", "diffHunk": "@@ -252,7 +252,7 @@ public void testInconsistentTypes() {\n         Types.BinaryType.get(), new VarBinaryType(VarBinaryType.MAX_LENGTH),\n         new VarBinaryType(100), Types.BinaryType.get());\n     checkInconsistentType(\n-        Types.TimeType.get(), new TimeType(6),\n+        Types.TimeType.get(), new TimeType(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9924db4dbd80d94ed97987e44671fa55d591f772"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY5NTYyOA==", "bodyText": "I will create one.", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r495695628", "createdAt": "2020-09-28T05:26:10Z", "author": {"login": "JingsongLi"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java", "diffHunk": "@@ -252,7 +252,7 @@ public void testInconsistentTypes() {\n         Types.BinaryType.get(), new VarBinaryType(VarBinaryType.MAX_LENGTH),\n         new VarBinaryType(100), Types.BinaryType.get());\n     checkInconsistentType(\n-        Types.TimeType.get(), new TimeType(6),\n+        Types.TimeType.get(), new TimeType(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NTc1NQ=="}, "originalCommit": {"oid": "9924db4dbd80d94ed97987e44671fa55d591f772"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY5NjkyMQ==", "bodyText": "Created #1521", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r495696921", "createdAt": "2020-09-28T05:30:54Z", "author": {"login": "JingsongLi"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java", "diffHunk": "@@ -252,7 +252,7 @@ public void testInconsistentTypes() {\n         Types.BinaryType.get(), new VarBinaryType(VarBinaryType.MAX_LENGTH),\n         new VarBinaryType(100), Types.BinaryType.get());\n     checkInconsistentType(\n-        Types.TimeType.get(), new TimeType(6),\n+        Types.TimeType.get(), new TimeType(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NTc1NQ=="}, "originalCommit": {"oid": "9924db4dbd80d94ed97987e44671fa55d591f772"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI2MDA5OA==", "bodyText": "Merged #1521.", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r496260098", "createdAt": "2020-09-28T21:59:28Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java", "diffHunk": "@@ -252,7 +252,7 @@ public void testInconsistentTypes() {\n         Types.BinaryType.get(), new VarBinaryType(VarBinaryType.MAX_LENGTH),\n         new VarBinaryType(100), Types.BinaryType.get());\n     checkInconsistentType(\n-        Types.TimeType.get(), new TimeType(6),\n+        Types.TimeType.get(), new TimeType(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NTc1NQ=="}, "originalCommit": {"oid": "9924db4dbd80d94ed97987e44671fa55d591f772"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5OTMzODg3OnYy", "diffSide": "RIGHT", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQxOToyMToxNFrOHYPxsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQwNToyNjowMFrOHYu25w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NjM1Mw==", "bodyText": "This should be called with timestampMillis because you want to wait until it is strictly after the current snapshot's timestamp.", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r495186353", "createdAt": "2020-09-25T19:21:14Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "diffHunk": "@@ -230,7 +215,7 @@ public void testSnapshotReads() throws Exception {\n     long timestampMillis = table.currentSnapshot().timestampMillis();\n \n     // produce another timestamp\n-    Thread.sleep(10);\n+    waitUntilAfter(10);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9924db4dbd80d94ed97987e44671fa55d591f772"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY5NTU5MQ==", "bodyText": "Yes, you are right.", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r495695591", "createdAt": "2020-09-28T05:26:00Z", "author": {"login": "JingsongLi"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "diffHunk": "@@ -230,7 +215,7 @@ public void testSnapshotReads() throws Exception {\n     long timestampMillis = table.currentSnapshot().timestampMillis();\n \n     // produce another timestamp\n-    Thread.sleep(10);\n+    waitUntilAfter(10);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTE4NjM1Mw=="}, "originalCommit": {"oid": "9924db4dbd80d94ed97987e44671fa55d591f772"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNjY4NDYyOnYy", "diffSide": "RIGHT", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMjowNDo0NlrOHZRcVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMjo0MzozOVrOHZWhxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI2MjIzMA==", "bodyText": "Why is this waiting until 10ms from now? That's basically equivalent to Thread.sleep. The purpose of using waitUntilAfter is to ensure the next call to System.currentTimeMillis() will be after some point in time. In most cases, that's the last snapshot's timestamp.\nI think this should be waitUntilAfter(table.currentSnapshot().timestampMillis());. Then the next append will be strictly after that snapshot's timestamp, and the query with as-of-timestamp will read the older snapshot. There should be no need to wait as long as 10ms.", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r496262230", "createdAt": "2020-09-28T22:04:46Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "diffHunk": "@@ -230,13 +246,16 @@ public void testSnapshotReads() throws Exception {\n     long timestampMillis = table.currentSnapshot().timestampMillis();\n \n     // produce another timestamp\n-    Thread.sleep(10);\n+    waitUntilAfter(System.currentTimeMillis() + 10);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a41dd8f8300bf16334dd5b7d9aab05f2851587de"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM0NTU0Mg==", "bodyText": "I see. You are right.", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r496345542", "createdAt": "2020-09-29T02:43:39Z", "author": {"login": "JingsongLi"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "diffHunk": "@@ -230,13 +246,16 @@ public void testSnapshotReads() throws Exception {\n     long timestampMillis = table.currentSnapshot().timestampMillis();\n \n     // produce another timestamp\n-    Thread.sleep(10);\n+    waitUntilAfter(System.currentTimeMillis() + 10);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI2MjIzMA=="}, "originalCommit": {"oid": "a41dd8f8300bf16334dd5b7d9aab05f2851587de"}, "originalPosition": 151}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNjcyMzQxOnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMjoyMDoxOVrOHZRy9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQwMDoyNjo1NFrOHaz71g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI2ODAyMA==", "bodyText": "We try to avoid using Hadoop classes in Iceberg APIs because they are hard to remove. Injecting a Hadoop Configuration is currently done in one place: to instantiate a catalog that requires it.\nThe catalog creates tables and tables are associated with a FileIO, so the configuration is passed down through that chain. The table's configuration should be used for any table configuration.\nMR also has a Hadoop Configuration, but that's required by the API so we can't remove it from the API there. But we still prefer using the table's Configuration when it is needed for components like HadoopFileIO.", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r496268020", "createdAt": "2020-09-28T22:20:19Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -89,8 +88,18 @@ public Builder table(Table newTable) {\n       return this;\n     }\n \n-    public Builder filters(List<Expression> newFilters) {\n-      this.filterExpressions = newFilters;\n+    public Builder hadoopConf(Configuration newConf) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a41dd8f8300bf16334dd5b7d9aab05f2851587de"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1MzU2OQ==", "bodyText": "+1 for avoiding using Hadoop Configuration.", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r496353569", "createdAt": "2020-09-29T03:15:41Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -89,8 +88,18 @@ public Builder table(Table newTable) {\n       return this;\n     }\n \n-    public Builder filters(List<Expression> newFilters) {\n-      this.filterExpressions = newFilters;\n+    public Builder hadoopConf(Configuration newConf) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI2ODAyMA=="}, "originalCommit": {"oid": "a41dd8f8300bf16334dd5b7d9aab05f2851587de"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NjIzNg==", "bodyText": "The reason why Hadoop conf needs to be passed now is because:\n\nJobManager needs the splits of the scan, so it needs to call Table.newScan.\nSo JobManager needs Table object, where can a table be generated? TableLoader -> from Catalog or HadoopTables.\nThe creation of Catalog and HadoopTables needs a Hadoop Configuration.\n\nMaybe we can pass this chain with an Iceberg object (FileIO may looks not good to the creation of catalog)?", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r496356236", "createdAt": "2020-09-29T03:26:47Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -89,8 +88,18 @@ public Builder table(Table newTable) {\n       return this;\n     }\n \n-    public Builder filters(List<Expression> newFilters) {\n-      this.filterExpressions = newFilters;\n+    public Builder hadoopConf(Configuration newConf) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI2ODAyMA=="}, "originalCommit": {"oid": "a41dd8f8300bf16334dd5b7d9aab05f2851587de"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg5NDkzOQ==", "bodyText": "Maybe CatalogLoader should serialize its own Configuration? It would make sense to pass one when creating a CatalogLoader for HadoopCatalog or HiveCatalog because those need a Configuration to create the catalog.", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r496894939", "createdAt": "2020-09-29T16:55:41Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -89,8 +88,18 @@ public Builder table(Table newTable) {\n       return this;\n     }\n \n-    public Builder filters(List<Expression> newFilters) {\n-      this.filterExpressions = newFilters;\n+    public Builder hadoopConf(Configuration newConf) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI2ODAyMA=="}, "originalCommit": {"oid": "a41dd8f8300bf16334dd5b7d9aab05f2851587de"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg5NjA5OA==", "bodyText": "Or maybe this could use the approach from the FlinkCatalogFactory:\n  public static Configuration clusterHadoopConf() {\n    return HadoopUtils.getHadoopConfiguration(GlobalConfiguration.loadConfiguration());\n  }\nUsing clusterHadoopConf internally here would avoid the need to expose this in the API and we could add Configuration to the loader later.", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r496896098", "createdAt": "2020-09-29T16:57:33Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -89,8 +88,18 @@ public Builder table(Table newTable) {\n       return this;\n     }\n \n-    public Builder filters(List<Expression> newFilters) {\n-      this.filterExpressions = newFilters;\n+    public Builder hadoopConf(Configuration newConf) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI2ODAyMA=="}, "originalCommit": {"oid": "a41dd8f8300bf16334dd5b7d9aab05f2851587de"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg3NTkyNg==", "bodyText": "Using clusterHadoopConf may not so flexible.\nI am +1 for CatalogLoader should serialize its own Configuration. I will create a PR later for source and sink.", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r497875926", "createdAt": "2020-10-01T00:26:54Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -89,8 +88,18 @@ public Builder table(Table newTable) {\n       return this;\n     }\n \n-    public Builder filters(List<Expression> newFilters) {\n-      this.filterExpressions = newFilters;\n+    public Builder hadoopConf(Configuration newConf) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI2ODAyMA=="}, "originalCommit": {"oid": "a41dd8f8300bf16334dd5b7d9aab05f2851587de"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNjcyNDA5OnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMjoyMDozNFrOHZRzSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNjo1Mjo1NFrOHZ39Dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI2ODEwNg==", "bodyText": "Why pass the conf here? I don't see it used.", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r496268106", "createdAt": "2020-09-28T22:20:34Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -45,30 +43,25 @@\n   private static final long serialVersionUID = 1L;\n \n   private final TableLoader tableLoader;\n-  private final Schema projectedSchema;\n-  private final ScanOptions options;\n-  private final List<Expression> filterExpressions;\n+  private final SerializableConfiguration serializableConf;\n   private final FileIO io;\n   private final EncryptionManager encryption;\n-  private final SerializableConfiguration serializableConf;\n+  private final ScanContext context;\n \n   private transient RowDataIterator iterator;\n \n-  FlinkInputFormat(\n-      TableLoader tableLoader, Schema projectedSchema, FileIO io, EncryptionManager encryption,\n-      List<Expression> filterExpressions, ScanOptions options, SerializableConfiguration serializableConf) {\n+  FlinkInputFormat(TableLoader tableLoader, SerializableConfiguration serializableConf, FileIO io,\n+                   EncryptionManager encryption, ScanContext context) {\n     this.tableLoader = tableLoader;\n-    this.projectedSchema = projectedSchema;\n+    this.serializableConf = serializableConf;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a41dd8f8300bf16334dd5b7d9aab05f2851587de"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM0NzczOA==", "bodyText": "It is used to open TableLoader.", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r496347738", "createdAt": "2020-09-29T02:52:32Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -45,30 +43,25 @@\n   private static final long serialVersionUID = 1L;\n \n   private final TableLoader tableLoader;\n-  private final Schema projectedSchema;\n-  private final ScanOptions options;\n-  private final List<Expression> filterExpressions;\n+  private final SerializableConfiguration serializableConf;\n   private final FileIO io;\n   private final EncryptionManager encryption;\n-  private final SerializableConfiguration serializableConf;\n+  private final ScanContext context;\n \n   private transient RowDataIterator iterator;\n \n-  FlinkInputFormat(\n-      TableLoader tableLoader, Schema projectedSchema, FileIO io, EncryptionManager encryption,\n-      List<Expression> filterExpressions, ScanOptions options, SerializableConfiguration serializableConf) {\n+  FlinkInputFormat(TableLoader tableLoader, SerializableConfiguration serializableConf, FileIO io,\n+                   EncryptionManager encryption, ScanContext context) {\n     this.tableLoader = tableLoader;\n-    this.projectedSchema = projectedSchema;\n+    this.serializableConf = serializableConf;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI2ODEwNg=="}, "originalCommit": {"oid": "a41dd8f8300bf16334dd5b7d9aab05f2851587de"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg5MzE5OQ==", "bodyText": "I see now. Thanks!", "url": "https://github.com/apache/iceberg/pull/1509#discussion_r496893199", "createdAt": "2020-09-29T16:52:54Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -45,30 +43,25 @@\n   private static final long serialVersionUID = 1L;\n \n   private final TableLoader tableLoader;\n-  private final Schema projectedSchema;\n-  private final ScanOptions options;\n-  private final List<Expression> filterExpressions;\n+  private final SerializableConfiguration serializableConf;\n   private final FileIO io;\n   private final EncryptionManager encryption;\n-  private final SerializableConfiguration serializableConf;\n+  private final ScanContext context;\n \n   private transient RowDataIterator iterator;\n \n-  FlinkInputFormat(\n-      TableLoader tableLoader, Schema projectedSchema, FileIO io, EncryptionManager encryption,\n-      List<Expression> filterExpressions, ScanOptions options, SerializableConfiguration serializableConf) {\n+  FlinkInputFormat(TableLoader tableLoader, SerializableConfiguration serializableConf, FileIO io,\n+                   EncryptionManager encryption, ScanContext context) {\n     this.tableLoader = tableLoader;\n-    this.projectedSchema = projectedSchema;\n+    this.serializableConf = serializableConf;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI2ODEwNg=="}, "originalCommit": {"oid": "a41dd8f8300bf16334dd5b7d9aab05f2851587de"}, "originalPosition": 38}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3534, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}