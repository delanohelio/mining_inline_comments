{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIyMDEwNzQ0", "number": 1776, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0MzoyMFrOE5vcNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMDoxMjoxOFrOE5ypuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4OTgxNTU3OnYy", "diffSide": "LEFT", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0MzoyMFrOH0Y7aA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0MzoyMFrOH0Y7aA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDY5NjQyNA==", "bodyText": "SparkWrite contains only common things now.", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524696424", "createdAt": "2020-11-16T22:43:20Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -72,52 +75,67 @@\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n \n-class SparkBatchWrite implements BatchWrite {\n-  private static final Logger LOG = LoggerFactory.getLogger(SparkBatchWrite.class);\n+class SparkWrite {\n+  private static final Logger LOG = LoggerFactory.getLogger(SparkWrite.class);\n \n   private final Table table;\n+  private final String queryId;\n   private final FileFormat format;\n   private final Broadcast<FileIO> io;\n   private final Broadcast<EncryptionManager> encryptionManager;\n-  private final boolean overwriteDynamic;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4OTgxOTQ1OnYy", "diffSide": "LEFT", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0NDowMlrOH0Y-Dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0NDowMlrOH0Y-Dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDY5NzEwMg==", "bodyText": "This logic moved to inner classes.", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524697102", "createdAt": "2020-11-16T22:44:02Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -170,45 +177,7 @@ protected void commitOperation(SnapshotUpdate<?> operation, int numFiles, String\n     LOG.info(\"Committed in {} ms\", duration);\n   }\n \n-  private void append(WriterCommitMessage[] messages) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4OTgyMjUwOnYy", "diffSide": "LEFT", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0NDozMFrOH0ZAPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0NDozMFrOH0ZAPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDY5NzY2Mg==", "bodyText": "This logic moved to base batch and streaming write classes.", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524697662", "createdAt": "2020-11-16T22:44:30Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -236,9 +201,171 @@ protected Table table() {\n     return ImmutableList.of();\n   }\n \n-  @Override\n-  public String toString() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "originalPosition": 209}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4OTg0Nzg5OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0ODo0N1rOH0ZRGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMjo0ODo0N1rOH0ZRGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDcwMTk3OA==", "bodyText": "In the future, Spark may have a write abstraction so we will make this class extend Write.", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524701978", "createdAt": "2020-11-16T22:48:47Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -72,52 +75,67 @@\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n \n-class SparkBatchWrite implements BatchWrite {\n-  private static final Logger LOG = LoggerFactory.getLogger(SparkBatchWrite.class);\n+class SparkWrite {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5MDM0MTY4OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwMDoxMjoxOFrOH0ewuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xN1QwODowMTozMFrOH0oUKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc5MTk5Mg==", "bodyText": "Since this searches through old snapshots, what about find instead of get here? That signals that the operation is doing more than just fetching a pre-computed value.", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524791992", "createdAt": "2020-11-17T00:12:18Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -236,9 +201,171 @@ protected Table table() {\n     return ImmutableList.of();\n   }\n \n-  @Override\n-  public String toString() {\n-    return String.format(\"IcebergWrite(table=%s, format=%s)\", table, format);\n+  private abstract class BaseBatchWrite implements BatchWrite {\n+    @Override\n+    public DataWriterFactory createBatchWriterFactory(PhysicalWriteInfo info) {\n+      return createWriterFactory();\n+    }\n+\n+    @Override\n+    public void abort(WriterCommitMessage[] messages) {\n+      SparkWrite.this.abort(messages);\n+    }\n+\n+    @Override\n+    public String toString() {\n+      return String.format(\"IcebergBatchWrite(table=%s, format=%s)\", table, format);\n+    }\n+  }\n+\n+  private class BatchAppend extends BaseBatchWrite {\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+      AppendFiles append = table.newAppend();\n+\n+      int numFiles = 0;\n+      for (DataFile file : files(messages)) {\n+        numFiles += 1;\n+        append.appendFile(file);\n+      }\n+\n+      commitOperation(append, String.format(\"append with %d new data files\", numFiles));\n+    }\n+  }\n+\n+  private class DynamicOverwrite extends BaseBatchWrite {\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+      ReplacePartitions dynamicOverwrite = table.newReplacePartitions();\n+\n+      int numFiles = 0;\n+      for (DataFile file : files(messages)) {\n+        numFiles += 1;\n+        dynamicOverwrite.addFile(file);\n+      }\n+\n+      commitOperation(dynamicOverwrite, String.format(\"dynamic partition overwrite with %d new data files\", numFiles));\n+    }\n+  }\n+\n+  private class OverwriteByFilter extends BaseBatchWrite {\n+    private final Expression overwriteExpr;\n+\n+    private OverwriteByFilter(Expression overwriteExpr) {\n+      this.overwriteExpr = overwriteExpr;\n+    }\n+\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+      OverwriteFiles overwriteFiles = table.newOverwrite();\n+      overwriteFiles.overwriteByRowFilter(overwriteExpr);\n+\n+      int numFiles = 0;\n+      for (DataFile file : files(messages)) {\n+        numFiles += 1;\n+        overwriteFiles.addFile(file);\n+      }\n+\n+      String commitMsg = String.format(\"overwrite by filter %s with %d new data files\", overwriteExpr, numFiles);\n+      commitOperation(overwriteFiles, commitMsg);\n+    }\n+  }\n+\n+  private abstract class BaseStreamingWrite implements StreamingWrite {\n+    private static final String QUERY_ID_PROPERTY = \"spark.sql.streaming.queryId\";\n+    private static final String EPOCH_ID_PROPERTY = \"spark.sql.streaming.epochId\";\n+\n+    protected abstract String mode();\n+\n+    @Override\n+    public StreamingDataWriterFactory createStreamingWriterFactory(PhysicalWriteInfo info) {\n+      return createWriterFactory();\n+    }\n+\n+    @Override\n+    public final void commit(long epochId, WriterCommitMessage[] messages) {\n+      LOG.info(\"Committing epoch {} for query {} in {} mode\", epochId, queryId, mode());\n+\n+      table.refresh();\n+      Long lastCommittedEpochId = getLastCommittedEpochId();\n+      if (lastCommittedEpochId != null && epochId <= lastCommittedEpochId) {\n+        LOG.info(\"Skipping epoch {} for query {} as it was already committed\", epochId, queryId);\n+        return;\n+      }\n+\n+      doCommit(epochId, messages);\n+    }\n+\n+    protected abstract void doCommit(long epochId, WriterCommitMessage[] messages);\n+\n+    protected <T> void commit(SnapshotUpdate<T> snapshotUpdate, long epochId, String description) {\n+      snapshotUpdate.set(QUERY_ID_PROPERTY, queryId);\n+      snapshotUpdate.set(EPOCH_ID_PROPERTY, Long.toString(epochId));\n+      commitOperation(snapshotUpdate, description);\n+    }\n+\n+    private Long getLastCommittedEpochId() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "originalPosition": 314}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDgxNzU0Mg==", "bodyText": "I kept the old name but I agree find is better. Let me update that.", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524817542", "createdAt": "2020-11-17T01:04:26Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -236,9 +201,171 @@ protected Table table() {\n     return ImmutableList.of();\n   }\n \n-  @Override\n-  public String toString() {\n-    return String.format(\"IcebergWrite(table=%s, format=%s)\", table, format);\n+  private abstract class BaseBatchWrite implements BatchWrite {\n+    @Override\n+    public DataWriterFactory createBatchWriterFactory(PhysicalWriteInfo info) {\n+      return createWriterFactory();\n+    }\n+\n+    @Override\n+    public void abort(WriterCommitMessage[] messages) {\n+      SparkWrite.this.abort(messages);\n+    }\n+\n+    @Override\n+    public String toString() {\n+      return String.format(\"IcebergBatchWrite(table=%s, format=%s)\", table, format);\n+    }\n+  }\n+\n+  private class BatchAppend extends BaseBatchWrite {\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+      AppendFiles append = table.newAppend();\n+\n+      int numFiles = 0;\n+      for (DataFile file : files(messages)) {\n+        numFiles += 1;\n+        append.appendFile(file);\n+      }\n+\n+      commitOperation(append, String.format(\"append with %d new data files\", numFiles));\n+    }\n+  }\n+\n+  private class DynamicOverwrite extends BaseBatchWrite {\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+      ReplacePartitions dynamicOverwrite = table.newReplacePartitions();\n+\n+      int numFiles = 0;\n+      for (DataFile file : files(messages)) {\n+        numFiles += 1;\n+        dynamicOverwrite.addFile(file);\n+      }\n+\n+      commitOperation(dynamicOverwrite, String.format(\"dynamic partition overwrite with %d new data files\", numFiles));\n+    }\n+  }\n+\n+  private class OverwriteByFilter extends BaseBatchWrite {\n+    private final Expression overwriteExpr;\n+\n+    private OverwriteByFilter(Expression overwriteExpr) {\n+      this.overwriteExpr = overwriteExpr;\n+    }\n+\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+      OverwriteFiles overwriteFiles = table.newOverwrite();\n+      overwriteFiles.overwriteByRowFilter(overwriteExpr);\n+\n+      int numFiles = 0;\n+      for (DataFile file : files(messages)) {\n+        numFiles += 1;\n+        overwriteFiles.addFile(file);\n+      }\n+\n+      String commitMsg = String.format(\"overwrite by filter %s with %d new data files\", overwriteExpr, numFiles);\n+      commitOperation(overwriteFiles, commitMsg);\n+    }\n+  }\n+\n+  private abstract class BaseStreamingWrite implements StreamingWrite {\n+    private static final String QUERY_ID_PROPERTY = \"spark.sql.streaming.queryId\";\n+    private static final String EPOCH_ID_PROPERTY = \"spark.sql.streaming.epochId\";\n+\n+    protected abstract String mode();\n+\n+    @Override\n+    public StreamingDataWriterFactory createStreamingWriterFactory(PhysicalWriteInfo info) {\n+      return createWriterFactory();\n+    }\n+\n+    @Override\n+    public final void commit(long epochId, WriterCommitMessage[] messages) {\n+      LOG.info(\"Committing epoch {} for query {} in {} mode\", epochId, queryId, mode());\n+\n+      table.refresh();\n+      Long lastCommittedEpochId = getLastCommittedEpochId();\n+      if (lastCommittedEpochId != null && epochId <= lastCommittedEpochId) {\n+        LOG.info(\"Skipping epoch {} for query {} as it was already committed\", epochId, queryId);\n+        return;\n+      }\n+\n+      doCommit(epochId, messages);\n+    }\n+\n+    protected abstract void doCommit(long epochId, WriterCommitMessage[] messages);\n+\n+    protected <T> void commit(SnapshotUpdate<T> snapshotUpdate, long epochId, String description) {\n+      snapshotUpdate.set(QUERY_ID_PROPERTY, queryId);\n+      snapshotUpdate.set(EPOCH_ID_PROPERTY, Long.toString(epochId));\n+      commitOperation(snapshotUpdate, description);\n+    }\n+\n+    private Long getLastCommittedEpochId() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc5MTk5Mg=="}, "originalCommit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "originalPosition": 314}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDk0ODUyMg==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/1776#discussion_r524948522", "createdAt": "2020-11-17T08:01:30Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkWrite.java", "diffHunk": "@@ -236,9 +201,171 @@ protected Table table() {\n     return ImmutableList.of();\n   }\n \n-  @Override\n-  public String toString() {\n-    return String.format(\"IcebergWrite(table=%s, format=%s)\", table, format);\n+  private abstract class BaseBatchWrite implements BatchWrite {\n+    @Override\n+    public DataWriterFactory createBatchWriterFactory(PhysicalWriteInfo info) {\n+      return createWriterFactory();\n+    }\n+\n+    @Override\n+    public void abort(WriterCommitMessage[] messages) {\n+      SparkWrite.this.abort(messages);\n+    }\n+\n+    @Override\n+    public String toString() {\n+      return String.format(\"IcebergBatchWrite(table=%s, format=%s)\", table, format);\n+    }\n+  }\n+\n+  private class BatchAppend extends BaseBatchWrite {\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+      AppendFiles append = table.newAppend();\n+\n+      int numFiles = 0;\n+      for (DataFile file : files(messages)) {\n+        numFiles += 1;\n+        append.appendFile(file);\n+      }\n+\n+      commitOperation(append, String.format(\"append with %d new data files\", numFiles));\n+    }\n+  }\n+\n+  private class DynamicOverwrite extends BaseBatchWrite {\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+      ReplacePartitions dynamicOverwrite = table.newReplacePartitions();\n+\n+      int numFiles = 0;\n+      for (DataFile file : files(messages)) {\n+        numFiles += 1;\n+        dynamicOverwrite.addFile(file);\n+      }\n+\n+      commitOperation(dynamicOverwrite, String.format(\"dynamic partition overwrite with %d new data files\", numFiles));\n+    }\n+  }\n+\n+  private class OverwriteByFilter extends BaseBatchWrite {\n+    private final Expression overwriteExpr;\n+\n+    private OverwriteByFilter(Expression overwriteExpr) {\n+      this.overwriteExpr = overwriteExpr;\n+    }\n+\n+    @Override\n+    public void commit(WriterCommitMessage[] messages) {\n+      OverwriteFiles overwriteFiles = table.newOverwrite();\n+      overwriteFiles.overwriteByRowFilter(overwriteExpr);\n+\n+      int numFiles = 0;\n+      for (DataFile file : files(messages)) {\n+        numFiles += 1;\n+        overwriteFiles.addFile(file);\n+      }\n+\n+      String commitMsg = String.format(\"overwrite by filter %s with %d new data files\", overwriteExpr, numFiles);\n+      commitOperation(overwriteFiles, commitMsg);\n+    }\n+  }\n+\n+  private abstract class BaseStreamingWrite implements StreamingWrite {\n+    private static final String QUERY_ID_PROPERTY = \"spark.sql.streaming.queryId\";\n+    private static final String EPOCH_ID_PROPERTY = \"spark.sql.streaming.epochId\";\n+\n+    protected abstract String mode();\n+\n+    @Override\n+    public StreamingDataWriterFactory createStreamingWriterFactory(PhysicalWriteInfo info) {\n+      return createWriterFactory();\n+    }\n+\n+    @Override\n+    public final void commit(long epochId, WriterCommitMessage[] messages) {\n+      LOG.info(\"Committing epoch {} for query {} in {} mode\", epochId, queryId, mode());\n+\n+      table.refresh();\n+      Long lastCommittedEpochId = getLastCommittedEpochId();\n+      if (lastCommittedEpochId != null && epochId <= lastCommittedEpochId) {\n+        LOG.info(\"Skipping epoch {} for query {} as it was already committed\", epochId, queryId);\n+        return;\n+      }\n+\n+      doCommit(epochId, messages);\n+    }\n+\n+    protected abstract void doCommit(long epochId, WriterCommitMessage[] messages);\n+\n+    protected <T> void commit(SnapshotUpdate<T> snapshotUpdate, long epochId, String description) {\n+      snapshotUpdate.set(QUERY_ID_PROPERTY, queryId);\n+      snapshotUpdate.set(EPOCH_ID_PROPERTY, Long.toString(epochId));\n+      commitOperation(snapshotUpdate, description);\n+    }\n+\n+    private Long getLastCommittedEpochId() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDc5MTk5Mg=="}, "originalCommit": {"oid": "2afa35f5c098f9588f22b7559d5d3f2dc7986eb6"}, "originalPosition": 314}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3460, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}