{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQxNDg3MTAy", "number": 1948, "title": "Spark: Add SQL commands evolve partition specs", "bodyText": "This adds two new ALTER TABLE commands to the extensions module, ADD PARTITION FIELD and DROP PARTITION FIELD.\nTests for the new commands are in TestAlterTablePartitionFields.", "createdAt": "2020-12-16T22:53:42Z", "url": "https://github.com/apache/iceberg/pull/1948", "merged": true, "mergeCommit": {"oid": "d2e5a078299477f9387dab78e0a8eae9d39c7ef2"}, "closed": true, "closedAt": "2020-12-18T23:11:41Z", "author": {"login": "rdblue"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdm3Aq0AH2gAyNTQxNDg3MTAyOjQwOTY4Njg3OWQ2N2NhM2MxMDFkNGRkNjgwOWI3ZjM5MzViYTVhZDU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdncnc_gFqTU1NTcyNTI5Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "409686879d67ca3c101d4dd6809b7f3935ba5ad5", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/409686879d67ca3c101d4dd6809b7f3935ba5ad5", "committedDate": "2020-12-16T22:51:52Z", "message": "Add SQL commands for ADD, DROP PARTITION FIELD."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/251fdf7ab61ea9e5e5f8f3da72b7c165abf274df", "committedDate": "2020-12-16T23:04:16Z", "message": "Support dropping partitions by name."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1MDcxMDU0", "url": "https://github.com/apache/iceberg/pull/1948#pullrequestreview-555071054", "createdAt": "2020-12-17T23:32:39Z", "commit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMzozMjo0MFrOIINGjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMzozMjo0MFrOIINGjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ3NDE5MA==", "bodyText": "So, this seems really unlikely and I can't figure out a good way around it, but if someone had a \"add partition field\" as a column name doing a Spark alter table this would probably give a false positive, but that probably doesn't matter since even then most of the time the iceberg parser is a superset of the delegate parser. Does that sound right or am I off base with the assumption this block?", "url": "https://github.com/apache/iceberg/pull/1948#discussion_r545474190", "createdAt": "2020-12-17T23:32:40Z", "author": {"login": "holdenk"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -94,13 +94,20 @@ class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserI\n    */\n   override def parsePlan(sqlText: String): LogicalPlan = {\n     val sqlTextAfterSubstitution = substitutor.substitute(sqlText)\n-    if (sqlTextAfterSubstitution.toLowerCase(Locale.ROOT).trim().startsWith(\"call\")) {\n+    if (isIcebergCommand(sqlTextAfterSubstitution)) {\n       parse(sqlTextAfterSubstitution) { parser => astBuilder.visit(parser.singleStatement()) }.asInstanceOf[LogicalPlan]\n     } else {\n       delegate.parsePlan(sqlText)\n     }\n   }\n \n+  private def isIcebergCommand(sqlText: String): Boolean = {\n+    val normalized = sqlText.toLowerCase(Locale.ROOT).trim()\n+    normalized.startsWith(\"call\") ||\n+        (normalized.startsWith(\"alter table\") && (\n+            normalized.contains(\"add partition field\") || normalized.contains(\"drop partition field\")))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "originalPosition": 16}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1NzA1NzI4", "url": "https://github.com/apache/iceberg/pull/1948#pullrequestreview-555705728", "createdAt": "2020-12-18T18:11:01Z", "commit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODoxMTowMVrOIItXGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODoxMTowMVrOIItXGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAwMjcxMg==", "bodyText": "Did the apply method not resolve here? Wondering why we needed to modify the anonymous arg", "url": "https://github.com/apache/iceberg/pull/1948#discussion_r546002712", "createdAt": "2020-12-18T18:11:01Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/iceberg/spark/extensions/IcebergSparkSessionExtensions.scala", "diffHunk": "@@ -37,6 +37,6 @@ class IcebergSparkSessionExtensions extends (SparkSessionExtensions => Unit) {\n     // TODO: PullupCorrelatedPredicates should handle row-level operations\n     extensions.injectOptimizerRule { _ => PullupCorrelatedPredicatesInRowLevelOperations }\n     extensions.injectOptimizerRule { _ => RewriteDelete }\n-    extensions.injectPlannerStrategy { _ => ExtendedDataSourceV2Strategy }\n+    extensions.injectPlannerStrategy { spark => ExtendedDataSourceV2Strategy(spark) }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1NzE1NjQ5", "url": "https://github.com/apache/iceberg/pull/1948#pullrequestreview-555715649", "createdAt": "2020-12-18T18:25:24Z", "commit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODoyNToyNVrOIIt3qg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODoyNToyNVrOIIt3qg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAxMTA1MA==", "bodyText": "extra newline?", "url": "https://github.com/apache/iceberg/pull/1948#discussion_r546011050", "createdAt": "2020-12-18T18:25:25Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DropPartitionFieldExec.scala", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.iceberg.spark.Spark3Util\n+import org.apache.iceberg.spark.source.SparkTable\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.connector.catalog.Identifier\n+import org.apache.spark.sql.connector.catalog.TableCatalog\n+import org.apache.spark.sql.connector.expressions.FieldReference\n+import org.apache.spark.sql.connector.expressions.IdentityTransform\n+import org.apache.spark.sql.connector.expressions.Transform\n+\n+case class DropPartitionFieldExec(\n+    catalog: TableCatalog,\n+    ident: Identifier,\n+    transform: Transform) extends V2CommandExec {\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._\n+\n+  override lazy val output: Seq[Attribute] = Nil\n+\n+  override protected def run(): Seq[InternalRow] = {\n+    catalog.loadTable(ident) match {\n+      case iceberg: SparkTable =>\n+        val schema = iceberg.table.schema\n+        transform match {\n+          case IdentityTransform(FieldReference(parts)) if parts.size == 1 && schema.findField(parts.head) == null =>\n+            // the name is not present in the Iceberg schema, so it must be a partition field name, not a column name\n+            iceberg.table.updateSpec()\n+                .removeField(parts.head)\n+                .commit()\n+\n+          case _ =>\n+            iceberg.table.updateSpec()\n+                .removeField(Spark3Util.toIcebergTerm(transform))\n+                .commit()\n+        }\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "originalPosition": 57}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4de1fbb623ae9773b2698cab6bbc56dccad3c60b", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/4de1fbb623ae9773b2698cab6bbc56dccad3c60b", "committedDate": "2020-12-18T18:32:00Z", "message": "Update DropPartitionFieldExec.scala"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1NzIyMDIx", "url": "https://github.com/apache/iceberg/pull/1948#pullrequestreview-555722021", "createdAt": "2020-12-18T18:35:27Z", "commit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODozNToyOFrOIIuLzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODozNToyOFrOIIuLzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAxNjIwNA==", "bodyText": "I don't think you need the inner paren's here", "url": "https://github.com/apache/iceberg/pull/1948#discussion_r546016204", "createdAt": "2020-12-18T18:35:28Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedDataSourceV2Strategy.scala", "diffHunk": "@@ -53,4 +76,18 @@ object ExtendedDataSourceV2Strategy extends Strategy {\n     }\n     new GenericInternalRow(values)\n   }\n+\n+  private object IcebergCatalogAndIdentifier {\n+    def unapply(identifier: Seq[String]): Option[(TableCatalog, Identifier)] = {\n+      val catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier.asJava)\n+      catalogAndIdentifier.catalog match {\n+        case icebergCatalog: SparkCatalog =>\n+          Some((icebergCatalog, catalogAndIdentifier.identifier))\n+        case icebergCatalog: SparkSessionCatalog[_] =>\n+          Some((icebergCatalog, catalogAndIdentifier.identifier))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "originalPosition": 67}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1NzI1Mjk2", "url": "https://github.com/apache/iceberg/pull/1948#pullrequestreview-555725296", "createdAt": "2020-12-18T18:40:43Z", "commit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3286, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}