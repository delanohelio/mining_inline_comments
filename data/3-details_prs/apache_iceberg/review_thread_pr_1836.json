{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI4MTM5NjMw", "number": 1836, "reviewThreads": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwOToxODozMlrOE96s_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQyMTozOToxNlrOE_WZwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMzMzYwMzgwOnYy", "diffSide": "RIGHT", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkAppenderFactory.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yN1QwOToxODozMlrOH62oLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjoxOTozNlrOH8QH2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTQ3NDQ3OA==", "bodyText": "@rdblue   Here I use the parquet writer which is built from <path, pos, row> schema, rather than the writer built from row schema.\nI tried to hide all the <path, pos, row> write logic inside the Parquet#buildPositionWriter, but failed to make it work.  I passed a function to this builder by creating a  parquet writer built by rowSchema, and construct a PositionDeleteStructWriter with a path string writer,  a pos long writer and the  row struct writer,   it does not work because the currentPath for here has been messed up.   Still figuring out how to make this work in a graceful way...", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r531474478", "createdAt": "2020-11-27T09:18:32Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkAppenderFactory.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.io.UncheckedIOException;\n+import java.util.Map;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.deletes.EqualityDeleteWriter;\n+import org.apache.iceberg.deletes.PositionDeleteWriter;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkOrcWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.io.DataWriter;\n+import org.apache.iceberg.io.DeleteSchemaUtil;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileAppenderFactory;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class FlinkAppenderFactory implements FileAppenderFactory<RowData>, Serializable {\n+  private final Schema schema;\n+  private final RowType flinkSchema;\n+  private final Map<String, String> props;\n+  private final PartitionSpec spec;\n+  private final int[] equalityFieldIds;\n+  private final Schema eqDeleteRowSchema;\n+  private final Schema posDeleteRowSchema;\n+\n+  private RowType eqDeleteFlinkSchema = null;\n+  private RowType posDeleteFlinkSchema = null;\n+\n+  public FlinkAppenderFactory(Schema schema, RowType flinkSchema, Map<String, String> props, PartitionSpec spec) {\n+    this(schema, flinkSchema, props, spec, null, schema, null);\n+  }\n+\n+  public FlinkAppenderFactory(Schema schema, RowType flinkSchema, Map<String, String> props,\n+                              PartitionSpec spec, int[] equalityFieldIds,\n+                              Schema eqDeleteRowSchema, Schema posDeleteRowSchema) {\n+    this.schema = schema;\n+    this.flinkSchema = flinkSchema;\n+    this.props = props;\n+    this.spec = spec;\n+    this.equalityFieldIds = equalityFieldIds;\n+    this.eqDeleteRowSchema = eqDeleteRowSchema;\n+    this.posDeleteRowSchema = posDeleteRowSchema;\n+  }\n+\n+  private RowType lazyEqDeleteFlinkSchema() {\n+    if (eqDeleteFlinkSchema == null) {\n+      Preconditions.checkNotNull(eqDeleteRowSchema, \"Equality delete row schema shouldn't be null\");\n+      this.eqDeleteFlinkSchema = FlinkSchemaUtil.convert(eqDeleteRowSchema);\n+    }\n+    return eqDeleteFlinkSchema;\n+  }\n+\n+  private RowType lazyPosDeleteFlinkSchema() {\n+    if (posDeleteFlinkSchema == null) {\n+      Preconditions.checkNotNull(posDeleteRowSchema, \"Pos-delete row schema shouldn't be null\");\n+      this.posDeleteFlinkSchema = FlinkSchemaUtil.convert(posDeleteRowSchema);\n+    }\n+    return this.posDeleteFlinkSchema;\n+  }\n+\n+  @Override\n+  public FileAppender<RowData> newAppender(OutputFile outputFile, FileFormat format) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.write(outputFile)\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(flinkSchema))\n+              .setAll(props)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        case ORC:\n+          return ORC.write(outputFile)\n+              .createWriterFunc((iSchema, typDesc) -> FlinkOrcWriter.buildWriter(flinkSchema, iSchema))\n+              .setAll(props)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        case PARQUET:\n+          return Parquet.write(outputFile)\n+              .createWriterFunc(msgType -> FlinkParquetWriters.buildWriter(flinkSchema, msgType))\n+              .setAll(props)\n+              .metricsConfig(metricsConfig)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public DataWriter<RowData> newDataWriter(EncryptedOutputFile file, FileFormat format, StructLike partition) {\n+    return new DataWriter<>(\n+        newAppender(file.encryptingOutputFile(), format), format,\n+        file.encryptingOutputFile().location(), spec, partition, file.keyMetadata());\n+  }\n+\n+  @Override\n+  public EqualityDeleteWriter<RowData> newEqDeleteWriter(EncryptedOutputFile outputFile, FileFormat format,\n+                                                         StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(lazyEqDeleteFlinkSchema()))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        case PARQUET:\n+          return Parquet.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(msgType -> FlinkParquetWriters.buildWriter(lazyEqDeleteFlinkSchema(), msgType))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .metricsConfig(metricsConfig)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public PositionDeleteWriter<RowData> newPosDeleteWriter(EncryptedOutputFile outputFile, FileFormat format,\n+                                                          StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(lazyPosDeleteFlinkSchema()))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .rowSchema(posDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .buildPositionWriter();\n+\n+        case PARQUET:\n+          RowType flinkPosDeleteSchema = FlinkSchemaUtil.convert(DeleteSchemaUtil.posDeleteSchema(posDeleteRowSchema));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjE1MTg5Mg==", "bodyText": "Let me provide more details, assume that we want to build a pos delete writer with row schema <id long, data string>.\nThe straightforward way is to build a struct writer like this:\n    PositionDeleteStructWriter\n               |\n               |---------->  StringWriter (file_path field)\n               |---------->  LongWriter   (pos       field)\n               |\n               |\n               |---------->  StructWriter (row  field)\n                                 |----------> LongWriter    (id   field)\n                                 |----------> StringWriter  (data field)\n\nThe question is how to build the PositionDeleteStructWriter, if we just provide a function to create the row field's StructWriter for flink, and then concat the <StringWriter, LongWriter, StructWriter> to PositionDeleteStructWriter constructor,\nthen the problem is:  the leaf node ColumnWriter will have an incorrect ColumnDescriptor:\n     PositionDeleteStructWriter\n               |\n               |---------->  StringWriter {<file_path>,type,maxRep,maxDef}\n               |---------->  LongWriter   {<pos>,type,maxRep,maxDef}\n               |\n               |\n               |---------->  StructWriter \n                                 |----------> LongWriter    {<id>, type, maxRep, maxDef}\n                                 |----------> StringWriter  {<data>,type, maxRep, maxDef}\nIn fact, for this example, the correct ColumnWriter should be:\n     PositionDeleteStructWriter\n               |\n               |---------->  StringWriter {<file_path>,type,maxRep,maxDef}\n               |---------->  LongWriter   {<pos>,type,maxRep,maxDef}\n               |\n               |\n               |---------->  StructWriter \n                                 |----------> LongWriter    {<row,id>, type, maxRep, maxDef}\n                                 |----------> StringWriter  {<row,data>,type, maxRep, maxDef}", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532151892", "createdAt": "2020-11-29T04:06:20Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkAppenderFactory.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.io.UncheckedIOException;\n+import java.util.Map;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.deletes.EqualityDeleteWriter;\n+import org.apache.iceberg.deletes.PositionDeleteWriter;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkOrcWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.io.DataWriter;\n+import org.apache.iceberg.io.DeleteSchemaUtil;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileAppenderFactory;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class FlinkAppenderFactory implements FileAppenderFactory<RowData>, Serializable {\n+  private final Schema schema;\n+  private final RowType flinkSchema;\n+  private final Map<String, String> props;\n+  private final PartitionSpec spec;\n+  private final int[] equalityFieldIds;\n+  private final Schema eqDeleteRowSchema;\n+  private final Schema posDeleteRowSchema;\n+\n+  private RowType eqDeleteFlinkSchema = null;\n+  private RowType posDeleteFlinkSchema = null;\n+\n+  public FlinkAppenderFactory(Schema schema, RowType flinkSchema, Map<String, String> props, PartitionSpec spec) {\n+    this(schema, flinkSchema, props, spec, null, schema, null);\n+  }\n+\n+  public FlinkAppenderFactory(Schema schema, RowType flinkSchema, Map<String, String> props,\n+                              PartitionSpec spec, int[] equalityFieldIds,\n+                              Schema eqDeleteRowSchema, Schema posDeleteRowSchema) {\n+    this.schema = schema;\n+    this.flinkSchema = flinkSchema;\n+    this.props = props;\n+    this.spec = spec;\n+    this.equalityFieldIds = equalityFieldIds;\n+    this.eqDeleteRowSchema = eqDeleteRowSchema;\n+    this.posDeleteRowSchema = posDeleteRowSchema;\n+  }\n+\n+  private RowType lazyEqDeleteFlinkSchema() {\n+    if (eqDeleteFlinkSchema == null) {\n+      Preconditions.checkNotNull(eqDeleteRowSchema, \"Equality delete row schema shouldn't be null\");\n+      this.eqDeleteFlinkSchema = FlinkSchemaUtil.convert(eqDeleteRowSchema);\n+    }\n+    return eqDeleteFlinkSchema;\n+  }\n+\n+  private RowType lazyPosDeleteFlinkSchema() {\n+    if (posDeleteFlinkSchema == null) {\n+      Preconditions.checkNotNull(posDeleteRowSchema, \"Pos-delete row schema shouldn't be null\");\n+      this.posDeleteFlinkSchema = FlinkSchemaUtil.convert(posDeleteRowSchema);\n+    }\n+    return this.posDeleteFlinkSchema;\n+  }\n+\n+  @Override\n+  public FileAppender<RowData> newAppender(OutputFile outputFile, FileFormat format) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.write(outputFile)\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(flinkSchema))\n+              .setAll(props)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        case ORC:\n+          return ORC.write(outputFile)\n+              .createWriterFunc((iSchema, typDesc) -> FlinkOrcWriter.buildWriter(flinkSchema, iSchema))\n+              .setAll(props)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        case PARQUET:\n+          return Parquet.write(outputFile)\n+              .createWriterFunc(msgType -> FlinkParquetWriters.buildWriter(flinkSchema, msgType))\n+              .setAll(props)\n+              .metricsConfig(metricsConfig)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public DataWriter<RowData> newDataWriter(EncryptedOutputFile file, FileFormat format, StructLike partition) {\n+    return new DataWriter<>(\n+        newAppender(file.encryptingOutputFile(), format), format,\n+        file.encryptingOutputFile().location(), spec, partition, file.keyMetadata());\n+  }\n+\n+  @Override\n+  public EqualityDeleteWriter<RowData> newEqDeleteWriter(EncryptedOutputFile outputFile, FileFormat format,\n+                                                         StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(lazyEqDeleteFlinkSchema()))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        case PARQUET:\n+          return Parquet.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(msgType -> FlinkParquetWriters.buildWriter(lazyEqDeleteFlinkSchema(), msgType))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .metricsConfig(metricsConfig)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public PositionDeleteWriter<RowData> newPosDeleteWriter(EncryptedOutputFile outputFile, FileFormat format,\n+                                                          StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(lazyPosDeleteFlinkSchema()))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .rowSchema(posDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .buildPositionWriter();\n+\n+        case PARQUET:\n+          RowType flinkPosDeleteSchema = FlinkSchemaUtil.convert(DeleteSchemaUtil.posDeleteSchema(posDeleteRowSchema));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTQ3NDQ3OA=="}, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjE1MzcwMw==", "bodyText": "In my mind, there are two ways to solve the problem, the first one is the current version: Just use the completed scheme which has included the file_path field and pos field to build the flink/spark struct writer, tradeoff\nis exposing few details to upper, such as we need to convert the CharSequence to flink's StringData before we write the file_path value (see the ParquetValueWriters.PathPosAccessor).  The current version is easy to implement.\nAnother way is: call the row schema's createWriterFunc in parquet visitor when building the whole struct writer by traversing the columns. In that visitor, we could add the row prefix to the fieldNames and finally the ColumnDescriptor won't be messed up.\nThat seems to be more complex when compared to the first version. The good thing is hiding all the constructing details inside the builder.\nIMO, I'd prefer to use the current version because the FlinkAppenderFactory and SparkAppenderFactory has already hidden the details for upper layer. People won't likely to write their own AppenderFactory, even if they want to write their own it will be easy when following the current flink/spark appender factory.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532153703", "createdAt": "2020-11-29T04:29:50Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkAppenderFactory.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.io.UncheckedIOException;\n+import java.util.Map;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.deletes.EqualityDeleteWriter;\n+import org.apache.iceberg.deletes.PositionDeleteWriter;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkOrcWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.io.DataWriter;\n+import org.apache.iceberg.io.DeleteSchemaUtil;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileAppenderFactory;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class FlinkAppenderFactory implements FileAppenderFactory<RowData>, Serializable {\n+  private final Schema schema;\n+  private final RowType flinkSchema;\n+  private final Map<String, String> props;\n+  private final PartitionSpec spec;\n+  private final int[] equalityFieldIds;\n+  private final Schema eqDeleteRowSchema;\n+  private final Schema posDeleteRowSchema;\n+\n+  private RowType eqDeleteFlinkSchema = null;\n+  private RowType posDeleteFlinkSchema = null;\n+\n+  public FlinkAppenderFactory(Schema schema, RowType flinkSchema, Map<String, String> props, PartitionSpec spec) {\n+    this(schema, flinkSchema, props, spec, null, schema, null);\n+  }\n+\n+  public FlinkAppenderFactory(Schema schema, RowType flinkSchema, Map<String, String> props,\n+                              PartitionSpec spec, int[] equalityFieldIds,\n+                              Schema eqDeleteRowSchema, Schema posDeleteRowSchema) {\n+    this.schema = schema;\n+    this.flinkSchema = flinkSchema;\n+    this.props = props;\n+    this.spec = spec;\n+    this.equalityFieldIds = equalityFieldIds;\n+    this.eqDeleteRowSchema = eqDeleteRowSchema;\n+    this.posDeleteRowSchema = posDeleteRowSchema;\n+  }\n+\n+  private RowType lazyEqDeleteFlinkSchema() {\n+    if (eqDeleteFlinkSchema == null) {\n+      Preconditions.checkNotNull(eqDeleteRowSchema, \"Equality delete row schema shouldn't be null\");\n+      this.eqDeleteFlinkSchema = FlinkSchemaUtil.convert(eqDeleteRowSchema);\n+    }\n+    return eqDeleteFlinkSchema;\n+  }\n+\n+  private RowType lazyPosDeleteFlinkSchema() {\n+    if (posDeleteFlinkSchema == null) {\n+      Preconditions.checkNotNull(posDeleteRowSchema, \"Pos-delete row schema shouldn't be null\");\n+      this.posDeleteFlinkSchema = FlinkSchemaUtil.convert(posDeleteRowSchema);\n+    }\n+    return this.posDeleteFlinkSchema;\n+  }\n+\n+  @Override\n+  public FileAppender<RowData> newAppender(OutputFile outputFile, FileFormat format) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.write(outputFile)\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(flinkSchema))\n+              .setAll(props)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        case ORC:\n+          return ORC.write(outputFile)\n+              .createWriterFunc((iSchema, typDesc) -> FlinkOrcWriter.buildWriter(flinkSchema, iSchema))\n+              .setAll(props)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        case PARQUET:\n+          return Parquet.write(outputFile)\n+              .createWriterFunc(msgType -> FlinkParquetWriters.buildWriter(flinkSchema, msgType))\n+              .setAll(props)\n+              .metricsConfig(metricsConfig)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public DataWriter<RowData> newDataWriter(EncryptedOutputFile file, FileFormat format, StructLike partition) {\n+    return new DataWriter<>(\n+        newAppender(file.encryptingOutputFile(), format), format,\n+        file.encryptingOutputFile().location(), spec, partition, file.keyMetadata());\n+  }\n+\n+  @Override\n+  public EqualityDeleteWriter<RowData> newEqDeleteWriter(EncryptedOutputFile outputFile, FileFormat format,\n+                                                         StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(lazyEqDeleteFlinkSchema()))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        case PARQUET:\n+          return Parquet.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(msgType -> FlinkParquetWriters.buildWriter(lazyEqDeleteFlinkSchema(), msgType))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .metricsConfig(metricsConfig)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public PositionDeleteWriter<RowData> newPosDeleteWriter(EncryptedOutputFile outputFile, FileFormat format,\n+                                                          StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(lazyPosDeleteFlinkSchema()))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .rowSchema(posDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .buildPositionWriter();\n+\n+        case PARQUET:\n+          RowType flinkPosDeleteSchema = FlinkSchemaUtil.convert(DeleteSchemaUtil.posDeleteSchema(posDeleteRowSchema));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTQ3NDQ3OA=="}, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjI2NTk4MQ==", "bodyText": "Sorry, I also hit the column descriptor problem when I tried to update this. I have it working for Avro, but not for Parquet. I should have commented with the results of my work so you didn't try it as well.\nI agree with you that this would be really difficult to solve and it is probably not worth it. We'll just have to go with the existing solution for Parquet until we get a better Parquet API that can load columns by ID rather than by path.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532265981", "createdAt": "2020-11-29T21:03:42Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkAppenderFactory.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.io.UncheckedIOException;\n+import java.util.Map;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.deletes.EqualityDeleteWriter;\n+import org.apache.iceberg.deletes.PositionDeleteWriter;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkOrcWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.io.DataWriter;\n+import org.apache.iceberg.io.DeleteSchemaUtil;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileAppenderFactory;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class FlinkAppenderFactory implements FileAppenderFactory<RowData>, Serializable {\n+  private final Schema schema;\n+  private final RowType flinkSchema;\n+  private final Map<String, String> props;\n+  private final PartitionSpec spec;\n+  private final int[] equalityFieldIds;\n+  private final Schema eqDeleteRowSchema;\n+  private final Schema posDeleteRowSchema;\n+\n+  private RowType eqDeleteFlinkSchema = null;\n+  private RowType posDeleteFlinkSchema = null;\n+\n+  public FlinkAppenderFactory(Schema schema, RowType flinkSchema, Map<String, String> props, PartitionSpec spec) {\n+    this(schema, flinkSchema, props, spec, null, schema, null);\n+  }\n+\n+  public FlinkAppenderFactory(Schema schema, RowType flinkSchema, Map<String, String> props,\n+                              PartitionSpec spec, int[] equalityFieldIds,\n+                              Schema eqDeleteRowSchema, Schema posDeleteRowSchema) {\n+    this.schema = schema;\n+    this.flinkSchema = flinkSchema;\n+    this.props = props;\n+    this.spec = spec;\n+    this.equalityFieldIds = equalityFieldIds;\n+    this.eqDeleteRowSchema = eqDeleteRowSchema;\n+    this.posDeleteRowSchema = posDeleteRowSchema;\n+  }\n+\n+  private RowType lazyEqDeleteFlinkSchema() {\n+    if (eqDeleteFlinkSchema == null) {\n+      Preconditions.checkNotNull(eqDeleteRowSchema, \"Equality delete row schema shouldn't be null\");\n+      this.eqDeleteFlinkSchema = FlinkSchemaUtil.convert(eqDeleteRowSchema);\n+    }\n+    return eqDeleteFlinkSchema;\n+  }\n+\n+  private RowType lazyPosDeleteFlinkSchema() {\n+    if (posDeleteFlinkSchema == null) {\n+      Preconditions.checkNotNull(posDeleteRowSchema, \"Pos-delete row schema shouldn't be null\");\n+      this.posDeleteFlinkSchema = FlinkSchemaUtil.convert(posDeleteRowSchema);\n+    }\n+    return this.posDeleteFlinkSchema;\n+  }\n+\n+  @Override\n+  public FileAppender<RowData> newAppender(OutputFile outputFile, FileFormat format) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.write(outputFile)\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(flinkSchema))\n+              .setAll(props)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        case ORC:\n+          return ORC.write(outputFile)\n+              .createWriterFunc((iSchema, typDesc) -> FlinkOrcWriter.buildWriter(flinkSchema, iSchema))\n+              .setAll(props)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        case PARQUET:\n+          return Parquet.write(outputFile)\n+              .createWriterFunc(msgType -> FlinkParquetWriters.buildWriter(flinkSchema, msgType))\n+              .setAll(props)\n+              .metricsConfig(metricsConfig)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public DataWriter<RowData> newDataWriter(EncryptedOutputFile file, FileFormat format, StructLike partition) {\n+    return new DataWriter<>(\n+        newAppender(file.encryptingOutputFile(), format), format,\n+        file.encryptingOutputFile().location(), spec, partition, file.keyMetadata());\n+  }\n+\n+  @Override\n+  public EqualityDeleteWriter<RowData> newEqDeleteWriter(EncryptedOutputFile outputFile, FileFormat format,\n+                                                         StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(lazyEqDeleteFlinkSchema()))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        case PARQUET:\n+          return Parquet.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(msgType -> FlinkParquetWriters.buildWriter(lazyEqDeleteFlinkSchema(), msgType))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .metricsConfig(metricsConfig)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public PositionDeleteWriter<RowData> newPosDeleteWriter(EncryptedOutputFile outputFile, FileFormat format,\n+                                                          StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(lazyPosDeleteFlinkSchema()))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .rowSchema(posDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .buildPositionWriter();\n+\n+        case PARQUET:\n+          RowType flinkPosDeleteSchema = FlinkSchemaUtil.convert(DeleteSchemaUtil.posDeleteSchema(posDeleteRowSchema));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTQ3NDQ3OA=="}, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjU2NTg5NA==", "bodyText": "we get a better Parquet API that can load columns by ID rather than by path.\n\nSounds like it's the correct direction to solve this issue, will that need much work to accomplish (I'm not quite sure).\nBTW,  do you have other concern about this patch ?", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532565894", "createdAt": "2020-11-30T12:37:31Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkAppenderFactory.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.io.UncheckedIOException;\n+import java.util.Map;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.deletes.EqualityDeleteWriter;\n+import org.apache.iceberg.deletes.PositionDeleteWriter;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkOrcWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.io.DataWriter;\n+import org.apache.iceberg.io.DeleteSchemaUtil;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileAppenderFactory;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class FlinkAppenderFactory implements FileAppenderFactory<RowData>, Serializable {\n+  private final Schema schema;\n+  private final RowType flinkSchema;\n+  private final Map<String, String> props;\n+  private final PartitionSpec spec;\n+  private final int[] equalityFieldIds;\n+  private final Schema eqDeleteRowSchema;\n+  private final Schema posDeleteRowSchema;\n+\n+  private RowType eqDeleteFlinkSchema = null;\n+  private RowType posDeleteFlinkSchema = null;\n+\n+  public FlinkAppenderFactory(Schema schema, RowType flinkSchema, Map<String, String> props, PartitionSpec spec) {\n+    this(schema, flinkSchema, props, spec, null, schema, null);\n+  }\n+\n+  public FlinkAppenderFactory(Schema schema, RowType flinkSchema, Map<String, String> props,\n+                              PartitionSpec spec, int[] equalityFieldIds,\n+                              Schema eqDeleteRowSchema, Schema posDeleteRowSchema) {\n+    this.schema = schema;\n+    this.flinkSchema = flinkSchema;\n+    this.props = props;\n+    this.spec = spec;\n+    this.equalityFieldIds = equalityFieldIds;\n+    this.eqDeleteRowSchema = eqDeleteRowSchema;\n+    this.posDeleteRowSchema = posDeleteRowSchema;\n+  }\n+\n+  private RowType lazyEqDeleteFlinkSchema() {\n+    if (eqDeleteFlinkSchema == null) {\n+      Preconditions.checkNotNull(eqDeleteRowSchema, \"Equality delete row schema shouldn't be null\");\n+      this.eqDeleteFlinkSchema = FlinkSchemaUtil.convert(eqDeleteRowSchema);\n+    }\n+    return eqDeleteFlinkSchema;\n+  }\n+\n+  private RowType lazyPosDeleteFlinkSchema() {\n+    if (posDeleteFlinkSchema == null) {\n+      Preconditions.checkNotNull(posDeleteRowSchema, \"Pos-delete row schema shouldn't be null\");\n+      this.posDeleteFlinkSchema = FlinkSchemaUtil.convert(posDeleteRowSchema);\n+    }\n+    return this.posDeleteFlinkSchema;\n+  }\n+\n+  @Override\n+  public FileAppender<RowData> newAppender(OutputFile outputFile, FileFormat format) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.write(outputFile)\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(flinkSchema))\n+              .setAll(props)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        case ORC:\n+          return ORC.write(outputFile)\n+              .createWriterFunc((iSchema, typDesc) -> FlinkOrcWriter.buildWriter(flinkSchema, iSchema))\n+              .setAll(props)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        case PARQUET:\n+          return Parquet.write(outputFile)\n+              .createWriterFunc(msgType -> FlinkParquetWriters.buildWriter(flinkSchema, msgType))\n+              .setAll(props)\n+              .metricsConfig(metricsConfig)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public DataWriter<RowData> newDataWriter(EncryptedOutputFile file, FileFormat format, StructLike partition) {\n+    return new DataWriter<>(\n+        newAppender(file.encryptingOutputFile(), format), format,\n+        file.encryptingOutputFile().location(), spec, partition, file.keyMetadata());\n+  }\n+\n+  @Override\n+  public EqualityDeleteWriter<RowData> newEqDeleteWriter(EncryptedOutputFile outputFile, FileFormat format,\n+                                                         StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(lazyEqDeleteFlinkSchema()))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        case PARQUET:\n+          return Parquet.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(msgType -> FlinkParquetWriters.buildWriter(lazyEqDeleteFlinkSchema(), msgType))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .metricsConfig(metricsConfig)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public PositionDeleteWriter<RowData> newPosDeleteWriter(EncryptedOutputFile outputFile, FileFormat format,\n+                                                          StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(lazyPosDeleteFlinkSchema()))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .rowSchema(posDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .buildPositionWriter();\n+\n+        case PARQUET:\n+          RowType flinkPosDeleteSchema = FlinkSchemaUtil.convert(DeleteSchemaUtil.posDeleteSchema(posDeleteRowSchema));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTQ3NDQ3OA=="}, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk0MDc2Mw==", "bodyText": "I'm reviewing it now. I just wanted to comment yesterday to let you know about the Parquet problem.\nFor the amount of work in Parquet, it shouldn't be too difficult for just this, but we need to support field IDs more generally over there and that is going to be a big project with all of the little things that need to be done.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532940763", "createdAt": "2020-11-30T22:19:36Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkAppenderFactory.java", "diffHunk": "@@ -0,0 +1,231 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.io.UncheckedIOException;\n+import java.util.Map;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.deletes.EqualityDeleteWriter;\n+import org.apache.iceberg.deletes.PositionDeleteWriter;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkOrcWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.io.DataWriter;\n+import org.apache.iceberg.io.DeleteSchemaUtil;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileAppenderFactory;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.parquet.ParquetValueWriters;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class FlinkAppenderFactory implements FileAppenderFactory<RowData>, Serializable {\n+  private final Schema schema;\n+  private final RowType flinkSchema;\n+  private final Map<String, String> props;\n+  private final PartitionSpec spec;\n+  private final int[] equalityFieldIds;\n+  private final Schema eqDeleteRowSchema;\n+  private final Schema posDeleteRowSchema;\n+\n+  private RowType eqDeleteFlinkSchema = null;\n+  private RowType posDeleteFlinkSchema = null;\n+\n+  public FlinkAppenderFactory(Schema schema, RowType flinkSchema, Map<String, String> props, PartitionSpec spec) {\n+    this(schema, flinkSchema, props, spec, null, schema, null);\n+  }\n+\n+  public FlinkAppenderFactory(Schema schema, RowType flinkSchema, Map<String, String> props,\n+                              PartitionSpec spec, int[] equalityFieldIds,\n+                              Schema eqDeleteRowSchema, Schema posDeleteRowSchema) {\n+    this.schema = schema;\n+    this.flinkSchema = flinkSchema;\n+    this.props = props;\n+    this.spec = spec;\n+    this.equalityFieldIds = equalityFieldIds;\n+    this.eqDeleteRowSchema = eqDeleteRowSchema;\n+    this.posDeleteRowSchema = posDeleteRowSchema;\n+  }\n+\n+  private RowType lazyEqDeleteFlinkSchema() {\n+    if (eqDeleteFlinkSchema == null) {\n+      Preconditions.checkNotNull(eqDeleteRowSchema, \"Equality delete row schema shouldn't be null\");\n+      this.eqDeleteFlinkSchema = FlinkSchemaUtil.convert(eqDeleteRowSchema);\n+    }\n+    return eqDeleteFlinkSchema;\n+  }\n+\n+  private RowType lazyPosDeleteFlinkSchema() {\n+    if (posDeleteFlinkSchema == null) {\n+      Preconditions.checkNotNull(posDeleteRowSchema, \"Pos-delete row schema shouldn't be null\");\n+      this.posDeleteFlinkSchema = FlinkSchemaUtil.convert(posDeleteRowSchema);\n+    }\n+    return this.posDeleteFlinkSchema;\n+  }\n+\n+  @Override\n+  public FileAppender<RowData> newAppender(OutputFile outputFile, FileFormat format) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.write(outputFile)\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(flinkSchema))\n+              .setAll(props)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        case ORC:\n+          return ORC.write(outputFile)\n+              .createWriterFunc((iSchema, typDesc) -> FlinkOrcWriter.buildWriter(flinkSchema, iSchema))\n+              .setAll(props)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        case PARQUET:\n+          return Parquet.write(outputFile)\n+              .createWriterFunc(msgType -> FlinkParquetWriters.buildWriter(flinkSchema, msgType))\n+              .setAll(props)\n+              .metricsConfig(metricsConfig)\n+              .schema(schema)\n+              .overwrite()\n+              .build();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public DataWriter<RowData> newDataWriter(EncryptedOutputFile file, FileFormat format, StructLike partition) {\n+    return new DataWriter<>(\n+        newAppender(file.encryptingOutputFile(), format), format,\n+        file.encryptingOutputFile().location(), spec, partition, file.keyMetadata());\n+  }\n+\n+  @Override\n+  public EqualityDeleteWriter<RowData> newEqDeleteWriter(EncryptedOutputFile outputFile, FileFormat format,\n+                                                         StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(lazyEqDeleteFlinkSchema()))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        case PARQUET:\n+          return Parquet.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(msgType -> FlinkParquetWriters.buildWriter(lazyEqDeleteFlinkSchema(), msgType))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .metricsConfig(metricsConfig)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public PositionDeleteWriter<RowData> newPosDeleteWriter(EncryptedOutputFile outputFile, FileFormat format,\n+                                                          StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(outputFile.encryptingOutputFile())\n+              .createWriterFunc(ignore -> new FlinkAvroWriter(lazyPosDeleteFlinkSchema()))\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(props)\n+              .rowSchema(posDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(outputFile.keyMetadata())\n+              .buildPositionWriter();\n+\n+        case PARQUET:\n+          RowType flinkPosDeleteSchema = FlinkSchemaUtil.convert(DeleteSchemaUtil.posDeleteSchema(posDeleteRowSchema));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTQ3NDQ3OA=="}, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 198}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzQ3MjQ0OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/org/apache/iceberg/data/GenericAppenderFactory.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMTo1OToyN1rOH8PhjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQyMToyNjo0MVrOH9AZ0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzMDk1Nw==", "bodyText": "Could this be the default implementation of newDataWriter? It looks the same across all of the implementations.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532930957", "createdAt": "2020-11-30T21:59:27Z", "author": {"login": "rdblue"}, "path": "data/src/main/java/org/apache/iceberg/data/GenericAppenderFactory.java", "diffHunk": "@@ -95,4 +115,88 @@ public GenericAppenderFactory setAll(Map<String, String> properties) {\n       throw new UncheckedIOException(e);\n     }\n   }\n+\n+  @Override\n+  public org.apache.iceberg.io.DataWriter<Record> newDataWriter(EncryptedOutputFile file, FileFormat format,\n+                                                                StructLike partition) {\n+    return new org.apache.iceberg.io.DataWriter<>(\n+        newAppender(file.encryptingOutputFile(), format), format,\n+        file.encryptingOutputFile().location(), spec, partition, file.keyMetadata());\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwNDk5OA==", "bodyText": "I did not move it to be an default implementation for newDataWriter  because the DataWriter construction will need the the partition spec.  It's not friendly to expose an extra method spec() in FileAppenderFactory ( so that those subclass could pass the correct spec object to DataWriter).", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r533104998", "createdAt": "2020-12-01T06:43:40Z", "author": {"login": "openinx"}, "path": "data/src/main/java/org/apache/iceberg/data/GenericAppenderFactory.java", "diffHunk": "@@ -95,4 +115,88 @@ public GenericAppenderFactory setAll(Map<String, String> properties) {\n       throw new UncheckedIOException(e);\n     }\n   }\n+\n+  @Override\n+  public org.apache.iceberg.io.DataWriter<Record> newDataWriter(EncryptedOutputFile file, FileFormat format,\n+                                                                StructLike partition) {\n+    return new org.apache.iceberg.io.DataWriter<>(\n+        newAppender(file.encryptingOutputFile(), format), format,\n+        file.encryptingOutputFile().location(), spec, partition, file.keyMetadata());\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzMDk1Nw=="}, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzczMTc5NQ==", "bodyText": "Sounds good, thanks for explaining it to me.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r533731795", "createdAt": "2020-12-01T21:26:41Z", "author": {"login": "rdblue"}, "path": "data/src/main/java/org/apache/iceberg/data/GenericAppenderFactory.java", "diffHunk": "@@ -95,4 +115,88 @@ public GenericAppenderFactory setAll(Map<String, String> properties) {\n       throw new UncheckedIOException(e);\n     }\n   }\n+\n+  @Override\n+  public org.apache.iceberg.io.DataWriter<Record> newDataWriter(EncryptedOutputFile file, FileFormat format,\n+                                                                StructLike partition) {\n+    return new org.apache.iceberg.io.DataWriter<>(\n+        newAppender(file.encryptingOutputFile(), format), format,\n+        file.encryptingOutputFile().location(), spec, partition, file.keyMetadata());\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzMDk1Nw=="}, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzQ3NTI3OnYy", "diffSide": "RIGHT", "path": "data/src/main/java/org/apache/iceberg/data/GenericAppenderFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjowMDoxN1rOH8PjQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjowMDoxN1rOH8PjQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzMTM5Mw==", "bodyText": "I think it should be \"unsupported\" rather than \"unknown\" because ORC is known, but not supported.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532931393", "createdAt": "2020-11-30T22:00:17Z", "author": {"login": "rdblue"}, "path": "data/src/main/java/org/apache/iceberg/data/GenericAppenderFactory.java", "diffHunk": "@@ -95,4 +115,88 @@ public GenericAppenderFactory setAll(Map<String, String> properties) {\n       throw new UncheckedIOException(e);\n     }\n   }\n+\n+  @Override\n+  public org.apache.iceberg.io.DataWriter<Record> newDataWriter(EncryptedOutputFile file, FileFormat format,\n+                                                                StructLike partition) {\n+    return new org.apache.iceberg.io.DataWriter<>(\n+        newAppender(file.encryptingOutputFile(), format), format,\n+        file.encryptingOutputFile().location(), spec, partition, file.keyMetadata());\n+  }\n+\n+  @Override\n+  public EqualityDeleteWriter<Record> newEqDeleteWriter(EncryptedOutputFile file, FileFormat format,\n+                                                        StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(config);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(file.encryptingOutputFile())\n+              .createWriterFunc(DataWriter::create)\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(config)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(file.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        case PARQUET:\n+          return Parquet.writeDeletes(file.encryptingOutputFile())\n+              .createWriterFunc(GenericParquetWriter::buildWriter)\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(config)\n+              .metricsConfig(metricsConfig)\n+              .rowSchema(eqDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(file.keyMetadata())\n+              .equalityFieldIds(equalityFieldIds)\n+              .buildEqualityWriter();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  @Override\n+  public PositionDeleteWriter<Record> newPosDeleteWriter(EncryptedOutputFile file, FileFormat format,\n+                                                         StructLike partition) {\n+    MetricsConfig metricsConfig = MetricsConfig.fromProperties(config);\n+    try {\n+      switch (format) {\n+        case AVRO:\n+          return Avro.writeDeletes(file.encryptingOutputFile())\n+              .createWriterFunc(DataWriter::create)\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(config)\n+              .rowSchema(posDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(file.keyMetadata())\n+              .buildPositionWriter();\n+\n+        case PARQUET:\n+          return Parquet.writeDeletes(file.encryptingOutputFile())\n+              .createWriterFunc(GenericParquetWriter::buildWriter)\n+              .withPartition(partition)\n+              .overwrite()\n+              .setAll(config)\n+              .metricsConfig(metricsConfig)\n+              .rowSchema(posDeleteRowSchema)\n+              .withSpec(spec)\n+              .withKeyMetadata(file.keyMetadata())\n+              .buildPositionWriter();\n+\n+        default:\n+          throw new UnsupportedOperationException(\"Cannot write unknown file format: \" + format);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzQ5ODYyOnYy", "diffSide": "RIGHT", "path": "data/src/main/java/org/apache/iceberg/data/GenericAppenderFactory.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjowNzo0NFrOH8Pxhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwNjoyOTowM1rOH8Z2DQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzNTA0Nw==", "bodyText": "Why not set eqDeleteRowSchema to null since equalityFieldIds is null?", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532935047", "createdAt": "2020-11-30T22:07:44Z", "author": {"login": "rdblue"}, "path": "data/src/main/java/org/apache/iceberg/data/GenericAppenderFactory.java", "diffHunk": "@@ -42,10 +47,25 @@\n public class GenericAppenderFactory implements FileAppenderFactory<Record> {\n \n   private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final int[] equalityFieldIds;\n+  private final Schema eqDeleteRowSchema;\n+  private final Schema posDeleteRowSchema;\n   private final Map<String, String> config = Maps.newHashMap();\n \n-  public GenericAppenderFactory(Schema schema) {\n+  public GenericAppenderFactory(Schema schema, PartitionSpec spec) {\n+    this(schema, spec, null, schema, null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEwMDA0NQ==", "bodyText": "OK, it's more reasonable to set the eqDeleteRowSchema to be null when equalityFieldIds is null.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r533100045", "createdAt": "2020-12-01T06:29:03Z", "author": {"login": "openinx"}, "path": "data/src/main/java/org/apache/iceberg/data/GenericAppenderFactory.java", "diffHunk": "@@ -42,10 +47,25 @@\n public class GenericAppenderFactory implements FileAppenderFactory<Record> {\n \n   private final Schema schema;\n+  private final PartitionSpec spec;\n+  private final int[] equalityFieldIds;\n+  private final Schema eqDeleteRowSchema;\n+  private final Schema posDeleteRowSchema;\n   private final Map<String, String> config = Maps.newHashMap();\n \n-  public GenericAppenderFactory(Schema schema) {\n+  public GenericAppenderFactory(Schema schema, PartitionSpec spec) {\n+    this(schema, spec, null, schema, null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzNTA0Nw=="}, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzUwMTc1OnYy", "diffSide": "RIGHT", "path": "data/src/test/java/org/apache/iceberg/data/GenericAppenderHelper.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjowODo1MFrOH8PzaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjozNjo1MVrOH8Qm7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzNTUyOQ==", "bodyText": "Could we keep the constructor this was using before so we don't need to change any tests that only use newAppender? There are 4 files just here that don't appear like they need to change just to add the spec that won't be used.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532935529", "createdAt": "2020-11-30T22:08:50Z", "author": {"login": "rdblue"}, "path": "data/src/test/java/org/apache/iceberg/data/GenericAppenderHelper.java", "diffHunk": "@@ -73,13 +74,12 @@ public DataFile writeFile(StructLike partition, List<Record> records) throws IOE\n     Preconditions.checkNotNull(table, \"table not set\");\n     File file = tmp.newFile();\n     Assert.assertTrue(file.delete());\n-    return appendToLocalFile(table, file, fileFormat, partition, records);\n+    return appendToLocalFile(table, file, fileFormat, partition, records, table.spec());\n   }\n \n-  private static DataFile appendToLocalFile(\n-      Table table, File file, FileFormat format, StructLike partition, List<Record> records)\n-      throws IOException {\n-    FileAppender<Record> appender = new GenericAppenderFactory(table.schema()).newAppender(\n+  private static DataFile appendToLocalFile(Table table, File file, FileFormat format, StructLike partition,\n+                                            List<Record> records, PartitionSpec spec) throws IOException {\n+    FileAppender<Record> appender = new GenericAppenderFactory(table.schema(), spec).newAppender(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk0ODcxNg==", "bodyText": "I think I've counted at least 10 files that would not need to change if we kept the original constructor.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532948716", "createdAt": "2020-11-30T22:36:51Z", "author": {"login": "rdblue"}, "path": "data/src/test/java/org/apache/iceberg/data/GenericAppenderHelper.java", "diffHunk": "@@ -73,13 +74,12 @@ public DataFile writeFile(StructLike partition, List<Record> records) throws IOE\n     Preconditions.checkNotNull(table, \"table not set\");\n     File file = tmp.newFile();\n     Assert.assertTrue(file.delete());\n-    return appendToLocalFile(table, file, fileFormat, partition, records);\n+    return appendToLocalFile(table, file, fileFormat, partition, records, table.spec());\n   }\n \n-  private static DataFile appendToLocalFile(\n-      Table table, File file, FileFormat format, StructLike partition, List<Record> records)\n-      throws IOException {\n-    FileAppender<Record> appender = new GenericAppenderFactory(table.schema()).newAppender(\n+  private static DataFile appendToLocalFile(Table table, File file, FileFormat format, StructLike partition,\n+                                            List<Record> records, PartitionSpec spec) throws IOException {\n+    FileAppender<Record> appender = new GenericAppenderFactory(table.schema(), spec).newAppender(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkzNTUyOQ=="}, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzU4MDQ5OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjozNDoxMVrOH8QiJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwNzo0NTo0MFrOH8bpuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk0NzQ5NA==", "bodyText": "I'd rather not add this to the build method. Nothing distinguishes it from other options, so I think we should add transformations as configuration methods, like we do for equalityFieldIds.\nI'm also thinking that it would be good to have a more light-weight way to add these transforms. Rather than an additional accessor that has two abstract methods, why not just register functions? It could look like this:\n  Avro.writeDeletes(outFile)\n      ...\n      .transformPaths(StringData::fromString)\n      .buildPositionWriter();\nThat way it's easier to use a method reference rather than creating a class. And nothing actually needs to transform pos yet, so we can just leave that out.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532947494", "createdAt": "2020-11-30T22:34:11Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -379,42 +378,40 @@ public DeleteWriteBuilder equalityFieldIds(int... fieldIds) {\n           appenderBuilder.build(), FileFormat.PARQUET, location, spec, partition, keyMetadata, equalityFieldIds);\n     }\n \n-\n-    public <T> PositionDeleteWriter<T> buildPositionWriter() throws IOException {\n+    public <T> PositionDeleteWriter<T> buildPositionWriter(ParquetValueWriters.PathPosAccessor<?, ?> accessor)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEyOTY1OQ==", "bodyText": "I like the idea about registering a light-weight function to convert the CharSequence to StringData.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r533129659", "createdAt": "2020-12-01T07:45:40Z", "author": {"login": "openinx"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -379,42 +378,40 @@ public DeleteWriteBuilder equalityFieldIds(int... fieldIds) {\n           appenderBuilder.build(), FileFormat.PARQUET, location, spec, partition, keyMetadata, equalityFieldIds);\n     }\n \n-\n-    public <T> PositionDeleteWriter<T> buildPositionWriter() throws IOException {\n+    public <T> PositionDeleteWriter<T> buildPositionWriter(ParquetValueWriters.PathPosAccessor<?, ?> accessor)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk0NzQ5NA=="}, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzYwNTQyOnYy", "diffSide": "RIGHT", "path": "core/src/test/java/org/apache/iceberg/io/TestAppenderFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjo0MjoxNVrOH8QwtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjo0MjoxNVrOH8QwtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk1MTIyMQ==", "bodyText": "I think this could just be table.schema(). No need to go through the spec to get the table schema.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532951221", "createdAt": "2020-11-30T22:42:15Z", "author": {"login": "rdblue"}, "path": "core/src/test/java/org/apache/iceberg/io/TestAppenderFactory.java", "diffHunk": "@@ -0,0 +1,265 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.deletes.EqualityDeleteWriter;\n+import org.apache.iceberg.deletes.PositionDelete;\n+import org.apache.iceberg.deletes.PositionDeleteWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestAppenderFactory<T> extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+\n+  private final FileFormat format;\n+  private final boolean partitioned;\n+\n+  private PartitionKey partition = null;\n+\n+  @Parameterized.Parameters(name = \"FileFormat={0}, Partitioned={1}\")\n+  public static Object[] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", false},\n+        new Object[] {\"avro\", true},\n+        new Object[] {\"parquet\", false},\n+        new Object[] {\"parquet\", true}\n+    };\n+  }\n+\n+\n+  public TestAppenderFactory(String fileFormat, boolean partitioned) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void setupTable() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    Assert.assertTrue(tableDir.delete()); // created by table create\n+\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+\n+    if (partitioned) {\n+      this.table = create(SCHEMA, SPEC);\n+    } else {\n+      this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n+    }\n+    this.partition = createPartitionKey();\n+\n+    table.updateProperties()\n+        .defaultFormat(format)\n+        .commit();\n+  }\n+\n+  protected abstract FileAppenderFactory<T> createAppenderFactory(List<Integer> equalityFieldIds,\n+                                                                  Schema eqDeleteSchema,\n+                                                                  Schema posDeleteRowSchema);\n+\n+  protected abstract T createRow(Integer id, String data);\n+\n+  protected abstract StructLikeSet expectedRowSet(Iterable<T> records) throws IOException;\n+\n+  protected abstract StructLikeSet actualRowSet(String... columns) throws IOException;\n+\n+  private OutputFileFactory createFileFactory() {\n+    return new OutputFileFactory(table.spec(), format, table.locationProvider(), table.io(),\n+        table.encryption(), 1, 1);\n+  }\n+\n+  private PartitionKey createPartitionKey() {\n+    if (table.spec().isUnpartitioned()) {\n+      return null;\n+    }\n+\n+    Record record = GenericRecord.create(table.spec().schema()).copy(ImmutableMap.of(\"data\", \"aaa\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzYxMDY1OnYy", "diffSide": "RIGHT", "path": "core/src/test/java/org/apache/iceberg/io/TestAppenderFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjo0NDowNlrOH8Qzyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjo0NDowNlrOH8Qzyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk1MjAxMQ==", "bodyText": "If the data here is bbb instead of ccc on purpose, then could you add a comment that this is testing that just id is used for comparison?", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532952011", "createdAt": "2020-11-30T22:44:06Z", "author": {"login": "rdblue"}, "path": "core/src/test/java/org/apache/iceberg/io/TestAppenderFactory.java", "diffHunk": "@@ -0,0 +1,265 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.deletes.EqualityDeleteWriter;\n+import org.apache.iceberg.deletes.PositionDelete;\n+import org.apache.iceberg.deletes.PositionDeleteWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestAppenderFactory<T> extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+\n+  private final FileFormat format;\n+  private final boolean partitioned;\n+\n+  private PartitionKey partition = null;\n+\n+  @Parameterized.Parameters(name = \"FileFormat={0}, Partitioned={1}\")\n+  public static Object[] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", false},\n+        new Object[] {\"avro\", true},\n+        new Object[] {\"parquet\", false},\n+        new Object[] {\"parquet\", true}\n+    };\n+  }\n+\n+\n+  public TestAppenderFactory(String fileFormat, boolean partitioned) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void setupTable() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    Assert.assertTrue(tableDir.delete()); // created by table create\n+\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+\n+    if (partitioned) {\n+      this.table = create(SCHEMA, SPEC);\n+    } else {\n+      this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n+    }\n+    this.partition = createPartitionKey();\n+\n+    table.updateProperties()\n+        .defaultFormat(format)\n+        .commit();\n+  }\n+\n+  protected abstract FileAppenderFactory<T> createAppenderFactory(List<Integer> equalityFieldIds,\n+                                                                  Schema eqDeleteSchema,\n+                                                                  Schema posDeleteRowSchema);\n+\n+  protected abstract T createRow(Integer id, String data);\n+\n+  protected abstract StructLikeSet expectedRowSet(Iterable<T> records) throws IOException;\n+\n+  protected abstract StructLikeSet actualRowSet(String... columns) throws IOException;\n+\n+  private OutputFileFactory createFileFactory() {\n+    return new OutputFileFactory(table.spec(), format, table.locationProvider(), table.io(),\n+        table.encryption(), 1, 1);\n+  }\n+\n+  private PartitionKey createPartitionKey() {\n+    if (table.spec().isUnpartitioned()) {\n+      return null;\n+    }\n+\n+    Record record = GenericRecord.create(table.spec().schema()).copy(ImmutableMap.of(\"data\", \"aaa\"));\n+\n+    PartitionKey partitionKey = new PartitionKey(table.spec(), table.schema());\n+    partitionKey.partition(record);\n+\n+    return partitionKey;\n+  }\n+\n+  private List<T> testRowSet() {\n+    return Lists.newArrayList(\n+        createRow(1, \"aaa\"),\n+        createRow(2, \"bbb\"),\n+        createRow(3, \"ccc\"),\n+        createRow(4, \"ddd\"),\n+        createRow(5, \"eee\")\n+    );\n+  }\n+\n+  private DataFile prepareDataFile(List<T> rowSet, FileAppenderFactory<T> appenderFactory,\n+                                   OutputFileFactory outputFileFactory) throws IOException {\n+    DataWriter<T> writer = appenderFactory.newDataWriter(outputFileFactory.newOutputFile(), format, partition);\n+    try (DataWriter<T> closeableWriter = writer) {\n+      for (T row : rowSet) {\n+        closeableWriter.add(row);\n+      }\n+    }\n+\n+    return writer.toDataFile();\n+  }\n+\n+  @Test\n+  public void testDataWriter() throws IOException {\n+    FileAppenderFactory<T> appenderFactory = createAppenderFactory(null, null, null);\n+    OutputFileFactory outputFileFactory = createFileFactory();\n+\n+    List<T> rowSet = testRowSet();\n+    DataFile dataFile = prepareDataFile(rowSet, appenderFactory, outputFileFactory);\n+\n+    table.newRowDelta()\n+        .addRows(dataFile)\n+        .commit();\n+\n+    Assert.assertEquals(\"Should have the expected records.\", expectedRowSet(rowSet), actualRowSet(\"*\"));\n+  }\n+\n+  @Test\n+  public void testEqDeleteWriter() throws IOException {\n+    List<Integer> equalityFieldIds = Lists.newArrayList(table.schema().findField(\"id\").fieldId());\n+    FileAppenderFactory<T> appenderFactory = createAppenderFactory(equalityFieldIds,\n+        table.schema().select(\"id\"), null);\n+    OutputFileFactory outputFileFactory = createFileFactory();\n+\n+    List<T> rowSet = testRowSet();\n+    DataFile dataFile = prepareDataFile(rowSet, appenderFactory, outputFileFactory);\n+\n+    table.newRowDelta()\n+        .addRows(dataFile)\n+        .commit();\n+\n+    List<T> deletes = Lists.newArrayList(\n+        createRow(1, \"aaa\"),\n+        createRow(3, \"bbb\"),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 173}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzYxODQ0OnYy", "diffSide": "RIGHT", "path": "core/src/test/java/org/apache/iceberg/io/TestAppenderFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjo0Njo0NFrOH8Q4aw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjo0Njo0NFrOH8Q4aw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk1MzE5NQ==", "bodyText": "Instead of reading from the table, I would rather see a test that the equality delete file contains the expected row data. In this case, it should not contain the data column. I would like to see that checked. And it would be good to add a case where the whole original row is written to the file.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532953195", "createdAt": "2020-11-30T22:46:44Z", "author": {"login": "rdblue"}, "path": "core/src/test/java/org/apache/iceberg/io/TestAppenderFactory.java", "diffHunk": "@@ -0,0 +1,265 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.deletes.EqualityDeleteWriter;\n+import org.apache.iceberg.deletes.PositionDelete;\n+import org.apache.iceberg.deletes.PositionDeleteWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestAppenderFactory<T> extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+\n+  private final FileFormat format;\n+  private final boolean partitioned;\n+\n+  private PartitionKey partition = null;\n+\n+  @Parameterized.Parameters(name = \"FileFormat={0}, Partitioned={1}\")\n+  public static Object[] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", false},\n+        new Object[] {\"avro\", true},\n+        new Object[] {\"parquet\", false},\n+        new Object[] {\"parquet\", true}\n+    };\n+  }\n+\n+\n+  public TestAppenderFactory(String fileFormat, boolean partitioned) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void setupTable() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    Assert.assertTrue(tableDir.delete()); // created by table create\n+\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+\n+    if (partitioned) {\n+      this.table = create(SCHEMA, SPEC);\n+    } else {\n+      this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n+    }\n+    this.partition = createPartitionKey();\n+\n+    table.updateProperties()\n+        .defaultFormat(format)\n+        .commit();\n+  }\n+\n+  protected abstract FileAppenderFactory<T> createAppenderFactory(List<Integer> equalityFieldIds,\n+                                                                  Schema eqDeleteSchema,\n+                                                                  Schema posDeleteRowSchema);\n+\n+  protected abstract T createRow(Integer id, String data);\n+\n+  protected abstract StructLikeSet expectedRowSet(Iterable<T> records) throws IOException;\n+\n+  protected abstract StructLikeSet actualRowSet(String... columns) throws IOException;\n+\n+  private OutputFileFactory createFileFactory() {\n+    return new OutputFileFactory(table.spec(), format, table.locationProvider(), table.io(),\n+        table.encryption(), 1, 1);\n+  }\n+\n+  private PartitionKey createPartitionKey() {\n+    if (table.spec().isUnpartitioned()) {\n+      return null;\n+    }\n+\n+    Record record = GenericRecord.create(table.spec().schema()).copy(ImmutableMap.of(\"data\", \"aaa\"));\n+\n+    PartitionKey partitionKey = new PartitionKey(table.spec(), table.schema());\n+    partitionKey.partition(record);\n+\n+    return partitionKey;\n+  }\n+\n+  private List<T> testRowSet() {\n+    return Lists.newArrayList(\n+        createRow(1, \"aaa\"),\n+        createRow(2, \"bbb\"),\n+        createRow(3, \"ccc\"),\n+        createRow(4, \"ddd\"),\n+        createRow(5, \"eee\")\n+    );\n+  }\n+\n+  private DataFile prepareDataFile(List<T> rowSet, FileAppenderFactory<T> appenderFactory,\n+                                   OutputFileFactory outputFileFactory) throws IOException {\n+    DataWriter<T> writer = appenderFactory.newDataWriter(outputFileFactory.newOutputFile(), format, partition);\n+    try (DataWriter<T> closeableWriter = writer) {\n+      for (T row : rowSet) {\n+        closeableWriter.add(row);\n+      }\n+    }\n+\n+    return writer.toDataFile();\n+  }\n+\n+  @Test\n+  public void testDataWriter() throws IOException {\n+    FileAppenderFactory<T> appenderFactory = createAppenderFactory(null, null, null);\n+    OutputFileFactory outputFileFactory = createFileFactory();\n+\n+    List<T> rowSet = testRowSet();\n+    DataFile dataFile = prepareDataFile(rowSet, appenderFactory, outputFileFactory);\n+\n+    table.newRowDelta()\n+        .addRows(dataFile)\n+        .commit();\n+\n+    Assert.assertEquals(\"Should have the expected records.\", expectedRowSet(rowSet), actualRowSet(\"*\"));\n+  }\n+\n+  @Test\n+  public void testEqDeleteWriter() throws IOException {\n+    List<Integer> equalityFieldIds = Lists.newArrayList(table.schema().findField(\"id\").fieldId());\n+    FileAppenderFactory<T> appenderFactory = createAppenderFactory(equalityFieldIds,\n+        table.schema().select(\"id\"), null);\n+    OutputFileFactory outputFileFactory = createFileFactory();\n+\n+    List<T> rowSet = testRowSet();\n+    DataFile dataFile = prepareDataFile(rowSet, appenderFactory, outputFileFactory);\n+\n+    table.newRowDelta()\n+        .addRows(dataFile)\n+        .commit();\n+\n+    List<T> deletes = Lists.newArrayList(\n+        createRow(1, \"aaa\"),\n+        createRow(3, \"bbb\"),\n+        createRow(5, \"ccc\")\n+    );\n+    EqualityDeleteWriter<T> eqDeleteWriter =\n+        appenderFactory.newEqDeleteWriter(outputFileFactory.newOutputFile(), format, partition);\n+    try (EqualityDeleteWriter<T> closeableWriter = eqDeleteWriter) {\n+      closeableWriter.deleteAll(deletes);\n+    }\n+\n+    table.newRowDelta()\n+        .addDeletes(eqDeleteWriter.toDeleteFile())\n+        .commit();\n+\n+    List<T> expected = Lists.newArrayList(\n+        createRow(2, \"bbb\"),\n+        createRow(4, \"ddd\")\n+    );\n+    Assert.assertEquals(\"Should have the expected records\", expectedRowSet(expected), actualRowSet(\"*\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 190}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0MzYyMDczOnYy", "diffSide": "RIGHT", "path": "core/src/test/java/org/apache/iceberg/io/TestAppenderFactory.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjo0NzozNlrOH8Q54A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMjo0NzozNlrOH8Q54A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjk1MzU2OA==", "bodyText": "Similar to above, I think this should check that only the path and position columns are written to the file and that they are the expected values. The test below should check that the row column is present and set correctly for each row.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r532953568", "createdAt": "2020-11-30T22:47:36Z", "author": {"login": "rdblue"}, "path": "core/src/test/java/org/apache/iceberg/io/TestAppenderFactory.java", "diffHunk": "@@ -0,0 +1,265 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.deletes.EqualityDeleteWriter;\n+import org.apache.iceberg.deletes.PositionDelete;\n+import org.apache.iceberg.deletes.PositionDeleteWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestAppenderFactory<T> extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+\n+  private final FileFormat format;\n+  private final boolean partitioned;\n+\n+  private PartitionKey partition = null;\n+\n+  @Parameterized.Parameters(name = \"FileFormat={0}, Partitioned={1}\")\n+  public static Object[] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", false},\n+        new Object[] {\"avro\", true},\n+        new Object[] {\"parquet\", false},\n+        new Object[] {\"parquet\", true}\n+    };\n+  }\n+\n+\n+  public TestAppenderFactory(String fileFormat, boolean partitioned) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void setupTable() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    Assert.assertTrue(tableDir.delete()); // created by table create\n+\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+\n+    if (partitioned) {\n+      this.table = create(SCHEMA, SPEC);\n+    } else {\n+      this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n+    }\n+    this.partition = createPartitionKey();\n+\n+    table.updateProperties()\n+        .defaultFormat(format)\n+        .commit();\n+  }\n+\n+  protected abstract FileAppenderFactory<T> createAppenderFactory(List<Integer> equalityFieldIds,\n+                                                                  Schema eqDeleteSchema,\n+                                                                  Schema posDeleteRowSchema);\n+\n+  protected abstract T createRow(Integer id, String data);\n+\n+  protected abstract StructLikeSet expectedRowSet(Iterable<T> records) throws IOException;\n+\n+  protected abstract StructLikeSet actualRowSet(String... columns) throws IOException;\n+\n+  private OutputFileFactory createFileFactory() {\n+    return new OutputFileFactory(table.spec(), format, table.locationProvider(), table.io(),\n+        table.encryption(), 1, 1);\n+  }\n+\n+  private PartitionKey createPartitionKey() {\n+    if (table.spec().isUnpartitioned()) {\n+      return null;\n+    }\n+\n+    Record record = GenericRecord.create(table.spec().schema()).copy(ImmutableMap.of(\"data\", \"aaa\"));\n+\n+    PartitionKey partitionKey = new PartitionKey(table.spec(), table.schema());\n+    partitionKey.partition(record);\n+\n+    return partitionKey;\n+  }\n+\n+  private List<T> testRowSet() {\n+    return Lists.newArrayList(\n+        createRow(1, \"aaa\"),\n+        createRow(2, \"bbb\"),\n+        createRow(3, \"ccc\"),\n+        createRow(4, \"ddd\"),\n+        createRow(5, \"eee\")\n+    );\n+  }\n+\n+  private DataFile prepareDataFile(List<T> rowSet, FileAppenderFactory<T> appenderFactory,\n+                                   OutputFileFactory outputFileFactory) throws IOException {\n+    DataWriter<T> writer = appenderFactory.newDataWriter(outputFileFactory.newOutputFile(), format, partition);\n+    try (DataWriter<T> closeableWriter = writer) {\n+      for (T row : rowSet) {\n+        closeableWriter.add(row);\n+      }\n+    }\n+\n+    return writer.toDataFile();\n+  }\n+\n+  @Test\n+  public void testDataWriter() throws IOException {\n+    FileAppenderFactory<T> appenderFactory = createAppenderFactory(null, null, null);\n+    OutputFileFactory outputFileFactory = createFileFactory();\n+\n+    List<T> rowSet = testRowSet();\n+    DataFile dataFile = prepareDataFile(rowSet, appenderFactory, outputFileFactory);\n+\n+    table.newRowDelta()\n+        .addRows(dataFile)\n+        .commit();\n+\n+    Assert.assertEquals(\"Should have the expected records.\", expectedRowSet(rowSet), actualRowSet(\"*\"));\n+  }\n+\n+  @Test\n+  public void testEqDeleteWriter() throws IOException {\n+    List<Integer> equalityFieldIds = Lists.newArrayList(table.schema().findField(\"id\").fieldId());\n+    FileAppenderFactory<T> appenderFactory = createAppenderFactory(equalityFieldIds,\n+        table.schema().select(\"id\"), null);\n+    OutputFileFactory outputFileFactory = createFileFactory();\n+\n+    List<T> rowSet = testRowSet();\n+    DataFile dataFile = prepareDataFile(rowSet, appenderFactory, outputFileFactory);\n+\n+    table.newRowDelta()\n+        .addRows(dataFile)\n+        .commit();\n+\n+    List<T> deletes = Lists.newArrayList(\n+        createRow(1, \"aaa\"),\n+        createRow(3, \"bbb\"),\n+        createRow(5, \"ccc\")\n+    );\n+    EqualityDeleteWriter<T> eqDeleteWriter =\n+        appenderFactory.newEqDeleteWriter(outputFileFactory.newOutputFile(), format, partition);\n+    try (EqualityDeleteWriter<T> closeableWriter = eqDeleteWriter) {\n+      closeableWriter.deleteAll(deletes);\n+    }\n+\n+    table.newRowDelta()\n+        .addDeletes(eqDeleteWriter.toDeleteFile())\n+        .commit();\n+\n+    List<T> expected = Lists.newArrayList(\n+        createRow(2, \"bbb\"),\n+        createRow(4, \"ddd\")\n+    );\n+    Assert.assertEquals(\"Should have the expected records\", expectedRowSet(expected), actualRowSet(\"*\"));\n+  }\n+\n+  @Test\n+  public void testPosDeleteWriter() throws IOException {\n+    // Initialize FileAppenderFactory without pos-delete row schema.\n+    FileAppenderFactory<T> appenderFactory = createAppenderFactory(null, null, null);\n+    OutputFileFactory outputFileFactory = createFileFactory();\n+\n+    List<T> rowSet = testRowSet();\n+    DataFile dataFile = prepareDataFile(rowSet, appenderFactory, outputFileFactory);\n+\n+    List<Pair<CharSequence, Long>> deletes = Lists.newArrayList(\n+        Pair.of(dataFile.path(), 0L),\n+        Pair.of(dataFile.path(), 2L),\n+        Pair.of(dataFile.path(), 4L)\n+    );\n+\n+    PositionDeleteWriter<T> eqDeleteWriter =\n+        appenderFactory.newPosDeleteWriter(outputFileFactory.newOutputFile(), format, partition);\n+    try (PositionDeleteWriter<T> closeableWriter = eqDeleteWriter) {\n+      for (Pair<CharSequence, Long> delete : deletes) {\n+        closeableWriter.delete(delete.first(), delete.second());\n+      }\n+    }\n+\n+    table.newRowDelta()\n+        .addRows(dataFile)\n+        .addDeletes(eqDeleteWriter.toDeleteFile())\n+        .validateDataFilesExist(eqDeleteWriter.referencedDataFiles())\n+        .validateDeletedFiles()\n+        .commit();\n+\n+    List<T> expected = Lists.newArrayList(\n+        createRow(2, \"bbb\"),\n+        createRow(4, \"ddd\")\n+    );\n+    Assert.assertEquals(\"Should have the expected records\", expectedRowSet(expected), actualRowSet(\"*\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d6cebc65cb8b2e9ede9feb37dd54cdcc880ab45c"}, "originalPosition": 227}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0NDg4Mzc2OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwODowNjo0MFrOH8cRig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQwODowNjo0MFrOH8cRig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzEzOTg1MA==", "bodyText": "Here I use the constructor that has spec argument because I believe we will use the DataWriter to append records once we switch to the RollingFileWriter. https://github.com/apache/iceberg/pull/1818/files#diff-fc9a9fd84d24c607fd85e053b08a559f56dd2dd2a46f1341c528e7a0269f873cR263.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r533139850", "createdAt": "2020-12-01T08:06:40Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -98,7 +98,7 @@ public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n         task, schema, schema, nameMapping, io.value(), encryptionManager.value(), caseSensitive);\n \n     StructType structType = SparkSchemaUtil.convert(schema);\n-    SparkAppenderFactory appenderFactory = new SparkAppenderFactory(properties, schema, structType);\n+    SparkAppenderFactory appenderFactory = new SparkAppenderFactory(properties, schema, structType, spec);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8e423fb34d02da49e78a9750b6204f6ce23cead2"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0ODYyMDM3OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQyMTozNjo1OVrOH9Au8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQyMTozNjo1OVrOH9Au8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzczNzIwMw==", "bodyText": "Nit: I think it would be better to use Function.identity().", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r533737203", "createdAt": "2020-12-01T21:36:59Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -281,6 +280,7 @@ public static DeleteWriteBuilder writeDeletes(OutputFile file) {\n     private StructLike partition = null;\n     private EncryptionKeyMetadata keyMetadata = null;\n     private int[] equalityFieldIds = null;\n+    private Function<CharSequence, ?> pathTransformFunc = t -> t;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4b2b04bd45bafa5ea794fd35366e819afc45b574"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0ODYyMzM3OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQyMTozNzo1NlrOH9Awtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwMTo0ODoxN1rOH9HG1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzczNzY1NQ==", "bodyText": "Does this line need to change? I'm fine removing the empty line, but I think throws can still fit on the previous line.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r533737655", "createdAt": "2020-12-01T21:37:56Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -379,37 +384,31 @@ public DeleteWriteBuilder equalityFieldIds(int... fieldIds) {\n           appenderBuilder.build(), FileFormat.PARQUET, location, spec, partition, keyMetadata, equalityFieldIds);\n     }\n \n-\n-    public <T> PositionDeleteWriter<T> buildPositionWriter() throws IOException {\n+    public <T> PositionDeleteWriter<T> buildPositionWriter()\n+        throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4b2b04bd45bafa5ea794fd35366e819afc45b574"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg0MTYyMg==", "bodyText": "I can revert it.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r533841622", "createdAt": "2020-12-02T01:48:17Z", "author": {"login": "openinx"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -379,37 +384,31 @@ public DeleteWriteBuilder equalityFieldIds(int... fieldIds) {\n           appenderBuilder.build(), FileFormat.PARQUET, location, spec, partition, keyMetadata, equalityFieldIds);\n     }\n \n-\n-    public <T> PositionDeleteWriter<T> buildPositionWriter() throws IOException {\n+    public <T> PositionDeleteWriter<T> buildPositionWriter()\n+        throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzczNzY1NQ=="}, "originalCommit": {"oid": "4b2b04bd45bafa5ea794fd35366e819afc45b574"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM0ODYyNzg0OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQyMTozOToxNlrOH9AzaQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQwMTo0NzoyNlrOH9HF5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzczODM0NQ==", "bodyText": "Shouldn't this pass pathTransformFunc as well?", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r533738345", "createdAt": "2020-12-01T21:39:16Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -379,37 +384,31 @@ public DeleteWriteBuilder equalityFieldIds(int... fieldIds) {\n           appenderBuilder.build(), FileFormat.PARQUET, location, spec, partition, keyMetadata, equalityFieldIds);\n     }\n \n-\n-    public <T> PositionDeleteWriter<T> buildPositionWriter() throws IOException {\n+    public <T> PositionDeleteWriter<T> buildPositionWriter()\n+        throws IOException {\n       Preconditions.checkState(equalityFieldIds == null, \"Cannot create position delete file using delete field ids\");\n \n       meta(\"delete-type\", \"position\");\n \n       if (rowSchema != null && createWriterFunc != null) {\n         // the appender uses the row schema wrapped with position fields\n-        appenderBuilder.schema(new org.apache.iceberg.Schema(\n-            MetadataColumns.DELETE_FILE_PATH,\n-            MetadataColumns.DELETE_FILE_POS,\n-            NestedField.optional(\n-                MetadataColumns.DELETE_FILE_ROW_FIELD_ID, \"row\", rowSchema.asStruct(),\n-                MetadataColumns.DELETE_FILE_ROW_DOC)));\n+        appenderBuilder.schema(DeleteSchemaUtil.posDeleteSchema(rowSchema));\n \n         appenderBuilder.createWriterFunc(parquetSchema -> {\n           ParquetValueWriter<?> writer = createWriterFunc.apply(parquetSchema);\n           if (writer instanceof StructWriter) {\n-            return new PositionDeleteStructWriter<T>((StructWriter<?>) writer);\n+            return new PositionDeleteStructWriter<T>((StructWriter<?>) writer, pathTransformFunc);\n           } else {\n             throw new UnsupportedOperationException(\"Cannot wrap writer for position deletes: \" + writer.getClass());\n           }\n         });\n \n       } else {\n-        appenderBuilder.schema(new org.apache.iceberg.Schema(\n-            MetadataColumns.DELETE_FILE_PATH,\n-            MetadataColumns.DELETE_FILE_POS));\n+        appenderBuilder.schema(DeleteSchemaUtil.pathPosSchema());\n \n         appenderBuilder.createWriterFunc(parquetSchema ->\n-            new PositionDeleteStructWriter<T>((StructWriter<?>) GenericParquetWriter.buildWriter(parquetSchema)));\n+            new PositionDeleteStructWriter<T>((StructWriter<?>) GenericParquetWriter.buildWriter(parquetSchema),\n+                t -> t));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4b2b04bd45bafa5ea794fd35366e819afc45b574"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg0MTIwNA==", "bodyText": "We shouldn't pass pathTransformFunc here, because in this path we will use GenericParquetWriter (Rather than FlinkParquetWriter or SparkParquetWriter) to write the PositionDelete,  if convert the path CharSequence to StringData,  the GenericParquetWriter could not find the correct writer to write values.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r533841204", "createdAt": "2020-12-02T01:46:52Z", "author": {"login": "openinx"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -379,37 +384,31 @@ public DeleteWriteBuilder equalityFieldIds(int... fieldIds) {\n           appenderBuilder.build(), FileFormat.PARQUET, location, spec, partition, keyMetadata, equalityFieldIds);\n     }\n \n-\n-    public <T> PositionDeleteWriter<T> buildPositionWriter() throws IOException {\n+    public <T> PositionDeleteWriter<T> buildPositionWriter()\n+        throws IOException {\n       Preconditions.checkState(equalityFieldIds == null, \"Cannot create position delete file using delete field ids\");\n \n       meta(\"delete-type\", \"position\");\n \n       if (rowSchema != null && createWriterFunc != null) {\n         // the appender uses the row schema wrapped with position fields\n-        appenderBuilder.schema(new org.apache.iceberg.Schema(\n-            MetadataColumns.DELETE_FILE_PATH,\n-            MetadataColumns.DELETE_FILE_POS,\n-            NestedField.optional(\n-                MetadataColumns.DELETE_FILE_ROW_FIELD_ID, \"row\", rowSchema.asStruct(),\n-                MetadataColumns.DELETE_FILE_ROW_DOC)));\n+        appenderBuilder.schema(DeleteSchemaUtil.posDeleteSchema(rowSchema));\n \n         appenderBuilder.createWriterFunc(parquetSchema -> {\n           ParquetValueWriter<?> writer = createWriterFunc.apply(parquetSchema);\n           if (writer instanceof StructWriter) {\n-            return new PositionDeleteStructWriter<T>((StructWriter<?>) writer);\n+            return new PositionDeleteStructWriter<T>((StructWriter<?>) writer, pathTransformFunc);\n           } else {\n             throw new UnsupportedOperationException(\"Cannot wrap writer for position deletes: \" + writer.getClass());\n           }\n         });\n \n       } else {\n-        appenderBuilder.schema(new org.apache.iceberg.Schema(\n-            MetadataColumns.DELETE_FILE_PATH,\n-            MetadataColumns.DELETE_FILE_POS));\n+        appenderBuilder.schema(DeleteSchemaUtil.pathPosSchema());\n \n         appenderBuilder.createWriterFunc(parquetSchema ->\n-            new PositionDeleteStructWriter<T>((StructWriter<?>) GenericParquetWriter.buildWriter(parquetSchema)));\n+            new PositionDeleteStructWriter<T>((StructWriter<?>) GenericParquetWriter.buildWriter(parquetSchema),\n+                t -> t));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzczODM0NQ=="}, "originalCommit": {"oid": "4b2b04bd45bafa5ea794fd35366e819afc45b574"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzg0MTM4Mw==", "bodyText": "It's good to use Function.identity() here.", "url": "https://github.com/apache/iceberg/pull/1836#discussion_r533841383", "createdAt": "2020-12-02T01:47:26Z", "author": {"login": "openinx"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -379,37 +384,31 @@ public DeleteWriteBuilder equalityFieldIds(int... fieldIds) {\n           appenderBuilder.build(), FileFormat.PARQUET, location, spec, partition, keyMetadata, equalityFieldIds);\n     }\n \n-\n-    public <T> PositionDeleteWriter<T> buildPositionWriter() throws IOException {\n+    public <T> PositionDeleteWriter<T> buildPositionWriter()\n+        throws IOException {\n       Preconditions.checkState(equalityFieldIds == null, \"Cannot create position delete file using delete field ids\");\n \n       meta(\"delete-type\", \"position\");\n \n       if (rowSchema != null && createWriterFunc != null) {\n         // the appender uses the row schema wrapped with position fields\n-        appenderBuilder.schema(new org.apache.iceberg.Schema(\n-            MetadataColumns.DELETE_FILE_PATH,\n-            MetadataColumns.DELETE_FILE_POS,\n-            NestedField.optional(\n-                MetadataColumns.DELETE_FILE_ROW_FIELD_ID, \"row\", rowSchema.asStruct(),\n-                MetadataColumns.DELETE_FILE_ROW_DOC)));\n+        appenderBuilder.schema(DeleteSchemaUtil.posDeleteSchema(rowSchema));\n \n         appenderBuilder.createWriterFunc(parquetSchema -> {\n           ParquetValueWriter<?> writer = createWriterFunc.apply(parquetSchema);\n           if (writer instanceof StructWriter) {\n-            return new PositionDeleteStructWriter<T>((StructWriter<?>) writer);\n+            return new PositionDeleteStructWriter<T>((StructWriter<?>) writer, pathTransformFunc);\n           } else {\n             throw new UnsupportedOperationException(\"Cannot wrap writer for position deletes: \" + writer.getClass());\n           }\n         });\n \n       } else {\n-        appenderBuilder.schema(new org.apache.iceberg.Schema(\n-            MetadataColumns.DELETE_FILE_PATH,\n-            MetadataColumns.DELETE_FILE_POS));\n+        appenderBuilder.schema(DeleteSchemaUtil.pathPosSchema());\n \n         appenderBuilder.createWriterFunc(parquetSchema ->\n-            new PositionDeleteStructWriter<T>((StructWriter<?>) GenericParquetWriter.buildWriter(parquetSchema)));\n+            new PositionDeleteStructWriter<T>((StructWriter<?>) GenericParquetWriter.buildWriter(parquetSchema),\n+                t -> t));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzczODM0NQ=="}, "originalCommit": {"oid": "4b2b04bd45bafa5ea794fd35366e819afc45b574"}, "originalPosition": 85}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3176, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}