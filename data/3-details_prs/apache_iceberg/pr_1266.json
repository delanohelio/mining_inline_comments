{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU4NDE3MDE3", "number": 1266, "title": "Flink: update parquet reader with schema visitor", "bodyText": "This is sub PR of #1237, it changes the current FlinkParquetReader to use a schema visitor.", "createdAt": "2020-07-29T12:48:00Z", "url": "https://github.com/apache/iceberg/pull/1266", "merged": true, "mergeCommit": {"oid": "6cb2db7acb06891502ac4af1845b239ed7cb521d"}, "closed": true, "closedAt": "2020-08-20T00:36:47Z", "author": {"login": "chenjunjiedada"}, "timelineItems": {"totalCount": 32, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc5qgYkgBqjM1OTg4OTM4NzI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc-YVAPgH2gAyNDU4NDE3MDE3OmUxNDA3ZDRkZWVjMjMyZjhmM2U5YmNkMjEzYjI4YzdkZmRlNDg5Njc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "749f7780af99c60628ac40b131bf999589d7be84", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/749f7780af99c60628ac40b131bf999589d7be84", "committedDate": "2020-07-29T20:47:44Z", "message": "Flink: update parquet reader with schema visitor"}, "afterCommit": {"oid": "2728b4fadf3c2c2f241760b506aa80280b8a3840", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/2728b4fadf3c2c2f241760b506aa80280b8a3840", "committedDate": "2020-07-29T20:50:48Z", "message": "Flink: update parquet reader to use schema visitor"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2728b4fadf3c2c2f241760b506aa80280b8a3840", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/2728b4fadf3c2c2f241760b506aa80280b8a3840", "committedDate": "2020-07-29T20:50:48Z", "message": "Flink: update parquet reader to use schema visitor"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e57b6dd17b7e66383a1d2b4e8e31bec87207f6ad", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/e57b6dd17b7e66383a1d2b4e8e31bec87207f6ad", "committedDate": "2020-07-30T17:54:18Z", "message": "remove useless class"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU5MDAzNzA1", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-459003705", "createdAt": "2020-07-31T08:29:51Z", "commit": {"oid": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwODoyOTo1MVrOG6AZKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwOToyNDowN1rOG6CAwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ3NzAzNQ==", "bodyText": "Here seems we could return the determinate parameter type RowData.   Can change it to ParquetValueReader<RowData>.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463477035", "createdAt": "2020-07-31T08:29:51Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,719 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4MTMyNQ==", "bodyText": "Q: Is there any problem here ?  The FallbackReadBuilder don't need to implement those methods ?\n\npublic T list(Types.ListType iList, GroupType array, T element) ;\npublic T map(Types.MapType iMap, GroupType map, T key, T value) ;\npublic T primitive(org.apache.iceberg.types.Type.PrimitiveType iPrimitive, PrimitiveType primitive)", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463481325", "createdAt": "2020-07-31T08:39:00Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,719 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4NzYyOQ==", "bodyText": "Q:  Is there neccessray to abstract a common ReadBuilder  and make the GenericParquetReader , SparkParquetReader, FlinkParquetReader to share it ?  I mean we could define different Reader(s) for different engine data type, for example we may have Flink's StructReader , and Spark's StructReader,  but the implementation of  TypeWithSchemaVisitor could be shared.   Does it make sense ?   CC @rdblue\nI raise this issue because I saw almost all the codes in ReadBuilder are the same, different copies may need extra resources (both contributor and reviewers) to maintain them.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463487629", "createdAt": "2020-07-31T08:51:32Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,719 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the IDs are missing\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5NDU1NA==", "bodyText": "This is timestamp with zone or without zone ?  If it's timestamp with zone,  then the value could be negative, then we could not just  use / and '%',  instead we should use Math.floorDiv and Math.floorMod.  Please see this pull request: #1271 .   ( for Java,  -5/2=-2,  while Math.floorDiv(-5, 2)=-3, actually we need the -3  when considering the epoch second).\nSpark's parquet TimestampMillisReader don't have this issue because it does not depend any div or mod operation.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463494554", "createdAt": "2020-07-31T09:05:34Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,719 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the IDs are missing\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new TimeMillisReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            return new TimestampMicroReader(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class TimestampMicroReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampMicroReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromInstant(Instant.ofEpochSecond(value / 1000_000, (value % 1000_000) * 1000));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5"}, "originalPosition": 325}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5NTkzOA==", "bodyText": "ditto.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463495938", "createdAt": "2020-07-31T09:08:32Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,719 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the IDs are missing\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new TimeMillisReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            return new TimestampMicroReader(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class TimestampMicroReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampMicroReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromInstant(Instant.ofEpochSecond(value / 1000_000, (value % 1000_000) * 1000));\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n+  }\n+\n+  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n+    StringReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public StringData read(StringData ignored) {\n+      Binary binary = column.nextBinary();\n+      ByteBuffer buffer = binary.toByteBuffer();\n+      if (buffer.hasArray()) {\n+        return StringData.fromBytes(\n+            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n+      } else {\n+        return StringData.fromBytes(binary.getBytes());\n+      }\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n-                                                       List<ParquetValueReader<?>> fieldReaders,\n-                                                       Types.StructType structType) {\n-    return new RowReader(types, fieldReaders, structType);\n+  private static class TimeMillisReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n+    TimeMillisReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public Integer read(Integer reuse) {\n+      // Flink only supports millisecond, so we discard microseconds in millisecond\n+      return (int) column.nextLong() / 1000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5"}, "originalPosition": 365}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5NzUwMQ==", "bodyText": "it could be removed now ?", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463497501", "createdAt": "2020-07-31T09:11:37Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,719 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the IDs are missing\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new TimeMillisReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            return new TimestampMicroReader(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class TimestampMicroReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampMicroReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromInstant(Instant.ofEpochSecond(value / 1000_000, (value % 1000_000) * 1000));\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n+  }\n+\n+  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n+    StringReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public StringData read(StringData ignored) {\n+      Binary binary = column.nextBinary();\n+      ByteBuffer buffer = binary.toByteBuffer();\n+      if (buffer.hasArray()) {\n+        return StringData.fromBytes(\n+            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n+      } else {\n+        return StringData.fromBytes(binary.getBytes());\n+      }\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n-                                                       List<ParquetValueReader<?>> fieldReaders,\n-                                                       Types.StructType structType) {\n-    return new RowReader(types, fieldReaders, structType);\n+  private static class TimeMillisReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n+    TimeMillisReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public Integer read(Integer reuse) {\n+      // Flink only supports millisecond, so we discard microseconds in millisecond\n+      return (int) column.nextLong() / 1000;\n+    }\n   }\n \n-  private static class RowReader extends ParquetValueReaders.StructReader<Row, Row> {\n-    private final Types.StructType structType;\n+  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n+      super(definitionLevel, repetitionLevel, reader);\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5"}, "originalPosition": 380}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzUwMzU1Mw==", "bodyText": "Is this right ?    should be old.length << 1  ?    CC @rdblue ,  spark's ReusableArrayData have the same issue ?", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463503553", "createdAt": "2020-07-31T09:24:07Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,719 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the IDs are missing\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new TimeMillisReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            return new TimestampMicroReader(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class TimestampMicroReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampMicroReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromInstant(Instant.ofEpochSecond(value / 1000_000, (value % 1000_000) * 1000));\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n+  }\n+\n+  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n+    StringReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public StringData read(StringData ignored) {\n+      Binary binary = column.nextBinary();\n+      ByteBuffer buffer = binary.toByteBuffer();\n+      if (buffer.hasArray()) {\n+        return StringData.fromBytes(\n+            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n+      } else {\n+        return StringData.fromBytes(binary.getBytes());\n+      }\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n-                                                       List<ParquetValueReader<?>> fieldReaders,\n-                                                       Types.StructType structType) {\n-    return new RowReader(types, fieldReaders, structType);\n+  private static class TimeMillisReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n+    TimeMillisReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public Integer read(Integer reuse) {\n+      // Flink only supports millisecond, so we discard microseconds in millisecond\n+      return (int) column.nextLong() / 1000;\n+    }\n   }\n \n-  private static class RowReader extends ParquetValueReaders.StructReader<Row, Row> {\n-    private final Types.StructType structType;\n+  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n+      super(definitionLevel, repetitionLevel, reader);\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected ReusableArrayData newListData(ArrayData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableArrayData) {\n+        return (ReusableArrayData) reuse;\n+      } else {\n+        return new ReusableArrayData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected E getElement(ReusableArrayData list) {\n+      E value = null;\n+      if (readPos < list.capacity()) {\n+        value = (E) list.values[readPos];\n+      }\n+\n+      readPos += 1;\n+\n+      return value;\n+    }\n+\n+    @Override\n+    protected void addElement(ReusableArrayData reused, E element) {\n+      if (writePos >= reused.capacity()) {\n+        reused.grow();\n+      }\n+\n+      reused.values[writePos] = element;\n+\n+      writePos += 1;\n+    }\n+\n+    @Override\n+    protected ArrayData buildList(ReusableArrayData list) {\n+      list.setNumElements(writePos);\n+      return list;\n+    }\n+  }\n+\n+  private static class MapReader<K, V> extends\n+      ParquetValueReaders.RepeatedKeyValueReader<MapData, ReusableMapData, K, V> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    private final ParquetValueReaders.ReusableEntry<K, V> entry = new ParquetValueReaders.ReusableEntry<>();\n+    private final ParquetValueReaders.ReusableEntry<K, V> nullEntry = new ParquetValueReaders.ReusableEntry<>();\n+\n+    MapReader(int definitionLevel, int repetitionLevel,\n+              ParquetValueReader<K> keyReader, ParquetValueReader<V> valueReader) {\n+      super(definitionLevel, repetitionLevel, keyReader, valueReader);\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected ReusableMapData newMapData(MapData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableMapData) {\n+        return (ReusableMapData) reuse;\n+      } else {\n+        return new ReusableMapData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected Map.Entry<K, V> getPair(ReusableMapData map) {\n+      Map.Entry<K, V> kv = nullEntry;\n+      if (readPos < map.capacity()) {\n+        entry.set((K) map.keys.values[readPos], (V) map.values.values[readPos]);\n+        kv = entry;\n+      }\n+\n+      readPos += 1;\n+\n+      return kv;\n+    }\n+\n+    @Override\n+    protected void addPair(ReusableMapData map, K key, V value) {\n+      if (writePos >= map.capacity()) {\n+        map.grow();\n+      }\n+\n+      map.keys.values[writePos] = key;\n+      map.values.values[writePos] = value;\n \n-    RowReader(List<Type> types, List<ParquetValueReader<?>> readers, Types.StructType struct) {\n+      writePos += 1;\n+    }\n+\n+    @Override\n+    protected MapData buildMap(ReusableMapData map) {\n+      map.setNumElements(writePos);\n+      return map;\n+    }\n+  }\n+\n+  private static class RowDataReader extends ParquetValueReaders.StructReader<RowData, GenericRowData> {\n+    private final int numFields;\n+\n+    RowDataReader(List<Type> types, List<ParquetValueReader<?>> readers) {\n       super(types, readers);\n-      this.structType = struct;\n+      this.numFields = readers.size();\n     }\n \n     @Override\n-    protected Row newStructData(Row reuse) {\n-      if (reuse != null) {\n-        return reuse;\n+    protected GenericRowData newStructData(RowData reuse) {\n+      if (reuse instanceof GenericRowData) {\n+        return (GenericRowData) reuse;\n       } else {\n-        return new Row(structType.fields().size());\n+        return new GenericRowData(numFields);\n       }\n     }\n \n     @Override\n-    protected Object getField(Row row, int pos) {\n-      return row.getField(pos);\n+    protected Object getField(GenericRowData intermediate, int pos) {\n+      return intermediate.getField(pos);\n+    }\n+\n+    @Override\n+    protected RowData buildStruct(GenericRowData struct) {\n+      return struct;\n+    }\n+\n+    @Override\n+    protected void set(GenericRowData row, int pos, Object value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setNull(GenericRowData row, int pos) {\n+      row.setField(pos, null);\n     }\n \n     @Override\n-    protected Row buildStruct(Row row) {\n-      return row;\n+    protected void setBoolean(GenericRowData row, int pos, boolean value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setInteger(GenericRowData row, int pos, int value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setLong(GenericRowData row, int pos, long value) {\n+      row.setField(pos, value);\n     }\n \n     @Override\n-    protected void set(Row row, int pos, Object value) {\n+    protected void setFloat(GenericRowData row, int pos, float value) {\n       row.setField(pos, value);\n     }\n+\n+    @Override\n+    protected void setDouble(GenericRowData row, int pos, double value) {\n+      row.setField(pos, value);\n+    }\n+  }\n+\n+  private static class ReusableMapData implements MapData {\n+    private final ReusableArrayData keys;\n+    private final ReusableArrayData values;\n+\n+    private int numElements;\n+\n+    private ReusableMapData() {\n+      this.keys = new ReusableArrayData();\n+      this.values = new ReusableArrayData();\n+    }\n+\n+    private void grow() {\n+      keys.grow();\n+      values.grow();\n+    }\n+\n+    private int capacity() {\n+      return keys.capacity();\n+    }\n+\n+    public void setNumElements(int numElements) {\n+      this.numElements = numElements;\n+      keys.setNumElements(numElements);\n+      values.setNumElements(numElements);\n+    }\n+\n+    @Override\n+    public int size() {\n+      return numElements;\n+    }\n+\n+    @Override\n+    public ReusableArrayData keyArray() {\n+      return keys;\n+    }\n+\n+    @Override\n+    public ReusableArrayData valueArray() {\n+      return values;\n+    }\n+  }\n+\n+  private static class ReusableArrayData implements ArrayData {\n+    private static final Object[] EMPTY = new Object[0];\n+\n+    private Object[] values = EMPTY;\n+    private int numElements = 0;\n+\n+    private void grow() {\n+      if (values.length == 0) {\n+        this.values = new Object[20];\n+      } else {\n+        Object[] old = values;\n+        this.values = new Object[old.length << 2];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5"}, "originalPosition": 609}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5", "committedDate": "2020-07-31T11:16:10Z", "message": "address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "df701a87aa1667c325c5d132d7e2ed958d7d36e9", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/df701a87aa1667c325c5d132d7e2ed958d7d36e9", "committedDate": "2020-07-31T13:45:04Z", "message": "address comments from openinx"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a", "committedDate": "2020-08-03T22:52:22Z", "message": "Remove fallback reader and update names"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYwNTgyMjEz", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-460582213", "createdAt": "2020-08-04T07:59:06Z", "commit": {"oid": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a"}, "state": "COMMENTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwNzo1OTowNlrOG7VbXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwOTowOTo1OFrOG7X7aw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg3MDIzNw==", "bodyText": "Better to create an issue to address this thing,  once the readers and writers get in,  we could do the refactor in that issue .", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464870237", "createdAt": "2020-08-04T07:59:06Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,719 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the IDs are missing\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4NzYyOQ=="}, "originalCommit": {"oid": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg3MTQ3Mg==", "bodyText": "nit: could code format it, seems we missing the indent.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464871472", "createdAt": "2020-08-04T08:01:17Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,701 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg3MTY2Mw==", "bodyText": "nit: indent ?", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464871663", "createdAt": "2020-08-04T08:01:36Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,701 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg4ODc2OA==", "bodyText": "For this PR ( flink parquet reader) , seems we don't need this writer visitor .", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464888768", "createdAt": "2020-08-04T08:32:01Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/ParquetWithFlinkSchemaVisitor.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.iceberg.avro.AvroSchemaUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class ParquetWithFlinkSchemaVisitor<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg5ODM1Ng==", "bodyText": "I still think we don't need to grow four times space,  actually double array space is enough.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464898356", "createdAt": "2020-08-04T08:47:55Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,719 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the IDs are missing\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new TimeMillisReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            return new TimestampMicroReader(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class TimestampMicroReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampMicroReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromInstant(Instant.ofEpochSecond(value / 1000_000, (value % 1000_000) * 1000));\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n+  }\n+\n+  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n+    StringReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public StringData read(StringData ignored) {\n+      Binary binary = column.nextBinary();\n+      ByteBuffer buffer = binary.toByteBuffer();\n+      if (buffer.hasArray()) {\n+        return StringData.fromBytes(\n+            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n+      } else {\n+        return StringData.fromBytes(binary.getBytes());\n+      }\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n-                                                       List<ParquetValueReader<?>> fieldReaders,\n-                                                       Types.StructType structType) {\n-    return new RowReader(types, fieldReaders, structType);\n+  private static class TimeMillisReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n+    TimeMillisReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public Integer read(Integer reuse) {\n+      // Flink only supports millisecond, so we discard microseconds in millisecond\n+      return (int) column.nextLong() / 1000;\n+    }\n   }\n \n-  private static class RowReader extends ParquetValueReaders.StructReader<Row, Row> {\n-    private final Types.StructType structType;\n+  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n+      super(definitionLevel, repetitionLevel, reader);\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected ReusableArrayData newListData(ArrayData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableArrayData) {\n+        return (ReusableArrayData) reuse;\n+      } else {\n+        return new ReusableArrayData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected E getElement(ReusableArrayData list) {\n+      E value = null;\n+      if (readPos < list.capacity()) {\n+        value = (E) list.values[readPos];\n+      }\n+\n+      readPos += 1;\n+\n+      return value;\n+    }\n+\n+    @Override\n+    protected void addElement(ReusableArrayData reused, E element) {\n+      if (writePos >= reused.capacity()) {\n+        reused.grow();\n+      }\n+\n+      reused.values[writePos] = element;\n+\n+      writePos += 1;\n+    }\n+\n+    @Override\n+    protected ArrayData buildList(ReusableArrayData list) {\n+      list.setNumElements(writePos);\n+      return list;\n+    }\n+  }\n+\n+  private static class MapReader<K, V> extends\n+      ParquetValueReaders.RepeatedKeyValueReader<MapData, ReusableMapData, K, V> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    private final ParquetValueReaders.ReusableEntry<K, V> entry = new ParquetValueReaders.ReusableEntry<>();\n+    private final ParquetValueReaders.ReusableEntry<K, V> nullEntry = new ParquetValueReaders.ReusableEntry<>();\n+\n+    MapReader(int definitionLevel, int repetitionLevel,\n+              ParquetValueReader<K> keyReader, ParquetValueReader<V> valueReader) {\n+      super(definitionLevel, repetitionLevel, keyReader, valueReader);\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected ReusableMapData newMapData(MapData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableMapData) {\n+        return (ReusableMapData) reuse;\n+      } else {\n+        return new ReusableMapData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected Map.Entry<K, V> getPair(ReusableMapData map) {\n+      Map.Entry<K, V> kv = nullEntry;\n+      if (readPos < map.capacity()) {\n+        entry.set((K) map.keys.values[readPos], (V) map.values.values[readPos]);\n+        kv = entry;\n+      }\n+\n+      readPos += 1;\n+\n+      return kv;\n+    }\n+\n+    @Override\n+    protected void addPair(ReusableMapData map, K key, V value) {\n+      if (writePos >= map.capacity()) {\n+        map.grow();\n+      }\n+\n+      map.keys.values[writePos] = key;\n+      map.values.values[writePos] = value;\n \n-    RowReader(List<Type> types, List<ParquetValueReader<?>> readers, Types.StructType struct) {\n+      writePos += 1;\n+    }\n+\n+    @Override\n+    protected MapData buildMap(ReusableMapData map) {\n+      map.setNumElements(writePos);\n+      return map;\n+    }\n+  }\n+\n+  private static class RowDataReader extends ParquetValueReaders.StructReader<RowData, GenericRowData> {\n+    private final int numFields;\n+\n+    RowDataReader(List<Type> types, List<ParquetValueReader<?>> readers) {\n       super(types, readers);\n-      this.structType = struct;\n+      this.numFields = readers.size();\n     }\n \n     @Override\n-    protected Row newStructData(Row reuse) {\n-      if (reuse != null) {\n-        return reuse;\n+    protected GenericRowData newStructData(RowData reuse) {\n+      if (reuse instanceof GenericRowData) {\n+        return (GenericRowData) reuse;\n       } else {\n-        return new Row(structType.fields().size());\n+        return new GenericRowData(numFields);\n       }\n     }\n \n     @Override\n-    protected Object getField(Row row, int pos) {\n-      return row.getField(pos);\n+    protected Object getField(GenericRowData intermediate, int pos) {\n+      return intermediate.getField(pos);\n+    }\n+\n+    @Override\n+    protected RowData buildStruct(GenericRowData struct) {\n+      return struct;\n+    }\n+\n+    @Override\n+    protected void set(GenericRowData row, int pos, Object value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setNull(GenericRowData row, int pos) {\n+      row.setField(pos, null);\n     }\n \n     @Override\n-    protected Row buildStruct(Row row) {\n-      return row;\n+    protected void setBoolean(GenericRowData row, int pos, boolean value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setInteger(GenericRowData row, int pos, int value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setLong(GenericRowData row, int pos, long value) {\n+      row.setField(pos, value);\n     }\n \n     @Override\n-    protected void set(Row row, int pos, Object value) {\n+    protected void setFloat(GenericRowData row, int pos, float value) {\n       row.setField(pos, value);\n     }\n+\n+    @Override\n+    protected void setDouble(GenericRowData row, int pos, double value) {\n+      row.setField(pos, value);\n+    }\n+  }\n+\n+  private static class ReusableMapData implements MapData {\n+    private final ReusableArrayData keys;\n+    private final ReusableArrayData values;\n+\n+    private int numElements;\n+\n+    private ReusableMapData() {\n+      this.keys = new ReusableArrayData();\n+      this.values = new ReusableArrayData();\n+    }\n+\n+    private void grow() {\n+      keys.grow();\n+      values.grow();\n+    }\n+\n+    private int capacity() {\n+      return keys.capacity();\n+    }\n+\n+    public void setNumElements(int numElements) {\n+      this.numElements = numElements;\n+      keys.setNumElements(numElements);\n+      values.setNumElements(numElements);\n+    }\n+\n+    @Override\n+    public int size() {\n+      return numElements;\n+    }\n+\n+    @Override\n+    public ReusableArrayData keyArray() {\n+      return keys;\n+    }\n+\n+    @Override\n+    public ReusableArrayData valueArray() {\n+      return values;\n+    }\n+  }\n+\n+  private static class ReusableArrayData implements ArrayData {\n+    private static final Object[] EMPTY = new Object[0];\n+\n+    private Object[] values = EMPTY;\n+    private int numElements = 0;\n+\n+    private void grow() {\n+      if (values.length == 0) {\n+        this.values = new Object[20];\n+      } else {\n+        Object[] old = values;\n+        this.values = new Object[old.length << 2];", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzUwMzU1Mw=="}, "originalCommit": {"oid": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5"}, "originalPosition": 609}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkwMzM1Ng==", "bodyText": "How about moving this method into org.apache.iceberg.data.RandomGenericData,  then we don't need to expose the RandomRecordGenerator to public ?\nbtw,  we could  integrate the genericIcebergGenerics method with RandomGenericData#generate .", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464903356", "createdAt": "2020-08-04T08:56:17Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "diffHunk": "@@ -88,20 +93,46 @@ public Row next() {\n     };\n   }\n \n+  private static Iterable<Record> generateIcebergGenerics(Schema schema, int numRecords,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkwMzYwNg==", "bodyText": "ditto", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464903606", "createdAt": "2020-08-04T08:56:43Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "diffHunk": "@@ -88,20 +93,46 @@ public Row next() {\n     };\n   }\n \n+  private static Iterable<Record> generateIcebergGenerics(Schema schema, int numRecords,\n+                                                   Supplier<RandomGenericData.RandomDataGenerator<Record>> supplier) {\n+    return () -> new Iterator<Record>() {\n+      private final RandomGenericData.RandomDataGenerator<Record> generator = supplier.get();\n+      private int count = 0;\n+\n+      @Override\n+      public boolean hasNext() {\n+        return count < numRecords;\n+      }\n+\n+      @Override\n+      public Record next() {\n+        if (!hasNext()) {\n+          throw new NoSuchElementException();\n+        }\n+        ++count;\n+        return (Record) TypeUtil.visit(schema, generator);\n+      }\n+    };\n+  }\n+\n+\n+  public static Iterable<Record> generateRecords(Schema schema, int numRecords, long seed) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkwMzY5OA==", "bodyText": "ditto.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464903698", "createdAt": "2020-08-04T08:56:51Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "diffHunk": "@@ -88,20 +93,46 @@ public Row next() {\n     };\n   }\n \n+  private static Iterable<Record> generateIcebergGenerics(Schema schema, int numRecords,\n+                                                   Supplier<RandomGenericData.RandomDataGenerator<Record>> supplier) {\n+    return () -> new Iterator<Record>() {\n+      private final RandomGenericData.RandomDataGenerator<Record> generator = supplier.get();\n+      private int count = 0;\n+\n+      @Override\n+      public boolean hasNext() {\n+        return count < numRecords;\n+      }\n+\n+      @Override\n+      public Record next() {\n+        if (!hasNext()) {\n+          throw new NoSuchElementException();\n+        }\n+        ++count;\n+        return (Record) TypeUtil.visit(schema, generator);\n+      }\n+    };\n+  }\n+\n+\n+  public static Iterable<Record> generateRecords(Schema schema, int numRecords, long seed) {\n+    return RandomGenericData.generate(schema, numRecords, seed);\n+  }\n+\n   public static Iterable<Row> generate(Schema schema, int numRecords, long seed) {\n     return generateData(schema, numRecords, () -> new RandomRowGenerator(seed));\n   }\n \n-  public static Iterable<Row> generateFallbackData(Schema schema, int numRecords, long seed, long numDictRows) {\n-    return generateData(schema, numRecords, () -> new FallbackGenerator(seed, numDictRows));\n+  public static Iterable<Record> generateFallbackRecords(Schema schema, int numRecords, long seed, long numDictRows) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkwMzc3MA==", "bodyText": "ditto.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464903770", "createdAt": "2020-08-04T08:56:56Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "diffHunk": "@@ -88,20 +93,46 @@ public Row next() {\n     };\n   }\n \n+  private static Iterable<Record> generateIcebergGenerics(Schema schema, int numRecords,\n+                                                   Supplier<RandomGenericData.RandomDataGenerator<Record>> supplier) {\n+    return () -> new Iterator<Record>() {\n+      private final RandomGenericData.RandomDataGenerator<Record> generator = supplier.get();\n+      private int count = 0;\n+\n+      @Override\n+      public boolean hasNext() {\n+        return count < numRecords;\n+      }\n+\n+      @Override\n+      public Record next() {\n+        if (!hasNext()) {\n+          throw new NoSuchElementException();\n+        }\n+        ++count;\n+        return (Record) TypeUtil.visit(schema, generator);\n+      }\n+    };\n+  }\n+\n+\n+  public static Iterable<Record> generateRecords(Schema schema, int numRecords, long seed) {\n+    return RandomGenericData.generate(schema, numRecords, seed);\n+  }\n+\n   public static Iterable<Row> generate(Schema schema, int numRecords, long seed) {\n     return generateData(schema, numRecords, () -> new RandomRowGenerator(seed));\n   }\n \n-  public static Iterable<Row> generateFallbackData(Schema schema, int numRecords, long seed, long numDictRows) {\n-    return generateData(schema, numRecords, () -> new FallbackGenerator(seed, numDictRows));\n+  public static Iterable<Record> generateFallbackRecords(Schema schema, int numRecords, long seed, long numDictRows) {\n+    return generateIcebergGenerics(schema, numRecords, () -> new FallbackGenerator(seed, numDictRows));\n   }\n \n-  public static Iterable<Row> generateDictionaryEncodableData(Schema schema, int numRecords, long seed) {\n-    return generateData(schema, numRecords, () -> new DictionaryEncodedGenerator(seed));\n+  public static Iterable<Record> generateDictionaryEncodableRecords(Schema schema, int numRecords, long seed) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkwNTkzNg==", "bodyText": "Better to extend the org.apache.iceberg.data.DataTest because it provides more test cases, which COMPLEX_SCHEMA  did not cover.   ( I used COMPLEX_SCHEMA before,  because I don't know there's a better testing method).", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464905936", "createdAt": "2020-08-04T09:00:44Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestFlinkParquetReader.java", "diffHunk": "@@ -35,51 +36,49 @@\n \n import static org.apache.iceberg.flink.data.RandomData.COMPLEX_SCHEMA;\n \n-public class TestFlinkParquetReaderWriter {\n+public class TestFlinkParquetReader {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkxMTIxMQ==", "bodyText": "Seems we share the same primitive value assertion between assertRowData and assertArrayValues, could it be a common method ?", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464911211", "createdAt": "2020-08-04T09:09:58Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, Record expected, RowData actual) {\n+    if (expected == null && actual == null) {\n+      return;\n+    }\n+\n+    List<Type> types = new ArrayList<>();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expected.get(i) == null) {\n+        Assert.assertTrue(actual.isNullAt(i));\n+        continue;\n+      }\n+      Type.TypeID typeId = types.get(i).typeId();\n+      Object value = expected.get(i);\n+      switch (typeId) {\n+        case BOOLEAN:\n+          Assert.assertEquals(\"boolean value should be equal\", value, actual.getBoolean(i));\n+          break;\n+        case INTEGER:\n+          Assert.assertEquals(\"int value should be equal\", value, actual.getInt(i));\n+          break;\n+        case LONG:\n+          Assert.assertEquals(\"long value should be equal\", value, actual.getLong(i));\n+          break;\n+        case FLOAT:\n+          Assert.assertEquals(\"float value should be equal\", value, actual.getFloat(i));\n+          break;\n+        case DOUBLE:\n+          Assert.assertEquals(\"double should be equal\", value, actual.getDouble(i));\n+          break;\n+        case STRING:\n+          Assert.assertTrue(\"Should expect a CharSequence\", value instanceof CharSequence);\n+          Assert.assertEquals(\"string should be equal\", String.valueOf(value), actual.getString(i).toString());\n+          break;\n+        case DATE:\n+          Assert.assertTrue(\"Should expect a Date\", value instanceof LocalDate);\n+          LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, actual.getInt(i));\n+          Assert.assertEquals(\"date should be equal\", value, date);\n+          break;\n+        case TIME:\n+          Assert.assertTrue(\"Should expect a LocalTime\", value instanceof LocalTime);\n+          int milliseconds = (int) (((LocalTime) value).toNanoOfDay() / 1000_000);\n+          Assert.assertEquals(\"time millis should be equal\", milliseconds, actual.getInt(i));\n+          break;\n+        case TIMESTAMP:\n+          if (((Types.TimestampType) type.asPrimitiveType()).shouldAdjustToUTC()) {\n+            Assert.assertTrue(\"Should expect a OffsetDataTime\", value instanceof OffsetDateTime);\n+            OffsetDateTime ts = (OffsetDateTime) value;\n+            Assert.assertEquals(\"OffsetDataTime should be equal\", ts.toLocalDateTime(),\n+                actual.getTimestamp(i, 6).toLocalDateTime());\n+          } else {\n+            Assert.assertTrue(\"Should expect a LocalDataTime\", value instanceof LocalDateTime);\n+            LocalDateTime ts = (LocalDateTime) value;\n+            Assert.assertEquals(\"LocalDataTime should be equal\", ts,\n+                actual.getTimestamp(i, 6).toLocalDateTime());\n+          }\n+          break;\n+        case FIXED:\n+          Assert.assertTrue(\"Should expect byte[]\", value instanceof byte[]);\n+          Assert.assertArrayEquals(\"binary should be equal\", (byte[]) value, actual.getBinary(i));\n+          break;\n+        case BINARY:\n+          Assert.assertTrue(\"Should expect a ByteBuffer\", value instanceof ByteBuffer);\n+          Assert.assertArrayEquals(\"binary should be equal\", ((ByteBuffer) value).array(), actual.getBinary(i));\n+          break;\n+        case DECIMAL:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a"}, "originalPosition": 118}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYwNjM1NzI5", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-460635729", "createdAt": "2020-08-04T09:11:12Z", "commit": {"oid": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwOToxMToxMlrOG7X-Jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwOToxMToxMlrOG7X-Jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkxMTkxMQ==", "bodyText": "This don't have to be public, see comment https://github.com/apache/iceberg/pull/1266/files#r464903356", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464911911", "createdAt": "2020-08-04T09:11:12Z", "author": {"login": "openinx"}, "path": "data/src/test/java/org/apache/iceberg/data/RandomGenericData.java", "diffHunk": "@@ -55,8 +55,8 @@ private RandomGenericData() {}\n     return records;\n   }\n \n-  private static class RandomRecordGenerator extends RandomDataGenerator<Record> {\n-    private RandomRecordGenerator(long seed) {\n+  public static class RandomRecordGenerator extends RandomDataGenerator<Record> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a"}, "originalPosition": 6}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5a62d44065e45034f8bdbca70923735607c434b3", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/5a62d44065e45034f8bdbca70923735607c434b3", "committedDate": "2020-08-04T11:47:33Z", "message": "refactor RandomGenericData"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5069eaa5860cb52e3d837eb00d61039233b840f5", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/5069eaa5860cb52e3d837eb00d61039233b840f5", "committedDate": "2020-08-04T14:11:04Z", "message": "fix failed UT"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9976af28536fa830db6f6c38c4474c339a9ccb0f", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/9976af28536fa830db6f6c38c4474c339a9ccb0f", "committedDate": "2020-08-04T14:24:36Z", "message": "double the size when reallocate the buffer"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f8e130fc17322975d0e970abfcc25f30abbc622d", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/f8e130fc17322975d0e970abfcc25f30abbc622d", "committedDate": "2020-08-07T01:54:10Z", "message": "minor update for naming"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyOTk3NDcy", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-462997472", "createdAt": "2020-08-07T02:35:58Z", "commit": {"oid": "f8e130fc17322975d0e970abfcc25f30abbc622d"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwMjozNTo1OFrOG9KenA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwMzozOTo1N1rOG9LZyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4Nzk5Ng==", "bodyText": "I think here we'd better to create the record lazily, for Iterable<Record> result, (similar to spark RandomData),  because if we wanna to generate lots of records, it will be not easy to OOM .", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466787996", "createdAt": "2020-08-07T02:35:58Z", "author": {"login": "openinx"}, "path": "data/src/test/java/org/apache/iceberg/data/RandomGenericData.java", "diffHunk": "@@ -55,6 +59,14 @@ private RandomGenericData() {}\n     return records;\n   }\n \n+  public static Iterable<Record> generateFallbackRecords(Schema schema, int numRecords, long seed, long numDictRows) {\n+    return generateRecords(schema, numRecords, new FallbackGenerator(seed, numDictRows));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e130fc17322975d0e970abfcc25f30abbc622d"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4ODQ2Mw==", "bodyText": "We usually don't expose this method to public because RandomRecordGenerator is a private static class and others could not access this method actually.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466788463", "createdAt": "2020-08-07T02:37:44Z", "author": {"login": "openinx"}, "path": "data/src/test/java/org/apache/iceberg/data/RandomGenericData.java", "diffHunk": "@@ -46,7 +46,11 @@\n   private RandomGenericData() {}\n \n   public static List<Record> generate(Schema schema, int numRecords, long seed) {\n-    RandomRecordGenerator generator = new RandomRecordGenerator(seed);\n+    return generateRecords(schema, numRecords, new RandomRecordGenerator(seed));\n+  }\n+\n+  public static List<Record> generateRecords(Schema schema, int numRecords,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e130fc17322975d0e970abfcc25f30abbc622d"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5MjY0Ng==", "bodyText": "The DATE type should always be an integer ?   For me,  seems more reasonable to move the DATE case to the place where INT_64 is, because they are surely to use UnboxedReader .", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466792646", "createdAt": "2020-08-07T02:55:00Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,701 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e130fc17322975d0e970abfcc25f30abbc622d"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5NDcyMg==", "bodyText": "Is it correct ?  For my understanding,   the timestamp with time zone don't need to convert to a LocalDateTime (because it has its own time zone),   while the timestamp without time zone need to.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466794722", "createdAt": "2020-08-07T03:03:53Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,701 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            if (((Types.TimestampType) expected).shouldAdjustToUTC()) {\n+              return new TimestampTzReader(desc);\n+            } else {\n+              return new TimestampReader(desc);\n+            }\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n+  }\n+\n+  private static class TimestampTzReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampTzReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromLocalDateTime(Instant.ofEpochSecond(Math.floorDiv(value, 1000_000),\n+          Math.floorMod(value, 1000_000) * 1000)\n+          .atOffset(ZoneOffset.UTC)\n+          .toLocalDateTime());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e130fc17322975d0e970abfcc25f30abbc622d"}, "originalPosition": 289}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5ODcwMA==", "bodyText": "Here we pass the bigDecimal.precision() to DecimalData.fromBigDecimal,  I think it's incorrect.  Because the precision expected for DecimalData is the precision of data type,  it will use this value to do ROUND  etc. You could see the comments in DecimalData:\n\t// The semantics of the fields are as follows:\n\t//  - `precision` and `scale` represent the precision and scale of SQL decimal type\n\t//  - If `decimalVal` is set, it represents the whole decimal value\n\t//  - Otherwise, the decimal value is longVal/(10^scale).\n\t//\n\t// Note that the (precision, scale) must be correct.\n\t// if precision > MAX_COMPACT_PRECISION,\n\t//   `decimalVal` represents the value. `longVal` is undefined\n\t// otherwise, (longVal, scale) represents the value\n\t//   `decimalVal` may be set and cached\n\n\tfinal int precision;\n\tfinal int scale;\nI think we need to pass the decimal type's precision and scale,   and the use the DecimalData.fromUnscaledBytes(binary.getBytes(), precision, scale);  to construct the DecimalData.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466798700", "createdAt": "2020-08-07T03:20:36Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,701 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            if (((Types.TimestampType) expected).shouldAdjustToUTC()) {\n+              return new TimestampTzReader(desc);\n+            } else {\n+              return new TimestampReader(desc);\n+            }\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e130fc17322975d0e970abfcc25f30abbc622d"}, "originalPosition": 242}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5OTEwOQ==", "bodyText": "BTW, is it possible that  we have few unit tests to address this ?", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466799109", "createdAt": "2020-08-07T03:22:29Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,701 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            if (((Types.TimestampType) expected).shouldAdjustToUTC()) {\n+              return new TimestampTzReader(desc);\n+            } else {\n+              return new TimestampReader(desc);\n+            }\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5ODcwMA=="}, "originalCommit": {"oid": "f8e130fc17322975d0e970abfcc25f30abbc622d"}, "originalPosition": 242}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgwMTE4NA==", "bodyText": "Q: when the case readPos >= list.capacity(), seems we don't need to increment the readPos ?  For my understanding,  in that case the getElement should return a null, means that we will read the value without a provided reuse element.  Then we also shouldn't move the readPos because we actually don't reuse any object from the list.\nOf course, if we increment readPos here,  the logic is also correct. but seem have less possibility to reuse the object in array.\nHow do you think ?", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466801184", "createdAt": "2020-08-07T03:31:03Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,701 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            if (((Types.TimestampType) expected).shouldAdjustToUTC()) {\n+              return new TimestampTzReader(desc);\n+            } else {\n+              return new TimestampReader(desc);\n+            }\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n+  }\n+\n+  private static class TimestampTzReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampTzReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromLocalDateTime(Instant.ofEpochSecond(Math.floorDiv(value, 1000_000),\n+          Math.floorMod(value, 1000_000) * 1000)\n+          .atOffset(ZoneOffset.UTC)\n+          .toLocalDateTime());\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class TimestampReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromInstant(Instant.ofEpochSecond(Math.floorDiv(value, 1000_000),\n+          Math.floorMod(value, 1000_000) * 1000));\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n-                                                       List<ParquetValueReader<?>> fieldReaders,\n-                                                       Types.StructType structType) {\n-    return new RowReader(types, fieldReaders, structType);\n+  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n+    StringReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public StringData read(StringData ignored) {\n+      Binary binary = column.nextBinary();\n+      ByteBuffer buffer = binary.toByteBuffer();\n+      if (buffer.hasArray()) {\n+        return StringData.fromBytes(\n+            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n+      } else {\n+        return StringData.fromBytes(binary.getBytes());\n+      }\n+    }\n+  }\n+\n+  private static class LossyMicrosToMillisTimeReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n+    LossyMicrosToMillisTimeReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public Integer read(Integer reuse) {\n+      // Discard microseconds since Flink uses millisecond unit for TIME type.\n+      return (int) Math.floorDiv(column.nextLong(), 1000);\n+    }\n+  }\n+\n+  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n+      super(definitionLevel, repetitionLevel, reader);\n+    }\n+\n+    @Override\n+    protected ReusableArrayData newListData(ArrayData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableArrayData) {\n+        return (ReusableArrayData) reuse;\n+      } else {\n+        return new ReusableArrayData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected E getElement(ReusableArrayData list) {\n+      E value = null;\n+      if (readPos < list.capacity()) {\n+        value = (E) list.values[readPos];\n+      }\n+\n+      readPos += 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e130fc17322975d0e970abfcc25f30abbc622d"}, "originalPosition": 381}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgwMTM5Mg==", "bodyText": "Same question .", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466801392", "createdAt": "2020-08-07T03:31:59Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,701 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            if (((Types.TimestampType) expected).shouldAdjustToUTC()) {\n+              return new TimestampTzReader(desc);\n+            } else {\n+              return new TimestampReader(desc);\n+            }\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n+  }\n+\n+  private static class TimestampTzReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampTzReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromLocalDateTime(Instant.ofEpochSecond(Math.floorDiv(value, 1000_000),\n+          Math.floorMod(value, 1000_000) * 1000)\n+          .atOffset(ZoneOffset.UTC)\n+          .toLocalDateTime());\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class TimestampReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromInstant(Instant.ofEpochSecond(Math.floorDiv(value, 1000_000),\n+          Math.floorMod(value, 1000_000) * 1000));\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n-                                                       List<ParquetValueReader<?>> fieldReaders,\n-                                                       Types.StructType structType) {\n-    return new RowReader(types, fieldReaders, structType);\n+  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n+    StringReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public StringData read(StringData ignored) {\n+      Binary binary = column.nextBinary();\n+      ByteBuffer buffer = binary.toByteBuffer();\n+      if (buffer.hasArray()) {\n+        return StringData.fromBytes(\n+            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n+      } else {\n+        return StringData.fromBytes(binary.getBytes());\n+      }\n+    }\n+  }\n+\n+  private static class LossyMicrosToMillisTimeReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n+    LossyMicrosToMillisTimeReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public Integer read(Integer reuse) {\n+      // Discard microseconds since Flink uses millisecond unit for TIME type.\n+      return (int) Math.floorDiv(column.nextLong(), 1000);\n+    }\n+  }\n+\n+  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n+      super(definitionLevel, repetitionLevel, reader);\n+    }\n+\n+    @Override\n+    protected ReusableArrayData newListData(ArrayData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableArrayData) {\n+        return (ReusableArrayData) reuse;\n+      } else {\n+        return new ReusableArrayData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected E getElement(ReusableArrayData list) {\n+      E value = null;\n+      if (readPos < list.capacity()) {\n+        value = (E) list.values[readPos];\n+      }\n+\n+      readPos += 1;\n+\n+      return value;\n+    }\n+\n+    @Override\n+    protected void addElement(ReusableArrayData reused, E element) {\n+      if (writePos >= reused.capacity()) {\n+        reused.grow();\n+      }\n+\n+      reused.values[writePos] = element;\n+\n+      writePos += 1;\n+    }\n+\n+    @Override\n+    protected ArrayData buildList(ReusableArrayData list) {\n+      list.setNumElements(writePos);\n+      return list;\n+    }\n   }\n \n-  private static class RowReader extends ParquetValueReaders.StructReader<Row, Row> {\n-    private final Types.StructType structType;\n+  private static class MapReader<K, V> extends\n+      ParquetValueReaders.RepeatedKeyValueReader<MapData, ReusableMapData, K, V> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    private final ParquetValueReaders.ReusableEntry<K, V> entry = new ParquetValueReaders.ReusableEntry<>();\n+    private final ParquetValueReaders.ReusableEntry<K, V> nullEntry = new ParquetValueReaders.ReusableEntry<>();\n+\n+    MapReader(int definitionLevel, int repetitionLevel,\n+              ParquetValueReader<K> keyReader, ParquetValueReader<V> valueReader) {\n+      super(definitionLevel, repetitionLevel, keyReader, valueReader);\n+    }\n+\n+    @Override\n+    protected ReusableMapData newMapData(MapData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableMapData) {\n+        return (ReusableMapData) reuse;\n+      } else {\n+        return new ReusableMapData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected Map.Entry<K, V> getPair(ReusableMapData map) {\n+      Map.Entry<K, V> kv = nullEntry;\n+      if (readPos < map.capacity()) {\n+        entry.set((K) map.keys.values[readPos], (V) map.values.values[readPos]);\n+        kv = entry;\n+      }\n+\n+      readPos += 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e130fc17322975d0e970abfcc25f30abbc622d"}, "originalPosition": 440}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgwMzE0Ng==", "bodyText": "nit:  Lists.newArrayList.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466803146", "createdAt": "2020-08-07T03:39:57Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8e130fc17322975d0e970abfcc25f30abbc622d"}, "originalPosition": 58}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "08b40f4a79884165dda109247f226293fd6a1ac4", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/08b40f4a79884165dda109247f226293fd6a1ac4", "committedDate": "2020-08-07T08:42:49Z", "message": "address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzMTcwOTMw", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-463170930", "createdAt": "2020-08-07T09:30:06Z", "commit": {"oid": "08b40f4a79884165dda109247f226293fd6a1ac4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwOTozMDowNlrOG9TJQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwOTozMDowNlrOG9TJQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjkyOTk4Nw==", "bodyText": "nit:  use ((Types.TimestampType) type).shouldAdjustToUTC().", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466929987", "createdAt": "2020-08-07T09:30:06Z", "author": {"login": "openinx"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+    }\n+\n+  }\n+\n+  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        Assert.assertEquals(\"boolean value should be equal\", expected, supplier.get());\n+        break;\n+      case INTEGER:\n+        Assert.assertEquals(\"int value should be equal\", expected, supplier.get());\n+        break;\n+      case LONG:\n+        Assert.assertEquals(\"long value should be equal\", expected, supplier.get());\n+        break;\n+      case FLOAT:\n+        Assert.assertEquals(\"float value should be equal\", expected, supplier.get());\n+        break;\n+      case DOUBLE:\n+        Assert.assertEquals(\"double value should be equal\", expected, supplier.get());\n+        break;\n+      case STRING:\n+        Assert.assertTrue(\"Should expect a CharSequence\", expected instanceof CharSequence);\n+        Assert.assertEquals(\"string should be equal\",\n+            String.valueOf(expected), supplier.get().toString());\n+        break;\n+      case DATE:\n+        Assert.assertTrue(\"Should expect a Date\", expected instanceof LocalDate);\n+        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (int) supplier.get());\n+        Assert.assertEquals(\"date should be equal\", expected, date);\n+        break;\n+      case TIME:\n+        Assert.assertTrue(\"Should expect a LocalTime\", expected instanceof LocalTime);\n+        int milliseconds = (int) (((LocalTime) expected).toNanoOfDay() / 1000_000);\n+        Assert.assertEquals(\"time millis should be equal\", milliseconds, supplier.get());\n+        break;\n+      case TIMESTAMP:\n+        if (((Types.TimestampType) type.asPrimitiveType()).shouldAdjustToUTC()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08b40f4a79884165dda109247f226293fd6a1ac4"}, "originalPosition": 119}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzMTk0NDIx", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-463194421", "createdAt": "2020-08-07T10:08:04Z", "commit": {"oid": "08b40f4a79884165dda109247f226293fd6a1ac4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMDowODowNFrOG9UQAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxMDowODowNFrOG9UQAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njk0ODA5Nw==", "bodyText": "For flink GenericRowData,  we don't provide setInteger/setLong/setFloat interfaces etc, so seems we don't have to override those methods from ParquetValueReaders.StructReader, because all of them have the default implementation, for example:\n    protected void setBoolean(I struct, int pos, boolean value) {\n      set(struct, pos, value);\n    }\nI mean override the set(I struct, int pos, Object value) is enough.\nBTW, I saw Setter interface in ParquetValueReaders.StructReader don't have meaningful usage. Is it necessary to create a separate pull request to remove all methods related to Setter  ?  cc @rdblue .", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466948097", "createdAt": "2020-08-07T10:08:04Z", "author": {"login": "openinx"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,703 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case DATE:\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            if (((Types.TimestampType) expected).shouldAdjustToUTC()) {\n+              return new TimestampTzReader(desc);\n+            } else {\n+              return new TimestampReader(desc);\n+            }\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, precision, scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n+  }\n+\n+  private static class TimestampTzReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampTzReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromLocalDateTime(Instant.ofEpochSecond(Math.floorDiv(value, 1000_000),\n+          Math.floorMod(value, 1000_000) * 1000)\n+          .atOffset(ZoneOffset.UTC)\n+          .toLocalDateTime());\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class TimestampReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromInstant(Instant.ofEpochSecond(Math.floorDiv(value, 1000_000),\n+          Math.floorMod(value, 1000_000) * 1000));\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n-                                                       List<ParquetValueReader<?>> fieldReaders,\n-                                                       Types.StructType structType) {\n-    return new RowReader(types, fieldReaders, structType);\n+  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n+    StringReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public StringData read(StringData ignored) {\n+      Binary binary = column.nextBinary();\n+      ByteBuffer buffer = binary.toByteBuffer();\n+      if (buffer.hasArray()) {\n+        return StringData.fromBytes(\n+            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n+      } else {\n+        return StringData.fromBytes(binary.getBytes());\n+      }\n+    }\n+  }\n+\n+  private static class LossyMicrosToMillisTimeReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n+    LossyMicrosToMillisTimeReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public Integer read(Integer reuse) {\n+      // Discard microseconds since Flink uses millisecond unit for TIME type.\n+      return (int) Math.floorDiv(column.nextLong(), 1000);\n+    }\n+  }\n+\n+  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n+      super(definitionLevel, repetitionLevel, reader);\n+    }\n+\n+    @Override\n+    protected ReusableArrayData newListData(ArrayData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableArrayData) {\n+        return (ReusableArrayData) reuse;\n+      } else {\n+        return new ReusableArrayData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected E getElement(ReusableArrayData list) {\n+      E value = null;\n+      if (readPos < list.capacity()) {\n+        value = (E) list.values[readPos];\n+      }\n+\n+      readPos += 1;\n+\n+      return value;\n+    }\n+\n+    @Override\n+    protected void addElement(ReusableArrayData reused, E element) {\n+      if (writePos >= reused.capacity()) {\n+        reused.grow();\n+      }\n+\n+      reused.values[writePos] = element;\n+\n+      writePos += 1;\n+    }\n+\n+    @Override\n+    protected ArrayData buildList(ReusableArrayData list) {\n+      list.setNumElements(writePos);\n+      return list;\n+    }\n   }\n \n-  private static class RowReader extends ParquetValueReaders.StructReader<Row, Row> {\n-    private final Types.StructType structType;\n+  private static class MapReader<K, V> extends\n+      ParquetValueReaders.RepeatedKeyValueReader<MapData, ReusableMapData, K, V> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    private final ParquetValueReaders.ReusableEntry<K, V> entry = new ParquetValueReaders.ReusableEntry<>();\n+    private final ParquetValueReaders.ReusableEntry<K, V> nullEntry = new ParquetValueReaders.ReusableEntry<>();\n+\n+    MapReader(int definitionLevel, int repetitionLevel,\n+              ParquetValueReader<K> keyReader, ParquetValueReader<V> valueReader) {\n+      super(definitionLevel, repetitionLevel, keyReader, valueReader);\n+    }\n+\n+    @Override\n+    protected ReusableMapData newMapData(MapData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableMapData) {\n+        return (ReusableMapData) reuse;\n+      } else {\n+        return new ReusableMapData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected Map.Entry<K, V> getPair(ReusableMapData map) {\n+      Map.Entry<K, V> kv = nullEntry;\n+      if (readPos < map.capacity()) {\n+        entry.set((K) map.keys.values[readPos], (V) map.values.values[readPos]);\n+        kv = entry;\n+      }\n+\n+      readPos += 1;\n+\n+      return kv;\n+    }\n+\n+    @Override\n+    protected void addPair(ReusableMapData map, K key, V value) {\n+      if (writePos >= map.capacity()) {\n+        map.grow();\n+      }\n+\n+      map.keys.values[writePos] = key;\n+      map.values.values[writePos] = value;\n+\n+      writePos += 1;\n+    }\n \n-    RowReader(List<Type> types, List<ParquetValueReader<?>> readers, Types.StructType struct) {\n+    @Override\n+    protected MapData buildMap(ReusableMapData map) {\n+      map.setNumElements(writePos);\n+      return map;\n+    }\n+  }\n+\n+  private static class RowDataReader extends ParquetValueReaders.StructReader<RowData, GenericRowData> {\n+    private final int numFields;\n+\n+    RowDataReader(List<Type> types, List<ParquetValueReader<?>> readers) {\n       super(types, readers);\n-      this.structType = struct;\n+      this.numFields = readers.size();\n     }\n \n     @Override\n-    protected Row newStructData(Row reuse) {\n-      if (reuse != null) {\n-        return reuse;\n+    protected GenericRowData newStructData(RowData reuse) {\n+      if (reuse instanceof GenericRowData) {\n+        return (GenericRowData) reuse;\n       } else {\n-        return new Row(structType.fields().size());\n+        return new GenericRowData(numFields);\n       }\n     }\n \n     @Override\n-    protected Object getField(Row row, int pos) {\n-      return row.getField(pos);\n+    protected Object getField(GenericRowData intermediate, int pos) {\n+      return intermediate.getField(pos);\n     }\n \n     @Override\n-    protected Row buildStruct(Row row) {\n-      return row;\n+    protected RowData buildStruct(GenericRowData struct) {\n+      return struct;\n     }\n \n     @Override\n-    protected void set(Row row, int pos, Object value) {\n+    protected void set(GenericRowData row, int pos, Object value) {\n       row.setField(pos, value);\n     }\n+\n+    @Override\n+    protected void setNull(GenericRowData row, int pos) {\n+      row.setField(pos, null);\n+    }\n+\n+    @Override\n+    protected void setBoolean(GenericRowData row, int pos, boolean value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setInteger(GenericRowData row, int pos, int value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setLong(GenericRowData row, int pos, long value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setFloat(GenericRowData row, int pos, float value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setDouble(GenericRowData row, int pos, double value) {\n+      row.setField(pos, value);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "08b40f4a79884165dda109247f226293fd6a1ac4"}, "originalPosition": 537}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/0f5bf8317204f59d4bafbefe8530a72a54635cd4", "committedDate": "2020-08-07T13:05:08Z", "message": "address comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MTE1NjA0", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-466115604", "createdAt": "2020-08-12T17:14:48Z", "commit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzoxNDo0OFrOG_q1XQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzoxNDo0OFrOG_q1XQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxNTI2MQ==", "bodyText": "If the objects to validate are Record and RowData, then type should be StructType.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469415261", "createdAt": "2020-08-12T17:14:48Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "originalPosition": 57}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MTE3NDM3", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-466117437", "createdAt": "2020-08-12T17:17:08Z", "commit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzoxNzowOFrOG_q7Iw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzoxNzowOFrOG_q7Iw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxNjczOQ==", "bodyText": "If this uses getFieldOrNull then there is no need to do a null check at the top of this loop. It can be done in assertEquals called here.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469416739", "createdAt": "2020-08-12T17:17:08Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "originalPosition": 81}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MTE4NTMw", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-466118530", "createdAt": "2020-08-12T17:18:33Z", "commit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzoxODozM1rOG_q-qg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzoxODozM1rOG_q-qg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxNzY0Mg==", "bodyText": "Why call supplier.get() in every case when you could pass an object instead of a supplier?", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469417642", "createdAt": "2020-08-12T17:18:33Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+    }\n+  }\n+\n+  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "originalPosition": 85}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MTE5Mjc4", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-466119278", "createdAt": "2020-08-12T17:19:33Z", "commit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzoxOTozM1rOG_rBLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzoxOTozM1rOG_rBLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxODI4NQ==", "bodyText": "Can this use the utility methods to convert instead?", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469418285", "createdAt": "2020-08-12T17:19:33Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+    }\n+  }\n+\n+  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        Assert.assertEquals(\"boolean value should be equal\", expected, supplier.get());\n+        break;\n+      case INTEGER:\n+        Assert.assertEquals(\"int value should be equal\", expected, supplier.get());\n+        break;\n+      case LONG:\n+        Assert.assertEquals(\"long value should be equal\", expected, supplier.get());\n+        break;\n+      case FLOAT:\n+        Assert.assertEquals(\"float value should be equal\", expected, supplier.get());\n+        break;\n+      case DOUBLE:\n+        Assert.assertEquals(\"double value should be equal\", expected, supplier.get());\n+        break;\n+      case STRING:\n+        Assert.assertTrue(\"Should expect a CharSequence\", expected instanceof CharSequence);\n+        Assert.assertEquals(\"string should be equal\",\n+            String.valueOf(expected), supplier.get().toString());\n+        break;\n+      case DATE:\n+        Assert.assertTrue(\"Should expect a Date\", expected instanceof LocalDate);\n+        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (int) supplier.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "originalPosition": 109}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MTIwNDUz", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-466120453", "createdAt": "2020-08-12T17:21:11Z", "commit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzoyMToxMVrOG_rE-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzoyMToxMVrOG_rE-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxOTI1Ng==", "bodyText": "It is good that this uses a different conversion to LocalDateTime.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469419256", "createdAt": "2020-08-12T17:21:11Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+    }\n+  }\n+\n+  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        Assert.assertEquals(\"boolean value should be equal\", expected, supplier.get());\n+        break;\n+      case INTEGER:\n+        Assert.assertEquals(\"int value should be equal\", expected, supplier.get());\n+        break;\n+      case LONG:\n+        Assert.assertEquals(\"long value should be equal\", expected, supplier.get());\n+        break;\n+      case FLOAT:\n+        Assert.assertEquals(\"float value should be equal\", expected, supplier.get());\n+        break;\n+      case DOUBLE:\n+        Assert.assertEquals(\"double value should be equal\", expected, supplier.get());\n+        break;\n+      case STRING:\n+        Assert.assertTrue(\"Should expect a CharSequence\", expected instanceof CharSequence);\n+        Assert.assertEquals(\"string should be equal\",\n+            String.valueOf(expected), supplier.get().toString());\n+        break;\n+      case DATE:\n+        Assert.assertTrue(\"Should expect a Date\", expected instanceof LocalDate);\n+        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (int) supplier.get());\n+        Assert.assertEquals(\"date should be equal\", expected, date);\n+        break;\n+      case TIME:\n+        Assert.assertTrue(\"Should expect a LocalTime\", expected instanceof LocalTime);\n+        int milliseconds = (int) (((LocalTime) expected).toNanoOfDay() / 1000_000);\n+        Assert.assertEquals(\"time millis should be equal\", milliseconds, supplier.get());\n+        break;\n+      case TIMESTAMP:\n+        if (((Types.TimestampType) type).shouldAdjustToUTC()) {\n+          Assert.assertTrue(\"Should expect a OffsetDataTime\", expected instanceof OffsetDateTime);\n+          OffsetDateTime ts = (OffsetDateTime) expected;\n+          Assert.assertEquals(\"OffsetDataTime should be equal\", ts.toLocalDateTime(),\n+              ((TimestampData) supplier.get()).toLocalDateTime());\n+        } else {\n+          Assert.assertTrue(\"Should expect a LocalDataTime\", expected instanceof LocalDateTime);\n+          LocalDateTime ts = (LocalDateTime) expected;\n+          Assert.assertEquals(\"LocalDataTime should be equal\", ts,\n+              ((TimestampData) supplier.get()).toLocalDateTime());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "originalPosition": 127}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MTIwODY0", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-466120864", "createdAt": "2020-08-12T17:21:44Z", "commit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzoyMTo0NFrOG_rGTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzoyMTo0NFrOG_rGTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxOTU5OQ==", "bodyText": "Why does this convert to LocalDateTime before validating? Can TimestampData be converted to OffsetDateTime?", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469419599", "createdAt": "2020-08-12T17:21:44Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+    }\n+  }\n+\n+  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        Assert.assertEquals(\"boolean value should be equal\", expected, supplier.get());\n+        break;\n+      case INTEGER:\n+        Assert.assertEquals(\"int value should be equal\", expected, supplier.get());\n+        break;\n+      case LONG:\n+        Assert.assertEquals(\"long value should be equal\", expected, supplier.get());\n+        break;\n+      case FLOAT:\n+        Assert.assertEquals(\"float value should be equal\", expected, supplier.get());\n+        break;\n+      case DOUBLE:\n+        Assert.assertEquals(\"double value should be equal\", expected, supplier.get());\n+        break;\n+      case STRING:\n+        Assert.assertTrue(\"Should expect a CharSequence\", expected instanceof CharSequence);\n+        Assert.assertEquals(\"string should be equal\",\n+            String.valueOf(expected), supplier.get().toString());\n+        break;\n+      case DATE:\n+        Assert.assertTrue(\"Should expect a Date\", expected instanceof LocalDate);\n+        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (int) supplier.get());\n+        Assert.assertEquals(\"date should be equal\", expected, date);\n+        break;\n+      case TIME:\n+        Assert.assertTrue(\"Should expect a LocalTime\", expected instanceof LocalTime);\n+        int milliseconds = (int) (((LocalTime) expected).toNanoOfDay() / 1000_000);\n+        Assert.assertEquals(\"time millis should be equal\", milliseconds, supplier.get());\n+        break;\n+      case TIMESTAMP:\n+        if (((Types.TimestampType) type).shouldAdjustToUTC()) {\n+          Assert.assertTrue(\"Should expect a OffsetDataTime\", expected instanceof OffsetDateTime);\n+          OffsetDateTime ts = (OffsetDateTime) expected;\n+          Assert.assertEquals(\"OffsetDataTime should be equal\", ts.toLocalDateTime(),\n+              ((TimestampData) supplier.get()).toLocalDateTime());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "originalPosition": 122}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MTIyMjY2", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-466122266", "createdAt": "2020-08-12T17:23:40Z", "commit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzoyMzo0MFrOG_rKkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzoyMzo0MFrOG_rKkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyMDY5MA==", "bodyText": "This test is brittle because it assumes:\n\nByteBuffer is a HeapByteBuffer\nThe position and limit of the ByteBuffer are 0 and buffer.array().length\nThe buffer's arrayOffset is 0\n\nIt would be better to convert the byte array to a ByteBuffer and validate equality with buffers.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469420690", "createdAt": "2020-08-12T17:23:40Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+    }\n+  }\n+\n+  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        Assert.assertEquals(\"boolean value should be equal\", expected, supplier.get());\n+        break;\n+      case INTEGER:\n+        Assert.assertEquals(\"int value should be equal\", expected, supplier.get());\n+        break;\n+      case LONG:\n+        Assert.assertEquals(\"long value should be equal\", expected, supplier.get());\n+        break;\n+      case FLOAT:\n+        Assert.assertEquals(\"float value should be equal\", expected, supplier.get());\n+        break;\n+      case DOUBLE:\n+        Assert.assertEquals(\"double value should be equal\", expected, supplier.get());\n+        break;\n+      case STRING:\n+        Assert.assertTrue(\"Should expect a CharSequence\", expected instanceof CharSequence);\n+        Assert.assertEquals(\"string should be equal\",\n+            String.valueOf(expected), supplier.get().toString());\n+        break;\n+      case DATE:\n+        Assert.assertTrue(\"Should expect a Date\", expected instanceof LocalDate);\n+        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (int) supplier.get());\n+        Assert.assertEquals(\"date should be equal\", expected, date);\n+        break;\n+      case TIME:\n+        Assert.assertTrue(\"Should expect a LocalTime\", expected instanceof LocalTime);\n+        int milliseconds = (int) (((LocalTime) expected).toNanoOfDay() / 1000_000);\n+        Assert.assertEquals(\"time millis should be equal\", milliseconds, supplier.get());\n+        break;\n+      case TIMESTAMP:\n+        if (((Types.TimestampType) type).shouldAdjustToUTC()) {\n+          Assert.assertTrue(\"Should expect a OffsetDataTime\", expected instanceof OffsetDateTime);\n+          OffsetDateTime ts = (OffsetDateTime) expected;\n+          Assert.assertEquals(\"OffsetDataTime should be equal\", ts.toLocalDateTime(),\n+              ((TimestampData) supplier.get()).toLocalDateTime());\n+        } else {\n+          Assert.assertTrue(\"Should expect a LocalDataTime\", expected instanceof LocalDateTime);\n+          LocalDateTime ts = (LocalDateTime) expected;\n+          Assert.assertEquals(\"LocalDataTime should be equal\", ts,\n+              ((TimestampData) supplier.get()).toLocalDateTime());\n+        }\n+        break;\n+      case BINARY:\n+        Assert.assertTrue(\"Should expect a ByteBuffer\", expected instanceof ByteBuffer);\n+        Assert.assertArrayEquals(\"binary should be equal\", ((ByteBuffer) expected).array(), (byte[]) supplier.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "originalPosition": 132}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MTI3MTUx", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-466127151", "createdAt": "2020-08-12T17:30:04Z", "commit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzozMDowNVrOG_rZjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzozMDowNVrOG_rZjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyNDUyNg==", "bodyText": "I think it would be better to keep this already large method a bit smaller and add a method to assert array equality, not just one for values. Same with maps. An assertEquals(MapType, LogicalType, Map, MapData) method would be helpful.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469424526", "createdAt": "2020-08-12T17:30:05Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+    }\n+  }\n+\n+  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        Assert.assertEquals(\"boolean value should be equal\", expected, supplier.get());\n+        break;\n+      case INTEGER:\n+        Assert.assertEquals(\"int value should be equal\", expected, supplier.get());\n+        break;\n+      case LONG:\n+        Assert.assertEquals(\"long value should be equal\", expected, supplier.get());\n+        break;\n+      case FLOAT:\n+        Assert.assertEquals(\"float value should be equal\", expected, supplier.get());\n+        break;\n+      case DOUBLE:\n+        Assert.assertEquals(\"double value should be equal\", expected, supplier.get());\n+        break;\n+      case STRING:\n+        Assert.assertTrue(\"Should expect a CharSequence\", expected instanceof CharSequence);\n+        Assert.assertEquals(\"string should be equal\",\n+            String.valueOf(expected), supplier.get().toString());\n+        break;\n+      case DATE:\n+        Assert.assertTrue(\"Should expect a Date\", expected instanceof LocalDate);\n+        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (int) supplier.get());\n+        Assert.assertEquals(\"date should be equal\", expected, date);\n+        break;\n+      case TIME:\n+        Assert.assertTrue(\"Should expect a LocalTime\", expected instanceof LocalTime);\n+        int milliseconds = (int) (((LocalTime) expected).toNanoOfDay() / 1000_000);\n+        Assert.assertEquals(\"time millis should be equal\", milliseconds, supplier.get());\n+        break;\n+      case TIMESTAMP:\n+        if (((Types.TimestampType) type).shouldAdjustToUTC()) {\n+          Assert.assertTrue(\"Should expect a OffsetDataTime\", expected instanceof OffsetDateTime);\n+          OffsetDateTime ts = (OffsetDateTime) expected;\n+          Assert.assertEquals(\"OffsetDataTime should be equal\", ts.toLocalDateTime(),\n+              ((TimestampData) supplier.get()).toLocalDateTime());\n+        } else {\n+          Assert.assertTrue(\"Should expect a LocalDataTime\", expected instanceof LocalDateTime);\n+          LocalDateTime ts = (LocalDateTime) expected;\n+          Assert.assertEquals(\"LocalDataTime should be equal\", ts,\n+              ((TimestampData) supplier.get()).toLocalDateTime());\n+        }\n+        break;\n+      case BINARY:\n+        Assert.assertTrue(\"Should expect a ByteBuffer\", expected instanceof ByteBuffer);\n+        Assert.assertArrayEquals(\"binary should be equal\", ((ByteBuffer) expected).array(), (byte[]) supplier.get());\n+        break;\n+      case DECIMAL:\n+        Assert.assertTrue(\"Should expect a BigDecimal\", expected instanceof BigDecimal);\n+        BigDecimal bd = (BigDecimal) expected;\n+        Assert.assertEquals(\"decimal value should be equal\", bd,\n+            ((DecimalData) supplier.get()).toBigDecimal());\n+        break;\n+      case LIST:\n+        Assert.assertTrue(\"Should expect a Collection\", expected instanceof Collection);\n+        Collection<?> expectedArrayData = (Collection<?>) expected;\n+        ArrayData actualArrayData = (ArrayData) supplier.get();\n+        LogicalType elementType = ((ArrayType) logicalType).getElementType();\n+        Assert.assertEquals(\"array length should be equal\", expectedArrayData.size(), actualArrayData.size());\n+        assertArrayValues(type.asListType().elementType(), elementType, expectedArrayData, actualArrayData);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "originalPosition": 146}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MTI5MTkx", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-466129191", "createdAt": "2020-08-12T17:32:46Z", "commit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzozMjo0N1rOG_rftw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzozMjo0N1rOG_rftw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyNjEwMw==", "bodyText": "keySet isn't required to be an ordered collection, so this could easily break if keys are not returned in the same order as they are in MapData. This also allows maps to be equal that actually are not, like Map('b' -> 1, 'a' -> 2) and Map('a' -> 1, 'b' -> 2) if the first map's keys are reordered.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469426103", "createdAt": "2020-08-12T17:32:47Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+    }\n+  }\n+\n+  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        Assert.assertEquals(\"boolean value should be equal\", expected, supplier.get());\n+        break;\n+      case INTEGER:\n+        Assert.assertEquals(\"int value should be equal\", expected, supplier.get());\n+        break;\n+      case LONG:\n+        Assert.assertEquals(\"long value should be equal\", expected, supplier.get());\n+        break;\n+      case FLOAT:\n+        Assert.assertEquals(\"float value should be equal\", expected, supplier.get());\n+        break;\n+      case DOUBLE:\n+        Assert.assertEquals(\"double value should be equal\", expected, supplier.get());\n+        break;\n+      case STRING:\n+        Assert.assertTrue(\"Should expect a CharSequence\", expected instanceof CharSequence);\n+        Assert.assertEquals(\"string should be equal\",\n+            String.valueOf(expected), supplier.get().toString());\n+        break;\n+      case DATE:\n+        Assert.assertTrue(\"Should expect a Date\", expected instanceof LocalDate);\n+        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (int) supplier.get());\n+        Assert.assertEquals(\"date should be equal\", expected, date);\n+        break;\n+      case TIME:\n+        Assert.assertTrue(\"Should expect a LocalTime\", expected instanceof LocalTime);\n+        int milliseconds = (int) (((LocalTime) expected).toNanoOfDay() / 1000_000);\n+        Assert.assertEquals(\"time millis should be equal\", milliseconds, supplier.get());\n+        break;\n+      case TIMESTAMP:\n+        if (((Types.TimestampType) type).shouldAdjustToUTC()) {\n+          Assert.assertTrue(\"Should expect a OffsetDataTime\", expected instanceof OffsetDateTime);\n+          OffsetDateTime ts = (OffsetDateTime) expected;\n+          Assert.assertEquals(\"OffsetDataTime should be equal\", ts.toLocalDateTime(),\n+              ((TimestampData) supplier.get()).toLocalDateTime());\n+        } else {\n+          Assert.assertTrue(\"Should expect a LocalDataTime\", expected instanceof LocalDateTime);\n+          LocalDateTime ts = (LocalDateTime) expected;\n+          Assert.assertEquals(\"LocalDataTime should be equal\", ts,\n+              ((TimestampData) supplier.get()).toLocalDateTime());\n+        }\n+        break;\n+      case BINARY:\n+        Assert.assertTrue(\"Should expect a ByteBuffer\", expected instanceof ByteBuffer);\n+        Assert.assertArrayEquals(\"binary should be equal\", ((ByteBuffer) expected).array(), (byte[]) supplier.get());\n+        break;\n+      case DECIMAL:\n+        Assert.assertTrue(\"Should expect a BigDecimal\", expected instanceof BigDecimal);\n+        BigDecimal bd = (BigDecimal) expected;\n+        Assert.assertEquals(\"decimal value should be equal\", bd,\n+            ((DecimalData) supplier.get()).toBigDecimal());\n+        break;\n+      case LIST:\n+        Assert.assertTrue(\"Should expect a Collection\", expected instanceof Collection);\n+        Collection<?> expectedArrayData = (Collection<?>) expected;\n+        ArrayData actualArrayData = (ArrayData) supplier.get();\n+        LogicalType elementType = ((ArrayType) logicalType).getElementType();\n+        Assert.assertEquals(\"array length should be equal\", expectedArrayData.size(), actualArrayData.size());\n+        assertArrayValues(type.asListType().elementType(), elementType, expectedArrayData, actualArrayData);\n+        break;\n+      case MAP:\n+        Assert.assertTrue(\"Should expect a Map\", expected instanceof Map);\n+        MapData actual = (MapData) supplier.get();\n+        Assert.assertEquals(\"map size should be equal\",\n+            ((Map<?, ?>) expected).size(), actual.size());\n+        Collection<?> expectedKeyArrayData = ((Map<?, ?>) expected).keySet();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "originalPosition": 153}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MTM0MTcz", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-466134173", "createdAt": "2020-08-12T17:39:34Z", "commit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzozOTozNFrOG_rvCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzozOTozNFrOG_rvCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQzMDAyNw==", "bodyText": "Shouldn't this check whether the timestamp is a TIMESTAMP WITH ZONE or TIMESTAMP WITHOUT ZONE and return the correct reader? Spark doesn't do this because it doesn't support timestamp without zone, so we know it is always a timestamptz.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469430027", "createdAt": "2020-08-12T17:39:34Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,678 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case DATE:\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            return new TimestampMicrosReader(desc);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "originalPosition": 175}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MTM1Mjk1", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-466135295", "createdAt": "2020-08-12T17:41:04Z", "commit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzo0MTowNFrOG_ryiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzo0MTowNFrOG_ryiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQzMDkyMA==", "bodyText": "What about TIME_MILLIS? It won't be written by Iceberg, but it is usually good to be able to read from imported files.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469430920", "createdAt": "2020-08-12T17:41:04Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,678 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "originalPosition": 169}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MTM1NDQ0", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-466135444", "createdAt": "2020-08-12T17:41:17Z", "commit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzo0MToxN1rOG_ry8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzo0MToxN1rOG_ry8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQzMTAyNQ==", "bodyText": "Why not support TIMESTAMP_MILLIS as well?", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469431025", "createdAt": "2020-08-12T17:41:17Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,678 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case DATE:\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "originalPosition": 174}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY2MTQ0MTI2", "url": "https://github.com/apache/iceberg/pull/1266#pullrequestreview-466144126", "createdAt": "2020-08-12T17:53:26Z", "commit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzo1MzoyNlrOG_sORw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQxNzo1MzoyNlrOG_sORw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQzODAyMw==", "bodyText": "I think we can use the same ReusableMapData class for both Spark and Flink, since they are so similar. Some of the data access methods in the concrete classes would need to be implemented in a concrete class the engine-specific interface, but the main parts could be shared. And I think that would mean we could share significant parts of the reader classes as well.\nI don't think we need to do this refactor now, but we should try to deduplicate this.", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469438023", "createdAt": "2020-08-12T17:53:26Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,678 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case DATE:\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            return new TimestampMicrosReader(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      // TODO: need a unit test to write-read-validate decimal via FlinkParquetWrite/Reader\n+      return DecimalData.fromBigDecimal(bigDecimal, precision, scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n-                                                       List<ParquetValueReader<?>> fieldReaders,\n-                                                       Types.StructType structType) {\n-    return new RowReader(types, fieldReaders, structType);\n+  private static class TimestampMicrosReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampMicrosReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long micros = readLong();\n+      return TimestampData.fromEpochMillis(Math.floorDiv(micros, 1_000),\n+          (int) Math.floorMod(micros, 1_000) * 1_000);\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n+  }\n+\n+  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n+    StringReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public StringData read(StringData ignored) {\n+      Binary binary = column.nextBinary();\n+      ByteBuffer buffer = binary.toByteBuffer();\n+      if (buffer.hasArray()) {\n+        return StringData.fromBytes(\n+            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n+      } else {\n+        return StringData.fromBytes(binary.getBytes());\n+      }\n+    }\n+  }\n+\n+  private static class LossyMicrosToMillisTimeReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n+    LossyMicrosToMillisTimeReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public Integer read(Integer reuse) {\n+      // Discard microseconds since Flink uses millisecond unit for TIME type.\n+      return (int) Math.floorDiv(column.nextLong(), 1000);\n+    }\n+  }\n+\n+  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n+      super(definitionLevel, repetitionLevel, reader);\n+    }\n+\n+    @Override\n+    protected ReusableArrayData newListData(ArrayData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableArrayData) {\n+        return (ReusableArrayData) reuse;\n+      } else {\n+        return new ReusableArrayData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected E getElement(ReusableArrayData list) {\n+      E value = null;\n+      if (readPos < list.capacity()) {\n+        value = (E) list.values[readPos];\n+      }\n+\n+      readPos += 1;\n+\n+      return value;\n+    }\n+\n+    @Override\n+    protected void addElement(ReusableArrayData reused, E element) {\n+      if (writePos >= reused.capacity()) {\n+        reused.grow();\n+      }\n+\n+      reused.values[writePos] = element;\n+\n+      writePos += 1;\n+    }\n+\n+    @Override\n+    protected ArrayData buildList(ReusableArrayData list) {\n+      list.setNumElements(writePos);\n+      return list;\n+    }\n+  }\n+\n+  private static class MapReader<K, V> extends\n+      ParquetValueReaders.RepeatedKeyValueReader<MapData, ReusableMapData, K, V> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    private final ParquetValueReaders.ReusableEntry<K, V> entry = new ParquetValueReaders.ReusableEntry<>();\n+    private final ParquetValueReaders.ReusableEntry<K, V> nullEntry = new ParquetValueReaders.ReusableEntry<>();\n+\n+    MapReader(int definitionLevel, int repetitionLevel,\n+              ParquetValueReader<K> keyReader, ParquetValueReader<V> valueReader) {\n+      super(definitionLevel, repetitionLevel, keyReader, valueReader);\n+    }\n+\n+    @Override\n+    protected ReusableMapData newMapData(MapData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableMapData) {\n+        return (ReusableMapData) reuse;\n+      } else {\n+        return new ReusableMapData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected Map.Entry<K, V> getPair(ReusableMapData map) {\n+      Map.Entry<K, V> kv = nullEntry;\n+      if (readPos < map.capacity()) {\n+        entry.set((K) map.keys.values[readPos], (V) map.values.values[readPos]);\n+        kv = entry;\n+      }\n+\n+      readPos += 1;\n+\n+      return kv;\n+    }\n+\n+    @Override\n+    protected void addPair(ReusableMapData map, K key, V value) {\n+      if (writePos >= map.capacity()) {\n+        map.grow();\n+      }\n+\n+      map.keys.values[writePos] = key;\n+      map.values.values[writePos] = value;\n+\n+      writePos += 1;\n+    }\n+\n+    @Override\n+    protected MapData buildMap(ReusableMapData map) {\n+      map.setNumElements(writePos);\n+      return map;\n+    }\n   }\n \n-  private static class RowReader extends ParquetValueReaders.StructReader<Row, Row> {\n-    private final Types.StructType structType;\n+  private static class RowDataReader extends ParquetValueReaders.StructReader<RowData, GenericRowData> {\n+    private final int numFields;\n \n-    RowReader(List<Type> types, List<ParquetValueReader<?>> readers, Types.StructType struct) {\n+    RowDataReader(List<Type> types, List<ParquetValueReader<?>> readers) {\n       super(types, readers);\n-      this.structType = struct;\n+      this.numFields = readers.size();\n     }\n \n     @Override\n-    protected Row newStructData(Row reuse) {\n-      if (reuse != null) {\n-        return reuse;\n+    protected GenericRowData newStructData(RowData reuse) {\n+      if (reuse instanceof GenericRowData) {\n+        return (GenericRowData) reuse;\n       } else {\n-        return new Row(structType.fields().size());\n+        return new GenericRowData(numFields);\n       }\n     }\n \n     @Override\n-    protected Object getField(Row row, int pos) {\n-      return row.getField(pos);\n+    protected Object getField(GenericRowData intermediate, int pos) {\n+      return intermediate.getField(pos);\n+    }\n+\n+    @Override\n+    protected RowData buildStruct(GenericRowData struct) {\n+      return struct;\n+    }\n+\n+    @Override\n+    protected void set(GenericRowData row, int pos, Object value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setNull(GenericRowData row, int pos) {\n+      row.setField(pos, null);\n+    }\n+\n+    @Override\n+    protected void setBoolean(GenericRowData row, int pos, boolean value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setInteger(GenericRowData row, int pos, int value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setLong(GenericRowData row, int pos, long value) {\n+      row.setField(pos, value);\n     }\n \n     @Override\n-    protected Row buildStruct(Row row) {\n-      return row;\n+    protected void setFloat(GenericRowData row, int pos, float value) {\n+      row.setField(pos, value);\n     }\n \n     @Override\n-    protected void set(Row row, int pos, Object value) {\n+    protected void setDouble(GenericRowData row, int pos, double value) {\n       row.setField(pos, value);\n     }\n   }\n+\n+  private static class ReusableMapData implements MapData {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4"}, "originalPosition": 515}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e1407d4deec232f8f3e9bcd213b28c7dfde48967", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/e1407d4deec232f8f3e9bcd213b28c7dfde48967", "committedDate": "2020-08-13T04:30:03Z", "message": "address comments"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4358, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}