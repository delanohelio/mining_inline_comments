{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE2MzIzNjkw", "number": 1728, "reviewThreads": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQyMDo1Mzo0MlrOE11xhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxODo1NDo1M1rOE2M40w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0ODkxMDE1OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/antlr/org.apache.spark.sql.catalyst.parser.extensions/IcebergSqlExtensions.g4", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQyMDo1Mzo0M1rOHuWDtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNjoyMToyMFrOHu0moA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM1Nzk0MA==", "bodyText": "It is debatable whether we should have Iceberg in names. We will add MIGRATE and SNAPSHOT here later too. However, CALL will go away once it is available in Spark.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518357940", "createdAt": "2020-11-05T20:53:43Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/antlr/org.apache.spark.sql.catalyst.parser.extensions/IcebergSqlExtensions.g4", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * This file is an adaptation of Presto's and Spark's grammar files.\n+ */\n+\n+grammar IcebergSqlExtensions;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM2MzAwOA==", "bodyText": "I'd also like to add ALTER TABLE ... [ADD|DROP] PARTITION FIELD ...\nI like having Iceberg in the name because it will be more clear whether something is coming from Iceberg or Spark.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518363008", "createdAt": "2020-11-05T21:03:42Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/antlr/org.apache.spark.sql.catalyst.parser.extensions/IcebergSqlExtensions.g4", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * This file is an adaptation of Presto's and Spark's grammar files.\n+ */\n+\n+grammar IcebergSqlExtensions;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM1Nzk0MA=="}, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU4NzIyMQ==", "bodyText": "It would be great if we can come up with a list of commands that Iceberg would like to add. If possible, I am planning to add VACUUM, OPTIMIZE and RESTORE after this module is added.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518587221", "createdAt": "2020-11-06T08:16:41Z", "author": {"login": "jackye1995"}, "path": "spark3-extensions/src/main/antlr/org.apache.spark.sql.catalyst.parser.extensions/IcebergSqlExtensions.g4", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * This file is an adaptation of Presto's and Spark's grammar files.\n+ */\n+\n+grammar IcebergSqlExtensions;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM1Nzk0MA=="}, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg1Njg4Nw==", "bodyText": "We have a design doc on which commands/stored procedures we are going to support. In addition, we have a design doc on how data compaction procedures will look like (i.e. OPTIMIZE). I need to update the latter one.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518856887", "createdAt": "2020-11-06T16:18:50Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/antlr/org.apache.spark.sql.catalyst.parser.extensions/IcebergSqlExtensions.g4", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * This file is an adaptation of Presto's and Spark's grammar files.\n+ */\n+\n+grammar IcebergSqlExtensions;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM1Nzk0MA=="}, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg1ODQwMA==", "bodyText": "While I think we want to support VACUUM in a way that is close to Redshift, the design of the command requires further consideration. I'd also represent OPTIMIZE as stored procedures.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518858400", "createdAt": "2020-11-06T16:21:20Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/antlr/org.apache.spark.sql.catalyst.parser.extensions/IcebergSqlExtensions.g4", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * This file is an adaptation of Presto's and Spark's grammar files.\n+ */\n+\n+grammar IcebergSqlExtensions;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM1Nzk0MA=="}, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTE3MTAxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQyMjowNTowN1rOHuYf6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNzoxNjozMFrOHu2mtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM5NzkyOA==", "bodyText": "Could we put this in a different file? I'd also like to separate the parts that are inherited from Spark from the ones that are Iceberg-specific. What do you think about creating an abstract CatalystAstBuilder and extending it in IcebergAstBuilder?", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518397928", "createdAt": "2020-11-05T22:05:07Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import java.util.Locale\n+import javax.xml.bind.DatatypeConverter\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.atn.PredictionMode\n+import org.antlr.v4.runtime.misc.{Interval, ParseCancellationException}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNodeImpl}\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.parser.{ParseErrorListener, ParseException, ParserInterface}\n+import org.apache.spark.sql.catalyst.parser.ParserUtils._\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.{CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.trees.Origin\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.{getZoneId, stringToDate, stringToTimestamp}\n+import org.apache.spark.sql.catalyst.util.IntervalUtils\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, CalendarIntervalType, DataType, DateType, DoubleType, FloatType, LongType, ShortType, StructType, TimestampType}\n+import org.apache.spark.unsafe.types.UTF8String\n+import scala.collection.JavaConverters._\n+\n+class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserInterface {\n+\n+  private val astBuilder = new IcebergSqlExtensionsAstBuilder()\n+\n+  /**\n+   * Parse a string to a DataType.\n+   */\n+  override def parseDataType(sqlText: String): DataType = {\n+    delegate.parseDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a raw DataType without CHAR/VARCHAR replacement.\n+   */\n+  override def parseRawDataType(sqlText: String): DataType = {\n+    delegate.parseRawDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to an Expression.\n+   */\n+  override def parseExpression(sqlText: String): Expression = {\n+    delegate.parseExpression(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a TableIdentifier.\n+   */\n+  override def parseTableIdentifier(sqlText: String): TableIdentifier = {\n+    delegate.parseTableIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a FunctionIdentifier.\n+   */\n+  override def parseFunctionIdentifier(sqlText: String): FunctionIdentifier = {\n+    delegate.parseFunctionIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a multi-part identifier.\n+   */\n+  override def parseMultipartIdentifier(sqlText: String): Seq[String] = {\n+    delegate.parseMultipartIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Creates StructType for a given SQL string, which is a comma separated list of field\n+   * definitions which will preserve the correct Hive metadata.\n+   */\n+  override def parseTableSchema(sqlText: String): StructType = {\n+    delegate.parseTableSchema(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a LogicalPlan.\n+   */\n+  override def parsePlan(sqlText: String): LogicalPlan = parse(sqlText) { parser =>\n+    astBuilder.visit(parser.singleStatement()) match {\n+      case plan: LogicalPlan => plan\n+      case _ => delegate.parsePlan(sqlText)\n+    }\n+  }\n+\n+  protected def parse[T](command: String)(toResult: IcebergSqlExtensionsParser => T): T = {\n+    val lexer = new IcebergSqlExtensionsLexer(new UpperCaseCharStream(CharStreams.fromString(command)))\n+    lexer.removeErrorListeners()\n+    lexer.addErrorListener(ParseErrorListener)\n+\n+    val tokenStream = new CommonTokenStream(lexer)\n+    val parser = new IcebergSqlExtensionsParser(tokenStream)\n+    parser.addParseListener(PostProcessor)\n+    parser.removeErrorListeners()\n+    parser.addErrorListener(ParseErrorListener)\n+\n+    try {\n+      try {\n+        // first, try parsing with potentially faster SLL mode\n+        parser.getInterpreter.setPredictionMode(PredictionMode.SLL)\n+        toResult(parser)\n+      }\n+      catch {\n+        case _: ParseCancellationException =>\n+          // if we fail, parse with LL mode\n+          tokenStream.seek(0) // rewind input stream\n+          parser.reset()\n+\n+          // Try Again.\n+          parser.getInterpreter.setPredictionMode(PredictionMode.LL)\n+          toResult(parser)\n+      }\n+    }\n+    catch {\n+      case e: ParseException if e.command.isDefined =>\n+        throw e\n+      case e: ParseException =>\n+        throw e.withCommand(command)\n+      case e: AnalysisException =>\n+        val position = Origin(e.line, e.startPosition)\n+        throw new ParseException(Option(command), e.message, position, position)\n+    }\n+  }\n+}\n+\n+// literal parsing is taken from Spark's AstBuilder\n+class IcebergSqlExtensionsAstBuilder extends IcebergSqlExtensionsBaseVisitor[AnyRef] {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MTE4OA==", "bodyText": "Done. I did not introduce CatalystAstBuilder since this class became pretty small right now.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518891188", "createdAt": "2020-11-06T17:16:30Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import java.util.Locale\n+import javax.xml.bind.DatatypeConverter\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.atn.PredictionMode\n+import org.antlr.v4.runtime.misc.{Interval, ParseCancellationException}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNodeImpl}\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.parser.{ParseErrorListener, ParseException, ParserInterface}\n+import org.apache.spark.sql.catalyst.parser.ParserUtils._\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.{CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.trees.Origin\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.{getZoneId, stringToDate, stringToTimestamp}\n+import org.apache.spark.sql.catalyst.util.IntervalUtils\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, CalendarIntervalType, DataType, DateType, DoubleType, FloatType, LongType, ShortType, StructType, TimestampType}\n+import org.apache.spark.unsafe.types.UTF8String\n+import scala.collection.JavaConverters._\n+\n+class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserInterface {\n+\n+  private val astBuilder = new IcebergSqlExtensionsAstBuilder()\n+\n+  /**\n+   * Parse a string to a DataType.\n+   */\n+  override def parseDataType(sqlText: String): DataType = {\n+    delegate.parseDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a raw DataType without CHAR/VARCHAR replacement.\n+   */\n+  override def parseRawDataType(sqlText: String): DataType = {\n+    delegate.parseRawDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to an Expression.\n+   */\n+  override def parseExpression(sqlText: String): Expression = {\n+    delegate.parseExpression(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a TableIdentifier.\n+   */\n+  override def parseTableIdentifier(sqlText: String): TableIdentifier = {\n+    delegate.parseTableIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a FunctionIdentifier.\n+   */\n+  override def parseFunctionIdentifier(sqlText: String): FunctionIdentifier = {\n+    delegate.parseFunctionIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a multi-part identifier.\n+   */\n+  override def parseMultipartIdentifier(sqlText: String): Seq[String] = {\n+    delegate.parseMultipartIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Creates StructType for a given SQL string, which is a comma separated list of field\n+   * definitions which will preserve the correct Hive metadata.\n+   */\n+  override def parseTableSchema(sqlText: String): StructType = {\n+    delegate.parseTableSchema(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a LogicalPlan.\n+   */\n+  override def parsePlan(sqlText: String): LogicalPlan = parse(sqlText) { parser =>\n+    astBuilder.visit(parser.singleStatement()) match {\n+      case plan: LogicalPlan => plan\n+      case _ => delegate.parsePlan(sqlText)\n+    }\n+  }\n+\n+  protected def parse[T](command: String)(toResult: IcebergSqlExtensionsParser => T): T = {\n+    val lexer = new IcebergSqlExtensionsLexer(new UpperCaseCharStream(CharStreams.fromString(command)))\n+    lexer.removeErrorListeners()\n+    lexer.addErrorListener(ParseErrorListener)\n+\n+    val tokenStream = new CommonTokenStream(lexer)\n+    val parser = new IcebergSqlExtensionsParser(tokenStream)\n+    parser.addParseListener(PostProcessor)\n+    parser.removeErrorListeners()\n+    parser.addErrorListener(ParseErrorListener)\n+\n+    try {\n+      try {\n+        // first, try parsing with potentially faster SLL mode\n+        parser.getInterpreter.setPredictionMode(PredictionMode.SLL)\n+        toResult(parser)\n+      }\n+      catch {\n+        case _: ParseCancellationException =>\n+          // if we fail, parse with LL mode\n+          tokenStream.seek(0) // rewind input stream\n+          parser.reset()\n+\n+          // Try Again.\n+          parser.getInterpreter.setPredictionMode(PredictionMode.LL)\n+          toResult(parser)\n+      }\n+    }\n+    catch {\n+      case e: ParseException if e.command.isDefined =>\n+        throw e\n+      case e: ParseException =>\n+        throw e.withCommand(command)\n+      case e: AnalysisException =>\n+        val position = Origin(e.line, e.startPosition)\n+        throw new ParseException(Option(command), e.message, position, position)\n+    }\n+  }\n+}\n+\n+// literal parsing is taken from Spark's AstBuilder\n+class IcebergSqlExtensionsAstBuilder extends IcebergSqlExtensionsBaseVisitor[AnyRef] {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM5NzkyOA=="}, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTE4NzMzOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQyMjoxMDozNVrOHuYprA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNzoxNjo0MlrOHu2nGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwMDQyOA==", "bodyText": "The rules in Spark all use withOrigin(ctx). Should we do the same?", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518400428", "createdAt": "2020-11-05T22:10:35Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import java.util.Locale\n+import javax.xml.bind.DatatypeConverter\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.atn.PredictionMode\n+import org.antlr.v4.runtime.misc.{Interval, ParseCancellationException}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNodeImpl}\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.parser.{ParseErrorListener, ParseException, ParserInterface}\n+import org.apache.spark.sql.catalyst.parser.ParserUtils._\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.{CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.trees.Origin\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.{getZoneId, stringToDate, stringToTimestamp}\n+import org.apache.spark.sql.catalyst.util.IntervalUtils\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, CalendarIntervalType, DataType, DateType, DoubleType, FloatType, LongType, ShortType, StructType, TimestampType}\n+import org.apache.spark.unsafe.types.UTF8String\n+import scala.collection.JavaConverters._\n+\n+class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserInterface {\n+\n+  private val astBuilder = new IcebergSqlExtensionsAstBuilder()\n+\n+  /**\n+   * Parse a string to a DataType.\n+   */\n+  override def parseDataType(sqlText: String): DataType = {\n+    delegate.parseDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a raw DataType without CHAR/VARCHAR replacement.\n+   */\n+  override def parseRawDataType(sqlText: String): DataType = {\n+    delegate.parseRawDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to an Expression.\n+   */\n+  override def parseExpression(sqlText: String): Expression = {\n+    delegate.parseExpression(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a TableIdentifier.\n+   */\n+  override def parseTableIdentifier(sqlText: String): TableIdentifier = {\n+    delegate.parseTableIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a FunctionIdentifier.\n+   */\n+  override def parseFunctionIdentifier(sqlText: String): FunctionIdentifier = {\n+    delegate.parseFunctionIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a multi-part identifier.\n+   */\n+  override def parseMultipartIdentifier(sqlText: String): Seq[String] = {\n+    delegate.parseMultipartIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Creates StructType for a given SQL string, which is a comma separated list of field\n+   * definitions which will preserve the correct Hive metadata.\n+   */\n+  override def parseTableSchema(sqlText: String): StructType = {\n+    delegate.parseTableSchema(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a LogicalPlan.\n+   */\n+  override def parsePlan(sqlText: String): LogicalPlan = parse(sqlText) { parser =>\n+    astBuilder.visit(parser.singleStatement()) match {\n+      case plan: LogicalPlan => plan\n+      case _ => delegate.parsePlan(sqlText)\n+    }\n+  }\n+\n+  protected def parse[T](command: String)(toResult: IcebergSqlExtensionsParser => T): T = {\n+    val lexer = new IcebergSqlExtensionsLexer(new UpperCaseCharStream(CharStreams.fromString(command)))\n+    lexer.removeErrorListeners()\n+    lexer.addErrorListener(ParseErrorListener)\n+\n+    val tokenStream = new CommonTokenStream(lexer)\n+    val parser = new IcebergSqlExtensionsParser(tokenStream)\n+    parser.addParseListener(PostProcessor)\n+    parser.removeErrorListeners()\n+    parser.addErrorListener(ParseErrorListener)\n+\n+    try {\n+      try {\n+        // first, try parsing with potentially faster SLL mode\n+        parser.getInterpreter.setPredictionMode(PredictionMode.SLL)\n+        toResult(parser)\n+      }\n+      catch {\n+        case _: ParseCancellationException =>\n+          // if we fail, parse with LL mode\n+          tokenStream.seek(0) // rewind input stream\n+          parser.reset()\n+\n+          // Try Again.\n+          parser.getInterpreter.setPredictionMode(PredictionMode.LL)\n+          toResult(parser)\n+      }\n+    }\n+    catch {\n+      case e: ParseException if e.command.isDefined =>\n+        throw e\n+      case e: ParseException =>\n+        throw e.withCommand(command)\n+      case e: AnalysisException =>\n+        val position = Origin(e.line, e.startPosition)\n+        throw new ParseException(Option(command), e.message, position, position)\n+    }\n+  }\n+}\n+\n+// literal parsing is taken from Spark's AstBuilder\n+class IcebergSqlExtensionsAstBuilder extends IcebergSqlExtensionsBaseVisitor[AnyRef] {\n+\n+  override def visitCall(ctx: CallContext): LogicalPlan = {\n+    val name = ctx.multipartIdentifier.parts.asScala.map(_.getText)\n+    val args = ctx.callArgument.asScala.map(typedVisit[CallArgument])\n+    CallStatement(name, args)\n+  }\n+\n+  override def visitPositionalArgument(ctx: PositionalArgumentContext): CallArgument = {\n+    val expr = typedVisit[Expression](ctx.expression)\n+    PositionalArgument(expr)\n+  }\n+\n+  override def visitNamedArgument(ctx: NamedArgumentContext): CallArgument = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MTI5MA==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518891290", "createdAt": "2020-11-06T17:16:42Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import java.util.Locale\n+import javax.xml.bind.DatatypeConverter\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.atn.PredictionMode\n+import org.antlr.v4.runtime.misc.{Interval, ParseCancellationException}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNodeImpl}\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.parser.{ParseErrorListener, ParseException, ParserInterface}\n+import org.apache.spark.sql.catalyst.parser.ParserUtils._\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.{CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.trees.Origin\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.{getZoneId, stringToDate, stringToTimestamp}\n+import org.apache.spark.sql.catalyst.util.IntervalUtils\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, CalendarIntervalType, DataType, DateType, DoubleType, FloatType, LongType, ShortType, StructType, TimestampType}\n+import org.apache.spark.unsafe.types.UTF8String\n+import scala.collection.JavaConverters._\n+\n+class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserInterface {\n+\n+  private val astBuilder = new IcebergSqlExtensionsAstBuilder()\n+\n+  /**\n+   * Parse a string to a DataType.\n+   */\n+  override def parseDataType(sqlText: String): DataType = {\n+    delegate.parseDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a raw DataType without CHAR/VARCHAR replacement.\n+   */\n+  override def parseRawDataType(sqlText: String): DataType = {\n+    delegate.parseRawDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to an Expression.\n+   */\n+  override def parseExpression(sqlText: String): Expression = {\n+    delegate.parseExpression(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a TableIdentifier.\n+   */\n+  override def parseTableIdentifier(sqlText: String): TableIdentifier = {\n+    delegate.parseTableIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a FunctionIdentifier.\n+   */\n+  override def parseFunctionIdentifier(sqlText: String): FunctionIdentifier = {\n+    delegate.parseFunctionIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a multi-part identifier.\n+   */\n+  override def parseMultipartIdentifier(sqlText: String): Seq[String] = {\n+    delegate.parseMultipartIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Creates StructType for a given SQL string, which is a comma separated list of field\n+   * definitions which will preserve the correct Hive metadata.\n+   */\n+  override def parseTableSchema(sqlText: String): StructType = {\n+    delegate.parseTableSchema(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a LogicalPlan.\n+   */\n+  override def parsePlan(sqlText: String): LogicalPlan = parse(sqlText) { parser =>\n+    astBuilder.visit(parser.singleStatement()) match {\n+      case plan: LogicalPlan => plan\n+      case _ => delegate.parsePlan(sqlText)\n+    }\n+  }\n+\n+  protected def parse[T](command: String)(toResult: IcebergSqlExtensionsParser => T): T = {\n+    val lexer = new IcebergSqlExtensionsLexer(new UpperCaseCharStream(CharStreams.fromString(command)))\n+    lexer.removeErrorListeners()\n+    lexer.addErrorListener(ParseErrorListener)\n+\n+    val tokenStream = new CommonTokenStream(lexer)\n+    val parser = new IcebergSqlExtensionsParser(tokenStream)\n+    parser.addParseListener(PostProcessor)\n+    parser.removeErrorListeners()\n+    parser.addErrorListener(ParseErrorListener)\n+\n+    try {\n+      try {\n+        // first, try parsing with potentially faster SLL mode\n+        parser.getInterpreter.setPredictionMode(PredictionMode.SLL)\n+        toResult(parser)\n+      }\n+      catch {\n+        case _: ParseCancellationException =>\n+          // if we fail, parse with LL mode\n+          tokenStream.seek(0) // rewind input stream\n+          parser.reset()\n+\n+          // Try Again.\n+          parser.getInterpreter.setPredictionMode(PredictionMode.LL)\n+          toResult(parser)\n+      }\n+    }\n+    catch {\n+      case e: ParseException if e.command.isDefined =>\n+        throw e\n+      case e: ParseException =>\n+        throw e.withCommand(command)\n+      case e: AnalysisException =>\n+        val position = Origin(e.line, e.startPosition)\n+        throw new ParseException(Option(command), e.message, position, position)\n+    }\n+  }\n+}\n+\n+// literal parsing is taken from Spark's AstBuilder\n+class IcebergSqlExtensionsAstBuilder extends IcebergSqlExtensionsBaseVisitor[AnyRef] {\n+\n+  override def visitCall(ctx: CallContext): LogicalPlan = {\n+    val name = ctx.multipartIdentifier.parts.asScala.map(_.getText)\n+    val args = ctx.callArgument.asScala.map(typedVisit[CallArgument])\n+    CallStatement(name, args)\n+  }\n+\n+  override def visitPositionalArgument(ctx: PositionalArgumentContext): CallArgument = {\n+    val expr = typedVisit[Expression](ctx.expression)\n+    PositionalArgument(expr)\n+  }\n+\n+  override def visitNamedArgument(ctx: NamedArgumentContext): CallArgument = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwMDQyOA=="}, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 161}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTE5Mjg5OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQyMjoxMjoyNVrOHuYs6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNzoxNzoyMFrOHu2oeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwMTI1Nw==", "bodyText": "Spark performs variable substitution before parsing:\n  private val substitutor = new VariableSubstitution(conf)\n\n  protected override def parse[T](command: String)(toResult: SqlBaseParser => T): T = {\n    super.parse(substitutor.substitute(command))(toResult)\n  }\nIf possible, I think we should do the same here.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518401257", "createdAt": "2020-11-05T22:12:25Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import java.util.Locale\n+import javax.xml.bind.DatatypeConverter\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.atn.PredictionMode\n+import org.antlr.v4.runtime.misc.{Interval, ParseCancellationException}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNodeImpl}\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.parser.{ParseErrorListener, ParseException, ParserInterface}\n+import org.apache.spark.sql.catalyst.parser.ParserUtils._\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.{CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.trees.Origin\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.{getZoneId, stringToDate, stringToTimestamp}\n+import org.apache.spark.sql.catalyst.util.IntervalUtils\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, CalendarIntervalType, DataType, DateType, DoubleType, FloatType, LongType, ShortType, StructType, TimestampType}\n+import org.apache.spark.unsafe.types.UTF8String\n+import scala.collection.JavaConverters._\n+\n+class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserInterface {\n+\n+  private val astBuilder = new IcebergSqlExtensionsAstBuilder()\n+\n+  /**\n+   * Parse a string to a DataType.\n+   */\n+  override def parseDataType(sqlText: String): DataType = {\n+    delegate.parseDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a raw DataType without CHAR/VARCHAR replacement.\n+   */\n+  override def parseRawDataType(sqlText: String): DataType = {\n+    delegate.parseRawDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to an Expression.\n+   */\n+  override def parseExpression(sqlText: String): Expression = {\n+    delegate.parseExpression(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a TableIdentifier.\n+   */\n+  override def parseTableIdentifier(sqlText: String): TableIdentifier = {\n+    delegate.parseTableIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a FunctionIdentifier.\n+   */\n+  override def parseFunctionIdentifier(sqlText: String): FunctionIdentifier = {\n+    delegate.parseFunctionIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a multi-part identifier.\n+   */\n+  override def parseMultipartIdentifier(sqlText: String): Seq[String] = {\n+    delegate.parseMultipartIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Creates StructType for a given SQL string, which is a comma separated list of field\n+   * definitions which will preserve the correct Hive metadata.\n+   */\n+  override def parseTableSchema(sqlText: String): StructType = {\n+    delegate.parseTableSchema(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a LogicalPlan.\n+   */\n+  override def parsePlan(sqlText: String): LogicalPlan = parse(sqlText) { parser =>\n+    astBuilder.visit(parser.singleStatement()) match {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 101}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MTY0Mg==", "bodyText": "I had to introduce a dependency on SQLConf and make sure the initialization is lazy. Seems to work, added tests.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518891642", "createdAt": "2020-11-06T17:17:20Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import java.util.Locale\n+import javax.xml.bind.DatatypeConverter\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.atn.PredictionMode\n+import org.antlr.v4.runtime.misc.{Interval, ParseCancellationException}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNodeImpl}\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.parser.{ParseErrorListener, ParseException, ParserInterface}\n+import org.apache.spark.sql.catalyst.parser.ParserUtils._\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.{CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.trees.Origin\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.{getZoneId, stringToDate, stringToTimestamp}\n+import org.apache.spark.sql.catalyst.util.IntervalUtils\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, CalendarIntervalType, DataType, DateType, DoubleType, FloatType, LongType, ShortType, StructType, TimestampType}\n+import org.apache.spark.unsafe.types.UTF8String\n+import scala.collection.JavaConverters._\n+\n+class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserInterface {\n+\n+  private val astBuilder = new IcebergSqlExtensionsAstBuilder()\n+\n+  /**\n+   * Parse a string to a DataType.\n+   */\n+  override def parseDataType(sqlText: String): DataType = {\n+    delegate.parseDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a raw DataType without CHAR/VARCHAR replacement.\n+   */\n+  override def parseRawDataType(sqlText: String): DataType = {\n+    delegate.parseRawDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to an Expression.\n+   */\n+  override def parseExpression(sqlText: String): Expression = {\n+    delegate.parseExpression(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a TableIdentifier.\n+   */\n+  override def parseTableIdentifier(sqlText: String): TableIdentifier = {\n+    delegate.parseTableIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a FunctionIdentifier.\n+   */\n+  override def parseFunctionIdentifier(sqlText: String): FunctionIdentifier = {\n+    delegate.parseFunctionIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a multi-part identifier.\n+   */\n+  override def parseMultipartIdentifier(sqlText: String): Seq[String] = {\n+    delegate.parseMultipartIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Creates StructType for a given SQL string, which is a comma separated list of field\n+   * definitions which will preserve the correct Hive metadata.\n+   */\n+  override def parseTableSchema(sqlText: String): StructType = {\n+    delegate.parseTableSchema(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a LogicalPlan.\n+   */\n+  override def parsePlan(sqlText: String): LogicalPlan = parse(sqlText) { parser =>\n+    astBuilder.visit(parser.singleStatement()) match {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwMTI1Nw=="}, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTIwMzE5OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQyMjoxNTo0M1rOHuYzIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNzoxODowMVrOHu2p2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwMjg0OQ==", "bodyText": "It would be good to have some documentation on these classes.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518402849", "createdAt": "2020-11-05T22:15:43Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+\n+case class CallStatement(name: Seq[String], args: Seq[CallArgument]) extends ParsedStatement", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MTk5Mg==", "bodyText": "Let me add docs before the release when we know how this is going to look like.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518891992", "createdAt": "2020-11-06T17:18:01Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+\n+case class CallStatement(name: Seq[String], args: Seq[CallArgument]) extends ParsedStatement", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwMjg0OQ=="}, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTIyNDg1OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestIcebergSparkSqlExtensionsParser.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQyMjoyMzowNVrOHuZACw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNzoxODowOFrOHu2qGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwNjE1NQ==", "bodyText": "Minor: there are a few casts in these tests that aren't checked. What about adding a checkCast method?\nprivate <T> T checkCast(Object value, Class<T> expectedClass) {\n  Assert.assertTrue(\"Expected instance of \" + expectedClass.getName(), expectedClass.isInstance(value));\n  return expectedClass.cast(value);\n}", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518406155", "createdAt": "2020-11-05T22:23:05Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestIcebergSparkSqlExtensionsParser.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.math.BigDecimal;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.expressions.Literal;\n+import org.apache.spark.sql.catalyst.expressions.Literal$;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.catalyst.plans.logical.CallArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.CallStatement;\n+import org.apache.spark.sql.catalyst.plans.logical.NamedArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.PositionalArgument;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import scala.collection.JavaConverters;\n+\n+public class TestIcebergSparkSqlExtensionsParser {\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private static SparkSession spark = null;\n+  private static ParserInterface parser = null;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestIcebergSparkSqlExtensionsParser.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n+        .getOrCreate();\n+    TestIcebergSparkSqlExtensionsParser.parser = spark.sessionState().sqlParser();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestIcebergSparkSqlExtensionsParser.spark;\n+    TestIcebergSparkSqlExtensionsParser.spark = null;\n+    TestIcebergSparkSqlExtensionsParser.parser = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Test\n+  public void testPositionalArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL c.n.func(1, '2', 3L, true, 1.0D, 9.0e1, 900e-1BD)\");\n+    Assert.assertEquals(ImmutableList.of(\"c\", \"n\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(7, call.args().size());\n+\n+    checkArg(call, 0, 1);\n+    checkArg(call, 1, \"2\");\n+    checkArg(call, 2, 3L);\n+    checkArg(call, 3, true);\n+    checkArg(call, 4, 1.0D);\n+    checkArg(call, 5, 9.0e1);\n+    checkArg(call, 6, new BigDecimal(\"900e-1\"));\n+  }\n+\n+  @Test\n+  public void testNamedArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL cat.system.func(c1 => 1, c2 => '2', c3 => true)\");\n+    Assert.assertEquals(ImmutableList.of(\"cat\", \"system\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(3, call.args().size());\n+\n+    checkArg(call, 0, \"c1\", 1);\n+    checkArg(call, 1, \"c2\", \"2\");\n+    checkArg(call, 2, \"c3\", true);\n+  }\n+\n+  private void checkArg(CallStatement call, int index, Object expectedValue) {\n+    checkArg(call, index, null, expectedValue);\n+  }\n+\n+  private void checkArg(CallStatement call, int index, String expectedName, Object expectedValue) {\n+    if (expectedName != null) {\n+      NamedArgument arg = (NamedArgument) call.args().apply(index);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MjA1OA==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518892058", "createdAt": "2020-11-06T17:18:08Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestIcebergSparkSqlExtensionsParser.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.math.BigDecimal;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.expressions.Literal;\n+import org.apache.spark.sql.catalyst.expressions.Literal$;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.catalyst.plans.logical.CallArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.CallStatement;\n+import org.apache.spark.sql.catalyst.plans.logical.NamedArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.PositionalArgument;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import scala.collection.JavaConverters;\n+\n+public class TestIcebergSparkSqlExtensionsParser {\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private static SparkSession spark = null;\n+  private static ParserInterface parser = null;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestIcebergSparkSqlExtensionsParser.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n+        .getOrCreate();\n+    TestIcebergSparkSqlExtensionsParser.parser = spark.sessionState().sqlParser();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestIcebergSparkSqlExtensionsParser.spark;\n+    TestIcebergSparkSqlExtensionsParser.spark = null;\n+    TestIcebergSparkSqlExtensionsParser.parser = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Test\n+  public void testPositionalArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL c.n.func(1, '2', 3L, true, 1.0D, 9.0e1, 900e-1BD)\");\n+    Assert.assertEquals(ImmutableList.of(\"c\", \"n\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(7, call.args().size());\n+\n+    checkArg(call, 0, 1);\n+    checkArg(call, 1, \"2\");\n+    checkArg(call, 2, 3L);\n+    checkArg(call, 3, true);\n+    checkArg(call, 4, 1.0D);\n+    checkArg(call, 5, 9.0e1);\n+    checkArg(call, 6, new BigDecimal(\"900e-1\"));\n+  }\n+\n+  @Test\n+  public void testNamedArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL cat.system.func(c1 => 1, c2 => '2', c3 => true)\");\n+    Assert.assertEquals(ImmutableList.of(\"cat\", \"system\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(3, call.args().size());\n+\n+    checkArg(call, 0, \"c1\", 1);\n+    checkArg(call, 1, \"c2\", \"2\");\n+    checkArg(call, 2, \"c3\", true);\n+  }\n+\n+  private void checkArg(CallStatement call, int index, Object expectedValue) {\n+    checkArg(call, index, null, expectedValue);\n+  }\n+\n+  private void checkArg(CallStatement call, int index, String expectedName, Object expectedValue) {\n+    if (expectedName != null) {\n+      NamedArgument arg = (NamedArgument) call.args().apply(index);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwNjE1NQ=="}, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTIzMjAwOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestIcebergSparkSqlExtensionsParser.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQyMjoyNToyNVrOHuZEIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNzoxOTowMlrOHu2r1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwNzIwMA==", "bodyText": "Can you also add testMixedArgs() that validates order doesn't change when positional and named arguments are mixed together? Even if this isn't allowed by a later rule, the parser accepts it.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518407200", "createdAt": "2020-11-05T22:25:25Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestIcebergSparkSqlExtensionsParser.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.math.BigDecimal;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.expressions.Literal;\n+import org.apache.spark.sql.catalyst.expressions.Literal$;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.catalyst.plans.logical.CallArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.CallStatement;\n+import org.apache.spark.sql.catalyst.plans.logical.NamedArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.PositionalArgument;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import scala.collection.JavaConverters;\n+\n+public class TestIcebergSparkSqlExtensionsParser {\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private static SparkSession spark = null;\n+  private static ParserInterface parser = null;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestIcebergSparkSqlExtensionsParser.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n+        .getOrCreate();\n+    TestIcebergSparkSqlExtensionsParser.parser = spark.sessionState().sqlParser();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestIcebergSparkSqlExtensionsParser.spark;\n+    TestIcebergSparkSqlExtensionsParser.spark = null;\n+    TestIcebergSparkSqlExtensionsParser.parser = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Test\n+  public void testPositionalArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL c.n.func(1, '2', 3L, true, 1.0D, 9.0e1, 900e-1BD)\");\n+    Assert.assertEquals(ImmutableList.of(\"c\", \"n\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(7, call.args().size());\n+\n+    checkArg(call, 0, 1);\n+    checkArg(call, 1, \"2\");\n+    checkArg(call, 2, 3L);\n+    checkArg(call, 3, true);\n+    checkArg(call, 4, 1.0D);\n+    checkArg(call, 5, 9.0e1);\n+    checkArg(call, 6, new BigDecimal(\"900e-1\"));\n+  }\n+\n+  @Test\n+  public void testNamedArgs() throws ParseException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MjUwMg==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518892502", "createdAt": "2020-11-06T17:19:02Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestIcebergSparkSqlExtensionsParser.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.math.BigDecimal;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.expressions.Literal;\n+import org.apache.spark.sql.catalyst.expressions.Literal$;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.catalyst.plans.logical.CallArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.CallStatement;\n+import org.apache.spark.sql.catalyst.plans.logical.NamedArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.PositionalArgument;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import scala.collection.JavaConverters;\n+\n+public class TestIcebergSparkSqlExtensionsParser {\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private static SparkSession spark = null;\n+  private static ParserInterface parser = null;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestIcebergSparkSqlExtensionsParser.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n+        .getOrCreate();\n+    TestIcebergSparkSqlExtensionsParser.parser = spark.sessionState().sqlParser();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestIcebergSparkSqlExtensionsParser.spark;\n+    TestIcebergSparkSqlExtensionsParser.spark = null;\n+    TestIcebergSparkSqlExtensionsParser.parser = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Test\n+  public void testPositionalArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL c.n.func(1, '2', 3L, true, 1.0D, 9.0e1, 900e-1BD)\");\n+    Assert.assertEquals(ImmutableList.of(\"c\", \"n\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(7, call.args().size());\n+\n+    checkArg(call, 0, 1);\n+    checkArg(call, 1, \"2\");\n+    checkArg(call, 2, 3L);\n+    checkArg(call, 3, true);\n+    checkArg(call, 4, 1.0D);\n+    checkArg(call, 5, 9.0e1);\n+    checkArg(call, 6, new BigDecimal(\"900e-1\"));\n+  }\n+\n+  @Test\n+  public void testNamedArgs() throws ParseException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwNzIwMA=="}, "originalCommit": {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MjM4MTMwOnYy", "diffSide": "RIGHT", "path": "build.gradle", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNzoxOTo0MFrOHu2tUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNzoxOTo0MFrOHu2tUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5Mjg4MQ==", "bodyText": "This is the same version Spark uses.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518892881", "createdAt": "2020-11-06T17:19:40Z", "author": {"login": "aokolnychyi"}, "path": "build.gradle", "diffHunk": "@@ -884,6 +884,49 @@ project(':iceberg-spark3') {\n   }\n }\n \n+project(\":iceberg-spark3-extensions\") {\n+  apply plugin: 'java'\n+  apply plugin: 'scala'\n+  apply plugin: 'antlr'\n+\n+  configurations {\n+    /*\n+     The Gradle Antlr plugin erroneously adds both antlr-build and runtime dependencies to the runtime path. This\n+     bug https://github.com/gradle/gradle/issues/820 exists because older versions of Antlr do not have separate\n+     runtime and compile dependencies and they do not want to break backwards compatibility. So to only end up with\n+     the runtime dependency on the runtime classpath we remove the dependencies added by the plugin here. Then add\n+     the runtime dependency back to only the runtime configuration manually.\n+    */\n+    compile {\n+      extendsFrom = extendsFrom.findAll { it != configurations.antlr }\n+    }\n+  }\n+\n+  dependencies {\n+    compileOnly project(':iceberg-spark3')\n+\n+    compileOnly \"org.scala-lang:scala-library\"\n+    compileOnly(\"org.apache.spark:spark-hive_2.12\") {\n+      exclude group: 'org.apache.avro', module: 'avro'\n+      exclude group: 'org.apache.arrow'\n+    }\n+\n+    testCompile project(path: ':iceberg-api', configuration: 'testArtifacts')\n+    testCompile project(path: ':iceberg-hive-metastore', configuration: 'testArtifacts')\n+    testCompile project(path: ':iceberg-spark', configuration: 'testArtifacts')\n+    testCompile project(path: ':iceberg-spark3', configuration: 'testArtifacts')\n+\n+    // Required because we remove antlr plugin dependencies from the compile configuration, see note above\n+    runtime \"org.antlr:antlr4-runtime:4.7.1\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MjM4Mzk5OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSqlExtensionsAstBuilder.scala", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxNzoyMDoyMlrOHu2u5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQyMDoyMDo1OFrOHu8YPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MzI4Ng==", "bodyText": "@rdblue, this is the part I was talking about.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518893286", "createdAt": "2020-11-06T17:20:22Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSqlExtensionsAstBuilder.scala", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.parser.ParserInterface\n+import org.apache.spark.sql.catalyst.parser.ParserUtils._\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.{CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import scala.collection.JavaConverters._\n+\n+class IcebergSqlExtensionsAstBuilder(delegate: ParserInterface) extends IcebergSqlExtensionsBaseVisitor[AnyRef] {\n+\n+  override def visitCall(ctx: CallContext): LogicalPlan = {\n+    val name = ctx.multipartIdentifier.parts.asScala.map(_.getText)\n+    val args = ctx.callArgument.asScala.map(typedVisit[CallArgument])\n+    CallStatement(name, args)\n+  }\n+\n+  override def visitPositionalArgument(ctx: PositionalArgumentContext): CallArgument = withOrigin(ctx) {\n+    val expr = typedVisit[Expression](ctx.expression)\n+    PositionalArgument(expr)\n+  }\n+\n+  override def visitNamedArgument(ctx: NamedArgumentContext): CallArgument = withOrigin(ctx) {\n+    val name = ctx.identifier.getText\n+    val expr = typedVisit[Expression](ctx.expression)\n+    NamedArgument(name, expr)\n+  }\n+\n+  // return null for any statement we cannot handle so it can be parsed with the main Spark parser\n+  override def visitNonIcebergCommand(ctx: NonIcebergCommandContext): LogicalPlan = null\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitExpression(ctx: ExpressionContext): Expression = {\n+    // reconstruct the SQL string and parse it using the main Spark parser\n+    // while we can avoid having the logic to build Spark expressions, we still have to parse them\n+    val sqlString = reconstructSqlString(ctx)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk0MDcxOA==", "bodyText": "ParserRuleContext extends RuleContext, which is where getText is defined. Why not just call getText on any context? It doesn't seem like the reconstructSqlString method is needed.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518940718", "createdAt": "2020-11-06T18:50:47Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSqlExtensionsAstBuilder.scala", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.parser.ParserInterface\n+import org.apache.spark.sql.catalyst.parser.ParserUtils._\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.{CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import scala.collection.JavaConverters._\n+\n+class IcebergSqlExtensionsAstBuilder(delegate: ParserInterface) extends IcebergSqlExtensionsBaseVisitor[AnyRef] {\n+\n+  override def visitCall(ctx: CallContext): LogicalPlan = {\n+    val name = ctx.multipartIdentifier.parts.asScala.map(_.getText)\n+    val args = ctx.callArgument.asScala.map(typedVisit[CallArgument])\n+    CallStatement(name, args)\n+  }\n+\n+  override def visitPositionalArgument(ctx: PositionalArgumentContext): CallArgument = withOrigin(ctx) {\n+    val expr = typedVisit[Expression](ctx.expression)\n+    PositionalArgument(expr)\n+  }\n+\n+  override def visitNamedArgument(ctx: NamedArgumentContext): CallArgument = withOrigin(ctx) {\n+    val name = ctx.identifier.getText\n+    val expr = typedVisit[Expression](ctx.expression)\n+    NamedArgument(name, expr)\n+  }\n+\n+  // return null for any statement we cannot handle so it can be parsed with the main Spark parser\n+  override def visitNonIcebergCommand(ctx: NonIcebergCommandContext): LogicalPlan = null\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitExpression(ctx: ExpressionContext): Expression = {\n+    // reconstruct the SQL string and parse it using the main Spark parser\n+    // while we can avoid having the logic to build Spark expressions, we still have to parse them\n+    val sqlString = reconstructSqlString(ctx)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MzI4Ng=="}, "originalCommit": {"oid": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4MTk4Ng==", "bodyText": "Unfortunately, calling getText on a node with multiple children messes up spaces.\nFor example, we will get back TIMESTAMP'2020-01-01 00:00:00' (without spaces).\nThat's why we need to recurse in reconstructSqlString.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518981986", "createdAt": "2020-11-06T20:12:27Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSqlExtensionsAstBuilder.scala", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.parser.ParserInterface\n+import org.apache.spark.sql.catalyst.parser.ParserUtils._\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.{CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import scala.collection.JavaConverters._\n+\n+class IcebergSqlExtensionsAstBuilder(delegate: ParserInterface) extends IcebergSqlExtensionsBaseVisitor[AnyRef] {\n+\n+  override def visitCall(ctx: CallContext): LogicalPlan = {\n+    val name = ctx.multipartIdentifier.parts.asScala.map(_.getText)\n+    val args = ctx.callArgument.asScala.map(typedVisit[CallArgument])\n+    CallStatement(name, args)\n+  }\n+\n+  override def visitPositionalArgument(ctx: PositionalArgumentContext): CallArgument = withOrigin(ctx) {\n+    val expr = typedVisit[Expression](ctx.expression)\n+    PositionalArgument(expr)\n+  }\n+\n+  override def visitNamedArgument(ctx: NamedArgumentContext): CallArgument = withOrigin(ctx) {\n+    val name = ctx.identifier.getText\n+    val expr = typedVisit[Expression](ctx.expression)\n+    NamedArgument(name, expr)\n+  }\n+\n+  // return null for any statement we cannot handle so it can be parsed with the main Spark parser\n+  override def visitNonIcebergCommand(ctx: NonIcebergCommandContext): LogicalPlan = null\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitExpression(ctx: ExpressionContext): Expression = {\n+    // reconstruct the SQL string and parse it using the main Spark parser\n+    // while we can avoid having the logic to build Spark expressions, we still have to parse them\n+    val sqlString = reconstructSqlString(ctx)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MzI4Ng=="}, "originalCommit": {"oid": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NDg1Mg==", "bodyText": "Let me update the comment.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518984852", "createdAt": "2020-11-06T20:18:49Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSqlExtensionsAstBuilder.scala", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.parser.ParserInterface\n+import org.apache.spark.sql.catalyst.parser.ParserUtils._\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.{CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import scala.collection.JavaConverters._\n+\n+class IcebergSqlExtensionsAstBuilder(delegate: ParserInterface) extends IcebergSqlExtensionsBaseVisitor[AnyRef] {\n+\n+  override def visitCall(ctx: CallContext): LogicalPlan = {\n+    val name = ctx.multipartIdentifier.parts.asScala.map(_.getText)\n+    val args = ctx.callArgument.asScala.map(typedVisit[CallArgument])\n+    CallStatement(name, args)\n+  }\n+\n+  override def visitPositionalArgument(ctx: PositionalArgumentContext): CallArgument = withOrigin(ctx) {\n+    val expr = typedVisit[Expression](ctx.expression)\n+    PositionalArgument(expr)\n+  }\n+\n+  override def visitNamedArgument(ctx: NamedArgumentContext): CallArgument = withOrigin(ctx) {\n+    val name = ctx.identifier.getText\n+    val expr = typedVisit[Expression](ctx.expression)\n+    NamedArgument(name, expr)\n+  }\n+\n+  // return null for any statement we cannot handle so it can be parsed with the main Spark parser\n+  override def visitNonIcebergCommand(ctx: NonIcebergCommandContext): LogicalPlan = null\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitExpression(ctx: ExpressionContext): Expression = {\n+    // reconstruct the SQL string and parse it using the main Spark parser\n+    // while we can avoid having the logic to build Spark expressions, we still have to parse them\n+    val sqlString = reconstructSqlString(ctx)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MzI4Ng=="}, "originalCommit": {"oid": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NTc4OA==", "bodyText": "Updated.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518985788", "createdAt": "2020-11-06T20:20:58Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSqlExtensionsAstBuilder.scala", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.parser.ParserInterface\n+import org.apache.spark.sql.catalyst.parser.ParserUtils._\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.{CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import scala.collection.JavaConverters._\n+\n+class IcebergSqlExtensionsAstBuilder(delegate: ParserInterface) extends IcebergSqlExtensionsBaseVisitor[AnyRef] {\n+\n+  override def visitCall(ctx: CallContext): LogicalPlan = {\n+    val name = ctx.multipartIdentifier.parts.asScala.map(_.getText)\n+    val args = ctx.callArgument.asScala.map(typedVisit[CallArgument])\n+    CallStatement(name, args)\n+  }\n+\n+  override def visitPositionalArgument(ctx: PositionalArgumentContext): CallArgument = withOrigin(ctx) {\n+    val expr = typedVisit[Expression](ctx.expression)\n+    PositionalArgument(expr)\n+  }\n+\n+  override def visitNamedArgument(ctx: NamedArgumentContext): CallArgument = withOrigin(ctx) {\n+    val name = ctx.identifier.getText\n+    val expr = typedVisit[Expression](ctx.expression)\n+    NamedArgument(name, expr)\n+  }\n+\n+  // return null for any statement we cannot handle so it can be parsed with the main Spark parser\n+  override def visitNonIcebergCommand(ctx: NonIcebergCommandContext): LogicalPlan = null\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitExpression(ctx: ExpressionContext): Expression = {\n+    // reconstruct the SQL string and parse it using the main Spark parser\n+    // while we can avoid having the logic to build Spark expressions, we still have to parse them\n+    val sqlString = reconstructSqlString(ctx)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MzI4Ng=="}, "originalCommit": {"oid": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MjY2NDgyOnYy", "diffSide": "RIGHT", "path": "LICENSE", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxODo0NDo0MlrOHu5cVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQyMDoyMjowOVrOHu8aBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkzNzY4Ng==", "bodyText": "I think it's quite a bit broader than this. How about \"portions of the extensions parser\"?", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518937686", "createdAt": "2020-11-06T18:44:42Z", "author": {"login": "rdblue"}, "path": "LICENSE", "diffHunk": "@@ -279,6 +280,9 @@ This product includes code from Apache Spark.\n \n * dev/check-license script\n * vectorized reading of definition levels in BaseVectorizedParquetValuesReader.java\n+* SQL grammar rules in IcebergSqlExtensions.g4\n+* SQL parsing logic in IcebergSparkSqlExtensionsParser.java\n+* parser post processing logic in IcebergSqlExtensionsPostProcessor.java", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4Mjk0MA==", "bodyText": "Sounds good to me.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518982940", "createdAt": "2020-11-06T20:14:04Z", "author": {"login": "aokolnychyi"}, "path": "LICENSE", "diffHunk": "@@ -279,6 +280,9 @@ This product includes code from Apache Spark.\n \n * dev/check-license script\n * vectorized reading of definition levels in BaseVectorizedParquetValuesReader.java\n+* SQL grammar rules in IcebergSqlExtensions.g4\n+* SQL parsing logic in IcebergSparkSqlExtensionsParser.java\n+* parser post processing logic in IcebergSqlExtensionsPostProcessor.java", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkzNzY4Ng=="}, "originalCommit": {"oid": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NjI0NA==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518986244", "createdAt": "2020-11-06T20:22:09Z", "author": {"login": "aokolnychyi"}, "path": "LICENSE", "diffHunk": "@@ -279,6 +280,9 @@ This product includes code from Apache Spark.\n \n * dev/check-license script\n * vectorized reading of definition levels in BaseVectorizedParquetValuesReader.java\n+* SQL grammar rules in IcebergSqlExtensions.g4\n+* SQL parsing logic in IcebergSparkSqlExtensionsParser.java\n+* parser post processing logic in IcebergSqlExtensionsPostProcessor.java", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkzNzY4Ng=="}, "originalCommit": {"oid": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MjY2ODIyOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxODo0NTo0N1rOHu5eYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxODo0NTo0N1rOHu5eYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkzODIwOQ==", "bodyText": "Thanks for updating the comment.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518938209", "createdAt": "2020-11-06T18:45:47Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.atn.PredictionMode\n+import org.antlr.v4.runtime.misc.ParseCancellationException\n+import org.antlr.v4.runtime.tree.TerminalNodeImpl\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.parser.{ParseErrorListener, ParseException, ParserInterface, UpperCaseCharStream}\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.trees.Origin\n+import org.apache.spark.sql.internal.{SQLConf, VariableSubstitution}\n+import org.apache.spark.sql.types.{DataType, StructType}\n+\n+class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserInterface {\n+\n+  private lazy val substitutor = new VariableSubstitution(SQLConf.get)\n+  private lazy val astBuilder = new IcebergSqlExtensionsAstBuilder(delegate)\n+\n+  /**\n+   * Parse a string to a DataType.\n+   */\n+  override def parseDataType(sqlText: String): DataType = {\n+    delegate.parseDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a raw DataType without CHAR/VARCHAR replacement.\n+   */\n+  override def parseRawDataType(sqlText: String): DataType = {\n+    delegate.parseRawDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to an Expression.\n+   */\n+  override def parseExpression(sqlText: String): Expression = {\n+    delegate.parseExpression(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a TableIdentifier.\n+   */\n+  override def parseTableIdentifier(sqlText: String): TableIdentifier = {\n+    delegate.parseTableIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a FunctionIdentifier.\n+   */\n+  override def parseFunctionIdentifier(sqlText: String): FunctionIdentifier = {\n+    delegate.parseFunctionIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a multi-part identifier.\n+   */\n+  override def parseMultipartIdentifier(sqlText: String): Seq[String] = {\n+    delegate.parseMultipartIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Creates StructType for a given SQL string, which is a comma separated list of field\n+   * definitions which will preserve the correct Hive metadata.\n+   */\n+  override def parseTableSchema(sqlText: String): StructType = {\n+    delegate.parseTableSchema(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a LogicalPlan.\n+   */\n+  override def parsePlan(sqlText: String): LogicalPlan = {\n+    val sqlTextAfterSubstitution = substitutor.substitute(sqlText)\n+    parse(sqlTextAfterSubstitution) { parser =>\n+      astBuilder.visit(parser.singleStatement()) match {\n+        case plan: LogicalPlan => plan\n+        case _ => delegate.parsePlan(sqlText)\n+      }\n+    }\n+  }\n+\n+  protected def parse[T](command: String)(toResult: IcebergSqlExtensionsParser => T): T = {\n+    val lexer = new IcebergSqlExtensionsLexer(new UpperCaseCharStream(CharStreams.fromString(command)))\n+    lexer.removeErrorListeners()\n+    lexer.addErrorListener(ParseErrorListener)\n+\n+    val tokenStream = new CommonTokenStream(lexer)\n+    val parser = new IcebergSqlExtensionsParser(tokenStream)\n+    parser.addParseListener(IcebergSqlExtensionsPostProcessor)\n+    parser.removeErrorListeners()\n+    parser.addErrorListener(ParseErrorListener)\n+\n+    try {\n+      try {\n+        // first, try parsing with potentially faster SLL mode\n+        parser.getInterpreter.setPredictionMode(PredictionMode.SLL)\n+        toResult(parser)\n+      }\n+      catch {\n+        case _: ParseCancellationException =>\n+          // if we fail, parse with LL mode\n+          tokenStream.seek(0) // rewind input stream\n+          parser.reset()\n+\n+          // Try Again.\n+          parser.getInterpreter.setPredictionMode(PredictionMode.LL)\n+          toResult(parser)\n+      }\n+    }\n+    catch {\n+      case e: ParseException if e.command.isDefined =>\n+        throw e\n+      case e: ParseException =>\n+        throw e.withCommand(command)\n+      case e: AnalysisException =>\n+        val position = Origin(e.line, e.startPosition)\n+        throw new ParseException(Option(command), e.message, position, position)\n+    }\n+  }\n+}\n+\n+/**\n+ * The post-processor validates & cleans-up the parse tree during the parse process.\n+ */\n+// while we reuse ParseErrorListener and ParseException, we have to copy and modify PostProcessor\n+// as it directly depends on classes generated from the extensions grammar file", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af"}, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1MjY5NzE1OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQxODo1NDo1M1rOHu5wkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQyMDoyMTo1MVrOHu8ZmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk0Mjg2Ng==", "bodyText": "For test portability, it's always better to construct Timestamp using an instant: Timestamp.from(Instant.parse(\"2020-01-01 00:00:00\")).", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518942866", "createdAt": "2020-11-06T18:54:53Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.math.BigDecimal;\n+import java.sql.Timestamp;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.expressions.Expression;\n+import org.apache.spark.sql.catalyst.expressions.Literal;\n+import org.apache.spark.sql.catalyst.expressions.Literal$;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.catalyst.plans.logical.CallArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.CallStatement;\n+import org.apache.spark.sql.catalyst.plans.logical.NamedArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.PositionalArgument;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import scala.collection.JavaConverters;\n+\n+public class TestCallStatementParser {\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private static SparkSession spark = null;\n+  private static ParserInterface parser = null;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestCallStatementParser.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n+        .config(\"spark.extra.prop\", \"value\")\n+        .getOrCreate();\n+    TestCallStatementParser.parser = spark.sessionState().sqlParser();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestCallStatementParser.spark;\n+    TestCallStatementParser.spark = null;\n+    TestCallStatementParser.parser = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Test\n+  public void testCallWithPositionalArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL c.n.func(1, '2', 3L, true, 1.0D, 9.0e1, 900e-1BD)\");\n+    Assert.assertEquals(ImmutableList.of(\"c\", \"n\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(7, call.args().size());\n+\n+    checkArg(call, 0, 1, DataTypes.IntegerType);\n+    checkArg(call, 1, \"2\", DataTypes.StringType);\n+    checkArg(call, 2, 3L, DataTypes.LongType);\n+    checkArg(call, 3, true, DataTypes.BooleanType);\n+    checkArg(call, 4, 1.0D, DataTypes.DoubleType);\n+    checkArg(call, 5, 9.0e1, DataTypes.DoubleType);\n+    checkArg(call, 6, new BigDecimal(\"900e-1\"), DataTypes.createDecimalType(3, 1));\n+  }\n+\n+  @Test\n+  public void testCallWithNamedArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL cat.system.func(c1 => 1, c2 => '2', c3 => true)\");\n+    Assert.assertEquals(ImmutableList.of(\"cat\", \"system\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(3, call.args().size());\n+\n+    checkArg(call, 0, \"c1\", 1, DataTypes.IntegerType);\n+    checkArg(call, 1, \"c2\", \"2\", DataTypes.StringType);\n+    checkArg(call, 2, \"c3\", true, DataTypes.BooleanType);\n+  }\n+\n+  @Test\n+  public void testCallWithMixedArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL cat.system.func(c1 => 1, '2')\");\n+    Assert.assertEquals(ImmutableList.of(\"cat\", \"system\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(2, call.args().size());\n+\n+    checkArg(call, 0, \"c1\", 1, DataTypes.IntegerType);\n+    checkArg(call, 1, \"2\", DataTypes.StringType);\n+  }\n+\n+  @Test\n+  public void testCallWithTimestampArg() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL cat.system.func(TIMESTAMP '2020-01-01 00:00:00')\");\n+    Assert.assertEquals(ImmutableList.of(\"cat\", \"system\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(1, call.args().size());\n+\n+    checkArg(call, 0, Timestamp.valueOf(\"2020-01-01 00:00:00\"), DataTypes.TimestampType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NjEzNg==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518986136", "createdAt": "2020-11-06T20:21:51Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.math.BigDecimal;\n+import java.sql.Timestamp;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.expressions.Expression;\n+import org.apache.spark.sql.catalyst.expressions.Literal;\n+import org.apache.spark.sql.catalyst.expressions.Literal$;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.catalyst.plans.logical.CallArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.CallStatement;\n+import org.apache.spark.sql.catalyst.plans.logical.NamedArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.PositionalArgument;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import scala.collection.JavaConverters;\n+\n+public class TestCallStatementParser {\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private static SparkSession spark = null;\n+  private static ParserInterface parser = null;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestCallStatementParser.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n+        .config(\"spark.extra.prop\", \"value\")\n+        .getOrCreate();\n+    TestCallStatementParser.parser = spark.sessionState().sqlParser();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestCallStatementParser.spark;\n+    TestCallStatementParser.spark = null;\n+    TestCallStatementParser.parser = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Test\n+  public void testCallWithPositionalArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL c.n.func(1, '2', 3L, true, 1.0D, 9.0e1, 900e-1BD)\");\n+    Assert.assertEquals(ImmutableList.of(\"c\", \"n\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(7, call.args().size());\n+\n+    checkArg(call, 0, 1, DataTypes.IntegerType);\n+    checkArg(call, 1, \"2\", DataTypes.StringType);\n+    checkArg(call, 2, 3L, DataTypes.LongType);\n+    checkArg(call, 3, true, DataTypes.BooleanType);\n+    checkArg(call, 4, 1.0D, DataTypes.DoubleType);\n+    checkArg(call, 5, 9.0e1, DataTypes.DoubleType);\n+    checkArg(call, 6, new BigDecimal(\"900e-1\"), DataTypes.createDecimalType(3, 1));\n+  }\n+\n+  @Test\n+  public void testCallWithNamedArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL cat.system.func(c1 => 1, c2 => '2', c3 => true)\");\n+    Assert.assertEquals(ImmutableList.of(\"cat\", \"system\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(3, call.args().size());\n+\n+    checkArg(call, 0, \"c1\", 1, DataTypes.IntegerType);\n+    checkArg(call, 1, \"c2\", \"2\", DataTypes.StringType);\n+    checkArg(call, 2, \"c3\", true, DataTypes.BooleanType);\n+  }\n+\n+  @Test\n+  public void testCallWithMixedArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL cat.system.func(c1 => 1, '2')\");\n+    Assert.assertEquals(ImmutableList.of(\"cat\", \"system\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(2, call.args().size());\n+\n+    checkArg(call, 0, \"c1\", 1, DataTypes.IntegerType);\n+    checkArg(call, 1, \"2\", DataTypes.StringType);\n+  }\n+\n+  @Test\n+  public void testCallWithTimestampArg() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL cat.system.func(TIMESTAMP '2020-01-01 00:00:00')\");\n+    Assert.assertEquals(ImmutableList.of(\"cat\", \"system\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(1, call.args().size());\n+\n+    checkArg(call, 0, Timestamp.valueOf(\"2020-01-01 00:00:00\"), DataTypes.TimestampType);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk0Mjg2Ng=="}, "originalCommit": {"oid": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af"}, "originalPosition": 117}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3399, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}