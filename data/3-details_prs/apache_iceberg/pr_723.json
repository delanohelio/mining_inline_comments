{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzU4Nzg2NzIy", "number": 723, "title": "Arrow changes for supporting vectorized reads", "bodyText": "Co-authored-by: gautamkowshik@gmail.com\nCo-authored-by: anjalinorwood@gmail.com", "createdAt": "2020-01-02T20:38:22Z", "url": "https://github.com/apache/iceberg/pull/723", "merged": true, "mergeCommit": {"oid": "2dc7689d0e78b2db37bfb6a33c8bc933f5947cf0"}, "closed": true, "closedAt": "2020-03-04T01:13:48Z", "author": {"login": "samarthjain"}, "timelineItems": {"totalCount": 96, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb2zBEfABqjI5MjA5NDk2MDk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcKLiyugFqTM2ODQxNzU0MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5846603e7321fbbfeae5003618c8ef8dd698a597", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/5846603e7321fbbfeae5003618c8ef8dd698a597", "committedDate": "2020-01-02T20:37:17Z", "message": "Arrow changes for supporting vectorized reads\n\nCo-authored-by: gautamkowshik@gmail.com\nCo-authored-by: anjalinorwood@gmail.com"}, "afterCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/60466f867fd9f7092b9d26b24a177c4e64735d5b", "committedDate": "2020-01-03T18:52:53Z", "message": "Arrow changes for supporting vectorized reads\n\nCo-authored-by: gautamkowshik@gmail.com\nCo-authored-by: anjalinorwood@gmail.com"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MTkyMTgy", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344192182", "createdAt": "2020-01-16T20:24:41Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDoyNDo0MlrOFemdCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDoyNDo0MlrOFemdCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzYzMTYyNw==", "bodyText": "Nit: checkstyle doesn't usually like static imports like this, so elsewhere we converted these to use NestedField.optional in the code. We should use the same style here.\nAlso, is checkstyle turned on for iceberg-arrow?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367631627", "createdAt": "2020-01-16T20:24:42Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/ArrowSchemaUtil.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.vector.types.DateUnit;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.TimeUnit;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.types.Types.ListType;\n+import org.apache.iceberg.types.Types.MapType;\n+import org.apache.iceberg.types.Types.NestedField;\n+import org.apache.iceberg.types.Types.StructType;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MTkyNTA2", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344192506", "createdAt": "2020-01-16T20:25:18Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDoyNToxOFrOFemd6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDoyNToxOFrOFemd6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzYzMTg1MA==", "bodyText": "Style: we don't use final, we let the compiler infer it.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367631850", "createdAt": "2020-01-16T20:25:18Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/ArrowSchemaUtil.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.vector.types.DateUnit;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.TimeUnit;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.types.Types.ListType;\n+import org.apache.iceberg.types.Types.MapType;\n+import org.apache.iceberg.types.Types.NestedField;\n+import org.apache.iceberg.types.Types.StructType;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+\n+public class ArrowSchemaUtil {\n+  static final String ORIGINAL_TYPE = \"originalType\";\n+  static final String MAP_TYPE = \"mapType\";\n+  static final String MAP_KEY = \"key\";\n+  static final String MAP_VALUE = \"value\";\n+\n+  private ArrowSchemaUtil() { }\n+\n+  /**\n+   * Convert Iceberg schema to Arrow Schema.\n+   *\n+   * @param schema iceberg schema\n+   * @return arrow schema\n+   */\n+  public static Schema convert(final org.apache.iceberg.Schema schema) {\n+    final ImmutableList.Builder<Field> fields = ImmutableList.builder();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MTkzMzE3", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344193317", "createdAt": "2020-01-16T20:26:46Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDoyNjo0NlrOFemgXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDoyNjo0NlrOFemgXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzYzMjQ3Ng==", "bodyText": "What is true? We usually add comments to clarify like this: fs.delete(path, true /* delete recursively */)", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367632476", "createdAt": "2020-01-16T20:26:46Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/ArrowSchemaUtil.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.vector.types.DateUnit;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.TimeUnit;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.types.Types.ListType;\n+import org.apache.iceberg.types.Types.MapType;\n+import org.apache.iceberg.types.Types.NestedField;\n+import org.apache.iceberg.types.Types.StructType;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+\n+public class ArrowSchemaUtil {\n+  static final String ORIGINAL_TYPE = \"originalType\";\n+  static final String MAP_TYPE = \"mapType\";\n+  static final String MAP_KEY = \"key\";\n+  static final String MAP_VALUE = \"value\";\n+\n+  private ArrowSchemaUtil() { }\n+\n+  /**\n+   * Convert Iceberg schema to Arrow Schema.\n+   *\n+   * @param schema iceberg schema\n+   * @return arrow schema\n+   */\n+  public static Schema convert(final org.apache.iceberg.Schema schema) {\n+    final ImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\n+    for (NestedField f : schema.columns()) {\n+      fields.add(convert(f));\n+    }\n+\n+    return new Schema(fields.build());\n+  }\n+\n+  public static Field convert(final NestedField field) {\n+    final ArrowType arrowType;\n+\n+    final List<Field> children = Lists.newArrayList();\n+    Map<String, String> metadata = null;\n+\n+    switch (field.type().typeId()) {\n+      case BINARY:\n+        // Spark doesn't support BYTE(fixed_size) type, so cast it to VarBinary\n+      case FIXED:\n+        arrowType = ArrowType.Binary.INSTANCE;\n+        break;\n+      case BOOLEAN:\n+        arrowType = ArrowType.Bool.INSTANCE;\n+        break;\n+      case INTEGER:\n+        arrowType = new ArrowType.Int(Integer.SIZE, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 84}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MTk4NDEw", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344198410", "createdAt": "2020-01-16T20:35:40Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDozNTo0MVrOFemwBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDozNTo0MVrOFemwBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzYzNjQ4NA==", "bodyText": "This should check whether the TimestampType has shouldAdjustToUTC set. If set, then \"UTC\" is correct. If not set, then this should pass null. See https://github.com/apache/arrow/blob/master/format/Schema.fbs#L187-L197", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367636484", "createdAt": "2020-01-16T20:35:41Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/ArrowSchemaUtil.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.vector.types.DateUnit;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.TimeUnit;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.types.Types.ListType;\n+import org.apache.iceberg.types.Types.MapType;\n+import org.apache.iceberg.types.Types.NestedField;\n+import org.apache.iceberg.types.Types.StructType;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+\n+public class ArrowSchemaUtil {\n+  static final String ORIGINAL_TYPE = \"originalType\";\n+  static final String MAP_TYPE = \"mapType\";\n+  static final String MAP_KEY = \"key\";\n+  static final String MAP_VALUE = \"value\";\n+\n+  private ArrowSchemaUtil() { }\n+\n+  /**\n+   * Convert Iceberg schema to Arrow Schema.\n+   *\n+   * @param schema iceberg schema\n+   * @return arrow schema\n+   */\n+  public static Schema convert(final org.apache.iceberg.Schema schema) {\n+    final ImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\n+    for (NestedField f : schema.columns()) {\n+      fields.add(convert(f));\n+    }\n+\n+    return new Schema(fields.build());\n+  }\n+\n+  public static Field convert(final NestedField field) {\n+    final ArrowType arrowType;\n+\n+    final List<Field> children = Lists.newArrayList();\n+    Map<String, String> metadata = null;\n+\n+    switch (field.type().typeId()) {\n+      case BINARY:\n+        // Spark doesn't support BYTE(fixed_size) type, so cast it to VarBinary\n+      case FIXED:\n+        arrowType = ArrowType.Binary.INSTANCE;\n+        break;\n+      case BOOLEAN:\n+        arrowType = ArrowType.Bool.INSTANCE;\n+        break;\n+      case INTEGER:\n+        arrowType = new ArrowType.Int(Integer.SIZE, true);\n+        break;\n+      case LONG:\n+        arrowType = new ArrowType.Int(Long.SIZE, true);\n+        break;\n+      case FLOAT:\n+        arrowType = new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE);\n+        break;\n+      case DOUBLE:\n+        arrowType = new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE);\n+        break;\n+      case DECIMAL:\n+        final Types.DecimalType decimalType = (Types.DecimalType) field.type();\n+        arrowType = new ArrowType.Decimal(decimalType.precision(), decimalType.scale());\n+        break;\n+      case STRING:\n+        arrowType = ArrowType.Utf8.INSTANCE;\n+        break;\n+      case TIME:\n+        arrowType = new ArrowType.Time(TimeUnit.MICROSECOND, Long.SIZE);\n+        break;\n+      case TIMESTAMP:\n+        arrowType = new ArrowType.Timestamp(TimeUnit.MICROSECOND, \"UTC\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 106}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjAwNTI5", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344200529", "createdAt": "2020-01-16T20:39:34Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDozOTozNFrOFem3dQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDozOTozNFrOFem3dQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzYzODM4OQ==", "bodyText": "There's now a Map type in metadata: https://github.com/apache/arrow/blob/master/format/Schema.fbs#L84-L87\nShould we use that?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367638389", "createdAt": "2020-01-16T20:39:34Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/ArrowSchemaUtil.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.vector.types.DateUnit;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.TimeUnit;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.types.Types.ListType;\n+import org.apache.iceberg.types.Types.MapType;\n+import org.apache.iceberg.types.Types.NestedField;\n+import org.apache.iceberg.types.Types.StructType;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+\n+public class ArrowSchemaUtil {\n+  static final String ORIGINAL_TYPE = \"originalType\";\n+  static final String MAP_TYPE = \"mapType\";\n+  static final String MAP_KEY = \"key\";\n+  static final String MAP_VALUE = \"value\";\n+\n+  private ArrowSchemaUtil() { }\n+\n+  /**\n+   * Convert Iceberg schema to Arrow Schema.\n+   *\n+   * @param schema iceberg schema\n+   * @return arrow schema\n+   */\n+  public static Schema convert(final org.apache.iceberg.Schema schema) {\n+    final ImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\n+    for (NestedField f : schema.columns()) {\n+      fields.add(convert(f));\n+    }\n+\n+    return new Schema(fields.build());\n+  }\n+\n+  public static Field convert(final NestedField field) {\n+    final ArrowType arrowType;\n+\n+    final List<Field> children = Lists.newArrayList();\n+    Map<String, String> metadata = null;\n+\n+    switch (field.type().typeId()) {\n+      case BINARY:\n+        // Spark doesn't support BYTE(fixed_size) type, so cast it to VarBinary\n+      case FIXED:\n+        arrowType = ArrowType.Binary.INSTANCE;\n+        break;\n+      case BOOLEAN:\n+        arrowType = ArrowType.Bool.INSTANCE;\n+        break;\n+      case INTEGER:\n+        arrowType = new ArrowType.Int(Integer.SIZE, true);\n+        break;\n+      case LONG:\n+        arrowType = new ArrowType.Int(Long.SIZE, true);\n+        break;\n+      case FLOAT:\n+        arrowType = new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE);\n+        break;\n+      case DOUBLE:\n+        arrowType = new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE);\n+        break;\n+      case DECIMAL:\n+        final Types.DecimalType decimalType = (Types.DecimalType) field.type();\n+        arrowType = new ArrowType.Decimal(decimalType.precision(), decimalType.scale());\n+        break;\n+      case STRING:\n+        arrowType = ArrowType.Utf8.INSTANCE;\n+        break;\n+      case TIME:\n+        arrowType = new ArrowType.Time(TimeUnit.MICROSECOND, Long.SIZE);\n+        break;\n+      case TIMESTAMP:\n+        arrowType = new ArrowType.Timestamp(TimeUnit.MICROSECOND, \"UTC\");\n+        break;\n+      case DATE:\n+        arrowType = new ArrowType.Date(DateUnit.DAY);\n+        break;\n+      case STRUCT:\n+        final StructType struct = field.type().asStructType();\n+        arrowType = ArrowType.Struct.INSTANCE;\n+\n+        for (NestedField nested : struct.fields()) {\n+          children.add(convert(nested));\n+        }\n+        break;\n+      case LIST:\n+        final ListType listType = field.type().asListType();\n+        arrowType = ArrowType.List.INSTANCE;\n+\n+        for (NestedField nested : listType.fields()) {\n+          children.add(convert(nested));\n+        }\n+        break;\n+      case MAP:\n+        //Maps are represented as List<Struct<key, value>>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 128}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjAyODkw", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344202890", "createdAt": "2020-01-16T20:43:44Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDo0Mzo0NFrOFem-YA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDo0Mzo0NFrOFem-YA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY0MDE2MA==", "bodyText": "This can use the fields from the Iceberg map type:\n  List<Field> entryFields = Lists.transform(mapType.fields(), ArrowSchemaUtil::convert);\n\nUsing the underlying fields will also pass whether the value is required or optional correctly. You can also access that using MapType.isValueOptional()", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367640160", "createdAt": "2020-01-16T20:43:44Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/ArrowSchemaUtil.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.vector.types.DateUnit;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.TimeUnit;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.types.Types.ListType;\n+import org.apache.iceberg.types.Types.MapType;\n+import org.apache.iceberg.types.Types.NestedField;\n+import org.apache.iceberg.types.Types.StructType;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+\n+public class ArrowSchemaUtil {\n+  static final String ORIGINAL_TYPE = \"originalType\";\n+  static final String MAP_TYPE = \"mapType\";\n+  static final String MAP_KEY = \"key\";\n+  static final String MAP_VALUE = \"value\";\n+\n+  private ArrowSchemaUtil() { }\n+\n+  /**\n+   * Convert Iceberg schema to Arrow Schema.\n+   *\n+   * @param schema iceberg schema\n+   * @return arrow schema\n+   */\n+  public static Schema convert(final org.apache.iceberg.Schema schema) {\n+    final ImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\n+    for (NestedField f : schema.columns()) {\n+      fields.add(convert(f));\n+    }\n+\n+    return new Schema(fields.build());\n+  }\n+\n+  public static Field convert(final NestedField field) {\n+    final ArrowType arrowType;\n+\n+    final List<Field> children = Lists.newArrayList();\n+    Map<String, String> metadata = null;\n+\n+    switch (field.type().typeId()) {\n+      case BINARY:\n+        // Spark doesn't support BYTE(fixed_size) type, so cast it to VarBinary\n+      case FIXED:\n+        arrowType = ArrowType.Binary.INSTANCE;\n+        break;\n+      case BOOLEAN:\n+        arrowType = ArrowType.Bool.INSTANCE;\n+        break;\n+      case INTEGER:\n+        arrowType = new ArrowType.Int(Integer.SIZE, true);\n+        break;\n+      case LONG:\n+        arrowType = new ArrowType.Int(Long.SIZE, true);\n+        break;\n+      case FLOAT:\n+        arrowType = new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE);\n+        break;\n+      case DOUBLE:\n+        arrowType = new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE);\n+        break;\n+      case DECIMAL:\n+        final Types.DecimalType decimalType = (Types.DecimalType) field.type();\n+        arrowType = new ArrowType.Decimal(decimalType.precision(), decimalType.scale());\n+        break;\n+      case STRING:\n+        arrowType = ArrowType.Utf8.INSTANCE;\n+        break;\n+      case TIME:\n+        arrowType = new ArrowType.Time(TimeUnit.MICROSECOND, Long.SIZE);\n+        break;\n+      case TIMESTAMP:\n+        arrowType = new ArrowType.Timestamp(TimeUnit.MICROSECOND, \"UTC\");\n+        break;\n+      case DATE:\n+        arrowType = new ArrowType.Date(DateUnit.DAY);\n+        break;\n+      case STRUCT:\n+        final StructType struct = field.type().asStructType();\n+        arrowType = ArrowType.Struct.INSTANCE;\n+\n+        for (NestedField nested : struct.fields()) {\n+          children.add(convert(nested));\n+        }\n+        break;\n+      case LIST:\n+        final ListType listType = field.type().asListType();\n+        arrowType = ArrowType.List.INSTANCE;\n+\n+        for (NestedField nested : listType.fields()) {\n+          children.add(convert(nested));\n+        }\n+        break;\n+      case MAP:\n+        //Maps are represented as List<Struct<key, value>>\n+        metadata = ImmutableMap.of(ORIGINAL_TYPE, MAP_TYPE);\n+        final MapType mapType = field.type().asMapType();\n+        arrowType = ArrowType.List.INSTANCE;\n+\n+        final List<Field> entryFields = Lists.newArrayList(\n+            convert(required(0, MAP_KEY, mapType.keyType())),\n+            convert(optional(0, MAP_VALUE, mapType.valueType()))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 135}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjAzMTc1", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344203175", "createdAt": "2020-01-16T20:44:20Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDo0NDoyMFrOFem_Sg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDo0NDoyMFrOFem_Sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY0MDM5NA==", "bodyText": "Please add a newline before the throw.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367640394", "createdAt": "2020-01-16T20:44:20Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/ArrowSchemaUtil.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.vector.types.DateUnit;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.TimeUnit;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.types.Types.ListType;\n+import org.apache.iceberg.types.Types.MapType;\n+import org.apache.iceberg.types.Types.NestedField;\n+import org.apache.iceberg.types.Types.StructType;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+\n+public class ArrowSchemaUtil {\n+  static final String ORIGINAL_TYPE = \"originalType\";\n+  static final String MAP_TYPE = \"mapType\";\n+  static final String MAP_KEY = \"key\";\n+  static final String MAP_VALUE = \"value\";\n+\n+  private ArrowSchemaUtil() { }\n+\n+  /**\n+   * Convert Iceberg schema to Arrow Schema.\n+   *\n+   * @param schema iceberg schema\n+   * @return arrow schema\n+   */\n+  public static Schema convert(final org.apache.iceberg.Schema schema) {\n+    final ImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\n+    for (NestedField f : schema.columns()) {\n+      fields.add(convert(f));\n+    }\n+\n+    return new Schema(fields.build());\n+  }\n+\n+  public static Field convert(final NestedField field) {\n+    final ArrowType arrowType;\n+\n+    final List<Field> children = Lists.newArrayList();\n+    Map<String, String> metadata = null;\n+\n+    switch (field.type().typeId()) {\n+      case BINARY:\n+        // Spark doesn't support BYTE(fixed_size) type, so cast it to VarBinary\n+      case FIXED:\n+        arrowType = ArrowType.Binary.INSTANCE;\n+        break;\n+      case BOOLEAN:\n+        arrowType = ArrowType.Bool.INSTANCE;\n+        break;\n+      case INTEGER:\n+        arrowType = new ArrowType.Int(Integer.SIZE, true);\n+        break;\n+      case LONG:\n+        arrowType = new ArrowType.Int(Long.SIZE, true);\n+        break;\n+      case FLOAT:\n+        arrowType = new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE);\n+        break;\n+      case DOUBLE:\n+        arrowType = new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE);\n+        break;\n+      case DECIMAL:\n+        final Types.DecimalType decimalType = (Types.DecimalType) field.type();\n+        arrowType = new ArrowType.Decimal(decimalType.precision(), decimalType.scale());\n+        break;\n+      case STRING:\n+        arrowType = ArrowType.Utf8.INSTANCE;\n+        break;\n+      case TIME:\n+        arrowType = new ArrowType.Time(TimeUnit.MICROSECOND, Long.SIZE);\n+        break;\n+      case TIMESTAMP:\n+        arrowType = new ArrowType.Timestamp(TimeUnit.MICROSECOND, \"UTC\");\n+        break;\n+      case DATE:\n+        arrowType = new ArrowType.Date(DateUnit.DAY);\n+        break;\n+      case STRUCT:\n+        final StructType struct = field.type().asStructType();\n+        arrowType = ArrowType.Struct.INSTANCE;\n+\n+        for (NestedField nested : struct.fields()) {\n+          children.add(convert(nested));\n+        }\n+        break;\n+      case LIST:\n+        final ListType listType = field.type().asListType();\n+        arrowType = ArrowType.List.INSTANCE;\n+\n+        for (NestedField nested : listType.fields()) {\n+          children.add(convert(nested));\n+        }\n+        break;\n+      case MAP:\n+        //Maps are represented as List<Struct<key, value>>\n+        metadata = ImmutableMap.of(ORIGINAL_TYPE, MAP_TYPE);\n+        final MapType mapType = field.type().asMapType();\n+        arrowType = ArrowType.List.INSTANCE;\n+\n+        final List<Field> entryFields = Lists.newArrayList(\n+            convert(required(0, MAP_KEY, mapType.keyType())),\n+            convert(optional(0, MAP_VALUE, mapType.valueType()))\n+        );\n+\n+        final Field entry = new Field(\"\",\n+            new FieldType(true, new ArrowType.Struct(), null), entryFields);\n+        children.add(entry);\n+        break;\n+      default: throw new UnsupportedOperationException(\"Unsupported field type: \" + field);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 142}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjA1Mjk4", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344205298", "createdAt": "2020-01-16T20:48:11Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDo0ODoxMlrOFenFhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDo0ODoxMlrOFenFhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY0MTk4OA==", "bodyText": "I don't think that the key/value struct should be optional.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367641988", "createdAt": "2020-01-16T20:48:12Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/ArrowSchemaUtil.java", "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.vector.types.DateUnit;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.TimeUnit;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.arrow.vector.types.pojo.Schema;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.types.Types.ListType;\n+import org.apache.iceberg.types.Types.MapType;\n+import org.apache.iceberg.types.Types.NestedField;\n+import org.apache.iceberg.types.Types.StructType;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+\n+public class ArrowSchemaUtil {\n+  static final String ORIGINAL_TYPE = \"originalType\";\n+  static final String MAP_TYPE = \"mapType\";\n+  static final String MAP_KEY = \"key\";\n+  static final String MAP_VALUE = \"value\";\n+\n+  private ArrowSchemaUtil() { }\n+\n+  /**\n+   * Convert Iceberg schema to Arrow Schema.\n+   *\n+   * @param schema iceberg schema\n+   * @return arrow schema\n+   */\n+  public static Schema convert(final org.apache.iceberg.Schema schema) {\n+    final ImmutableList.Builder<Field> fields = ImmutableList.builder();\n+\n+    for (NestedField f : schema.columns()) {\n+      fields.add(convert(f));\n+    }\n+\n+    return new Schema(fields.build());\n+  }\n+\n+  public static Field convert(final NestedField field) {\n+    final ArrowType arrowType;\n+\n+    final List<Field> children = Lists.newArrayList();\n+    Map<String, String> metadata = null;\n+\n+    switch (field.type().typeId()) {\n+      case BINARY:\n+        // Spark doesn't support BYTE(fixed_size) type, so cast it to VarBinary\n+      case FIXED:\n+        arrowType = ArrowType.Binary.INSTANCE;\n+        break;\n+      case BOOLEAN:\n+        arrowType = ArrowType.Bool.INSTANCE;\n+        break;\n+      case INTEGER:\n+        arrowType = new ArrowType.Int(Integer.SIZE, true);\n+        break;\n+      case LONG:\n+        arrowType = new ArrowType.Int(Long.SIZE, true);\n+        break;\n+      case FLOAT:\n+        arrowType = new ArrowType.FloatingPoint(FloatingPointPrecision.SINGLE);\n+        break;\n+      case DOUBLE:\n+        arrowType = new ArrowType.FloatingPoint(FloatingPointPrecision.DOUBLE);\n+        break;\n+      case DECIMAL:\n+        final Types.DecimalType decimalType = (Types.DecimalType) field.type();\n+        arrowType = new ArrowType.Decimal(decimalType.precision(), decimalType.scale());\n+        break;\n+      case STRING:\n+        arrowType = ArrowType.Utf8.INSTANCE;\n+        break;\n+      case TIME:\n+        arrowType = new ArrowType.Time(TimeUnit.MICROSECOND, Long.SIZE);\n+        break;\n+      case TIMESTAMP:\n+        arrowType = new ArrowType.Timestamp(TimeUnit.MICROSECOND, \"UTC\");\n+        break;\n+      case DATE:\n+        arrowType = new ArrowType.Date(DateUnit.DAY);\n+        break;\n+      case STRUCT:\n+        final StructType struct = field.type().asStructType();\n+        arrowType = ArrowType.Struct.INSTANCE;\n+\n+        for (NestedField nested : struct.fields()) {\n+          children.add(convert(nested));\n+        }\n+        break;\n+      case LIST:\n+        final ListType listType = field.type().asListType();\n+        arrowType = ArrowType.List.INSTANCE;\n+\n+        for (NestedField nested : listType.fields()) {\n+          children.add(convert(nested));\n+        }\n+        break;\n+      case MAP:\n+        //Maps are represented as List<Struct<key, value>>\n+        metadata = ImmutableMap.of(ORIGINAL_TYPE, MAP_TYPE);\n+        final MapType mapType = field.type().asMapType();\n+        arrowType = ArrowType.List.INSTANCE;\n+\n+        final List<Field> entryFields = Lists.newArrayList(\n+            convert(required(0, MAP_KEY, mapType.keyType())),\n+            convert(optional(0, MAP_VALUE, mapType.valueType()))\n+        );\n+\n+        final Field entry = new Field(\"\",\n+            new FieldType(true, new ArrowType.Struct(), null), entryFields);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 139}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjA4ODEx", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344208811", "createdAt": "2020-01-16T20:54:29Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDo1NDoyOVrOFenPdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMDo1NDoyOVrOFenPdA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY0NDUzMg==", "bodyText": "Looks like there are quite a few of these classes that exist just to override Arrow's validity vector check. Can we locate these classes somewhere to signal that they all have the same purpose? That could be making them static classes in a container class or moving them into a different sub-package.\nI like the container class approach:\n/**\n * Iceberg extensions for Arrow vectors that override the {@code isSet(int)} methods for performance.\n */\npublic class IcebergArrowVectors {\n  public static class DecimalArrowVector extends DecimalVector { ... }\n\n  public static class VarBinaryArrowVector extends VarBinaryVector { ... }\n}", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367644532", "createdAt": "2020-01-16T20:54:29Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/IcebergDecimalArrowVector.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.DecimalVector;\n+\n+/**\n+ * Extension of Arrow's @{@link DecimalVector}. The whole reason of having this implementation is to override the\n+ * expensive {@link DecimalVector#isSet(int)} method used by  {@link DecimalVector#getObject(int)}.\n+ */\n+public class IcebergDecimalArrowVector extends DecimalVector {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 29}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjEzNzgx", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344213781", "createdAt": "2020-01-16T21:03:11Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTowMzoxMVrOFend6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTowMzoxMVrOFend6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY0ODIzMg==", "bodyText": "All of the uses of isNullAt (at least, in this PR) convert boolean to 0 or 1 using a ternary operator. Should we store isNull as byte[] instead? Would that improve or hurt performance? Maybe there are uses elsewhere that make boolean the right choice here.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367648232", "createdAt": "2020-01-16T21:03:11Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/NullabilityHolder.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+public class NullabilityHolder {\n+  private final boolean[] isNull;\n+  private int numNulls;\n+\n+  public NullabilityHolder(int batchSize) {\n+    this.isNull = new boolean[batchSize];\n+  }\n+\n+  public void setNull(int idx) {\n+    isNull[idx] = true;\n+    numNulls++;\n+  }\n+\n+  public boolean isNullAt(int idx) {\n+    return isNull[idx];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 36}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjIwNDYy", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344220462", "createdAt": "2020-01-16T21:14:57Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToxNDo1N1rOFenxaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToxNDo1N1rOFenxaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1MzIyNA==", "bodyText": "Style: We don't use get in getter methods. As a verb, it doesn't add any clarity to what is happening, in contrast to verbs like fetch or aggregate that tell you some expensive action is occurring. It also makes expressions longer and is awkward when used from languages like Scala that don't use this convention.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367653224", "createdAt": "2020-01-16T21:14:57Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import javax.annotation.Nullable;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+\n+/**\n+ * Container class for holding the Arrow vector holding a batch of values along with other state needed for reading\n+ * values out of it.\n+ */\n+public class VectorHolder {\n+  private final ColumnDescriptor columnDescriptor;\n+  private final FieldVector vector;\n+  private final boolean isDictionaryEncoded;\n+\n+  @Nullable\n+  private final Dictionary dictionary;\n+  private final NullabilityHolder nullabilityHolder;\n+\n+  public static final VectorHolder NULL_VECTOR_HOLDER = new VectorHolder(null, null, false, null, null);\n+\n+  public VectorHolder(\n+      ColumnDescriptor columnDescriptor,\n+      FieldVector vector,\n+      boolean isDictionaryEncoded,\n+      Dictionary dictionary,\n+      NullabilityHolder holder) {\n+    this.columnDescriptor = columnDescriptor;\n+    this.vector = vector;\n+    this.isDictionaryEncoded = isDictionaryEncoded;\n+    this.dictionary = dictionary;\n+    this.nullabilityHolder = holder;\n+  }\n+\n+  public ColumnDescriptor getDescriptor() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjIxNDQ5", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344221449", "createdAt": "2020-01-16T21:16:43Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToxNjo0NFrOFen0dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToxNjo0NFrOFen0dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1NDAwNw==", "bodyText": "Is any of this from Spark or based on code from Spark? If so, we should note in comments where it came from and add a list of affected files to LICENSE.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367654007", "createdAt": "2020-01-16T21:16:44Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,387 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 57}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjIyNzIx", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344222721", "createdAt": "2020-01-16T21:18:50Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToxODo1MFrOFen4lA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToxODo1MFrOFen4lA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1NTA2MA==", "bodyText": "Why handle nullability and dictionary encoding differently here than with the Iceberg vector classes above? This wraps vectors and those extend vectors.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367655060", "createdAt": "2020-01-16T21:18:50Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorHolder.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import javax.annotation.Nullable;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+\n+/**\n+ * Container class for holding the Arrow vector holding a batch of values along with other state needed for reading\n+ * values out of it.\n+ */\n+public class VectorHolder {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 31}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjI0MTQ1", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344224145", "createdAt": "2020-01-16T21:21:14Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToyMToxNVrOFen8vA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToyMToxNVrOFen8vA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1NjEyNA==", "bodyText": "I'd normally expect an allocateSomething method to return the something, not a related int. Could this set the type width and return void instead? Or can the type width be derived from the vector that was allocated so this can return one thing?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367656124", "createdAt": "2020-01-16T21:21:15Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,387 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {\n+  public static final int DEFAULT_BATCH_SIZE = 5000;\n+  public static final int UNKNOWN_WIDTH = -1;\n+\n+  private final ColumnDescriptor columnDescriptor;\n+  private final int batchSize;\n+  private final VectorizedColumnIterator vectorizedColumnIterator;\n+  private final boolean isFixedLengthDecimal;\n+  private final boolean isVarWidthType;\n+  private final boolean isFixedWidthBinary;\n+  private final boolean isBooleanType;\n+  private final boolean isPaddedDecimal;\n+  private final boolean isIntType;\n+  private final boolean isLongType;\n+  private final boolean isFloatType;\n+  private final boolean isDoubleType;\n+  private final Types.NestedField icebergField;\n+  private final BufferAllocator rootAlloc;\n+  private FieldVector vec;\n+  private int typeWidth;\n+  private boolean reuseContainers = true;\n+  private NullabilityHolder nullabilityHolder;\n+\n+  // In cases when Parquet employs fall back to plain encoding, we eagerly decode the dictionary encoded pages\n+  // before storing the values in the Arrow vector. This means even if the dictionary is present, data\n+  // present in the vector may not necessarily be dictionary encoded.\n+  private Dictionary dictionary;\n+  private boolean allPagesDictEncoded;\n+\n+  // This value is copied from Arrow's BaseVariableWidthVector. We may need to change\n+  // this value if Arrow ends up changing this default.\n+  private static final int DEFAULT_RECORD_BYTE_COUNT = 8;\n+\n+  public VectorizedArrowReader(\n+      ColumnDescriptor desc,\n+      Types.NestedField icebergField,\n+      BufferAllocator ra,\n+      int batchSize) {\n+    this.icebergField = icebergField;\n+    this.batchSize = (batchSize == 0) ? DEFAULT_BATCH_SIZE : batchSize;\n+    this.columnDescriptor = desc;\n+    this.rootAlloc = ra;\n+    this.isFixedLengthDecimal = isFixedLengthDecimal(desc);\n+    this.isVarWidthType = isVarWidthType(desc);\n+    this.isFixedWidthBinary = isFixedWidthBinary(desc);\n+    this.isBooleanType = isBooleanType(desc);\n+    this.isPaddedDecimal = isIntLongBackedDecimal(desc);\n+    this.isIntType = isIntType(desc);\n+    this.isLongType = isLongType(desc);\n+    this.isFloatType = isFloatType(desc);\n+    this.isDoubleType = isDoubleType(desc);\n+    this.vectorizedColumnIterator = new VectorizedColumnIterator(desc, \"\", batchSize);\n+  }\n+\n+  private VectorizedArrowReader() {\n+    this.icebergField = null;\n+    this.batchSize = DEFAULT_BATCH_SIZE;\n+    this.columnDescriptor = null;\n+    this.rootAlloc = null;\n+    this.isFixedLengthDecimal = false;\n+    this.isVarWidthType = false;\n+    this.isFixedWidthBinary = false;\n+    this.isBooleanType = false;\n+    this.isPaddedDecimal = false;\n+    this.isIntType = false;\n+    this.isLongType = false;\n+    this.isFloatType = false;\n+    this.isDoubleType = false;\n+    this.vectorizedColumnIterator = null;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  @Override\n+  public VectorHolder read(int numValsToRead) {\n+    if (vec == null || !reuseContainers) {\n+      typeWidth = allocateFieldVector();\n+    }\n+    vec.setValueCount(0);\n+    nullabilityHolder = new NullabilityHolder(batchSize);\n+    if (vectorizedColumnIterator.hasNext()) {\n+      if (allPagesDictEncoded) {\n+        vectorizedColumnIterator.nextBatchDictionaryIds((IntVector) vec, nullabilityHolder);\n+      } else {\n+        if (isFixedLengthDecimal) {\n+          vectorizedColumnIterator.nextBatchFixedLengthDecimal(vec, typeWidth, nullabilityHolder);\n+          ((IcebergDecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+        } else if (isFixedWidthBinary) {\n+          // Fixed width binary type values are stored in an IcebergVarBinaryArrowVector as well\n+          if (vec instanceof IcebergVarBinaryArrowVector) {\n+            ((IcebergVarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+          }\n+          vectorizedColumnIterator.nextBatchFixedWidthBinary(vec, typeWidth, nullabilityHolder);\n+        } else if (isVarWidthType) {\n+          if (vec instanceof IcebergVarcharArrowVector) {\n+            ((IcebergVarcharArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+          } else if (vec instanceof IcebergVarBinaryArrowVector) {\n+            ((IcebergVarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+          }\n+          vectorizedColumnIterator.nextBatchVarWidthType(vec, nullabilityHolder);\n+        } else if (isBooleanType) {\n+          vectorizedColumnIterator.nextBatchBoolean(vec, nullabilityHolder);\n+        } else if (isPaddedDecimal) {\n+          ((IcebergDecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+          vectorizedColumnIterator.nextBatchIntLongBackedDecimal(vec, typeWidth, nullabilityHolder);\n+        } else if (isIntType) {\n+          vectorizedColumnIterator.nextBatchIntegers(vec, typeWidth, nullabilityHolder);\n+        } else if (isLongType) {\n+          vectorizedColumnIterator.nextBatchLongs(vec, typeWidth, nullabilityHolder);\n+        } else if (isFloatType) {\n+          vectorizedColumnIterator.nextBatchFloats(vec, typeWidth, nullabilityHolder);\n+        } else if (isDoubleType) {\n+          vectorizedColumnIterator.nextBatchDoubles(vec, typeWidth, nullabilityHolder);\n+        }\n+      }\n+    }\n+    if (vec.getValueCount() != numValsToRead) {\n+      throw new IllegalStateException(\"Number of values read into the vector, \" +\n+          vec.getValueCount() + \" is not the same as the expected count of \" + numValsToRead);\n+    }\n+    return new VectorHolder(columnDescriptor, vec, allPagesDictEncoded, dictionary, nullabilityHolder);\n+  }\n+\n+  private int allocateFieldVector() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 179}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjI0NTIy", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344224522", "createdAt": "2020-01-16T21:21:55Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToyMTo1NVrOFen92Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToyMTo1NVrOFen92Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1NjQwOQ==", "bodyText": "Why is a new nullability holder always used instead of reusing the existing ones?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367656409", "createdAt": "2020-01-16T21:21:55Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,387 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {\n+  public static final int DEFAULT_BATCH_SIZE = 5000;\n+  public static final int UNKNOWN_WIDTH = -1;\n+\n+  private final ColumnDescriptor columnDescriptor;\n+  private final int batchSize;\n+  private final VectorizedColumnIterator vectorizedColumnIterator;\n+  private final boolean isFixedLengthDecimal;\n+  private final boolean isVarWidthType;\n+  private final boolean isFixedWidthBinary;\n+  private final boolean isBooleanType;\n+  private final boolean isPaddedDecimal;\n+  private final boolean isIntType;\n+  private final boolean isLongType;\n+  private final boolean isFloatType;\n+  private final boolean isDoubleType;\n+  private final Types.NestedField icebergField;\n+  private final BufferAllocator rootAlloc;\n+  private FieldVector vec;\n+  private int typeWidth;\n+  private boolean reuseContainers = true;\n+  private NullabilityHolder nullabilityHolder;\n+\n+  // In cases when Parquet employs fall back to plain encoding, we eagerly decode the dictionary encoded pages\n+  // before storing the values in the Arrow vector. This means even if the dictionary is present, data\n+  // present in the vector may not necessarily be dictionary encoded.\n+  private Dictionary dictionary;\n+  private boolean allPagesDictEncoded;\n+\n+  // This value is copied from Arrow's BaseVariableWidthVector. We may need to change\n+  // this value if Arrow ends up changing this default.\n+  private static final int DEFAULT_RECORD_BYTE_COUNT = 8;\n+\n+  public VectorizedArrowReader(\n+      ColumnDescriptor desc,\n+      Types.NestedField icebergField,\n+      BufferAllocator ra,\n+      int batchSize) {\n+    this.icebergField = icebergField;\n+    this.batchSize = (batchSize == 0) ? DEFAULT_BATCH_SIZE : batchSize;\n+    this.columnDescriptor = desc;\n+    this.rootAlloc = ra;\n+    this.isFixedLengthDecimal = isFixedLengthDecimal(desc);\n+    this.isVarWidthType = isVarWidthType(desc);\n+    this.isFixedWidthBinary = isFixedWidthBinary(desc);\n+    this.isBooleanType = isBooleanType(desc);\n+    this.isPaddedDecimal = isIntLongBackedDecimal(desc);\n+    this.isIntType = isIntType(desc);\n+    this.isLongType = isLongType(desc);\n+    this.isFloatType = isFloatType(desc);\n+    this.isDoubleType = isDoubleType(desc);\n+    this.vectorizedColumnIterator = new VectorizedColumnIterator(desc, \"\", batchSize);\n+  }\n+\n+  private VectorizedArrowReader() {\n+    this.icebergField = null;\n+    this.batchSize = DEFAULT_BATCH_SIZE;\n+    this.columnDescriptor = null;\n+    this.rootAlloc = null;\n+    this.isFixedLengthDecimal = false;\n+    this.isVarWidthType = false;\n+    this.isFixedWidthBinary = false;\n+    this.isBooleanType = false;\n+    this.isPaddedDecimal = false;\n+    this.isIntType = false;\n+    this.isLongType = false;\n+    this.isFloatType = false;\n+    this.isDoubleType = false;\n+    this.vectorizedColumnIterator = null;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  @Override\n+  public VectorHolder read(int numValsToRead) {\n+    if (vec == null || !reuseContainers) {\n+      typeWidth = allocateFieldVector();\n+    }\n+    vec.setValueCount(0);\n+    nullabilityHolder = new NullabilityHolder(batchSize);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 135}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjI0OTIy", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344224922", "createdAt": "2020-01-16T21:22:36Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToyMjozNlrOFen-_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToyMjozNlrOFen-_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1NjcwMg==", "bodyText": "ra isn't very descriptive. Can we use allocator instead?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367656702", "createdAt": "2020-01-16T21:22:36Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,387 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {\n+  public static final int DEFAULT_BATCH_SIZE = 5000;\n+  public static final int UNKNOWN_WIDTH = -1;\n+\n+  private final ColumnDescriptor columnDescriptor;\n+  private final int batchSize;\n+  private final VectorizedColumnIterator vectorizedColumnIterator;\n+  private final boolean isFixedLengthDecimal;\n+  private final boolean isVarWidthType;\n+  private final boolean isFixedWidthBinary;\n+  private final boolean isBooleanType;\n+  private final boolean isPaddedDecimal;\n+  private final boolean isIntType;\n+  private final boolean isLongType;\n+  private final boolean isFloatType;\n+  private final boolean isDoubleType;\n+  private final Types.NestedField icebergField;\n+  private final BufferAllocator rootAlloc;\n+  private FieldVector vec;\n+  private int typeWidth;\n+  private boolean reuseContainers = true;\n+  private NullabilityHolder nullabilityHolder;\n+\n+  // In cases when Parquet employs fall back to plain encoding, we eagerly decode the dictionary encoded pages\n+  // before storing the values in the Arrow vector. This means even if the dictionary is present, data\n+  // present in the vector may not necessarily be dictionary encoded.\n+  private Dictionary dictionary;\n+  private boolean allPagesDictEncoded;\n+\n+  // This value is copied from Arrow's BaseVariableWidthVector. We may need to change\n+  // this value if Arrow ends up changing this default.\n+  private static final int DEFAULT_RECORD_BYTE_COUNT = 8;\n+\n+  public VectorizedArrowReader(\n+      ColumnDescriptor desc,\n+      Types.NestedField icebergField,\n+      BufferAllocator ra,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 93}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjI2MjAz", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344226203", "createdAt": "2020-01-16T21:24:47Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToyNDo0N1rOFeoDAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToyNDo0N1rOFeoDAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1NzczMA==", "bodyText": "Would it make sense to use an enum instead of 9 booleans?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367657730", "createdAt": "2020-01-16T21:24:47Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,387 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {\n+  public static final int DEFAULT_BATCH_SIZE = 5000;\n+  public static final int UNKNOWN_WIDTH = -1;\n+\n+  private final ColumnDescriptor columnDescriptor;\n+  private final int batchSize;\n+  private final VectorizedColumnIterator vectorizedColumnIterator;\n+  private final boolean isFixedLengthDecimal;\n+  private final boolean isVarWidthType;\n+  private final boolean isFixedWidthBinary;\n+  private final boolean isBooleanType;\n+  private final boolean isPaddedDecimal;\n+  private final boolean isIntType;\n+  private final boolean isLongType;\n+  private final boolean isFloatType;\n+  private final boolean isDoubleType;\n+  private final Types.NestedField icebergField;\n+  private final BufferAllocator rootAlloc;\n+  private FieldVector vec;\n+  private int typeWidth;\n+  private boolean reuseContainers = true;\n+  private NullabilityHolder nullabilityHolder;\n+\n+  // In cases when Parquet employs fall back to plain encoding, we eagerly decode the dictionary encoded pages\n+  // before storing the values in the Arrow vector. This means even if the dictionary is present, data\n+  // present in the vector may not necessarily be dictionary encoded.\n+  private Dictionary dictionary;\n+  private boolean allPagesDictEncoded;\n+\n+  // This value is copied from Arrow's BaseVariableWidthVector. We may need to change\n+  // this value if Arrow ends up changing this default.\n+  private static final int DEFAULT_RECORD_BYTE_COUNT = 8;\n+\n+  public VectorizedArrowReader(\n+      ColumnDescriptor desc,\n+      Types.NestedField icebergField,\n+      BufferAllocator ra,\n+      int batchSize) {\n+    this.icebergField = icebergField;\n+    this.batchSize = (batchSize == 0) ? DEFAULT_BATCH_SIZE : batchSize;\n+    this.columnDescriptor = desc;\n+    this.rootAlloc = ra;\n+    this.isFixedLengthDecimal = isFixedLengthDecimal(desc);\n+    this.isVarWidthType = isVarWidthType(desc);\n+    this.isFixedWidthBinary = isFixedWidthBinary(desc);\n+    this.isBooleanType = isBooleanType(desc);\n+    this.isPaddedDecimal = isIntLongBackedDecimal(desc);\n+    this.isIntType = isIntType(desc);\n+    this.isLongType = isLongType(desc);\n+    this.isFloatType = isFloatType(desc);\n+    this.isDoubleType = isDoubleType(desc);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 107}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjI3MTUz", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344227153", "createdAt": "2020-01-16T21:26:25Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToyNjoyNVrOFeoF4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMToyNjoyNVrOFeoF4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY1ODQ2Nw==", "bodyText": "Why does this use an unsafe cast, while fixed width below checks the vector class?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367658467", "createdAt": "2020-01-16T21:26:25Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,387 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {\n+  public static final int DEFAULT_BATCH_SIZE = 5000;\n+  public static final int UNKNOWN_WIDTH = -1;\n+\n+  private final ColumnDescriptor columnDescriptor;\n+  private final int batchSize;\n+  private final VectorizedColumnIterator vectorizedColumnIterator;\n+  private final boolean isFixedLengthDecimal;\n+  private final boolean isVarWidthType;\n+  private final boolean isFixedWidthBinary;\n+  private final boolean isBooleanType;\n+  private final boolean isPaddedDecimal;\n+  private final boolean isIntType;\n+  private final boolean isLongType;\n+  private final boolean isFloatType;\n+  private final boolean isDoubleType;\n+  private final Types.NestedField icebergField;\n+  private final BufferAllocator rootAlloc;\n+  private FieldVector vec;\n+  private int typeWidth;\n+  private boolean reuseContainers = true;\n+  private NullabilityHolder nullabilityHolder;\n+\n+  // In cases when Parquet employs fall back to plain encoding, we eagerly decode the dictionary encoded pages\n+  // before storing the values in the Arrow vector. This means even if the dictionary is present, data\n+  // present in the vector may not necessarily be dictionary encoded.\n+  private Dictionary dictionary;\n+  private boolean allPagesDictEncoded;\n+\n+  // This value is copied from Arrow's BaseVariableWidthVector. We may need to change\n+  // this value if Arrow ends up changing this default.\n+  private static final int DEFAULT_RECORD_BYTE_COUNT = 8;\n+\n+  public VectorizedArrowReader(\n+      ColumnDescriptor desc,\n+      Types.NestedField icebergField,\n+      BufferAllocator ra,\n+      int batchSize) {\n+    this.icebergField = icebergField;\n+    this.batchSize = (batchSize == 0) ? DEFAULT_BATCH_SIZE : batchSize;\n+    this.columnDescriptor = desc;\n+    this.rootAlloc = ra;\n+    this.isFixedLengthDecimal = isFixedLengthDecimal(desc);\n+    this.isVarWidthType = isVarWidthType(desc);\n+    this.isFixedWidthBinary = isFixedWidthBinary(desc);\n+    this.isBooleanType = isBooleanType(desc);\n+    this.isPaddedDecimal = isIntLongBackedDecimal(desc);\n+    this.isIntType = isIntType(desc);\n+    this.isLongType = isLongType(desc);\n+    this.isFloatType = isFloatType(desc);\n+    this.isDoubleType = isDoubleType(desc);\n+    this.vectorizedColumnIterator = new VectorizedColumnIterator(desc, \"\", batchSize);\n+  }\n+\n+  private VectorizedArrowReader() {\n+    this.icebergField = null;\n+    this.batchSize = DEFAULT_BATCH_SIZE;\n+    this.columnDescriptor = null;\n+    this.rootAlloc = null;\n+    this.isFixedLengthDecimal = false;\n+    this.isVarWidthType = false;\n+    this.isFixedWidthBinary = false;\n+    this.isBooleanType = false;\n+    this.isPaddedDecimal = false;\n+    this.isIntType = false;\n+    this.isLongType = false;\n+    this.isFloatType = false;\n+    this.isDoubleType = false;\n+    this.vectorizedColumnIterator = null;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  @Override\n+  public VectorHolder read(int numValsToRead) {\n+    if (vec == null || !reuseContainers) {\n+      typeWidth = allocateFieldVector();\n+    }\n+    vec.setValueCount(0);\n+    nullabilityHolder = new NullabilityHolder(batchSize);\n+    if (vectorizedColumnIterator.hasNext()) {\n+      if (allPagesDictEncoded) {\n+        vectorizedColumnIterator.nextBatchDictionaryIds((IntVector) vec, nullabilityHolder);\n+      } else {\n+        if (isFixedLengthDecimal) {\n+          vectorizedColumnIterator.nextBatchFixedLengthDecimal(vec, typeWidth, nullabilityHolder);\n+          ((IcebergDecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 142}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjM1MDk0", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344235094", "createdAt": "2020-01-16T21:40:55Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo0MDo1NVrOFeodgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo0MDo1NVrOFeodgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY2NDUxMg==", "bodyText": "Right now, allocation and reads use separate logic. Reads use the isXType booleans, while allocateFieldVector uses the Parquet type.\nI think I understand why those are slightly different: allocation needs to create, for example, a DateDayVector but reading calls nextBatchIntergers because it writes into the underlying buffer directly.\nI think that having these completely decoupled could lead to problems, though. I'd rather allocate a vector and then base the type of read on the vector that was produced. Something like this:\n  public VectorizedArrowReader(...) {\n    ...\n    this.vector = allocateFieldVector();\n    if (this.vector instanceof IntVector || this.vector instanceof DateDayVector) {\n      this.readMethod = ReadMethod.INT;\n    } else if (this.vector instanceof LongVector || this.vector instanceof TimeStampMicroTZVector) {\n      this.readMethod = ReadMethod.LONG;\n    }\n  }\n\nThe logic would probably be more complicated because it needs to rely on both the incoming Parquet encoding and the outgoing vector type. But the main thing is that it would be good to check that the vector and the read method are compatible.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367664512", "createdAt": "2020-01-16T21:40:55Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,387 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {\n+  public static final int DEFAULT_BATCH_SIZE = 5000;\n+  public static final int UNKNOWN_WIDTH = -1;\n+\n+  private final ColumnDescriptor columnDescriptor;\n+  private final int batchSize;\n+  private final VectorizedColumnIterator vectorizedColumnIterator;\n+  private final boolean isFixedLengthDecimal;\n+  private final boolean isVarWidthType;\n+  private final boolean isFixedWidthBinary;\n+  private final boolean isBooleanType;\n+  private final boolean isPaddedDecimal;\n+  private final boolean isIntType;\n+  private final boolean isLongType;\n+  private final boolean isFloatType;\n+  private final boolean isDoubleType;\n+  private final Types.NestedField icebergField;\n+  private final BufferAllocator rootAlloc;\n+  private FieldVector vec;\n+  private int typeWidth;\n+  private boolean reuseContainers = true;\n+  private NullabilityHolder nullabilityHolder;\n+\n+  // In cases when Parquet employs fall back to plain encoding, we eagerly decode the dictionary encoded pages\n+  // before storing the values in the Arrow vector. This means even if the dictionary is present, data\n+  // present in the vector may not necessarily be dictionary encoded.\n+  private Dictionary dictionary;\n+  private boolean allPagesDictEncoded;\n+\n+  // This value is copied from Arrow's BaseVariableWidthVector. We may need to change\n+  // this value if Arrow ends up changing this default.\n+  private static final int DEFAULT_RECORD_BYTE_COUNT = 8;\n+\n+  public VectorizedArrowReader(\n+      ColumnDescriptor desc,\n+      Types.NestedField icebergField,\n+      BufferAllocator ra,\n+      int batchSize) {\n+    this.icebergField = icebergField;\n+    this.batchSize = (batchSize == 0) ? DEFAULT_BATCH_SIZE : batchSize;\n+    this.columnDescriptor = desc;\n+    this.rootAlloc = ra;\n+    this.isFixedLengthDecimal = isFixedLengthDecimal(desc);\n+    this.isVarWidthType = isVarWidthType(desc);\n+    this.isFixedWidthBinary = isFixedWidthBinary(desc);\n+    this.isBooleanType = isBooleanType(desc);\n+    this.isPaddedDecimal = isIntLongBackedDecimal(desc);\n+    this.isIntType = isIntType(desc);\n+    this.isLongType = isLongType(desc);\n+    this.isFloatType = isFloatType(desc);\n+    this.isDoubleType = isDoubleType(desc);\n+    this.vectorizedColumnIterator = new VectorizedColumnIterator(desc, \"\", batchSize);\n+  }\n+\n+  private VectorizedArrowReader() {\n+    this.icebergField = null;\n+    this.batchSize = DEFAULT_BATCH_SIZE;\n+    this.columnDescriptor = null;\n+    this.rootAlloc = null;\n+    this.isFixedLengthDecimal = false;\n+    this.isVarWidthType = false;\n+    this.isFixedWidthBinary = false;\n+    this.isBooleanType = false;\n+    this.isPaddedDecimal = false;\n+    this.isIntType = false;\n+    this.isLongType = false;\n+    this.isFloatType = false;\n+    this.isDoubleType = false;\n+    this.vectorizedColumnIterator = null;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  @Override\n+  public VectorHolder read(int numValsToRead) {\n+    if (vec == null || !reuseContainers) {\n+      typeWidth = allocateFieldVector();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 132}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjM2MzQ1", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344236345", "createdAt": "2020-01-16T21:43:13Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo0MzoxM1rOFeohWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo0MzoxM1rOFeohWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY2NTQ5Ng==", "bodyText": "Using Preconditions could save some space and make this a format string:\nPreconditions.checkState(vec.getValueCount() == numValsToRead,\n    \"Number of values read, %s, does not equal expected, %s\", vec.getValueCount(), numValsToRead);", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367665496", "createdAt": "2020-01-16T21:43:13Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,387 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {\n+  public static final int DEFAULT_BATCH_SIZE = 5000;\n+  public static final int UNKNOWN_WIDTH = -1;\n+\n+  private final ColumnDescriptor columnDescriptor;\n+  private final int batchSize;\n+  private final VectorizedColumnIterator vectorizedColumnIterator;\n+  private final boolean isFixedLengthDecimal;\n+  private final boolean isVarWidthType;\n+  private final boolean isFixedWidthBinary;\n+  private final boolean isBooleanType;\n+  private final boolean isPaddedDecimal;\n+  private final boolean isIntType;\n+  private final boolean isLongType;\n+  private final boolean isFloatType;\n+  private final boolean isDoubleType;\n+  private final Types.NestedField icebergField;\n+  private final BufferAllocator rootAlloc;\n+  private FieldVector vec;\n+  private int typeWidth;\n+  private boolean reuseContainers = true;\n+  private NullabilityHolder nullabilityHolder;\n+\n+  // In cases when Parquet employs fall back to plain encoding, we eagerly decode the dictionary encoded pages\n+  // before storing the values in the Arrow vector. This means even if the dictionary is present, data\n+  // present in the vector may not necessarily be dictionary encoded.\n+  private Dictionary dictionary;\n+  private boolean allPagesDictEncoded;\n+\n+  // This value is copied from Arrow's BaseVariableWidthVector. We may need to change\n+  // this value if Arrow ends up changing this default.\n+  private static final int DEFAULT_RECORD_BYTE_COUNT = 8;\n+\n+  public VectorizedArrowReader(\n+      ColumnDescriptor desc,\n+      Types.NestedField icebergField,\n+      BufferAllocator ra,\n+      int batchSize) {\n+    this.icebergField = icebergField;\n+    this.batchSize = (batchSize == 0) ? DEFAULT_BATCH_SIZE : batchSize;\n+    this.columnDescriptor = desc;\n+    this.rootAlloc = ra;\n+    this.isFixedLengthDecimal = isFixedLengthDecimal(desc);\n+    this.isVarWidthType = isVarWidthType(desc);\n+    this.isFixedWidthBinary = isFixedWidthBinary(desc);\n+    this.isBooleanType = isBooleanType(desc);\n+    this.isPaddedDecimal = isIntLongBackedDecimal(desc);\n+    this.isIntType = isIntType(desc);\n+    this.isLongType = isLongType(desc);\n+    this.isFloatType = isFloatType(desc);\n+    this.isDoubleType = isDoubleType(desc);\n+    this.vectorizedColumnIterator = new VectorizedColumnIterator(desc, \"\", batchSize);\n+  }\n+\n+  private VectorizedArrowReader() {\n+    this.icebergField = null;\n+    this.batchSize = DEFAULT_BATCH_SIZE;\n+    this.columnDescriptor = null;\n+    this.rootAlloc = null;\n+    this.isFixedLengthDecimal = false;\n+    this.isVarWidthType = false;\n+    this.isFixedWidthBinary = false;\n+    this.isBooleanType = false;\n+    this.isPaddedDecimal = false;\n+    this.isIntType = false;\n+    this.isLongType = false;\n+    this.isFloatType = false;\n+    this.isDoubleType = false;\n+    this.vectorizedColumnIterator = null;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  @Override\n+  public VectorHolder read(int numValsToRead) {\n+    if (vec == null || !reuseContainers) {\n+      typeWidth = allocateFieldVector();\n+    }\n+    vec.setValueCount(0);\n+    nullabilityHolder = new NullabilityHolder(batchSize);\n+    if (vectorizedColumnIterator.hasNext()) {\n+      if (allPagesDictEncoded) {\n+        vectorizedColumnIterator.nextBatchDictionaryIds((IntVector) vec, nullabilityHolder);\n+      } else {\n+        if (isFixedLengthDecimal) {\n+          vectorizedColumnIterator.nextBatchFixedLengthDecimal(vec, typeWidth, nullabilityHolder);\n+          ((IcebergDecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+        } else if (isFixedWidthBinary) {\n+          // Fixed width binary type values are stored in an IcebergVarBinaryArrowVector as well\n+          if (vec instanceof IcebergVarBinaryArrowVector) {\n+            ((IcebergVarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+          }\n+          vectorizedColumnIterator.nextBatchFixedWidthBinary(vec, typeWidth, nullabilityHolder);\n+        } else if (isVarWidthType) {\n+          if (vec instanceof IcebergVarcharArrowVector) {\n+            ((IcebergVarcharArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+          } else if (vec instanceof IcebergVarBinaryArrowVector) {\n+            ((IcebergVarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+          }\n+          vectorizedColumnIterator.nextBatchVarWidthType(vec, nullabilityHolder);\n+        } else if (isBooleanType) {\n+          vectorizedColumnIterator.nextBatchBoolean(vec, nullabilityHolder);\n+        } else if (isPaddedDecimal) {\n+          ((IcebergDecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+          vectorizedColumnIterator.nextBatchIntLongBackedDecimal(vec, typeWidth, nullabilityHolder);\n+        } else if (isIntType) {\n+          vectorizedColumnIterator.nextBatchIntegers(vec, typeWidth, nullabilityHolder);\n+        } else if (isLongType) {\n+          vectorizedColumnIterator.nextBatchLongs(vec, typeWidth, nullabilityHolder);\n+        } else if (isFloatType) {\n+          vectorizedColumnIterator.nextBatchFloats(vec, typeWidth, nullabilityHolder);\n+        } else if (isDoubleType) {\n+          vectorizedColumnIterator.nextBatchDoubles(vec, typeWidth, nullabilityHolder);\n+        }\n+      }\n+    }\n+    if (vec.getValueCount() != numValsToRead) {\n+      throw new IllegalStateException(\"Number of values read into the vector, \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 173}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjM3Mjgy", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344237282", "createdAt": "2020-01-16T21:44:50Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo0NDo1MFrOFeokDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo0NDo1MFrOFeokDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY2NjE5MA==", "bodyText": "Doesn't look like this is used anywhere?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367666190", "createdAt": "2020-01-16T21:44:50Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,387 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {\n+  public static final int DEFAULT_BATCH_SIZE = 5000;\n+  public static final int UNKNOWN_WIDTH = -1;\n+\n+  private final ColumnDescriptor columnDescriptor;\n+  private final int batchSize;\n+  private final VectorizedColumnIterator vectorizedColumnIterator;\n+  private final boolean isFixedLengthDecimal;\n+  private final boolean isVarWidthType;\n+  private final boolean isFixedWidthBinary;\n+  private final boolean isBooleanType;\n+  private final boolean isPaddedDecimal;\n+  private final boolean isIntType;\n+  private final boolean isLongType;\n+  private final boolean isFloatType;\n+  private final boolean isDoubleType;\n+  private final Types.NestedField icebergField;\n+  private final BufferAllocator rootAlloc;\n+  private FieldVector vec;\n+  private int typeWidth;\n+  private boolean reuseContainers = true;\n+  private NullabilityHolder nullabilityHolder;\n+\n+  // In cases when Parquet employs fall back to plain encoding, we eagerly decode the dictionary encoded pages\n+  // before storing the values in the Arrow vector. This means even if the dictionary is present, data\n+  // present in the vector may not necessarily be dictionary encoded.\n+  private Dictionary dictionary;\n+  private boolean allPagesDictEncoded;\n+\n+  // This value is copied from Arrow's BaseVariableWidthVector. We may need to change\n+  // this value if Arrow ends up changing this default.\n+  private static final int DEFAULT_RECORD_BYTE_COUNT = 8;\n+\n+  public VectorizedArrowReader(\n+      ColumnDescriptor desc,\n+      Types.NestedField icebergField,\n+      BufferAllocator ra,\n+      int batchSize) {\n+    this.icebergField = icebergField;\n+    this.batchSize = (batchSize == 0) ? DEFAULT_BATCH_SIZE : batchSize;\n+    this.columnDescriptor = desc;\n+    this.rootAlloc = ra;\n+    this.isFixedLengthDecimal = isFixedLengthDecimal(desc);\n+    this.isVarWidthType = isVarWidthType(desc);\n+    this.isFixedWidthBinary = isFixedWidthBinary(desc);\n+    this.isBooleanType = isBooleanType(desc);\n+    this.isPaddedDecimal = isIntLongBackedDecimal(desc);\n+    this.isIntType = isIntType(desc);\n+    this.isLongType = isLongType(desc);\n+    this.isFloatType = isFloatType(desc);\n+    this.isDoubleType = isDoubleType(desc);\n+    this.vectorizedColumnIterator = new VectorizedColumnIterator(desc, \"\", batchSize);\n+  }\n+\n+  private VectorizedArrowReader() {\n+    this.icebergField = null;\n+    this.batchSize = DEFAULT_BATCH_SIZE;\n+    this.columnDescriptor = null;\n+    this.rootAlloc = null;\n+    this.isFixedLengthDecimal = false;\n+    this.isVarWidthType = false;\n+    this.isFixedWidthBinary = false;\n+    this.isBooleanType = false;\n+    this.isPaddedDecimal = false;\n+    this.isIntType = false;\n+    this.isLongType = false;\n+    this.isFloatType = false;\n+    this.isDoubleType = false;\n+    this.vectorizedColumnIterator = null;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  @Override\n+  public VectorHolder read(int numValsToRead) {\n+    if (vec == null || !reuseContainers) {\n+      typeWidth = allocateFieldVector();\n+    }\n+    vec.setValueCount(0);\n+    nullabilityHolder = new NullabilityHolder(batchSize);\n+    if (vectorizedColumnIterator.hasNext()) {\n+      if (allPagesDictEncoded) {\n+        vectorizedColumnIterator.nextBatchDictionaryIds((IntVector) vec, nullabilityHolder);\n+      } else {\n+        if (isFixedLengthDecimal) {\n+          vectorizedColumnIterator.nextBatchFixedLengthDecimal(vec, typeWidth, nullabilityHolder);\n+          ((IcebergDecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+        } else if (isFixedWidthBinary) {\n+          // Fixed width binary type values are stored in an IcebergVarBinaryArrowVector as well\n+          if (vec instanceof IcebergVarBinaryArrowVector) {\n+            ((IcebergVarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+          }\n+          vectorizedColumnIterator.nextBatchFixedWidthBinary(vec, typeWidth, nullabilityHolder);\n+        } else if (isVarWidthType) {\n+          if (vec instanceof IcebergVarcharArrowVector) {\n+            ((IcebergVarcharArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+          } else if (vec instanceof IcebergVarBinaryArrowVector) {\n+            ((IcebergVarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+          }\n+          vectorizedColumnIterator.nextBatchVarWidthType(vec, nullabilityHolder);\n+        } else if (isBooleanType) {\n+          vectorizedColumnIterator.nextBatchBoolean(vec, nullabilityHolder);\n+        } else if (isPaddedDecimal) {\n+          ((IcebergDecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+          vectorizedColumnIterator.nextBatchIntLongBackedDecimal(vec, typeWidth, nullabilityHolder);\n+        } else if (isIntType) {\n+          vectorizedColumnIterator.nextBatchIntegers(vec, typeWidth, nullabilityHolder);\n+        } else if (isLongType) {\n+          vectorizedColumnIterator.nextBatchLongs(vec, typeWidth, nullabilityHolder);\n+        } else if (isFloatType) {\n+          vectorizedColumnIterator.nextBatchFloats(vec, typeWidth, nullabilityHolder);\n+        } else if (isDoubleType) {\n+          vectorizedColumnIterator.nextBatchDoubles(vec, typeWidth, nullabilityHolder);\n+        }\n+      }\n+    }\n+    if (vec.getValueCount() != numValsToRead) {\n+      throw new IllegalStateException(\"Number of values read into the vector, \" +\n+          vec.getValueCount() + \" is not the same as the expected count of \" + numValsToRead);\n+    }\n+    return new VectorHolder(columnDescriptor, vec, allPagesDictEncoded, dictionary, nullabilityHolder);\n+  }\n+\n+  private int allocateFieldVector() {\n+    if (allPagesDictEncoded) {\n+      Field field = new Field(\n+          icebergField.name(),\n+          new FieldType(icebergField.isOptional(), new ArrowType.Int(Integer.SIZE, true), null, null),\n+          null);\n+      this.vec = field.createVector(rootAlloc);\n+      ((IntVector) vec).allocateNew(batchSize);\n+      return IntVector.TYPE_WIDTH;\n+    } else {\n+      PrimitiveType primitive = columnDescriptor.getPrimitiveType();\n+      if (primitive.getOriginalType() != null) {\n+        switch (columnDescriptor.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            this.vec = new IcebergVarcharArrowVector(icebergField.name(), rootAlloc);\n+            //TODO: Possibly use the uncompressed page size info to set the initial capacity\n+            vec.setInitialCapacity(batchSize * 10);\n+            vec.allocateNewSafe();\n+            return UNKNOWN_WIDTH;\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((IntVector) vec).allocateNew(batchSize);\n+            return IntVector.TYPE_WIDTH;\n+          case DATE:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((DateDayVector) vec).allocateNew(batchSize);\n+            return IntVector.TYPE_WIDTH;\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((BigIntVector) vec).allocateNew(batchSize);\n+            return BigIntVector.TYPE_WIDTH;\n+          case TIMESTAMP_MICROS:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((TimeStampMicroTZVector) vec).allocateNew(batchSize);\n+            return BigIntVector.TYPE_WIDTH;\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            this.vec = new IcebergDecimalArrowVector(icebergField.name(), rootAlloc, decimal.getPrecision(),\n+                decimal.getScale());\n+            ((DecimalVector) vec).allocateNew(batchSize);\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return primitive.getTypeLength();\n+              case INT64:\n+                return BigIntVector.TYPE_WIDTH;\n+              case INT32:\n+                return IntVector.TYPE_WIDTH;\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+            int len = ((Types.FixedType) icebergField.type()).length();\n+            this.vec = new IcebergVarBinaryArrowVector(icebergField.name(), rootAlloc);\n+            int factor = (len + DEFAULT_RECORD_BYTE_COUNT - 1) / DEFAULT_RECORD_BYTE_COUNT;\n+            vec.setInitialCapacity(batchSize * factor);\n+            vec.allocateNew();\n+            return len;\n+          case BINARY:\n+            this.vec = new IcebergVarBinaryArrowVector(icebergField.name(), rootAlloc);\n+            //TODO: Possibly use the uncompressed page size info to set the initial capacity\n+            vec.setInitialCapacity(batchSize * 10);\n+            vec.allocateNewSafe();\n+            return UNKNOWN_WIDTH;\n+          case INT32:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((IntVector) vec).allocateNew(batchSize);\n+            return IntVector.TYPE_WIDTH;\n+          case FLOAT:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((Float4Vector) vec).allocateNew(batchSize);\n+            return Float4Vector.TYPE_WIDTH;\n+          case BOOLEAN:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((BitVector) vec).allocateNew(batchSize);\n+            return UNKNOWN_WIDTH;\n+          case INT64:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((BigIntVector) vec).allocateNew(batchSize);\n+            return BigIntVector.TYPE_WIDTH;\n+          case DOUBLE:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((Float8Vector) vec).allocateNew(batchSize);\n+            return Float8Vector.TYPE_WIDTH;\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void setRowGroupInfo(PageReadStore source, Map<ColumnPath, ColumnChunkMetaData> metadata) {\n+    ColumnChunkMetaData chunkMetaData = metadata.get(ColumnPath.get(columnDescriptor.getPath()));\n+    allPagesDictEncoded = !ParquetUtil.hasNonDictionaryPages(chunkMetaData);\n+    dictionary = vectorizedColumnIterator.setRowGroupInfo(source, allPagesDictEncoded);\n+  }\n+\n+  @Override\n+  public void reuseContainers(boolean reuse) {\n+    this.reuseContainers = reuse;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return columnDescriptor.toString();\n+  }\n+\n+  public static final VectorizedArrowReader NULL_VALUES_READER =\n+      new VectorizedArrowReader() {\n+        @Override\n+        public VectorHolder read(int numValsToRead) {\n+          return VectorHolder.NULL_VECTOR_HOLDER;\n+        }\n+\n+        @Override\n+        public void setRowGroupInfo(PageReadStore source, Map<ColumnPath, ColumnChunkMetaData> metadata) {\n+        }\n+      };\n+\n+  private static boolean isFixedLengthDecimal(ColumnDescriptor desc) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    return primitive.getOriginalType() != null &&\n+        primitive.getOriginalType() == OriginalType.DECIMAL &&\n+        (primitive.getPrimitiveTypeName() == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY ||\n+            primitive.getPrimitiveTypeName() == PrimitiveType.PrimitiveTypeName.BINARY);\n+  }\n+\n+  private static boolean isIntLongBackedDecimal(ColumnDescriptor desc) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    return primitive.getOriginalType() != null &&\n+        primitive.getOriginalType() == OriginalType.DECIMAL &&\n+        (primitive.getPrimitiveTypeName() == PrimitiveType.PrimitiveTypeName.INT64 ||\n+            primitive.getPrimitiveTypeName() == PrimitiveType.PrimitiveTypeName.INT32);\n+  }\n+\n+  private static boolean isVarWidthType(ColumnDescriptor desc) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    OriginalType originalType = primitive.getOriginalType();\n+    if (originalType != null &&\n+        originalType != OriginalType.DECIMAL &&\n+        (originalType == OriginalType.ENUM ||\n+            originalType == OriginalType.JSON ||\n+            originalType == OriginalType.UTF8 ||\n+            originalType == OriginalType.BSON)) {\n+      return true;\n+    }\n+    if (originalType == null && primitive.getPrimitiveTypeName() == PrimitiveType.PrimitiveTypeName.BINARY) {\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  private static boolean isBooleanType(ColumnDescriptor desc) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    OriginalType originalType = primitive.getOriginalType();\n+    return originalType == null && primitive.getPrimitiveTypeName() == PrimitiveType.PrimitiveTypeName.BOOLEAN;\n+  }\n+\n+  private static boolean isFixedWidthBinary(ColumnDescriptor desc) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    OriginalType originalType = primitive.getOriginalType();\n+    if (originalType == null &&\n+        primitive.getPrimitiveTypeName() == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  private static boolean isIntType(ColumnDescriptor desc) {\n+    return desc.getPrimitiveType().getPrimitiveTypeName() == PrimitiveType.PrimitiveTypeName.INT32;\n+  }\n+\n+  private static boolean isLongType(ColumnDescriptor desc) {\n+    return desc.getPrimitiveType().getPrimitiveTypeName() == PrimitiveType.PrimitiveTypeName.INT64;\n+  }\n+\n+  private static boolean isDoubleType(ColumnDescriptor desc) {\n+    return desc.getPrimitiveType().getPrimitiveTypeName() == PrimitiveType.PrimitiveTypeName.DOUBLE;\n+  }\n+\n+  private static boolean isFloatType(ColumnDescriptor desc) {\n+    return desc.getPrimitiveType().getPrimitiveTypeName() == PrimitiveType.PrimitiveTypeName.FLOAT;\n+  }\n+\n+  private static Map<ColumnPath, Boolean> buildColumnDictEncodedMap(BlockMetaData blockMetaData) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 377}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjM4MDI2", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344238026", "createdAt": "2020-01-16T21:46:15Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo0NjoxNVrOFeomVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo0NjoxNVrOFeomVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY2Njc3Mg==", "bodyText": "This will require updates to our binary LICENSE and NOTICE files.\nDo we know if this pulls in any transitive dependencies as well?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367666772", "createdAt": "2020-01-16T21:46:15Z", "author": {"login": "rdblue"}, "path": "build.gradle", "diffHunk": "@@ -337,6 +353,9 @@ project(':iceberg-spark-runtime') {\n     relocate 'org.codehaus.jackson', 'org.apache.iceberg.shaded.org.apache.parquet.shaded.org.codehaus.jackson'\n     relocate 'org.apache.orc', 'org.apache.iceberg.shaded.org.apache.orc'\n     relocate 'io.airlift', 'org.apache.iceberg.shaded.io.airlift'\n+    // relocate Arrow and related deps to shade Iceberg specific version\n+    relocate 'io.netty.buffer', 'org.apache.iceberg.shaded.io.netty.buffer'\n+    relocate 'org.apache.arrow', 'org.apache.iceberg.shaded.org.apache.arrow'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 36}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjM4Mzk4", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344238398", "createdAt": "2020-01-16T21:46:52Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo0Njo1MlrOFeonfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo0Njo1MlrOFeonfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY2NzA2OA==", "bodyText": "Why exclude Netty? Isn't this needed for tests?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367667068", "createdAt": "2020-01-16T21:46:52Z", "author": {"login": "rdblue"}, "path": "build.gradle", "diffHunk": "@@ -228,6 +228,21 @@ project(':iceberg-parquet') {\n   }\n }\n \n+project(':iceberg-arrow') {\n+  dependencies {\n+    compile project(':iceberg-api')\n+    compile project(':iceberg-parquet')\n+\n+    compile(\"org.apache.arrow:arrow-vector\") {\n+      exclude group: 'io.netty', module: 'netty-buffer'\n+      exclude group: 'io.netty', module: 'netty-common'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 11}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjM4OTIy", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344238922", "createdAt": "2020-01-16T21:47:45Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo0Nzo0NVrOFeopLA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo0Nzo0NVrOFeopLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY2NzUwMA==", "bodyText": "0.15.1 is out. Does that version work?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367667500", "createdAt": "2020-01-16T21:47:45Z", "author": {"login": "rdblue"}, "path": "versions.lock", "diffHunk": "@@ -95,9 +96,9 @@ org.antlr:antlr4-runtime:4.7 (1 constraints: 7a0e125f)\n org.antlr:stringtemplate:3.2.1 (1 constraints: c10a3bc6)\n org.apache.ant:ant:1.9.1 (3 constraints: a721ed14)\n org.apache.ant:ant-launcher:1.9.1 (1 constraints: 69082485)\n-org.apache.arrow:arrow-format:0.10.0 (1 constraints: 1f0de721)\n-org.apache.arrow:arrow-memory:0.10.0 (1 constraints: 1f0de721)\n-org.apache.arrow:arrow-vector:0.10.0 (1 constraints: e90c9734)\n+org.apache.arrow:arrow-format:0.14.1 (1 constraints: 240df421)\n+org.apache.arrow:arrow-memory:0.14.1 (1 constraints: 240df421)\n+org.apache.arrow:arrow-vector:0.14.1 (2 constraints: 2012a545)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 57}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjM5NzM2", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344239736", "createdAt": "2020-01-16T21:49:10Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo0OToxMFrOFeoraw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo0OToxMFrOFeoraw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY2ODA3NQ==", "bodyText": "Can we move the asserts to a separate line? And also use Assert.assertEquals to match our no-static-method-import style.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367668075", "createdAt": "2020-01-16T21:49:10Z", "author": {"login": "rdblue"}, "path": "arrow/src/test/java/org/apache/iceberg/arrow/ArrowSchemaUtilTest.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow;\n+\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.types.Types.BooleanType;\n+import org.apache.iceberg.types.Types.DateType;\n+import org.apache.iceberg.types.Types.DoubleType;\n+import org.apache.iceberg.types.Types.ListType;\n+import org.apache.iceberg.types.Types.LongType;\n+import org.apache.iceberg.types.Types.MapType;\n+import org.apache.iceberg.types.Types.StringType;\n+import org.apache.iceberg.types.Types.TimestampType;\n+import org.junit.Test;\n+\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.Bool;\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.Date;\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.FloatingPoint;\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.Int;\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.List;\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.Timestamp;\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+\n+\n+public class ArrowSchemaUtilTest {\n+\n+  @Test\n+  public void convertPrimitive() {\n+    Schema iceberg = new Schema(\n+        optional(0, \"i\", Types.IntegerType.get()),\n+        optional(1, \"b\", BooleanType.get()),\n+        required(2, \"d\", DoubleType.get()),\n+        required(3, \"s\", StringType.get()),\n+        optional(4, \"d2\", DateType.get()),\n+        optional(5, \"ts\", TimestampType.withoutZone())\n+    );\n+    org.apache.arrow.vector.types.pojo.Schema arrow = ArrowSchemaUtil.convert(iceberg);\n+    validate(iceberg, arrow);\n+  }\n+\n+  @Test\n+  public void convertComplex() {\n+    Schema iceberg = new Schema(\n+        optional(0, \"m\", MapType.ofOptional(\n+            1, 2, StringType.get(),\n+            LongType.get())\n+        ),\n+        required(3, \"m2\", MapType.ofOptional(\n+            4, 5, StringType.get(),\n+            ListType.ofOptional(6, TimestampType.withoutZone()))\n+        )\n+    );\n+    org.apache.arrow.vector.types.pojo.Schema arrow = ArrowSchemaUtil.convert(iceberg);\n+    assertEquals(iceberg.columns().size(), arrow.getFields().size());\n+  }\n+\n+  private void validate(Schema iceberg, org.apache.arrow.vector.types.pojo.Schema arrow) {\n+    assertEquals(iceberg.columns().size(), arrow.getFields().size());\n+\n+    for (Types.NestedField nf : iceberg.columns()) {\n+      Field field = arrow.findField(nf.name());\n+      assertNotNull(\"Missing filed: \" + nf, field);\n+\n+      validate(nf.type(), field.getType());\n+    }\n+  }\n+\n+  private void validate(Type iceberg, ArrowType arrow) {\n+    switch (iceberg.typeId()) {\n+      case BOOLEAN: assertEquals(Bool, arrow.getTypeID());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 94}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjM5ODk1", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344239895", "createdAt": "2020-01-16T21:49:30Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo0OTozMFrOFeor7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo0OTozMFrOFeor7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY2ODIwNw==", "bodyText": "Can we test withZone instead?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367668207", "createdAt": "2020-01-16T21:49:30Z", "author": {"login": "rdblue"}, "path": "arrow/src/test/java/org/apache/iceberg/arrow/ArrowSchemaUtilTest.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow;\n+\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.types.Types.BooleanType;\n+import org.apache.iceberg.types.Types.DateType;\n+import org.apache.iceberg.types.Types.DoubleType;\n+import org.apache.iceberg.types.Types.ListType;\n+import org.apache.iceberg.types.Types.LongType;\n+import org.apache.iceberg.types.Types.MapType;\n+import org.apache.iceberg.types.Types.StringType;\n+import org.apache.iceberg.types.Types.TimestampType;\n+import org.junit.Test;\n+\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.Bool;\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.Date;\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.FloatingPoint;\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.Int;\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.List;\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.Timestamp;\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+\n+\n+public class ArrowSchemaUtilTest {\n+\n+  @Test\n+  public void convertPrimitive() {\n+    Schema iceberg = new Schema(\n+        optional(0, \"i\", Types.IntegerType.get()),\n+        optional(1, \"b\", BooleanType.get()),\n+        required(2, \"d\", DoubleType.get()),\n+        required(3, \"s\", StringType.get()),\n+        optional(4, \"d2\", DateType.get()),\n+        optional(5, \"ts\", TimestampType.withoutZone())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjQyMjQz", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344242243", "createdAt": "2020-01-16T21:53:51Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo1Mzo1MVrOFeoy4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo1Mzo1MVrOFeoy4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY2OTk4NA==", "bodyText": "Only flat schemas are supported, right? Should this add some preconditions to validate that it isn't passed a column descriptor for a nested column?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367669984", "createdAt": "2020-01-16T21:53:51Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedColumnIterator.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import java.io.IOException;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.DataPage;\n+import org.apache.parquet.column.page.DictionaryPage;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.column.page.PageReader;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * Vectorized version of the ColumnIterator that reads column values in data pages of a column in a row group in a\n+ * batched fashion.\n+ */\n+public class VectorizedColumnIterator {\n+\n+  private final ColumnDescriptor desc;\n+  private final VectorizedPageIterator vectorizedPageIterator;\n+\n+  // state reset for each row group\n+  private PageReader columnPageReader = null;\n+  private long totalValuesCount = 0L;\n+  private long valuesRead = 0L;\n+  private long advanceNextPageCount = 0L;\n+  private final int batchSize;\n+\n+  public VectorizedColumnIterator(ColumnDescriptor desc, String writerVersion, int batchSize) {\n+    this.desc = desc;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjQyNDIx", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344242421", "createdAt": "2020-01-16T21:54:08Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo1NDowOVrOFeozbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo1NDowOVrOFeozbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY3MDEyNw==", "bodyText": "The other iterator uses store.getTotalValueCount, not the one in columnPageReader. Are those guaranteed to be the same?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367670127", "createdAt": "2020-01-16T21:54:09Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedColumnIterator.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import java.io.IOException;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.DataPage;\n+import org.apache.parquet.column.page.DictionaryPage;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.column.page.PageReader;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * Vectorized version of the ColumnIterator that reads column values in data pages of a column in a row group in a\n+ * batched fashion.\n+ */\n+public class VectorizedColumnIterator {\n+\n+  private final ColumnDescriptor desc;\n+  private final VectorizedPageIterator vectorizedPageIterator;\n+\n+  // state reset for each row group\n+  private PageReader columnPageReader = null;\n+  private long totalValuesCount = 0L;\n+  private long valuesRead = 0L;\n+  private long advanceNextPageCount = 0L;\n+  private final int batchSize;\n+\n+  public VectorizedColumnIterator(ColumnDescriptor desc, String writerVersion, int batchSize) {\n+    this.desc = desc;\n+    this.batchSize = batchSize;\n+    this.vectorizedPageIterator = new VectorizedPageIterator(desc, writerVersion, batchSize);\n+  }\n+\n+  public Dictionary setRowGroupInfo(PageReadStore store, boolean allPagesDictEncoded) {\n+    this.columnPageReader = store.getPageReader(desc);\n+    this.totalValuesCount = columnPageReader.getTotalValueCount();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 58}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MjQ1MDAy", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344245002", "createdAt": "2020-01-16T21:59:00Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo1OTowMVrOFeo7IA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xNlQyMTo1OTowMVrOFeo7IA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzY3MjA5Ng==", "bodyText": "This readDictionaryForColumn uses desc and calls getPageReader that is called just above to initialize columnPageReader. I think it would be better to use the same readDictionary static method that is used by the non-vectorized path. For one thing, it would eliminate the duplicate call to getPageReader because you'd pass columnPageReader in, and it would also get rid of the unknown use of desc because it is static. And we'd not need to maintain two copies of the method that do pretty much the same thing.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367672096", "createdAt": "2020-01-16T21:59:01Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedColumnIterator.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import java.io.IOException;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.DataPage;\n+import org.apache.parquet.column.page.DictionaryPage;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.column.page.PageReader;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * Vectorized version of the ColumnIterator that reads column values in data pages of a column in a row group in a\n+ * batched fashion.\n+ */\n+public class VectorizedColumnIterator {\n+\n+  private final ColumnDescriptor desc;\n+  private final VectorizedPageIterator vectorizedPageIterator;\n+\n+  // state reset for each row group\n+  private PageReader columnPageReader = null;\n+  private long totalValuesCount = 0L;\n+  private long valuesRead = 0L;\n+  private long advanceNextPageCount = 0L;\n+  private final int batchSize;\n+\n+  public VectorizedColumnIterator(ColumnDescriptor desc, String writerVersion, int batchSize) {\n+    this.desc = desc;\n+    this.batchSize = batchSize;\n+    this.vectorizedPageIterator = new VectorizedPageIterator(desc, writerVersion, batchSize);\n+  }\n+\n+  public Dictionary setRowGroupInfo(PageReadStore store, boolean allPagesDictEncoded) {\n+    this.columnPageReader = store.getPageReader(desc);\n+    this.totalValuesCount = columnPageReader.getTotalValueCount();\n+    this.valuesRead = 0L;\n+    this.advanceNextPageCount = 0L;\n+    this.vectorizedPageIterator.reset();\n+    Dictionary dict = readDictionaryForColumn(store);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 62}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzAwMTM1", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344300135", "createdAt": "2020-01-17T00:21:55Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDoyMTo1NVrOFermGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDoyMTo1NVrOFermGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcxNTg2NA==", "bodyText": "Why does this need to be a PageReadStore instead of a PageReader? This implementation is nearly identical to setPageSource in the non-vectorized read path. I think we could refactor into a superclass and reuse setPageSource and readDictionary if we were to refactor into a common superclass. Probably don't need to do that in this PR, but it would be nice to avoid unnecessary drift between the two read implementations.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367715864", "createdAt": "2020-01-17T00:21:55Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedColumnIterator.java", "diffHunk": "@@ -0,0 +1,258 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import java.io.IOException;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.DataPage;\n+import org.apache.parquet.column.page.DictionaryPage;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.column.page.PageReader;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * Vectorized version of the ColumnIterator that reads column values in data pages of a column in a row group in a\n+ * batched fashion.\n+ */\n+public class VectorizedColumnIterator {\n+\n+  private final ColumnDescriptor desc;\n+  private final VectorizedPageIterator vectorizedPageIterator;\n+\n+  // state reset for each row group\n+  private PageReader columnPageReader = null;\n+  private long totalValuesCount = 0L;\n+  private long valuesRead = 0L;\n+  private long advanceNextPageCount = 0L;\n+  private final int batchSize;\n+\n+  public VectorizedColumnIterator(ColumnDescriptor desc, String writerVersion, int batchSize) {\n+    this.desc = desc;\n+    this.batchSize = batchSize;\n+    this.vectorizedPageIterator = new VectorizedPageIterator(desc, writerVersion, batchSize);\n+  }\n+\n+  public Dictionary setRowGroupInfo(PageReadStore store, boolean allPagesDictEncoded) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzAyOTg2", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344302986", "createdAt": "2020-01-17T00:31:31Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDozMTozMVrOFeruuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDozMTozMVrOFeruuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcxODA3NQ==", "bodyText": "Looks like you don't need this method any more. It is only used in setPage and the nextBatch methods all use this.hasNext = triplesRead < triplesCount;", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367718075", "createdAt": "2020-01-17T00:31:31Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java", "diffHunk": "@@ -0,0 +1,565 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.IOException;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.CorruptDeltaByteArrays;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.Encoding;\n+import org.apache.parquet.column.ValuesType;\n+import org.apache.parquet.column.page.DataPage;\n+import org.apache.parquet.column.page.DataPageV1;\n+import org.apache.parquet.column.page.DataPageV2;\n+import org.apache.parquet.column.values.RequiresPreviousReader;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.rle.RunLengthBitPackingHybridDecoder;\n+import org.apache.parquet.io.ParquetDecodingException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class VectorizedPageIterator {\n+  private static final Logger LOG = LoggerFactory.getLogger(VectorizedPageIterator.class);\n+\n+  public VectorizedPageIterator(ColumnDescriptor desc, String writerVersion, int batchSize) {\n+    this.desc = desc;\n+    this.writerVersion = writerVersion;\n+  }\n+\n+  private final ColumnDescriptor desc;\n+  private final String writerVersion;\n+\n+  // iterator state\n+  private boolean hasNext = false;\n+  private int triplesRead = 0;\n+\n+  // page bookkeeping\n+  private Dictionary dictionary = null;\n+  private DataPage page = null;\n+  private int triplesCount = 0;\n+\n+  // Needed once we add support for complex types. Unused for now.\n+  private IntIterator repetitionLevels = null;\n+  private int currentRL = 0;\n+\n+  private VectorizedParquetValuesReader definitionLevelReader;\n+  private boolean eagerDecodeDictionary;\n+  private ValuesAsBytesReader plainValuesReader = null;\n+  private VectorizedParquetValuesReader dictionaryEncodedValuesReader = null;\n+  private boolean allPagesDictEncoded;\n+\n+  public void setPage(DataPage dataPage) {\n+    this.page = Preconditions.checkNotNull(dataPage, \"Cannot read from null page\");\n+    this.page.accept(new DataPage.Visitor<ValuesReader>() {\n+      @Override\n+      public ValuesReader visit(DataPageV1 dataPageV1) {\n+        initFromPage(dataPageV1);\n+        return null;\n+      }\n+\n+      @Override\n+      public ValuesReader visit(DataPageV2 dataPageV2) {\n+        initFromPage(dataPageV2);\n+        return null;\n+      }\n+    });\n+    this.triplesRead = 0;\n+    advance();\n+  }\n+\n+  // Dictionary is set per row group\n+  public void setDictionaryForColumn(Dictionary dict, boolean allDictEncoded) {\n+    this.dictionary = dict;\n+    this.allPagesDictEncoded = allDictEncoded;\n+  }\n+\n+  public void reset() {\n+    this.page = null;\n+    this.triplesCount = 0;\n+    this.triplesRead = 0;\n+    this.repetitionLevels = null;\n+    this.plainValuesReader = null;\n+    this.definitionLevelReader = null;\n+    this.hasNext = false;\n+  }\n+\n+  public int currentPageCount() {\n+    return triplesCount;\n+  }\n+\n+  public boolean hasNext() {\n+    return hasNext;\n+  }\n+\n+  /**\n+   * Method for reading a batch of dictionary ids from the dicitonary encoded data pages. Like definition levels,\n+   * dictionary ids in Parquet are RLE/bin-packed encoded as well.\n+   */\n+  public int nextBatchDictionaryIds(\n+      final IntVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    definitionLevelReader.readBatchOfDictionaryIds(\n+        vector,\n+        numValsInVector,\n+        actualBatchSize,\n+        holder,\n+        dictionaryEncodedValuesReader);\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of INT32 data type\n+   */\n+  public int nextBatchIntegers(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      definitionLevelReader.readBatchOfDictionaryEncodedIntegers(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      definitionLevelReader.readBatchOfIntegers(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of INT64 data type\n+   */\n+  public int nextBatchLongs(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      definitionLevelReader.readBatchOfDictionaryEncodedLongs(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      definitionLevelReader.readBatchOfLongs(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of FLOAT data type.\n+   */\n+  public int nextBatchFloats(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      definitionLevelReader.readBatchOfDictionaryEncodedFloats(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      definitionLevelReader.readBatchOfFloats(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of DOUBLE data type\n+   */\n+  public int nextBatchDoubles(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      definitionLevelReader.readBatchOfDictionaryEncodedDoubles(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      definitionLevelReader.readBatchOfDoubles(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  private int getActualBatchSize(int expectedBatchSize) {\n+    return Math.min(expectedBatchSize, triplesCount - triplesRead);\n+  }\n+\n+  /**\n+   * Method for reading a batch of decimals backed by INT32 and INT64 parquet data types. Since Arrow stores all\n+   * decimals in 16 bytes, byte arrays are appropriately padded before being written to Arrow data buffers.\n+   */\n+  public int nextBatchIntLongBackedDecimal(\n+      final FieldVector vector, final int expectedBatchSize, final int numValsInVector,\n+      final int typeWidth, NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      definitionLevelReader.readBatchOfDictionaryEncodedIntLongBackedDecimals(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      definitionLevelReader.readBatchOfIntLongBackedDecimals(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of decimals backed by fixed length byte array parquet data type. Arrow stores all\n+   * decimals in 16 bytes. This method provides the necessary padding to the decimals read. Moreover, Arrow interprets\n+   * the decimals in Arrow buffer as little endian. Parquet stores fixed length decimals as big endian. So, this method\n+   * uses {@link DecimalVector#setBigEndian(int, byte[])} method so that the data in Arrow vector is indeed little\n+   * endian.\n+   */\n+  public int nextBatchFixedLengthDecimal(\n+      final FieldVector vector, final int expectedBatchSize, final int numValsInVector,\n+      final int typeWidth, NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      definitionLevelReader.readBatchOfDictionaryEncodedFixedLengthDecimals(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      definitionLevelReader.readBatchOfFixedLengthDecimals(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of variable width data type (ENUM, JSON, UTF8, BSON).\n+   */\n+  public int nextBatchVarWidthType(\n+      final FieldVector vector,\n+      final int expectedBatchSize,\n+      final int numValsInVector,\n+      NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      definitionLevelReader.readBatchOfDictionaryEncodedVarWidth(\n+          vector,\n+          numValsInVector,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      definitionLevelReader.readBatchVarWidth(\n+          vector,\n+          numValsInVector,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading batches of fixed width binary type (e.g. BYTE[7]). Spark does not support fixed width binary\n+   * data type. To work around this limitation, the data is read as fixed width binary from parquet and stored in a\n+   * {@link VarBinaryVector} in Arrow.\n+   */\n+  public int nextBatchFixedWidthBinary(\n+      final FieldVector vector, final int expectedBatchSize, final int numValsInVector,\n+      final int typeWidth, NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      definitionLevelReader.readBatchOfDictionaryEncodedFixedWidthBinary(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      definitionLevelReader.readBatchOfFixedWidthBinary(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading batches of booleans.\n+   */\n+  public int nextBatchBoolean(\n+      final FieldVector vector,\n+      final int expectedBatchSize,\n+      final int numValsInVector,\n+      NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    definitionLevelReader.readBatchOfBooleans(vector, numValsInVector, actualBatchSize,\n+        nullabilityHolder, plainValuesReader);\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  private void advance() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 442}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzAzMjM0", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344303234", "createdAt": "2020-01-17T00:32:25Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDozMjoyNVrOFervjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDozMjoyNVrOFervjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcxODI4Ng==", "bodyText": "We should also try to make a superclass for this iterator and the row-based PageIterator because there is a fair amount of duplication here as well.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367718286", "createdAt": "2020-01-17T00:32:25Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java", "diffHunk": "@@ -0,0 +1,565 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.IOException;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.CorruptDeltaByteArrays;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.Encoding;\n+import org.apache.parquet.column.ValuesType;\n+import org.apache.parquet.column.page.DataPage;\n+import org.apache.parquet.column.page.DataPageV1;\n+import org.apache.parquet.column.page.DataPageV2;\n+import org.apache.parquet.column.values.RequiresPreviousReader;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.rle.RunLengthBitPackingHybridDecoder;\n+import org.apache.parquet.io.ParquetDecodingException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class VectorizedPageIterator {\n+  private static final Logger LOG = LoggerFactory.getLogger(VectorizedPageIterator.class);\n+\n+  public VectorizedPageIterator(ColumnDescriptor desc, String writerVersion, int batchSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 51}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzA0NTE3", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344304517", "createdAt": "2020-01-17T00:36:48Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDozNjo0OFrOFerzrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDozNjo0OFrOFerzrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcxOTM0Mg==", "bodyText": "I don't think dlReader is needed any more?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367719342", "createdAt": "2020-01-17T00:36:48Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java", "diffHunk": "@@ -0,0 +1,565 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.IOException;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.CorruptDeltaByteArrays;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.Encoding;\n+import org.apache.parquet.column.ValuesType;\n+import org.apache.parquet.column.page.DataPage;\n+import org.apache.parquet.column.page.DataPageV1;\n+import org.apache.parquet.column.page.DataPageV2;\n+import org.apache.parquet.column.values.RequiresPreviousReader;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.rle.RunLengthBitPackingHybridDecoder;\n+import org.apache.parquet.io.ParquetDecodingException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class VectorizedPageIterator {\n+  private static final Logger LOG = LoggerFactory.getLogger(VectorizedPageIterator.class);\n+\n+  public VectorizedPageIterator(ColumnDescriptor desc, String writerVersion, int batchSize) {\n+    this.desc = desc;\n+    this.writerVersion = writerVersion;\n+  }\n+\n+  private final ColumnDescriptor desc;\n+  private final String writerVersion;\n+\n+  // iterator state\n+  private boolean hasNext = false;\n+  private int triplesRead = 0;\n+\n+  // page bookkeeping\n+  private Dictionary dictionary = null;\n+  private DataPage page = null;\n+  private int triplesCount = 0;\n+\n+  // Needed once we add support for complex types. Unused for now.\n+  private IntIterator repetitionLevels = null;\n+  private int currentRL = 0;\n+\n+  private VectorizedParquetValuesReader definitionLevelReader;\n+  private boolean eagerDecodeDictionary;\n+  private ValuesAsBytesReader plainValuesReader = null;\n+  private VectorizedParquetValuesReader dictionaryEncodedValuesReader = null;\n+  private boolean allPagesDictEncoded;\n+\n+  public void setPage(DataPage dataPage) {\n+    this.page = Preconditions.checkNotNull(dataPage, \"Cannot read from null page\");\n+    this.page.accept(new DataPage.Visitor<ValuesReader>() {\n+      @Override\n+      public ValuesReader visit(DataPageV1 dataPageV1) {\n+        initFromPage(dataPageV1);\n+        return null;\n+      }\n+\n+      @Override\n+      public ValuesReader visit(DataPageV2 dataPageV2) {\n+        initFromPage(dataPageV2);\n+        return null;\n+      }\n+    });\n+    this.triplesRead = 0;\n+    advance();\n+  }\n+\n+  // Dictionary is set per row group\n+  public void setDictionaryForColumn(Dictionary dict, boolean allDictEncoded) {\n+    this.dictionary = dict;\n+    this.allPagesDictEncoded = allDictEncoded;\n+  }\n+\n+  public void reset() {\n+    this.page = null;\n+    this.triplesCount = 0;\n+    this.triplesRead = 0;\n+    this.repetitionLevels = null;\n+    this.plainValuesReader = null;\n+    this.definitionLevelReader = null;\n+    this.hasNext = false;\n+  }\n+\n+  public int currentPageCount() {\n+    return triplesCount;\n+  }\n+\n+  public boolean hasNext() {\n+    return hasNext;\n+  }\n+\n+  /**\n+   * Method for reading a batch of dictionary ids from the dicitonary encoded data pages. Like definition levels,\n+   * dictionary ids in Parquet are RLE/bin-packed encoded as well.\n+   */\n+  public int nextBatchDictionaryIds(\n+      final IntVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    definitionLevelReader.readBatchOfDictionaryIds(\n+        vector,\n+        numValsInVector,\n+        actualBatchSize,\n+        holder,\n+        dictionaryEncodedValuesReader);\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of INT32 data type\n+   */\n+  public int nextBatchIntegers(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      definitionLevelReader.readBatchOfDictionaryEncodedIntegers(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      definitionLevelReader.readBatchOfIntegers(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of INT64 data type\n+   */\n+  public int nextBatchLongs(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      definitionLevelReader.readBatchOfDictionaryEncodedLongs(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      definitionLevelReader.readBatchOfLongs(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of FLOAT data type.\n+   */\n+  public int nextBatchFloats(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      definitionLevelReader.readBatchOfDictionaryEncodedFloats(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      definitionLevelReader.readBatchOfFloats(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of DOUBLE data type\n+   */\n+  public int nextBatchDoubles(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      definitionLevelReader.readBatchOfDictionaryEncodedDoubles(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      definitionLevelReader.readBatchOfDoubles(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  private int getActualBatchSize(int expectedBatchSize) {\n+    return Math.min(expectedBatchSize, triplesCount - triplesRead);\n+  }\n+\n+  /**\n+   * Method for reading a batch of decimals backed by INT32 and INT64 parquet data types. Since Arrow stores all\n+   * decimals in 16 bytes, byte arrays are appropriately padded before being written to Arrow data buffers.\n+   */\n+  public int nextBatchIntLongBackedDecimal(\n+      final FieldVector vector, final int expectedBatchSize, final int numValsInVector,\n+      final int typeWidth, NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      definitionLevelReader.readBatchOfDictionaryEncodedIntLongBackedDecimals(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      definitionLevelReader.readBatchOfIntLongBackedDecimals(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of decimals backed by fixed length byte array parquet data type. Arrow stores all\n+   * decimals in 16 bytes. This method provides the necessary padding to the decimals read. Moreover, Arrow interprets\n+   * the decimals in Arrow buffer as little endian. Parquet stores fixed length decimals as big endian. So, this method\n+   * uses {@link DecimalVector#setBigEndian(int, byte[])} method so that the data in Arrow vector is indeed little\n+   * endian.\n+   */\n+  public int nextBatchFixedLengthDecimal(\n+      final FieldVector vector, final int expectedBatchSize, final int numValsInVector,\n+      final int typeWidth, NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      definitionLevelReader.readBatchOfDictionaryEncodedFixedLengthDecimals(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      definitionLevelReader.readBatchOfFixedLengthDecimals(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of variable width data type (ENUM, JSON, UTF8, BSON).\n+   */\n+  public int nextBatchVarWidthType(\n+      final FieldVector vector,\n+      final int expectedBatchSize,\n+      final int numValsInVector,\n+      NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      definitionLevelReader.readBatchOfDictionaryEncodedVarWidth(\n+          vector,\n+          numValsInVector,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      definitionLevelReader.readBatchVarWidth(\n+          vector,\n+          numValsInVector,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading batches of fixed width binary type (e.g. BYTE[7]). Spark does not support fixed width binary\n+   * data type. To work around this limitation, the data is read as fixed width binary from parquet and stored in a\n+   * {@link VarBinaryVector} in Arrow.\n+   */\n+  public int nextBatchFixedWidthBinary(\n+      final FieldVector vector, final int expectedBatchSize, final int numValsInVector,\n+      final int typeWidth, NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      definitionLevelReader.readBatchOfDictionaryEncodedFixedWidthBinary(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      definitionLevelReader.readBatchOfFixedWidthBinary(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading batches of booleans.\n+   */\n+  public int nextBatchBoolean(\n+      final FieldVector vector,\n+      final int expectedBatchSize,\n+      final int numValsInVector,\n+      NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    definitionLevelReader.readBatchOfBooleans(vector, numValsInVector, actualBatchSize,\n+        nullabilityHolder, plainValuesReader);\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  private void advance() {\n+    if (triplesRead < triplesCount) {\n+      this.hasNext = true;\n+    } else {\n+      this.hasNext = false;\n+    }\n+  }\n+\n+  private void initDataReader(Encoding dataEncoding, ByteBufferInputStream in, int valueCount) {\n+    ValuesReader previousReader = plainValuesReader;\n+    this.eagerDecodeDictionary = dataEncoding.usesDictionary() && dictionary != null && !allPagesDictEncoded;\n+    if (dataEncoding.usesDictionary()) {\n+      if (dictionary == null) {\n+        throw new ParquetDecodingException(\n+            \"could not read page in col \" + desc + \" as the dictionary was missing for encoding \" + dataEncoding);\n+      }\n+      try {\n+        dictionaryEncodedValuesReader =\n+            new VectorizedParquetValuesReader(desc.getMaxDefinitionLevel());\n+        dictionaryEncodedValuesReader.initFromPage(valueCount, in);\n+      } catch (IOException e) {\n+        throw new ParquetDecodingException(\"could not read page in col \" + desc, e);\n+      }\n+    } else {\n+      plainValuesReader = new ValuesAsBytesReader();\n+      plainValuesReader.initFromPage(valueCount, in);\n+    }\n+    if (CorruptDeltaByteArrays.requiresSequentialReads(writerVersion, dataEncoding) &&\n+        previousReader != null && previousReader instanceof RequiresPreviousReader) {\n+      // previous reader can only be set if reading sequentially\n+      ((RequiresPreviousReader) plainValuesReader).setPreviousReader(previousReader);\n+    }\n+  }\n+\n+  private void initFromPage(DataPageV1 dataPageV1) {\n+    this.triplesCount = dataPageV1.getValueCount();\n+    ValuesReader rlReader = dataPageV1.getRlEncoding().getValuesReader(desc, ValuesType.REPETITION_LEVEL);\n+    ValuesReader dlReader;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 479}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzA2MjQw", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344306240", "createdAt": "2020-01-17T00:42:48Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo0Mjo0OFrOFer5Cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo0Mjo0OFrOFer5Cg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyMDcxNA==", "bodyText": "Style: Can we update these names to match the others? Also, I don't think we need to have each argument on a separate line.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367720714", "createdAt": "2020-01-17T00:42:48Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {\n+\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  private enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  private MODE mode;\n+  private int currentCount;\n+  private int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  private int[] packedValuesBuffer = new int[16];\n+  private int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  private final int maxDefLevel;\n+\n+  public VectorizedParquetValuesReader(int maxDefLevel) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    init(bitWidth);\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bw,\n+      boolean rl,\n+      int mdl) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 103}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzA3MzY5", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344307369", "createdAt": "2020-01-17T00:46:38Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo0NjozOFrOFer8ww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo0NjozOFrOFer8ww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyMTY2Nw==", "bodyText": "Can we prefix with this. to show that this is setting an instance field?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367721667", "createdAt": "2020-01-17T00:46:38Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {\n+\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  private enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  private MODE mode;\n+  private int currentCount;\n+  private int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  private int[] packedValuesBuffer = new int[16];\n+  private int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  private final int maxDefLevel;\n+\n+  public VectorizedParquetValuesReader(int maxDefLevel) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    init(bitWidth);\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bw,\n+      boolean rl,\n+      int mdl) {\n+    this.fixedWidth = true;\n+    this.readLength = rl;\n+    this.maxDefLevel = mdl;\n+    init(bw);\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {\n+    this.inputStream = in;\n+    if (fixedWidth) {\n+      // initialize for repetition and definition levels\n+      if (readLength) {\n+        int length = readIntLittleEndian();\n+        this.inputStream = in.sliceStream(length);\n+      }\n+    } else {\n+      // initialize for values\n+      if (in.available() > 0) {\n+        init(in.read());\n+      }\n+    }\n+    if (bitWidth == 0) {\n+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.\n+      this.mode = MODE.RLE;\n+      this.currentCount = valueCount;\n+      this.currentValue = 0;\n+    } else {\n+      this.currentCount = 0;\n+    }\n+  }\n+\n+  /**\n+   * Initializes the internal state for decoding ints of `bitWidth`.\n+   */\n+  private void init(int bw) {\n+    Preconditions.checkArgument(bw >= 0 && bw <= 32, \"bitWidth must be >= 0 and <= 32\");\n+    this.bitWidth = bw;\n+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);\n+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);\n+  }\n+\n+  /**\n+   * Reads the next varint encoded int.\n+   */\n+  private int readUnsignedVarInt() throws IOException {\n+    int value = 0;\n+    int shift = 0;\n+    int byteRead;\n+    do {\n+      byteRead = inputStream.read();\n+      value |= (byteRead & 0x7F) << shift;\n+      shift += 7;\n+    } while ((byteRead & 0x80) != 0);\n+    return value;\n+  }\n+\n+  /**\n+   * Reads the next 4 byte little endian int.\n+   */\n+  private int readIntLittleEndian() throws IOException {\n+    int ch4 = inputStream.read();\n+    int ch3 = inputStream.read();\n+    int ch2 = inputStream.read();\n+    int ch1 = inputStream.read();\n+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);\n+  }\n+\n+  /**\n+   * Reads the next byteWidth little endian int.\n+   */\n+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n+    switch (bytesWidth) {\n+      case 0:\n+        return 0;\n+      case 1:\n+        return inputStream.read();\n+      case 2: {\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 8) + ch2;\n+      }\n+      case 3: {\n+        int ch3 = inputStream.read();\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n+      }\n+      case 4: {\n+        return readIntLittleEndian();\n+      }\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  /**\n+   * Reads the next group.\n+   */\n+  private void readNextGroup() {\n+    try {\n+      int header = readUnsignedVarInt();\n+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;\n+      switch (mode) {\n+        case RLE:\n+          this.currentCount = header >>> 1;\n+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();\n+          return;\n+        case PACKED:\n+          int numGroups = header >>> 1;\n+          this.currentCount = numGroups * 8;\n+\n+          if (this.packedValuesBuffer.length < this.currentCount) {\n+            this.packedValuesBuffer = new int[this.currentCount];\n+          }\n+          packedValuesBufferIdx = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 217}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzA3NzI4", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344307728", "createdAt": "2020-01-17T00:47:57Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo0Nzo1N1rOFer-DA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo0Nzo1N1rOFer-DA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyMTk5Ng==", "bodyText": "Is this correct for all types? Seems like this might be incorrect for variable-length or 8-byte types.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367721996", "createdAt": "2020-01-17T00:47:57Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {\n+\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  private enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  private MODE mode;\n+  private int currentCount;\n+  private int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  private int[] packedValuesBuffer = new int[16];\n+  private int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  private final int maxDefLevel;\n+\n+  public VectorizedParquetValuesReader(int maxDefLevel) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    init(bitWidth);\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bw,\n+      boolean rl,\n+      int mdl) {\n+    this.fixedWidth = true;\n+    this.readLength = rl;\n+    this.maxDefLevel = mdl;\n+    init(bw);\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {\n+    this.inputStream = in;\n+    if (fixedWidth) {\n+      // initialize for repetition and definition levels\n+      if (readLength) {\n+        int length = readIntLittleEndian();\n+        this.inputStream = in.sliceStream(length);\n+      }\n+    } else {\n+      // initialize for values\n+      if (in.available() > 0) {\n+        init(in.read());\n+      }\n+    }\n+    if (bitWidth == 0) {\n+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.\n+      this.mode = MODE.RLE;\n+      this.currentCount = valueCount;\n+      this.currentValue = 0;\n+    } else {\n+      this.currentCount = 0;\n+    }\n+  }\n+\n+  /**\n+   * Initializes the internal state for decoding ints of `bitWidth`.\n+   */\n+  private void init(int bw) {\n+    Preconditions.checkArgument(bw >= 0 && bw <= 32, \"bitWidth must be >= 0 and <= 32\");\n+    this.bitWidth = bw;\n+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);\n+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);\n+  }\n+\n+  /**\n+   * Reads the next varint encoded int.\n+   */\n+  private int readUnsignedVarInt() throws IOException {\n+    int value = 0;\n+    int shift = 0;\n+    int byteRead;\n+    do {\n+      byteRead = inputStream.read();\n+      value |= (byteRead & 0x7F) << shift;\n+      shift += 7;\n+    } while ((byteRead & 0x80) != 0);\n+    return value;\n+  }\n+\n+  /**\n+   * Reads the next 4 byte little endian int.\n+   */\n+  private int readIntLittleEndian() throws IOException {\n+    int ch4 = inputStream.read();\n+    int ch3 = inputStream.read();\n+    int ch2 = inputStream.read();\n+    int ch1 = inputStream.read();\n+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);\n+  }\n+\n+  /**\n+   * Reads the next byteWidth little endian int.\n+   */\n+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n+    switch (bytesWidth) {\n+      case 0:\n+        return 0;\n+      case 1:\n+        return inputStream.read();\n+      case 2: {\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 8) + ch2;\n+      }\n+      case 3: {\n+        int ch3 = inputStream.read();\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n+      }\n+      case 4: {\n+        return readIntLittleEndian();\n+      }\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  /**\n+   * Reads the next group.\n+   */\n+  private void readNextGroup() {\n+    try {\n+      int header = readUnsignedVarInt();\n+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;\n+      switch (mode) {\n+        case RLE:\n+          this.currentCount = header >>> 1;\n+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();\n+          return;\n+        case PACKED:\n+          int numGroups = header >>> 1;\n+          this.currentCount = numGroups * 8;\n+\n+          if (this.packedValuesBuffer.length < this.currentCount) {\n+            this.packedValuesBuffer = new int[this.currentCount];\n+          }\n+          packedValuesBufferIdx = 0;\n+          int valueIndex = 0;\n+          while (valueIndex < this.currentCount) {\n+            // values are bit packed 8 at a time, so reading bitWidth will always work\n+            ByteBuffer buffer = inputStream.slice(bitWidth);\n+            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);\n+            valueIndex += 8;\n+          }\n+          return;\n+        default:\n+          throw new ParquetDecodingException(\"not a valid mode \" + this.mode);\n+      }\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"Failed to read from input stream\", e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean readBoolean() {\n+    return this.readInteger() != 0;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    this.readInteger();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 241}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzA4NzQ4", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344308748", "createdAt": "2020-01-17T00:51:37Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo1MTozOFrOFesBiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo1MTozOFrOFesBiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyMjg4OA==", "bodyText": "Giving a bit more context about what violated assumptions here is usually good. I'd probably update it to this:\nthrow new RuntimeException(\"Unsupported hybrid decode mode: \" + mode);", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367722888", "createdAt": "2020-01-17T00:51:38Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {\n+\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  private enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  private MODE mode;\n+  private int currentCount;\n+  private int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  private int[] packedValuesBuffer = new int[16];\n+  private int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  private final int maxDefLevel;\n+\n+  public VectorizedParquetValuesReader(int maxDefLevel) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    init(bitWidth);\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bw,\n+      boolean rl,\n+      int mdl) {\n+    this.fixedWidth = true;\n+    this.readLength = rl;\n+    this.maxDefLevel = mdl;\n+    init(bw);\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {\n+    this.inputStream = in;\n+    if (fixedWidth) {\n+      // initialize for repetition and definition levels\n+      if (readLength) {\n+        int length = readIntLittleEndian();\n+        this.inputStream = in.sliceStream(length);\n+      }\n+    } else {\n+      // initialize for values\n+      if (in.available() > 0) {\n+        init(in.read());\n+      }\n+    }\n+    if (bitWidth == 0) {\n+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.\n+      this.mode = MODE.RLE;\n+      this.currentCount = valueCount;\n+      this.currentValue = 0;\n+    } else {\n+      this.currentCount = 0;\n+    }\n+  }\n+\n+  /**\n+   * Initializes the internal state for decoding ints of `bitWidth`.\n+   */\n+  private void init(int bw) {\n+    Preconditions.checkArgument(bw >= 0 && bw <= 32, \"bitWidth must be >= 0 and <= 32\");\n+    this.bitWidth = bw;\n+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);\n+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);\n+  }\n+\n+  /**\n+   * Reads the next varint encoded int.\n+   */\n+  private int readUnsignedVarInt() throws IOException {\n+    int value = 0;\n+    int shift = 0;\n+    int byteRead;\n+    do {\n+      byteRead = inputStream.read();\n+      value |= (byteRead & 0x7F) << shift;\n+      shift += 7;\n+    } while ((byteRead & 0x80) != 0);\n+    return value;\n+  }\n+\n+  /**\n+   * Reads the next 4 byte little endian int.\n+   */\n+  private int readIntLittleEndian() throws IOException {\n+    int ch4 = inputStream.read();\n+    int ch3 = inputStream.read();\n+    int ch2 = inputStream.read();\n+    int ch1 = inputStream.read();\n+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);\n+  }\n+\n+  /**\n+   * Reads the next byteWidth little endian int.\n+   */\n+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n+    switch (bytesWidth) {\n+      case 0:\n+        return 0;\n+      case 1:\n+        return inputStream.read();\n+      case 2: {\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 8) + ch2;\n+      }\n+      case 3: {\n+        int ch3 = inputStream.read();\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n+      }\n+      case 4: {\n+        return readIntLittleEndian();\n+      }\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  /**\n+   * Reads the next group.\n+   */\n+  private void readNextGroup() {\n+    try {\n+      int header = readUnsignedVarInt();\n+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;\n+      switch (mode) {\n+        case RLE:\n+          this.currentCount = header >>> 1;\n+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();\n+          return;\n+        case PACKED:\n+          int numGroups = header >>> 1;\n+          this.currentCount = numGroups * 8;\n+\n+          if (this.packedValuesBuffer.length < this.currentCount) {\n+            this.packedValuesBuffer = new int[this.currentCount];\n+          }\n+          packedValuesBufferIdx = 0;\n+          int valueIndex = 0;\n+          while (valueIndex < this.currentCount) {\n+            // values are bit packed 8 at a time, so reading bitWidth will always work\n+            ByteBuffer buffer = inputStream.slice(bitWidth);\n+            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);\n+            valueIndex += 8;\n+          }\n+          return;\n+        default:\n+          throw new ParquetDecodingException(\"not a valid mode \" + this.mode);\n+      }\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"Failed to read from input stream\", e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean readBoolean() {\n+    return this.readInteger() != 0;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    this.readInteger();\n+  }\n+\n+  @Override\n+  public int readValueDictionaryId() {\n+    return readInteger();\n+  }\n+\n+  @Override\n+  public int readInteger() {\n+    if (this.currentCount == 0) {\n+      this.readNextGroup();\n+    }\n+\n+    this.currentCount--;\n+    switch (mode) {\n+      case RLE:\n+        return this.currentValue;\n+      case PACKED:\n+        return this.packedValuesBuffer[packedValuesBufferIdx++];\n+    }\n+    throw new RuntimeException(\"Unreachable\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 262}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzA5MzU1", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344309355", "createdAt": "2020-01-17T00:53:52Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo1Mzo1MlrOFesDfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo1Mzo1MlrOFesDfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyMzM4OQ==", "bodyText": "Should this be called offset since it is the offset to copy into the output vector?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367723389", "createdAt": "2020-01-17T00:53:52Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {\n+\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  private enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  private MODE mode;\n+  private int currentCount;\n+  private int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  private int[] packedValuesBuffer = new int[16];\n+  private int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  private final int maxDefLevel;\n+\n+  public VectorizedParquetValuesReader(int maxDefLevel) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    init(bitWidth);\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bw,\n+      boolean rl,\n+      int mdl) {\n+    this.fixedWidth = true;\n+    this.readLength = rl;\n+    this.maxDefLevel = mdl;\n+    init(bw);\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {\n+    this.inputStream = in;\n+    if (fixedWidth) {\n+      // initialize for repetition and definition levels\n+      if (readLength) {\n+        int length = readIntLittleEndian();\n+        this.inputStream = in.sliceStream(length);\n+      }\n+    } else {\n+      // initialize for values\n+      if (in.available() > 0) {\n+        init(in.read());\n+      }\n+    }\n+    if (bitWidth == 0) {\n+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.\n+      this.mode = MODE.RLE;\n+      this.currentCount = valueCount;\n+      this.currentValue = 0;\n+    } else {\n+      this.currentCount = 0;\n+    }\n+  }\n+\n+  /**\n+   * Initializes the internal state for decoding ints of `bitWidth`.\n+   */\n+  private void init(int bw) {\n+    Preconditions.checkArgument(bw >= 0 && bw <= 32, \"bitWidth must be >= 0 and <= 32\");\n+    this.bitWidth = bw;\n+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);\n+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);\n+  }\n+\n+  /**\n+   * Reads the next varint encoded int.\n+   */\n+  private int readUnsignedVarInt() throws IOException {\n+    int value = 0;\n+    int shift = 0;\n+    int byteRead;\n+    do {\n+      byteRead = inputStream.read();\n+      value |= (byteRead & 0x7F) << shift;\n+      shift += 7;\n+    } while ((byteRead & 0x80) != 0);\n+    return value;\n+  }\n+\n+  /**\n+   * Reads the next 4 byte little endian int.\n+   */\n+  private int readIntLittleEndian() throws IOException {\n+    int ch4 = inputStream.read();\n+    int ch3 = inputStream.read();\n+    int ch2 = inputStream.read();\n+    int ch1 = inputStream.read();\n+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);\n+  }\n+\n+  /**\n+   * Reads the next byteWidth little endian int.\n+   */\n+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n+    switch (bytesWidth) {\n+      case 0:\n+        return 0;\n+      case 1:\n+        return inputStream.read();\n+      case 2: {\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 8) + ch2;\n+      }\n+      case 3: {\n+        int ch3 = inputStream.read();\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n+      }\n+      case 4: {\n+        return readIntLittleEndian();\n+      }\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  /**\n+   * Reads the next group.\n+   */\n+  private void readNextGroup() {\n+    try {\n+      int header = readUnsignedVarInt();\n+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;\n+      switch (mode) {\n+        case RLE:\n+          this.currentCount = header >>> 1;\n+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();\n+          return;\n+        case PACKED:\n+          int numGroups = header >>> 1;\n+          this.currentCount = numGroups * 8;\n+\n+          if (this.packedValuesBuffer.length < this.currentCount) {\n+            this.packedValuesBuffer = new int[this.currentCount];\n+          }\n+          packedValuesBufferIdx = 0;\n+          int valueIndex = 0;\n+          while (valueIndex < this.currentCount) {\n+            // values are bit packed 8 at a time, so reading bitWidth will always work\n+            ByteBuffer buffer = inputStream.slice(bitWidth);\n+            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);\n+            valueIndex += 8;\n+          }\n+          return;\n+        default:\n+          throw new ParquetDecodingException(\"not a valid mode \" + this.mode);\n+      }\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"Failed to read from input stream\", e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean readBoolean() {\n+    return this.readInteger() != 0;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    this.readInteger();\n+  }\n+\n+  @Override\n+  public int readValueDictionaryId() {\n+    return readInteger();\n+  }\n+\n+  @Override\n+  public int readInteger() {\n+    if (this.currentCount == 0) {\n+      this.readNextGroup();\n+    }\n+\n+    this.currentCount--;\n+    switch (mode) {\n+      case RLE:\n+        return this.currentValue;\n+      case PACKED:\n+        return this.packedValuesBuffer[packedValuesBufferIdx++];\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  public void readBatchOfDictionaryIds(\n+      final IntVector vector,\n+      final int numValsInVector,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 267}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzEwMzc2", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344310376", "createdAt": "2020-01-17T00:57:35Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo1NzozNVrOFesHAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMDo1NzozNVrOFesHAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyNDI4OA==", "bodyText": "I think this name is slightly misleading. I expected it to be the vector capacity, but it looks like this is the number of values to read. Can we rename to clarify?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367724288", "createdAt": "2020-01-17T00:57:35Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {\n+\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  private enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  private MODE mode;\n+  private int currentCount;\n+  private int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  private int[] packedValuesBuffer = new int[16];\n+  private int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  private final int maxDefLevel;\n+\n+  public VectorizedParquetValuesReader(int maxDefLevel) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    init(bitWidth);\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bw,\n+      boolean rl,\n+      int mdl) {\n+    this.fixedWidth = true;\n+    this.readLength = rl;\n+    this.maxDefLevel = mdl;\n+    init(bw);\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {\n+    this.inputStream = in;\n+    if (fixedWidth) {\n+      // initialize for repetition and definition levels\n+      if (readLength) {\n+        int length = readIntLittleEndian();\n+        this.inputStream = in.sliceStream(length);\n+      }\n+    } else {\n+      // initialize for values\n+      if (in.available() > 0) {\n+        init(in.read());\n+      }\n+    }\n+    if (bitWidth == 0) {\n+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.\n+      this.mode = MODE.RLE;\n+      this.currentCount = valueCount;\n+      this.currentValue = 0;\n+    } else {\n+      this.currentCount = 0;\n+    }\n+  }\n+\n+  /**\n+   * Initializes the internal state for decoding ints of `bitWidth`.\n+   */\n+  private void init(int bw) {\n+    Preconditions.checkArgument(bw >= 0 && bw <= 32, \"bitWidth must be >= 0 and <= 32\");\n+    this.bitWidth = bw;\n+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);\n+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);\n+  }\n+\n+  /**\n+   * Reads the next varint encoded int.\n+   */\n+  private int readUnsignedVarInt() throws IOException {\n+    int value = 0;\n+    int shift = 0;\n+    int byteRead;\n+    do {\n+      byteRead = inputStream.read();\n+      value |= (byteRead & 0x7F) << shift;\n+      shift += 7;\n+    } while ((byteRead & 0x80) != 0);\n+    return value;\n+  }\n+\n+  /**\n+   * Reads the next 4 byte little endian int.\n+   */\n+  private int readIntLittleEndian() throws IOException {\n+    int ch4 = inputStream.read();\n+    int ch3 = inputStream.read();\n+    int ch2 = inputStream.read();\n+    int ch1 = inputStream.read();\n+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);\n+  }\n+\n+  /**\n+   * Reads the next byteWidth little endian int.\n+   */\n+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n+    switch (bytesWidth) {\n+      case 0:\n+        return 0;\n+      case 1:\n+        return inputStream.read();\n+      case 2: {\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 8) + ch2;\n+      }\n+      case 3: {\n+        int ch3 = inputStream.read();\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n+      }\n+      case 4: {\n+        return readIntLittleEndian();\n+      }\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  /**\n+   * Reads the next group.\n+   */\n+  private void readNextGroup() {\n+    try {\n+      int header = readUnsignedVarInt();\n+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;\n+      switch (mode) {\n+        case RLE:\n+          this.currentCount = header >>> 1;\n+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();\n+          return;\n+        case PACKED:\n+          int numGroups = header >>> 1;\n+          this.currentCount = numGroups * 8;\n+\n+          if (this.packedValuesBuffer.length < this.currentCount) {\n+            this.packedValuesBuffer = new int[this.currentCount];\n+          }\n+          packedValuesBufferIdx = 0;\n+          int valueIndex = 0;\n+          while (valueIndex < this.currentCount) {\n+            // values are bit packed 8 at a time, so reading bitWidth will always work\n+            ByteBuffer buffer = inputStream.slice(bitWidth);\n+            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);\n+            valueIndex += 8;\n+          }\n+          return;\n+        default:\n+          throw new ParquetDecodingException(\"not a valid mode \" + this.mode);\n+      }\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"Failed to read from input stream\", e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean readBoolean() {\n+    return this.readInteger() != 0;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    this.readInteger();\n+  }\n+\n+  @Override\n+  public int readValueDictionaryId() {\n+    return readInteger();\n+  }\n+\n+  @Override\n+  public int readInteger() {\n+    if (this.currentCount == 0) {\n+      this.readNextGroup();\n+    }\n+\n+    this.currentCount--;\n+    switch (mode) {\n+      case RLE:\n+        return this.currentValue;\n+      case PACKED:\n+        return this.packedValuesBuffer[packedValuesBufferIdx++];\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  public void readBatchOfDictionaryIds(\n+      final IntVector vector,\n+      final int numValsInVector,\n+      final int batchSize,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 268}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzEzMDUw", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344313050", "createdAt": "2020-01-17T01:06:50Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMTowNjo1MFrOFesP6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMTowNjo1MFrOFesP6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyNjU3MQ==", "bodyText": "Why doesn't this check currentValue?\nIt looks like this is using the vectorized reader as a definition-level reader that will read runs of data values. Is that not the case?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367726571", "createdAt": "2020-01-17T01:06:50Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {\n+\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  private enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  private MODE mode;\n+  private int currentCount;\n+  private int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  private int[] packedValuesBuffer = new int[16];\n+  private int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  private final int maxDefLevel;\n+\n+  public VectorizedParquetValuesReader(int maxDefLevel) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    init(bitWidth);\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bw,\n+      boolean rl,\n+      int mdl) {\n+    this.fixedWidth = true;\n+    this.readLength = rl;\n+    this.maxDefLevel = mdl;\n+    init(bw);\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {\n+    this.inputStream = in;\n+    if (fixedWidth) {\n+      // initialize for repetition and definition levels\n+      if (readLength) {\n+        int length = readIntLittleEndian();\n+        this.inputStream = in.sliceStream(length);\n+      }\n+    } else {\n+      // initialize for values\n+      if (in.available() > 0) {\n+        init(in.read());\n+      }\n+    }\n+    if (bitWidth == 0) {\n+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.\n+      this.mode = MODE.RLE;\n+      this.currentCount = valueCount;\n+      this.currentValue = 0;\n+    } else {\n+      this.currentCount = 0;\n+    }\n+  }\n+\n+  /**\n+   * Initializes the internal state for decoding ints of `bitWidth`.\n+   */\n+  private void init(int bw) {\n+    Preconditions.checkArgument(bw >= 0 && bw <= 32, \"bitWidth must be >= 0 and <= 32\");\n+    this.bitWidth = bw;\n+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);\n+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);\n+  }\n+\n+  /**\n+   * Reads the next varint encoded int.\n+   */\n+  private int readUnsignedVarInt() throws IOException {\n+    int value = 0;\n+    int shift = 0;\n+    int byteRead;\n+    do {\n+      byteRead = inputStream.read();\n+      value |= (byteRead & 0x7F) << shift;\n+      shift += 7;\n+    } while ((byteRead & 0x80) != 0);\n+    return value;\n+  }\n+\n+  /**\n+   * Reads the next 4 byte little endian int.\n+   */\n+  private int readIntLittleEndian() throws IOException {\n+    int ch4 = inputStream.read();\n+    int ch3 = inputStream.read();\n+    int ch2 = inputStream.read();\n+    int ch1 = inputStream.read();\n+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);\n+  }\n+\n+  /**\n+   * Reads the next byteWidth little endian int.\n+   */\n+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n+    switch (bytesWidth) {\n+      case 0:\n+        return 0;\n+      case 1:\n+        return inputStream.read();\n+      case 2: {\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 8) + ch2;\n+      }\n+      case 3: {\n+        int ch3 = inputStream.read();\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n+      }\n+      case 4: {\n+        return readIntLittleEndian();\n+      }\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  /**\n+   * Reads the next group.\n+   */\n+  private void readNextGroup() {\n+    try {\n+      int header = readUnsignedVarInt();\n+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;\n+      switch (mode) {\n+        case RLE:\n+          this.currentCount = header >>> 1;\n+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();\n+          return;\n+        case PACKED:\n+          int numGroups = header >>> 1;\n+          this.currentCount = numGroups * 8;\n+\n+          if (this.packedValuesBuffer.length < this.currentCount) {\n+            this.packedValuesBuffer = new int[this.currentCount];\n+          }\n+          packedValuesBufferIdx = 0;\n+          int valueIndex = 0;\n+          while (valueIndex < this.currentCount) {\n+            // values are bit packed 8 at a time, so reading bitWidth will always work\n+            ByteBuffer buffer = inputStream.slice(bitWidth);\n+            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);\n+            valueIndex += 8;\n+          }\n+          return;\n+        default:\n+          throw new ParquetDecodingException(\"not a valid mode \" + this.mode);\n+      }\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"Failed to read from input stream\", e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean readBoolean() {\n+    return this.readInteger() != 0;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    this.readInteger();\n+  }\n+\n+  @Override\n+  public int readValueDictionaryId() {\n+    return readInteger();\n+  }\n+\n+  @Override\n+  public int readInteger() {\n+    if (this.currentCount == 0) {\n+      this.readNextGroup();\n+    }\n+\n+    this.currentCount--;\n+    switch (mode) {\n+      case RLE:\n+        return this.currentValue;\n+      case PACKED:\n+        return this.packedValuesBuffer[packedValuesBufferIdx++];\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  public void readBatchOfDictionaryIds(\n+      final IntVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader dictionaryEncodedValuesReader) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readDictionaryIdsInternal(vector, idx, numValues);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, vector.getValidityBuffer());\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.set(idx, dictionaryEncodedValuesReader.readInteger());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  // Used for reading dictionary ids in a vectorized fashion. Unlike other methods, this doesn't\n+  // check definition level.\n+  private void readDictionaryIdsInternal(\n+      final IntVector intVector,\n+      final int numValsInVector,\n+      final int numValuesToRead) {\n+    int left = numValuesToRead;\n+    int idx = numValsInVector;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, currentValue);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, packedValuesBuffer[packedValuesBufferIdx]);\n+            packedValuesBufferIdx++;\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfLongs(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 348}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzEzNzk5", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344313799", "createdAt": "2020-01-17T01:09:36Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMTowOTozNlrOFesSbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMTowOTozNlrOFesSbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyNzIxMg==", "bodyText": "Can we clear the validity buffer at the start of each batch read and then only set the validity buffer and nullability holder when the value is defined?\nWe may also be able to detect whether a column is mostly nulls by looking at the column stats. That way we could set all values to non-null and call this for every non-null value, depending on what is more likely for a column.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367727212", "createdAt": "2020-01-17T01:09:36Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {\n+\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  private enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  private MODE mode;\n+  private int currentCount;\n+  private int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  private int[] packedValuesBuffer = new int[16];\n+  private int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  private final int maxDefLevel;\n+\n+  public VectorizedParquetValuesReader(int maxDefLevel) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    init(bitWidth);\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bw,\n+      boolean rl,\n+      int mdl) {\n+    this.fixedWidth = true;\n+    this.readLength = rl;\n+    this.maxDefLevel = mdl;\n+    init(bw);\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {\n+    this.inputStream = in;\n+    if (fixedWidth) {\n+      // initialize for repetition and definition levels\n+      if (readLength) {\n+        int length = readIntLittleEndian();\n+        this.inputStream = in.sliceStream(length);\n+      }\n+    } else {\n+      // initialize for values\n+      if (in.available() > 0) {\n+        init(in.read());\n+      }\n+    }\n+    if (bitWidth == 0) {\n+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.\n+      this.mode = MODE.RLE;\n+      this.currentCount = valueCount;\n+      this.currentValue = 0;\n+    } else {\n+      this.currentCount = 0;\n+    }\n+  }\n+\n+  /**\n+   * Initializes the internal state for decoding ints of `bitWidth`.\n+   */\n+  private void init(int bw) {\n+    Preconditions.checkArgument(bw >= 0 && bw <= 32, \"bitWidth must be >= 0 and <= 32\");\n+    this.bitWidth = bw;\n+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);\n+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);\n+  }\n+\n+  /**\n+   * Reads the next varint encoded int.\n+   */\n+  private int readUnsignedVarInt() throws IOException {\n+    int value = 0;\n+    int shift = 0;\n+    int byteRead;\n+    do {\n+      byteRead = inputStream.read();\n+      value |= (byteRead & 0x7F) << shift;\n+      shift += 7;\n+    } while ((byteRead & 0x80) != 0);\n+    return value;\n+  }\n+\n+  /**\n+   * Reads the next 4 byte little endian int.\n+   */\n+  private int readIntLittleEndian() throws IOException {\n+    int ch4 = inputStream.read();\n+    int ch3 = inputStream.read();\n+    int ch2 = inputStream.read();\n+    int ch1 = inputStream.read();\n+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);\n+  }\n+\n+  /**\n+   * Reads the next byteWidth little endian int.\n+   */\n+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n+    switch (bytesWidth) {\n+      case 0:\n+        return 0;\n+      case 1:\n+        return inputStream.read();\n+      case 2: {\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 8) + ch2;\n+      }\n+      case 3: {\n+        int ch3 = inputStream.read();\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n+      }\n+      case 4: {\n+        return readIntLittleEndian();\n+      }\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  /**\n+   * Reads the next group.\n+   */\n+  private void readNextGroup() {\n+    try {\n+      int header = readUnsignedVarInt();\n+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;\n+      switch (mode) {\n+        case RLE:\n+          this.currentCount = header >>> 1;\n+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();\n+          return;\n+        case PACKED:\n+          int numGroups = header >>> 1;\n+          this.currentCount = numGroups * 8;\n+\n+          if (this.packedValuesBuffer.length < this.currentCount) {\n+            this.packedValuesBuffer = new int[this.currentCount];\n+          }\n+          packedValuesBufferIdx = 0;\n+          int valueIndex = 0;\n+          while (valueIndex < this.currentCount) {\n+            // values are bit packed 8 at a time, so reading bitWidth will always work\n+            ByteBuffer buffer = inputStream.slice(bitWidth);\n+            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);\n+            valueIndex += 8;\n+          }\n+          return;\n+        default:\n+          throw new ParquetDecodingException(\"not a valid mode \" + this.mode);\n+      }\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"Failed to read from input stream\", e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean readBoolean() {\n+    return this.readInteger() != 0;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    this.readInteger();\n+  }\n+\n+  @Override\n+  public int readValueDictionaryId() {\n+    return readInteger();\n+  }\n+\n+  @Override\n+  public int readInteger() {\n+    if (this.currentCount == 0) {\n+      this.readNextGroup();\n+    }\n+\n+    this.currentCount--;\n+    switch (mode) {\n+      case RLE:\n+        return this.currentValue;\n+      case PACKED:\n+        return this.packedValuesBuffer[packedValuesBufferIdx++];\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  public void readBatchOfDictionaryIds(\n+      final IntVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader dictionaryEncodedValuesReader) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readDictionaryIdsInternal(vector, idx, numValues);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, vector.getValidityBuffer());\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.set(idx, dictionaryEncodedValuesReader.readInteger());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  // Used for reading dictionary ids in a vectorized fashion. Unlike other methods, this doesn't\n+  // check definition level.\n+  private void readDictionaryIdsInternal(\n+      final IntVector intVector,\n+      final int numValsInVector,\n+      final int numValuesToRead) {\n+    int left = numValuesToRead;\n+    int idx = numValsInVector;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, currentValue);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, packedValuesBuffer[packedValuesBufferIdx]);\n+            packedValuesBufferIdx++;\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfLongs(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              numValues);\n+          bufferIdx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(\n+                  typeWidth,\n+                  valuesReader,\n+                  bufferIdx,\n+                  vector.getValidityBuffer(),\n+                  vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 367}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzE1NTYy", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344315562", "createdAt": "2020-01-17T01:16:01Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMToxNjowMVrOFesX5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMToxNjowMVrOFesX5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyODYxNQ==", "bodyText": "Is it better to set both buffers at the same time, or better to loop twice?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367728615", "createdAt": "2020-01-17T01:16:01Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {\n+\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  private enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  private MODE mode;\n+  private int currentCount;\n+  private int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  private int[] packedValuesBuffer = new int[16];\n+  private int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  private final int maxDefLevel;\n+\n+  public VectorizedParquetValuesReader(int maxDefLevel) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    init(bitWidth);\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bw,\n+      boolean rl,\n+      int mdl) {\n+    this.fixedWidth = true;\n+    this.readLength = rl;\n+    this.maxDefLevel = mdl;\n+    init(bw);\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {\n+    this.inputStream = in;\n+    if (fixedWidth) {\n+      // initialize for repetition and definition levels\n+      if (readLength) {\n+        int length = readIntLittleEndian();\n+        this.inputStream = in.sliceStream(length);\n+      }\n+    } else {\n+      // initialize for values\n+      if (in.available() > 0) {\n+        init(in.read());\n+      }\n+    }\n+    if (bitWidth == 0) {\n+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.\n+      this.mode = MODE.RLE;\n+      this.currentCount = valueCount;\n+      this.currentValue = 0;\n+    } else {\n+      this.currentCount = 0;\n+    }\n+  }\n+\n+  /**\n+   * Initializes the internal state for decoding ints of `bitWidth`.\n+   */\n+  private void init(int bw) {\n+    Preconditions.checkArgument(bw >= 0 && bw <= 32, \"bitWidth must be >= 0 and <= 32\");\n+    this.bitWidth = bw;\n+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);\n+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);\n+  }\n+\n+  /**\n+   * Reads the next varint encoded int.\n+   */\n+  private int readUnsignedVarInt() throws IOException {\n+    int value = 0;\n+    int shift = 0;\n+    int byteRead;\n+    do {\n+      byteRead = inputStream.read();\n+      value |= (byteRead & 0x7F) << shift;\n+      shift += 7;\n+    } while ((byteRead & 0x80) != 0);\n+    return value;\n+  }\n+\n+  /**\n+   * Reads the next 4 byte little endian int.\n+   */\n+  private int readIntLittleEndian() throws IOException {\n+    int ch4 = inputStream.read();\n+    int ch3 = inputStream.read();\n+    int ch2 = inputStream.read();\n+    int ch1 = inputStream.read();\n+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);\n+  }\n+\n+  /**\n+   * Reads the next byteWidth little endian int.\n+   */\n+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n+    switch (bytesWidth) {\n+      case 0:\n+        return 0;\n+      case 1:\n+        return inputStream.read();\n+      case 2: {\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 8) + ch2;\n+      }\n+      case 3: {\n+        int ch3 = inputStream.read();\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n+      }\n+      case 4: {\n+        return readIntLittleEndian();\n+      }\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  /**\n+   * Reads the next group.\n+   */\n+  private void readNextGroup() {\n+    try {\n+      int header = readUnsignedVarInt();\n+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;\n+      switch (mode) {\n+        case RLE:\n+          this.currentCount = header >>> 1;\n+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();\n+          return;\n+        case PACKED:\n+          int numGroups = header >>> 1;\n+          this.currentCount = numGroups * 8;\n+\n+          if (this.packedValuesBuffer.length < this.currentCount) {\n+            this.packedValuesBuffer = new int[this.currentCount];\n+          }\n+          packedValuesBufferIdx = 0;\n+          int valueIndex = 0;\n+          while (valueIndex < this.currentCount) {\n+            // values are bit packed 8 at a time, so reading bitWidth will always work\n+            ByteBuffer buffer = inputStream.slice(bitWidth);\n+            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);\n+            valueIndex += 8;\n+          }\n+          return;\n+        default:\n+          throw new ParquetDecodingException(\"not a valid mode \" + this.mode);\n+      }\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"Failed to read from input stream\", e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean readBoolean() {\n+    return this.readInteger() != 0;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    this.readInteger();\n+  }\n+\n+  @Override\n+  public int readValueDictionaryId() {\n+    return readInteger();\n+  }\n+\n+  @Override\n+  public int readInteger() {\n+    if (this.currentCount == 0) {\n+      this.readNextGroup();\n+    }\n+\n+    this.currentCount--;\n+    switch (mode) {\n+      case RLE:\n+        return this.currentValue;\n+      case PACKED:\n+        return this.packedValuesBuffer[packedValuesBufferIdx++];\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  public void readBatchOfDictionaryIds(\n+      final IntVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader dictionaryEncodedValuesReader) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readDictionaryIdsInternal(vector, idx, numValues);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, vector.getValidityBuffer());\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.set(idx, dictionaryEncodedValuesReader.readInteger());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  // Used for reading dictionary ids in a vectorized fashion. Unlike other methods, this doesn't\n+  // check definition level.\n+  private void readDictionaryIdsInternal(\n+      final IntVector intVector,\n+      final int numValsInVector,\n+      final int numValuesToRead) {\n+    int left = numValuesToRead;\n+    int idx = numValsInVector;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, currentValue);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, packedValuesBuffer[packedValuesBufferIdx]);\n+            packedValuesBufferIdx++;\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfLongs(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              numValues);\n+          bufferIdx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(\n+                  typeWidth,\n+                  valuesReader,\n+                  bufferIdx,\n+                  vector.getValidityBuffer(),\n+                  vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedLongs(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedLongsInternal(vector, typeWidth, idx, numValues, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, validityBuffer);\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setLong(idx, dict.decodeToLong(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedLongsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer().setLong(idx, dict.decodeToLong(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer()\n+                .setLong(idx, dict.decodeToLong(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfIntegers(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedIntegers(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedIntegersInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setInt(idx, dict.decodeToInt(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedIntegersInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf dataBuffer = vector.getDataBuffer();\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            dataBuffer.setInt(idx, dict.decodeToInt(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            dataBuffer.setInt(idx, dict.decodeToInt(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFloats(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void setValue(\n+      int typeWidth,\n+      ValuesAsBytesReader valuesReader,\n+      int bufferIdx,\n+      ArrowBuf validityBuffer,\n+      ArrowBuf dataBuffer) {\n+    dataBuffer.setBytes(bufferIdx * typeWidth, valuesReader.getBuffer(typeWidth));\n+    BitVectorHelper.setValidityBitToOne(validityBuffer, bufferIdx);\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFloats(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFloatsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, validityBuffer);\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFloatsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDoubles(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedDoubles(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedDoublesInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedDoublesInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedWidthBinary(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFixedWidthBinary(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFixedWidthBinaryInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer()\n+                  .setBytes(idx * typeWidth, dict.decodeToBinary(valuesReader.readInteger()).getBytesUnsafe());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFixedWidthBinaryInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setBytes(idx * typeWidth, dict.decodeToBinary(currentValue).getBytesUnsafe());\n+            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer()\n+                .setBytes(\n+                    idx * typeWidth,\n+                    dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe());\n+            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedLengthDecimals(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(bufferIdx, byteArray);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(bufferIdx, byteArray);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFixedLengthDecimals(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFixedLengthDecimalsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              byte[] decimalBytes = dict.decodeToBinary(valuesReader.readInteger()).getBytesUnsafe();\n+              byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+              System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFixedLengthDecimalsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            byte[] decimalBytes = dict.decodeToBinary(currentValue).getBytesUnsafe();\n+            byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+            System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+            ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            byte[] decimalBytes = dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe();\n+            byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+            System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+            ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  /**\n+   * Method for reading a batch of non-decimal numeric data types (INT32, INT64, FLOAT, DOUBLE, DATE, TIMESTAMP) This\n+   * method reads batches of bytes from Parquet and writes them into the data buffer underneath the Arrow vector. It\n+   * appropriately sets the validity buffer in the Arrow vector.\n+   */\n+  public void readBatchVarWidth(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              setVarWidthBinaryValue(vector, valuesReader, bufferIdx);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setVarWidthBinaryValue(vector, valuesReader, bufferIdx);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void setVarWidthBinaryValue(FieldVector vector, ValuesAsBytesReader valuesReader, int bufferIdx) {\n+    int len = valuesReader.readInteger();\n+    ByteBuffer buffer = valuesReader.getBuffer(len);\n+    // Calling setValueLengthSafe takes care of allocating a larger buffer if\n+    // running out of space.\n+    ((BaseVariableWidthVector) vector).setValueLengthSafe(bufferIdx, len);\n+    // It is possible that the data buffer was reallocated. So it is important to\n+    // not cache the data buffer reference but instead use vector.getDataBuffer().\n+    vector.getDataBuffer().writeBytes(buffer.array(), buffer.position(), buffer.limit() - buffer.position());\n+    // Similarly, we need to get the latest reference to the validity buffer as well\n+    // since reallocation changes reference of the validity buffers as well.\n+    BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), bufferIdx);\n+  }\n+\n+  public void readBatchOfDictionaryEncodedVarWidth(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader dictionaryEncodedValuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedVarWidthBinaryInternal(vector, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              ((BaseVariableWidthVector) vector).setSafe(\n+                  idx,\n+                  dict.decodeToBinary(dictionaryEncodedValuesReader.readInteger()).getBytesUnsafe());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedVarWidthBinaryInternal(\n+      FieldVector vector,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            ((BaseVariableWidthVector) vector).setSafe(idx, dict.decodeToBinary(currentValue).getBytesUnsafe());\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            ((BaseVariableWidthVector) vector).setSafe(\n+                idx,\n+                dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe());\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfIntLongBackedDecimals(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, 0, typeWidth);\n+              vector.getDataBuffer().setBytes(bufferIdx * DecimalVector.TYPE_WIDTH, byteArray);\n+              BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), bufferIdx);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, 0, typeWidth);\n+              vector.getDataBuffer().setBytes(bufferIdx * DecimalVector.TYPE_WIDTH, byteArray);\n+              BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), bufferIdx);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedIntLongBackedDecimals(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedIntLongBackedDecimalsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              ((DecimalVector) vector).set(\n+                  idx,\n+                  typeWidth == Integer.BYTES ?\n+                      dict.decodeToInt(valuesReader.readInteger())\n+                      : dict.decodeToLong(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedIntLongBackedDecimalsInternal(\n+      FieldVector vector,\n+      final int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            ((DecimalVector) vector).set(\n+                idx,\n+                typeWidth == Integer.BYTES ? dict.decodeToInt(currentValue) : dict.decodeToLong(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            ((DecimalVector) vector).set(\n+                idx,\n+                typeWidth == Integer.BYTES ?\n+                    dict.decodeToInt(currentValue)\n+                    : dict.decodeToLong(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfBooleans(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              ((BitVector) vector).setSafe(bufferIdx, valuesReader.readBoolean() ? 1 : 0);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              ((BitVector) vector).setSafe(bufferIdx, valuesReader.readBoolean() ? 1 : 0);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void setBinaryInVector(\n+      VarBinaryVector vector,\n+      int typeWidth,\n+      ValuesAsBytesReader valuesReader,\n+      int bufferIdx) {\n+    byte[] byteArray = new byte[typeWidth];\n+    valuesReader.getBuffer(typeWidth).get(byteArray);\n+    vector.setSafe(bufferIdx, byteArray);\n+  }\n+\n+  private void setNextNValuesInVector(\n+      int typeWidth, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader, int bufferIdx, FieldVector vector, int numValues) {\n+    ArrowBuf validityBuffer = vector.getValidityBuffer();\n+    int validityBufferIdx = bufferIdx;\n+    if (currentValue == maxDefLevel) {\n+      for (int i = 0; i < numValues; i++) {\n+        BitVectorHelper.setValidityBitToOne(validityBuffer, validityBufferIdx);\n+        validityBufferIdx++;\n+      }\n+      ByteBuffer buffer = valuesReader.getBuffer(numValues * typeWidth);\n+      vector.getDataBuffer().setBytes(bufferIdx * typeWidth, buffer);\n+    } else {\n+      setNulls(nullabilityHolder, bufferIdx, numValues, validityBuffer);\n+    }\n+  }\n+\n+  private void setNull(NullabilityHolder nullabilityHolder, int bufferIdx, ArrowBuf validityBuffer) {\n+    nullabilityHolder.setNull(bufferIdx);\n+    BitVectorHelper.setValidityBit(validityBuffer, bufferIdx, 0);\n+  }\n+\n+  private void setNulls(NullabilityHolder nullabilityHolder, int idx, int numValues, ArrowBuf validityBuffer) {\n+    int bufferIdx = idx;\n+    for (int i = 0; i < numValues; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 1369}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzE1Njcx", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344315671", "createdAt": "2020-01-17T01:16:22Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMToxNjoyM1rOFesYOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMToxNjoyM1rOFesYOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyODY5OQ==", "bodyText": "I think there should be a few more tests for this.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367728699", "createdAt": "2020-01-17T01:16:23Z", "author": {"login": "rdblue"}, "path": "arrow/src/test/java/org/apache/iceberg/arrow/ArrowSchemaUtilTest.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow;\n+\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.types.Types.BooleanType;\n+import org.apache.iceberg.types.Types.DateType;\n+import org.apache.iceberg.types.Types.DoubleType;\n+import org.apache.iceberg.types.Types.ListType;\n+import org.apache.iceberg.types.Types.LongType;\n+import org.apache.iceberg.types.Types.MapType;\n+import org.apache.iceberg.types.Types.StringType;\n+import org.apache.iceberg.types.Types.TimestampType;\n+import org.junit.Test;\n+\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.Bool;\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.Date;\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.FloatingPoint;\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.Int;\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.List;\n+import static org.apache.arrow.vector.types.pojo.ArrowType.ArrowTypeID.Timestamp;\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+\n+\n+public class ArrowSchemaUtilTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzE2NzM3", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344316737", "createdAt": "2020-01-17T01:20:18Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMToyMDoxOFrOFesbpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMToyMDoxOFrOFesbpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzcyOTU3Mg==", "bodyText": "I think that getBytesUnsafe will result in a copy, although it will be reused if the same dictionary value is seen again. Is it possible to get a ByteBuffer instead?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367729572", "createdAt": "2020-01-17T01:20:18Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {\n+\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  private enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  private MODE mode;\n+  private int currentCount;\n+  private int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  private int[] packedValuesBuffer = new int[16];\n+  private int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  private final int maxDefLevel;\n+\n+  public VectorizedParquetValuesReader(int maxDefLevel) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    init(bitWidth);\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bw,\n+      boolean rl,\n+      int mdl) {\n+    this.fixedWidth = true;\n+    this.readLength = rl;\n+    this.maxDefLevel = mdl;\n+    init(bw);\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {\n+    this.inputStream = in;\n+    if (fixedWidth) {\n+      // initialize for repetition and definition levels\n+      if (readLength) {\n+        int length = readIntLittleEndian();\n+        this.inputStream = in.sliceStream(length);\n+      }\n+    } else {\n+      // initialize for values\n+      if (in.available() > 0) {\n+        init(in.read());\n+      }\n+    }\n+    if (bitWidth == 0) {\n+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.\n+      this.mode = MODE.RLE;\n+      this.currentCount = valueCount;\n+      this.currentValue = 0;\n+    } else {\n+      this.currentCount = 0;\n+    }\n+  }\n+\n+  /**\n+   * Initializes the internal state for decoding ints of `bitWidth`.\n+   */\n+  private void init(int bw) {\n+    Preconditions.checkArgument(bw >= 0 && bw <= 32, \"bitWidth must be >= 0 and <= 32\");\n+    this.bitWidth = bw;\n+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);\n+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);\n+  }\n+\n+  /**\n+   * Reads the next varint encoded int.\n+   */\n+  private int readUnsignedVarInt() throws IOException {\n+    int value = 0;\n+    int shift = 0;\n+    int byteRead;\n+    do {\n+      byteRead = inputStream.read();\n+      value |= (byteRead & 0x7F) << shift;\n+      shift += 7;\n+    } while ((byteRead & 0x80) != 0);\n+    return value;\n+  }\n+\n+  /**\n+   * Reads the next 4 byte little endian int.\n+   */\n+  private int readIntLittleEndian() throws IOException {\n+    int ch4 = inputStream.read();\n+    int ch3 = inputStream.read();\n+    int ch2 = inputStream.read();\n+    int ch1 = inputStream.read();\n+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);\n+  }\n+\n+  /**\n+   * Reads the next byteWidth little endian int.\n+   */\n+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n+    switch (bytesWidth) {\n+      case 0:\n+        return 0;\n+      case 1:\n+        return inputStream.read();\n+      case 2: {\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 8) + ch2;\n+      }\n+      case 3: {\n+        int ch3 = inputStream.read();\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n+      }\n+      case 4: {\n+        return readIntLittleEndian();\n+      }\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  /**\n+   * Reads the next group.\n+   */\n+  private void readNextGroup() {\n+    try {\n+      int header = readUnsignedVarInt();\n+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;\n+      switch (mode) {\n+        case RLE:\n+          this.currentCount = header >>> 1;\n+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();\n+          return;\n+        case PACKED:\n+          int numGroups = header >>> 1;\n+          this.currentCount = numGroups * 8;\n+\n+          if (this.packedValuesBuffer.length < this.currentCount) {\n+            this.packedValuesBuffer = new int[this.currentCount];\n+          }\n+          packedValuesBufferIdx = 0;\n+          int valueIndex = 0;\n+          while (valueIndex < this.currentCount) {\n+            // values are bit packed 8 at a time, so reading bitWidth will always work\n+            ByteBuffer buffer = inputStream.slice(bitWidth);\n+            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);\n+            valueIndex += 8;\n+          }\n+          return;\n+        default:\n+          throw new ParquetDecodingException(\"not a valid mode \" + this.mode);\n+      }\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"Failed to read from input stream\", e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean readBoolean() {\n+    return this.readInteger() != 0;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    this.readInteger();\n+  }\n+\n+  @Override\n+  public int readValueDictionaryId() {\n+    return readInteger();\n+  }\n+\n+  @Override\n+  public int readInteger() {\n+    if (this.currentCount == 0) {\n+      this.readNextGroup();\n+    }\n+\n+    this.currentCount--;\n+    switch (mode) {\n+      case RLE:\n+        return this.currentValue;\n+      case PACKED:\n+        return this.packedValuesBuffer[packedValuesBufferIdx++];\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  public void readBatchOfDictionaryIds(\n+      final IntVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader dictionaryEncodedValuesReader) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readDictionaryIdsInternal(vector, idx, numValues);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, vector.getValidityBuffer());\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.set(idx, dictionaryEncodedValuesReader.readInteger());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  // Used for reading dictionary ids in a vectorized fashion. Unlike other methods, this doesn't\n+  // check definition level.\n+  private void readDictionaryIdsInternal(\n+      final IntVector intVector,\n+      final int numValsInVector,\n+      final int numValuesToRead) {\n+    int left = numValuesToRead;\n+    int idx = numValsInVector;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, currentValue);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, packedValuesBuffer[packedValuesBufferIdx]);\n+            packedValuesBufferIdx++;\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfLongs(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              numValues);\n+          bufferIdx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(\n+                  typeWidth,\n+                  valuesReader,\n+                  bufferIdx,\n+                  vector.getValidityBuffer(),\n+                  vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedLongs(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedLongsInternal(vector, typeWidth, idx, numValues, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, validityBuffer);\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setLong(idx, dict.decodeToLong(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedLongsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer().setLong(idx, dict.decodeToLong(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer()\n+                .setLong(idx, dict.decodeToLong(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfIntegers(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedIntegers(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedIntegersInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setInt(idx, dict.decodeToInt(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedIntegersInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf dataBuffer = vector.getDataBuffer();\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            dataBuffer.setInt(idx, dict.decodeToInt(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            dataBuffer.setInt(idx, dict.decodeToInt(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFloats(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void setValue(\n+      int typeWidth,\n+      ValuesAsBytesReader valuesReader,\n+      int bufferIdx,\n+      ArrowBuf validityBuffer,\n+      ArrowBuf dataBuffer) {\n+    dataBuffer.setBytes(bufferIdx * typeWidth, valuesReader.getBuffer(typeWidth));\n+    BitVectorHelper.setValidityBitToOne(validityBuffer, bufferIdx);\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFloats(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFloatsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, validityBuffer);\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFloatsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDoubles(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedDoubles(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedDoublesInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedDoublesInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedWidthBinary(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFixedWidthBinary(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFixedWidthBinaryInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer()\n+                  .setBytes(idx * typeWidth, dict.decodeToBinary(valuesReader.readInteger()).getBytesUnsafe());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 859}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzE3ODky", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344317892", "createdAt": "2020-01-17T01:24:26Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMToyNDoyNlrOFesfIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMToyNDoyNlrOFesfIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzczMDQ2NQ==", "bodyText": "This shouldn't allocate a new byte array each time through the loop. It should create the buffer at the start of the method call and reuse it for each value. We could probably also cache these to avoid the allocation entirely, but that would require a size-based buffer cache and a thread-local.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367730465", "createdAt": "2020-01-17T01:24:26Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {\n+\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  private enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  private MODE mode;\n+  private int currentCount;\n+  private int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  private int[] packedValuesBuffer = new int[16];\n+  private int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  private final int maxDefLevel;\n+\n+  public VectorizedParquetValuesReader(int maxDefLevel) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    init(bitWidth);\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bw,\n+      boolean rl,\n+      int mdl) {\n+    this.fixedWidth = true;\n+    this.readLength = rl;\n+    this.maxDefLevel = mdl;\n+    init(bw);\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {\n+    this.inputStream = in;\n+    if (fixedWidth) {\n+      // initialize for repetition and definition levels\n+      if (readLength) {\n+        int length = readIntLittleEndian();\n+        this.inputStream = in.sliceStream(length);\n+      }\n+    } else {\n+      // initialize for values\n+      if (in.available() > 0) {\n+        init(in.read());\n+      }\n+    }\n+    if (bitWidth == 0) {\n+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.\n+      this.mode = MODE.RLE;\n+      this.currentCount = valueCount;\n+      this.currentValue = 0;\n+    } else {\n+      this.currentCount = 0;\n+    }\n+  }\n+\n+  /**\n+   * Initializes the internal state for decoding ints of `bitWidth`.\n+   */\n+  private void init(int bw) {\n+    Preconditions.checkArgument(bw >= 0 && bw <= 32, \"bitWidth must be >= 0 and <= 32\");\n+    this.bitWidth = bw;\n+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);\n+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);\n+  }\n+\n+  /**\n+   * Reads the next varint encoded int.\n+   */\n+  private int readUnsignedVarInt() throws IOException {\n+    int value = 0;\n+    int shift = 0;\n+    int byteRead;\n+    do {\n+      byteRead = inputStream.read();\n+      value |= (byteRead & 0x7F) << shift;\n+      shift += 7;\n+    } while ((byteRead & 0x80) != 0);\n+    return value;\n+  }\n+\n+  /**\n+   * Reads the next 4 byte little endian int.\n+   */\n+  private int readIntLittleEndian() throws IOException {\n+    int ch4 = inputStream.read();\n+    int ch3 = inputStream.read();\n+    int ch2 = inputStream.read();\n+    int ch1 = inputStream.read();\n+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);\n+  }\n+\n+  /**\n+   * Reads the next byteWidth little endian int.\n+   */\n+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n+    switch (bytesWidth) {\n+      case 0:\n+        return 0;\n+      case 1:\n+        return inputStream.read();\n+      case 2: {\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 8) + ch2;\n+      }\n+      case 3: {\n+        int ch3 = inputStream.read();\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n+      }\n+      case 4: {\n+        return readIntLittleEndian();\n+      }\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  /**\n+   * Reads the next group.\n+   */\n+  private void readNextGroup() {\n+    try {\n+      int header = readUnsignedVarInt();\n+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;\n+      switch (mode) {\n+        case RLE:\n+          this.currentCount = header >>> 1;\n+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();\n+          return;\n+        case PACKED:\n+          int numGroups = header >>> 1;\n+          this.currentCount = numGroups * 8;\n+\n+          if (this.packedValuesBuffer.length < this.currentCount) {\n+            this.packedValuesBuffer = new int[this.currentCount];\n+          }\n+          packedValuesBufferIdx = 0;\n+          int valueIndex = 0;\n+          while (valueIndex < this.currentCount) {\n+            // values are bit packed 8 at a time, so reading bitWidth will always work\n+            ByteBuffer buffer = inputStream.slice(bitWidth);\n+            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);\n+            valueIndex += 8;\n+          }\n+          return;\n+        default:\n+          throw new ParquetDecodingException(\"not a valid mode \" + this.mode);\n+      }\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"Failed to read from input stream\", e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean readBoolean() {\n+    return this.readInteger() != 0;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    this.readInteger();\n+  }\n+\n+  @Override\n+  public int readValueDictionaryId() {\n+    return readInteger();\n+  }\n+\n+  @Override\n+  public int readInteger() {\n+    if (this.currentCount == 0) {\n+      this.readNextGroup();\n+    }\n+\n+    this.currentCount--;\n+    switch (mode) {\n+      case RLE:\n+        return this.currentValue;\n+      case PACKED:\n+        return this.packedValuesBuffer[packedValuesBufferIdx++];\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  public void readBatchOfDictionaryIds(\n+      final IntVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader dictionaryEncodedValuesReader) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readDictionaryIdsInternal(vector, idx, numValues);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, vector.getValidityBuffer());\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.set(idx, dictionaryEncodedValuesReader.readInteger());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  // Used for reading dictionary ids in a vectorized fashion. Unlike other methods, this doesn't\n+  // check definition level.\n+  private void readDictionaryIdsInternal(\n+      final IntVector intVector,\n+      final int numValsInVector,\n+      final int numValuesToRead) {\n+    int left = numValuesToRead;\n+    int idx = numValsInVector;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, currentValue);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, packedValuesBuffer[packedValuesBufferIdx]);\n+            packedValuesBufferIdx++;\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfLongs(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              numValues);\n+          bufferIdx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(\n+                  typeWidth,\n+                  valuesReader,\n+                  bufferIdx,\n+                  vector.getValidityBuffer(),\n+                  vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedLongs(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedLongsInternal(vector, typeWidth, idx, numValues, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, validityBuffer);\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setLong(idx, dict.decodeToLong(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedLongsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer().setLong(idx, dict.decodeToLong(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer()\n+                .setLong(idx, dict.decodeToLong(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfIntegers(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedIntegers(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedIntegersInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setInt(idx, dict.decodeToInt(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedIntegersInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf dataBuffer = vector.getDataBuffer();\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            dataBuffer.setInt(idx, dict.decodeToInt(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            dataBuffer.setInt(idx, dict.decodeToInt(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFloats(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void setValue(\n+      int typeWidth,\n+      ValuesAsBytesReader valuesReader,\n+      int bufferIdx,\n+      ArrowBuf validityBuffer,\n+      ArrowBuf dataBuffer) {\n+    dataBuffer.setBytes(bufferIdx * typeWidth, valuesReader.getBuffer(typeWidth));\n+    BitVectorHelper.setValidityBitToOne(validityBuffer, bufferIdx);\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFloats(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFloatsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, validityBuffer);\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFloatsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDoubles(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedDoubles(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedDoublesInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedDoublesInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedWidthBinary(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFixedWidthBinary(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFixedWidthBinaryInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer()\n+                  .setBytes(idx * typeWidth, dict.decodeToBinary(valuesReader.readInteger()).getBytesUnsafe());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFixedWidthBinaryInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setBytes(idx * typeWidth, dict.decodeToBinary(currentValue).getBytesUnsafe());\n+            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer()\n+                .setBytes(\n+                    idx * typeWidth,\n+                    dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe());\n+            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedLengthDecimals(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 924}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzE4NTAz", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344318503", "createdAt": "2020-01-17T01:26:35Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMToyNjozNVrOFeshHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMToyNjozNVrOFeshHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzczMDk3Mg==", "bodyText": "Same here. It would be great to follow up and get rid of this copy.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367730972", "createdAt": "2020-01-17T01:26:35Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {\n+\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  private enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  private MODE mode;\n+  private int currentCount;\n+  private int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  private int[] packedValuesBuffer = new int[16];\n+  private int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  private final int maxDefLevel;\n+\n+  public VectorizedParquetValuesReader(int maxDefLevel) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    init(bitWidth);\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bw,\n+      boolean rl,\n+      int mdl) {\n+    this.fixedWidth = true;\n+    this.readLength = rl;\n+    this.maxDefLevel = mdl;\n+    init(bw);\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {\n+    this.inputStream = in;\n+    if (fixedWidth) {\n+      // initialize for repetition and definition levels\n+      if (readLength) {\n+        int length = readIntLittleEndian();\n+        this.inputStream = in.sliceStream(length);\n+      }\n+    } else {\n+      // initialize for values\n+      if (in.available() > 0) {\n+        init(in.read());\n+      }\n+    }\n+    if (bitWidth == 0) {\n+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.\n+      this.mode = MODE.RLE;\n+      this.currentCount = valueCount;\n+      this.currentValue = 0;\n+    } else {\n+      this.currentCount = 0;\n+    }\n+  }\n+\n+  /**\n+   * Initializes the internal state for decoding ints of `bitWidth`.\n+   */\n+  private void init(int bw) {\n+    Preconditions.checkArgument(bw >= 0 && bw <= 32, \"bitWidth must be >= 0 and <= 32\");\n+    this.bitWidth = bw;\n+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);\n+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);\n+  }\n+\n+  /**\n+   * Reads the next varint encoded int.\n+   */\n+  private int readUnsignedVarInt() throws IOException {\n+    int value = 0;\n+    int shift = 0;\n+    int byteRead;\n+    do {\n+      byteRead = inputStream.read();\n+      value |= (byteRead & 0x7F) << shift;\n+      shift += 7;\n+    } while ((byteRead & 0x80) != 0);\n+    return value;\n+  }\n+\n+  /**\n+   * Reads the next 4 byte little endian int.\n+   */\n+  private int readIntLittleEndian() throws IOException {\n+    int ch4 = inputStream.read();\n+    int ch3 = inputStream.read();\n+    int ch2 = inputStream.read();\n+    int ch1 = inputStream.read();\n+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);\n+  }\n+\n+  /**\n+   * Reads the next byteWidth little endian int.\n+   */\n+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n+    switch (bytesWidth) {\n+      case 0:\n+        return 0;\n+      case 1:\n+        return inputStream.read();\n+      case 2: {\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 8) + ch2;\n+      }\n+      case 3: {\n+        int ch3 = inputStream.read();\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n+      }\n+      case 4: {\n+        return readIntLittleEndian();\n+      }\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  /**\n+   * Reads the next group.\n+   */\n+  private void readNextGroup() {\n+    try {\n+      int header = readUnsignedVarInt();\n+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;\n+      switch (mode) {\n+        case RLE:\n+          this.currentCount = header >>> 1;\n+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();\n+          return;\n+        case PACKED:\n+          int numGroups = header >>> 1;\n+          this.currentCount = numGroups * 8;\n+\n+          if (this.packedValuesBuffer.length < this.currentCount) {\n+            this.packedValuesBuffer = new int[this.currentCount];\n+          }\n+          packedValuesBufferIdx = 0;\n+          int valueIndex = 0;\n+          while (valueIndex < this.currentCount) {\n+            // values are bit packed 8 at a time, so reading bitWidth will always work\n+            ByteBuffer buffer = inputStream.slice(bitWidth);\n+            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);\n+            valueIndex += 8;\n+          }\n+          return;\n+        default:\n+          throw new ParquetDecodingException(\"not a valid mode \" + this.mode);\n+      }\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"Failed to read from input stream\", e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean readBoolean() {\n+    return this.readInteger() != 0;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    this.readInteger();\n+  }\n+\n+  @Override\n+  public int readValueDictionaryId() {\n+    return readInteger();\n+  }\n+\n+  @Override\n+  public int readInteger() {\n+    if (this.currentCount == 0) {\n+      this.readNextGroup();\n+    }\n+\n+    this.currentCount--;\n+    switch (mode) {\n+      case RLE:\n+        return this.currentValue;\n+      case PACKED:\n+        return this.packedValuesBuffer[packedValuesBufferIdx++];\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  public void readBatchOfDictionaryIds(\n+      final IntVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader dictionaryEncodedValuesReader) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readDictionaryIdsInternal(vector, idx, numValues);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, vector.getValidityBuffer());\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.set(idx, dictionaryEncodedValuesReader.readInteger());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  // Used for reading dictionary ids in a vectorized fashion. Unlike other methods, this doesn't\n+  // check definition level.\n+  private void readDictionaryIdsInternal(\n+      final IntVector intVector,\n+      final int numValsInVector,\n+      final int numValuesToRead) {\n+    int left = numValuesToRead;\n+    int idx = numValsInVector;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, currentValue);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, packedValuesBuffer[packedValuesBufferIdx]);\n+            packedValuesBufferIdx++;\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfLongs(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              numValues);\n+          bufferIdx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(\n+                  typeWidth,\n+                  valuesReader,\n+                  bufferIdx,\n+                  vector.getValidityBuffer(),\n+                  vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedLongs(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedLongsInternal(vector, typeWidth, idx, numValues, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, validityBuffer);\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setLong(idx, dict.decodeToLong(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedLongsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer().setLong(idx, dict.decodeToLong(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer()\n+                .setLong(idx, dict.decodeToLong(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfIntegers(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedIntegers(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedIntegersInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setInt(idx, dict.decodeToInt(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedIntegersInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf dataBuffer = vector.getDataBuffer();\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            dataBuffer.setInt(idx, dict.decodeToInt(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            dataBuffer.setInt(idx, dict.decodeToInt(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFloats(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void setValue(\n+      int typeWidth,\n+      ValuesAsBytesReader valuesReader,\n+      int bufferIdx,\n+      ArrowBuf validityBuffer,\n+      ArrowBuf dataBuffer) {\n+    dataBuffer.setBytes(bufferIdx * typeWidth, valuesReader.getBuffer(typeWidth));\n+    BitVectorHelper.setValidityBitToOne(validityBuffer, bufferIdx);\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFloats(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFloatsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, validityBuffer);\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFloatsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDoubles(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedDoubles(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedDoublesInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedDoublesInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedWidthBinary(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFixedWidthBinary(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFixedWidthBinaryInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer()\n+                  .setBytes(idx * typeWidth, dict.decodeToBinary(valuesReader.readInteger()).getBytesUnsafe());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFixedWidthBinaryInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setBytes(idx * typeWidth, dict.decodeToBinary(currentValue).getBytesUnsafe());\n+            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer()\n+                .setBytes(\n+                    idx * typeWidth,\n+                    dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe());\n+            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedLengthDecimals(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(bufferIdx, byteArray);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(bufferIdx, byteArray);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFixedLengthDecimals(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFixedLengthDecimalsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              byte[] decimalBytes = dict.decodeToBinary(valuesReader.readInteger()).getBytesUnsafe();\n+              byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+              System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFixedLengthDecimalsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            byte[] decimalBytes = dict.decodeToBinary(currentValue).getBytesUnsafe();\n+            byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+            System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+            ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            byte[] decimalBytes = dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 1020}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzE4Njk4", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344318698", "createdAt": "2020-01-17T01:27:27Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMToyNzoyN1rOFeshuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMToyNzoyN1rOFeshuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzczMTEyOQ==", "bodyText": "Is this correct?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367731129", "createdAt": "2020-01-17T01:27:27Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {\n+\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  private enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  private MODE mode;\n+  private int currentCount;\n+  private int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  private int[] packedValuesBuffer = new int[16];\n+  private int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  private final int maxDefLevel;\n+\n+  public VectorizedParquetValuesReader(int maxDefLevel) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    init(bitWidth);\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bw,\n+      boolean rl,\n+      int mdl) {\n+    this.fixedWidth = true;\n+    this.readLength = rl;\n+    this.maxDefLevel = mdl;\n+    init(bw);\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {\n+    this.inputStream = in;\n+    if (fixedWidth) {\n+      // initialize for repetition and definition levels\n+      if (readLength) {\n+        int length = readIntLittleEndian();\n+        this.inputStream = in.sliceStream(length);\n+      }\n+    } else {\n+      // initialize for values\n+      if (in.available() > 0) {\n+        init(in.read());\n+      }\n+    }\n+    if (bitWidth == 0) {\n+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.\n+      this.mode = MODE.RLE;\n+      this.currentCount = valueCount;\n+      this.currentValue = 0;\n+    } else {\n+      this.currentCount = 0;\n+    }\n+  }\n+\n+  /**\n+   * Initializes the internal state for decoding ints of `bitWidth`.\n+   */\n+  private void init(int bw) {\n+    Preconditions.checkArgument(bw >= 0 && bw <= 32, \"bitWidth must be >= 0 and <= 32\");\n+    this.bitWidth = bw;\n+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);\n+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);\n+  }\n+\n+  /**\n+   * Reads the next varint encoded int.\n+   */\n+  private int readUnsignedVarInt() throws IOException {\n+    int value = 0;\n+    int shift = 0;\n+    int byteRead;\n+    do {\n+      byteRead = inputStream.read();\n+      value |= (byteRead & 0x7F) << shift;\n+      shift += 7;\n+    } while ((byteRead & 0x80) != 0);\n+    return value;\n+  }\n+\n+  /**\n+   * Reads the next 4 byte little endian int.\n+   */\n+  private int readIntLittleEndian() throws IOException {\n+    int ch4 = inputStream.read();\n+    int ch3 = inputStream.read();\n+    int ch2 = inputStream.read();\n+    int ch1 = inputStream.read();\n+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);\n+  }\n+\n+  /**\n+   * Reads the next byteWidth little endian int.\n+   */\n+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n+    switch (bytesWidth) {\n+      case 0:\n+        return 0;\n+      case 1:\n+        return inputStream.read();\n+      case 2: {\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 8) + ch2;\n+      }\n+      case 3: {\n+        int ch3 = inputStream.read();\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n+      }\n+      case 4: {\n+        return readIntLittleEndian();\n+      }\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  /**\n+   * Reads the next group.\n+   */\n+  private void readNextGroup() {\n+    try {\n+      int header = readUnsignedVarInt();\n+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;\n+      switch (mode) {\n+        case RLE:\n+          this.currentCount = header >>> 1;\n+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();\n+          return;\n+        case PACKED:\n+          int numGroups = header >>> 1;\n+          this.currentCount = numGroups * 8;\n+\n+          if (this.packedValuesBuffer.length < this.currentCount) {\n+            this.packedValuesBuffer = new int[this.currentCount];\n+          }\n+          packedValuesBufferIdx = 0;\n+          int valueIndex = 0;\n+          while (valueIndex < this.currentCount) {\n+            // values are bit packed 8 at a time, so reading bitWidth will always work\n+            ByteBuffer buffer = inputStream.slice(bitWidth);\n+            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);\n+            valueIndex += 8;\n+          }\n+          return;\n+        default:\n+          throw new ParquetDecodingException(\"not a valid mode \" + this.mode);\n+      }\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"Failed to read from input stream\", e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean readBoolean() {\n+    return this.readInteger() != 0;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    this.readInteger();\n+  }\n+\n+  @Override\n+  public int readValueDictionaryId() {\n+    return readInteger();\n+  }\n+\n+  @Override\n+  public int readInteger() {\n+    if (this.currentCount == 0) {\n+      this.readNextGroup();\n+    }\n+\n+    this.currentCount--;\n+    switch (mode) {\n+      case RLE:\n+        return this.currentValue;\n+      case PACKED:\n+        return this.packedValuesBuffer[packedValuesBufferIdx++];\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  public void readBatchOfDictionaryIds(\n+      final IntVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader dictionaryEncodedValuesReader) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readDictionaryIdsInternal(vector, idx, numValues);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, vector.getValidityBuffer());\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.set(idx, dictionaryEncodedValuesReader.readInteger());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  // Used for reading dictionary ids in a vectorized fashion. Unlike other methods, this doesn't\n+  // check definition level.\n+  private void readDictionaryIdsInternal(\n+      final IntVector intVector,\n+      final int numValsInVector,\n+      final int numValuesToRead) {\n+    int left = numValuesToRead;\n+    int idx = numValsInVector;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, currentValue);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, packedValuesBuffer[packedValuesBufferIdx]);\n+            packedValuesBufferIdx++;\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfLongs(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              numValues);\n+          bufferIdx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(\n+                  typeWidth,\n+                  valuesReader,\n+                  bufferIdx,\n+                  vector.getValidityBuffer(),\n+                  vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedLongs(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedLongsInternal(vector, typeWidth, idx, numValues, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, validityBuffer);\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setLong(idx, dict.decodeToLong(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedLongsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer().setLong(idx, dict.decodeToLong(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer()\n+                .setLong(idx, dict.decodeToLong(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfIntegers(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedIntegers(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedIntegersInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setInt(idx, dict.decodeToInt(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedIntegersInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf dataBuffer = vector.getDataBuffer();\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            dataBuffer.setInt(idx, dict.decodeToInt(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            dataBuffer.setInt(idx, dict.decodeToInt(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFloats(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void setValue(\n+      int typeWidth,\n+      ValuesAsBytesReader valuesReader,\n+      int bufferIdx,\n+      ArrowBuf validityBuffer,\n+      ArrowBuf dataBuffer) {\n+    dataBuffer.setBytes(bufferIdx * typeWidth, valuesReader.getBuffer(typeWidth));\n+    BitVectorHelper.setValidityBitToOne(validityBuffer, bufferIdx);\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFloats(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFloatsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, validityBuffer);\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFloatsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDoubles(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedDoubles(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedDoublesInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedDoublesInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedWidthBinary(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFixedWidthBinary(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFixedWidthBinaryInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer()\n+                  .setBytes(idx * typeWidth, dict.decodeToBinary(valuesReader.readInteger()).getBytesUnsafe());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFixedWidthBinaryInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setBytes(idx * typeWidth, dict.decodeToBinary(currentValue).getBytesUnsafe());\n+            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer()\n+                .setBytes(\n+                    idx * typeWidth,\n+                    dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe());\n+            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedLengthDecimals(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(bufferIdx, byteArray);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(bufferIdx, byteArray);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFixedLengthDecimals(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFixedLengthDecimalsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              byte[] decimalBytes = dict.decodeToBinary(valuesReader.readInteger()).getBytesUnsafe();\n+              byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+              System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFixedLengthDecimalsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            byte[] decimalBytes = dict.decodeToBinary(currentValue).getBytesUnsafe();\n+            byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+            System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+            ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            byte[] decimalBytes = dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe();\n+            byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+            System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+            ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  /**\n+   * Method for reading a batch of non-decimal numeric data types (INT32, INT64, FLOAT, DOUBLE, DATE, TIMESTAMP) This", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 1034}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzE5NTg0", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344319584", "createdAt": "2020-01-17T01:30:40Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMTozMDo0MFrOFeskxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMTozMDo0MFrOFeskxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzczMTkwOQ==", "bodyText": "I don't think this is safe. Don't you need to use offset = buffer.position() + buffer.arrayOffset() when accessing the backing array?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367731909", "createdAt": "2020-01-17T01:30:40Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {\n+\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  private enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  private MODE mode;\n+  private int currentCount;\n+  private int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  private int[] packedValuesBuffer = new int[16];\n+  private int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  private final int maxDefLevel;\n+\n+  public VectorizedParquetValuesReader(int maxDefLevel) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    init(bitWidth);\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bw,\n+      boolean rl,\n+      int mdl) {\n+    this.fixedWidth = true;\n+    this.readLength = rl;\n+    this.maxDefLevel = mdl;\n+    init(bw);\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {\n+    this.inputStream = in;\n+    if (fixedWidth) {\n+      // initialize for repetition and definition levels\n+      if (readLength) {\n+        int length = readIntLittleEndian();\n+        this.inputStream = in.sliceStream(length);\n+      }\n+    } else {\n+      // initialize for values\n+      if (in.available() > 0) {\n+        init(in.read());\n+      }\n+    }\n+    if (bitWidth == 0) {\n+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.\n+      this.mode = MODE.RLE;\n+      this.currentCount = valueCount;\n+      this.currentValue = 0;\n+    } else {\n+      this.currentCount = 0;\n+    }\n+  }\n+\n+  /**\n+   * Initializes the internal state for decoding ints of `bitWidth`.\n+   */\n+  private void init(int bw) {\n+    Preconditions.checkArgument(bw >= 0 && bw <= 32, \"bitWidth must be >= 0 and <= 32\");\n+    this.bitWidth = bw;\n+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);\n+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);\n+  }\n+\n+  /**\n+   * Reads the next varint encoded int.\n+   */\n+  private int readUnsignedVarInt() throws IOException {\n+    int value = 0;\n+    int shift = 0;\n+    int byteRead;\n+    do {\n+      byteRead = inputStream.read();\n+      value |= (byteRead & 0x7F) << shift;\n+      shift += 7;\n+    } while ((byteRead & 0x80) != 0);\n+    return value;\n+  }\n+\n+  /**\n+   * Reads the next 4 byte little endian int.\n+   */\n+  private int readIntLittleEndian() throws IOException {\n+    int ch4 = inputStream.read();\n+    int ch3 = inputStream.read();\n+    int ch2 = inputStream.read();\n+    int ch1 = inputStream.read();\n+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);\n+  }\n+\n+  /**\n+   * Reads the next byteWidth little endian int.\n+   */\n+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n+    switch (bytesWidth) {\n+      case 0:\n+        return 0;\n+      case 1:\n+        return inputStream.read();\n+      case 2: {\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 8) + ch2;\n+      }\n+      case 3: {\n+        int ch3 = inputStream.read();\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n+      }\n+      case 4: {\n+        return readIntLittleEndian();\n+      }\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  /**\n+   * Reads the next group.\n+   */\n+  private void readNextGroup() {\n+    try {\n+      int header = readUnsignedVarInt();\n+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;\n+      switch (mode) {\n+        case RLE:\n+          this.currentCount = header >>> 1;\n+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();\n+          return;\n+        case PACKED:\n+          int numGroups = header >>> 1;\n+          this.currentCount = numGroups * 8;\n+\n+          if (this.packedValuesBuffer.length < this.currentCount) {\n+            this.packedValuesBuffer = new int[this.currentCount];\n+          }\n+          packedValuesBufferIdx = 0;\n+          int valueIndex = 0;\n+          while (valueIndex < this.currentCount) {\n+            // values are bit packed 8 at a time, so reading bitWidth will always work\n+            ByteBuffer buffer = inputStream.slice(bitWidth);\n+            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);\n+            valueIndex += 8;\n+          }\n+          return;\n+        default:\n+          throw new ParquetDecodingException(\"not a valid mode \" + this.mode);\n+      }\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"Failed to read from input stream\", e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean readBoolean() {\n+    return this.readInteger() != 0;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    this.readInteger();\n+  }\n+\n+  @Override\n+  public int readValueDictionaryId() {\n+    return readInteger();\n+  }\n+\n+  @Override\n+  public int readInteger() {\n+    if (this.currentCount == 0) {\n+      this.readNextGroup();\n+    }\n+\n+    this.currentCount--;\n+    switch (mode) {\n+      case RLE:\n+        return this.currentValue;\n+      case PACKED:\n+        return this.packedValuesBuffer[packedValuesBufferIdx++];\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  public void readBatchOfDictionaryIds(\n+      final IntVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader dictionaryEncodedValuesReader) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readDictionaryIdsInternal(vector, idx, numValues);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, vector.getValidityBuffer());\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.set(idx, dictionaryEncodedValuesReader.readInteger());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  // Used for reading dictionary ids in a vectorized fashion. Unlike other methods, this doesn't\n+  // check definition level.\n+  private void readDictionaryIdsInternal(\n+      final IntVector intVector,\n+      final int numValsInVector,\n+      final int numValuesToRead) {\n+    int left = numValuesToRead;\n+    int idx = numValsInVector;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, currentValue);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, packedValuesBuffer[packedValuesBufferIdx]);\n+            packedValuesBufferIdx++;\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfLongs(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              numValues);\n+          bufferIdx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(\n+                  typeWidth,\n+                  valuesReader,\n+                  bufferIdx,\n+                  vector.getValidityBuffer(),\n+                  vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedLongs(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedLongsInternal(vector, typeWidth, idx, numValues, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, validityBuffer);\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setLong(idx, dict.decodeToLong(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedLongsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer().setLong(idx, dict.decodeToLong(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer()\n+                .setLong(idx, dict.decodeToLong(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfIntegers(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedIntegers(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedIntegersInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setInt(idx, dict.decodeToInt(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedIntegersInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf dataBuffer = vector.getDataBuffer();\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            dataBuffer.setInt(idx, dict.decodeToInt(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            dataBuffer.setInt(idx, dict.decodeToInt(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFloats(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void setValue(\n+      int typeWidth,\n+      ValuesAsBytesReader valuesReader,\n+      int bufferIdx,\n+      ArrowBuf validityBuffer,\n+      ArrowBuf dataBuffer) {\n+    dataBuffer.setBytes(bufferIdx * typeWidth, valuesReader.getBuffer(typeWidth));\n+    BitVectorHelper.setValidityBitToOne(validityBuffer, bufferIdx);\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFloats(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFloatsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, validityBuffer);\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFloatsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDoubles(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedDoubles(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedDoublesInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedDoublesInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedWidthBinary(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFixedWidthBinary(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFixedWidthBinaryInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer()\n+                  .setBytes(idx * typeWidth, dict.decodeToBinary(valuesReader.readInteger()).getBytesUnsafe());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFixedWidthBinaryInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setBytes(idx * typeWidth, dict.decodeToBinary(currentValue).getBytesUnsafe());\n+            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer()\n+                .setBytes(\n+                    idx * typeWidth,\n+                    dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe());\n+            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedLengthDecimals(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(bufferIdx, byteArray);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(bufferIdx, byteArray);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFixedLengthDecimals(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFixedLengthDecimalsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              byte[] decimalBytes = dict.decodeToBinary(valuesReader.readInteger()).getBytesUnsafe();\n+              byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+              System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFixedLengthDecimalsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            byte[] decimalBytes = dict.decodeToBinary(currentValue).getBytesUnsafe();\n+            byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+            System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+            ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            byte[] decimalBytes = dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe();\n+            byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+            System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+            ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  /**\n+   * Method for reading a batch of non-decimal numeric data types (INT32, INT64, FLOAT, DOUBLE, DATE, TIMESTAMP) This\n+   * method reads batches of bytes from Parquet and writes them into the data buffer underneath the Arrow vector. It\n+   * appropriately sets the validity buffer in the Arrow vector.\n+   */\n+  public void readBatchVarWidth(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              setVarWidthBinaryValue(vector, valuesReader, bufferIdx);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setVarWidthBinaryValue(vector, valuesReader, bufferIdx);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void setVarWidthBinaryValue(FieldVector vector, ValuesAsBytesReader valuesReader, int bufferIdx) {\n+    int len = valuesReader.readInteger();\n+    ByteBuffer buffer = valuesReader.getBuffer(len);\n+    // Calling setValueLengthSafe takes care of allocating a larger buffer if\n+    // running out of space.\n+    ((BaseVariableWidthVector) vector).setValueLengthSafe(bufferIdx, len);\n+    // It is possible that the data buffer was reallocated. So it is important to\n+    // not cache the data buffer reference but instead use vector.getDataBuffer().\n+    vector.getDataBuffer().writeBytes(buffer.array(), buffer.position(), buffer.limit() - buffer.position());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 1087}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzIwNTUx", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344320551", "createdAt": "2020-01-17T01:34:37Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMTozNDozN1rOFesoQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMTozNDozN1rOFesoQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzczMjgwMA==", "bodyText": "Is there any benefit to returning 0 or 1 directly? We could use readBooleanAsInt and use a shift to get either 1 or 0. Then we wouldn't need to branch here.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367732800", "createdAt": "2020-01-17T01:34:37Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {\n+\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  private enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  private MODE mode;\n+  private int currentCount;\n+  private int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  private int[] packedValuesBuffer = new int[16];\n+  private int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  private final int maxDefLevel;\n+\n+  public VectorizedParquetValuesReader(int maxDefLevel) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    init(bitWidth);\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bw,\n+      boolean rl,\n+      int mdl) {\n+    this.fixedWidth = true;\n+    this.readLength = rl;\n+    this.maxDefLevel = mdl;\n+    init(bw);\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {\n+    this.inputStream = in;\n+    if (fixedWidth) {\n+      // initialize for repetition and definition levels\n+      if (readLength) {\n+        int length = readIntLittleEndian();\n+        this.inputStream = in.sliceStream(length);\n+      }\n+    } else {\n+      // initialize for values\n+      if (in.available() > 0) {\n+        init(in.read());\n+      }\n+    }\n+    if (bitWidth == 0) {\n+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.\n+      this.mode = MODE.RLE;\n+      this.currentCount = valueCount;\n+      this.currentValue = 0;\n+    } else {\n+      this.currentCount = 0;\n+    }\n+  }\n+\n+  /**\n+   * Initializes the internal state for decoding ints of `bitWidth`.\n+   */\n+  private void init(int bw) {\n+    Preconditions.checkArgument(bw >= 0 && bw <= 32, \"bitWidth must be >= 0 and <= 32\");\n+    this.bitWidth = bw;\n+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);\n+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);\n+  }\n+\n+  /**\n+   * Reads the next varint encoded int.\n+   */\n+  private int readUnsignedVarInt() throws IOException {\n+    int value = 0;\n+    int shift = 0;\n+    int byteRead;\n+    do {\n+      byteRead = inputStream.read();\n+      value |= (byteRead & 0x7F) << shift;\n+      shift += 7;\n+    } while ((byteRead & 0x80) != 0);\n+    return value;\n+  }\n+\n+  /**\n+   * Reads the next 4 byte little endian int.\n+   */\n+  private int readIntLittleEndian() throws IOException {\n+    int ch4 = inputStream.read();\n+    int ch3 = inputStream.read();\n+    int ch2 = inputStream.read();\n+    int ch1 = inputStream.read();\n+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);\n+  }\n+\n+  /**\n+   * Reads the next byteWidth little endian int.\n+   */\n+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n+    switch (bytesWidth) {\n+      case 0:\n+        return 0;\n+      case 1:\n+        return inputStream.read();\n+      case 2: {\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 8) + ch2;\n+      }\n+      case 3: {\n+        int ch3 = inputStream.read();\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n+      }\n+      case 4: {\n+        return readIntLittleEndian();\n+      }\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  /**\n+   * Reads the next group.\n+   */\n+  private void readNextGroup() {\n+    try {\n+      int header = readUnsignedVarInt();\n+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;\n+      switch (mode) {\n+        case RLE:\n+          this.currentCount = header >>> 1;\n+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();\n+          return;\n+        case PACKED:\n+          int numGroups = header >>> 1;\n+          this.currentCount = numGroups * 8;\n+\n+          if (this.packedValuesBuffer.length < this.currentCount) {\n+            this.packedValuesBuffer = new int[this.currentCount];\n+          }\n+          packedValuesBufferIdx = 0;\n+          int valueIndex = 0;\n+          while (valueIndex < this.currentCount) {\n+            // values are bit packed 8 at a time, so reading bitWidth will always work\n+            ByteBuffer buffer = inputStream.slice(bitWidth);\n+            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);\n+            valueIndex += 8;\n+          }\n+          return;\n+        default:\n+          throw new ParquetDecodingException(\"not a valid mode \" + this.mode);\n+      }\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"Failed to read from input stream\", e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean readBoolean() {\n+    return this.readInteger() != 0;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    this.readInteger();\n+  }\n+\n+  @Override\n+  public int readValueDictionaryId() {\n+    return readInteger();\n+  }\n+\n+  @Override\n+  public int readInteger() {\n+    if (this.currentCount == 0) {\n+      this.readNextGroup();\n+    }\n+\n+    this.currentCount--;\n+    switch (mode) {\n+      case RLE:\n+        return this.currentValue;\n+      case PACKED:\n+        return this.packedValuesBuffer[packedValuesBufferIdx++];\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  public void readBatchOfDictionaryIds(\n+      final IntVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader dictionaryEncodedValuesReader) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readDictionaryIdsInternal(vector, idx, numValues);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, vector.getValidityBuffer());\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.set(idx, dictionaryEncodedValuesReader.readInteger());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  // Used for reading dictionary ids in a vectorized fashion. Unlike other methods, this doesn't\n+  // check definition level.\n+  private void readDictionaryIdsInternal(\n+      final IntVector intVector,\n+      final int numValsInVector,\n+      final int numValuesToRead) {\n+    int left = numValuesToRead;\n+    int idx = numValsInVector;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, currentValue);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, packedValuesBuffer[packedValuesBufferIdx]);\n+            packedValuesBufferIdx++;\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfLongs(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              numValues);\n+          bufferIdx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(\n+                  typeWidth,\n+                  valuesReader,\n+                  bufferIdx,\n+                  vector.getValidityBuffer(),\n+                  vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedLongs(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedLongsInternal(vector, typeWidth, idx, numValues, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, validityBuffer);\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setLong(idx, dict.decodeToLong(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedLongsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer().setLong(idx, dict.decodeToLong(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer()\n+                .setLong(idx, dict.decodeToLong(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfIntegers(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedIntegers(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedIntegersInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setInt(idx, dict.decodeToInt(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedIntegersInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf dataBuffer = vector.getDataBuffer();\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            dataBuffer.setInt(idx, dict.decodeToInt(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            dataBuffer.setInt(idx, dict.decodeToInt(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFloats(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void setValue(\n+      int typeWidth,\n+      ValuesAsBytesReader valuesReader,\n+      int bufferIdx,\n+      ArrowBuf validityBuffer,\n+      ArrowBuf dataBuffer) {\n+    dataBuffer.setBytes(bufferIdx * typeWidth, valuesReader.getBuffer(typeWidth));\n+    BitVectorHelper.setValidityBitToOne(validityBuffer, bufferIdx);\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFloats(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFloatsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, validityBuffer);\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFloatsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDoubles(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedDoubles(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedDoublesInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedDoublesInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedWidthBinary(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFixedWidthBinary(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFixedWidthBinaryInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer()\n+                  .setBytes(idx * typeWidth, dict.decodeToBinary(valuesReader.readInteger()).getBytesUnsafe());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFixedWidthBinaryInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setBytes(idx * typeWidth, dict.decodeToBinary(currentValue).getBytesUnsafe());\n+            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer()\n+                .setBytes(\n+                    idx * typeWidth,\n+                    dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe());\n+            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedLengthDecimals(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(bufferIdx, byteArray);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(bufferIdx, byteArray);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFixedLengthDecimals(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFixedLengthDecimalsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              byte[] decimalBytes = dict.decodeToBinary(valuesReader.readInteger()).getBytesUnsafe();\n+              byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+              System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFixedLengthDecimalsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            byte[] decimalBytes = dict.decodeToBinary(currentValue).getBytesUnsafe();\n+            byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+            System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+            ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            byte[] decimalBytes = dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe();\n+            byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+            System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+            ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  /**\n+   * Method for reading a batch of non-decimal numeric data types (INT32, INT64, FLOAT, DOUBLE, DATE, TIMESTAMP) This\n+   * method reads batches of bytes from Parquet and writes them into the data buffer underneath the Arrow vector. It\n+   * appropriately sets the validity buffer in the Arrow vector.\n+   */\n+  public void readBatchVarWidth(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              setVarWidthBinaryValue(vector, valuesReader, bufferIdx);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setVarWidthBinaryValue(vector, valuesReader, bufferIdx);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void setVarWidthBinaryValue(FieldVector vector, ValuesAsBytesReader valuesReader, int bufferIdx) {\n+    int len = valuesReader.readInteger();\n+    ByteBuffer buffer = valuesReader.getBuffer(len);\n+    // Calling setValueLengthSafe takes care of allocating a larger buffer if\n+    // running out of space.\n+    ((BaseVariableWidthVector) vector).setValueLengthSafe(bufferIdx, len);\n+    // It is possible that the data buffer was reallocated. So it is important to\n+    // not cache the data buffer reference but instead use vector.getDataBuffer().\n+    vector.getDataBuffer().writeBytes(buffer.array(), buffer.position(), buffer.limit() - buffer.position());\n+    // Similarly, we need to get the latest reference to the validity buffer as well\n+    // since reallocation changes reference of the validity buffers as well.\n+    BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), bufferIdx);\n+  }\n+\n+  public void readBatchOfDictionaryEncodedVarWidth(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader dictionaryEncodedValuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedVarWidthBinaryInternal(vector, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              ((BaseVariableWidthVector) vector).setSafe(\n+                  idx,\n+                  dict.decodeToBinary(dictionaryEncodedValuesReader.readInteger()).getBytesUnsafe());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedVarWidthBinaryInternal(\n+      FieldVector vector,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            ((BaseVariableWidthVector) vector).setSafe(idx, dict.decodeToBinary(currentValue).getBytesUnsafe());\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            ((BaseVariableWidthVector) vector).setSafe(\n+                idx,\n+                dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe());\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfIntLongBackedDecimals(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, 0, typeWidth);\n+              vector.getDataBuffer().setBytes(bufferIdx * DecimalVector.TYPE_WIDTH, byteArray);\n+              BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), bufferIdx);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, 0, typeWidth);\n+              vector.getDataBuffer().setBytes(bufferIdx * DecimalVector.TYPE_WIDTH, byteArray);\n+              BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), bufferIdx);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedIntLongBackedDecimals(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedIntLongBackedDecimalsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              ((DecimalVector) vector).set(\n+                  idx,\n+                  typeWidth == Integer.BYTES ?\n+                      dict.decodeToInt(valuesReader.readInteger())\n+                      : dict.decodeToLong(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedIntLongBackedDecimalsInternal(\n+      FieldVector vector,\n+      final int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            ((DecimalVector) vector).set(\n+                idx,\n+                typeWidth == Integer.BYTES ? dict.decodeToInt(currentValue) : dict.decodeToLong(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            ((DecimalVector) vector).set(\n+                idx,\n+                typeWidth == Integer.BYTES ?\n+                    dict.decodeToInt(currentValue)\n+                    : dict.decodeToLong(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfBooleans(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              ((BitVector) vector).setSafe(bufferIdx, valuesReader.readBoolean() ? 1 : 0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 1311}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzIwOTM1", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344320935", "createdAt": "2020-01-17T01:36:08Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMTozNjowOVrOFespZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMTozNjowOVrOFespZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzczMzA5Mg==", "bodyText": "We use this to trigger reallocation if necessary?\nIs there a setSafe that accepts a ByteBuffer to avoid this copy? If not, we should contribute one back to Arrow.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367733092", "createdAt": "2020-01-17T01:36:09Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {\n+\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  private enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  private MODE mode;\n+  private int currentCount;\n+  private int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  private int[] packedValuesBuffer = new int[16];\n+  private int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  private final int maxDefLevel;\n+\n+  public VectorizedParquetValuesReader(int maxDefLevel) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    init(bitWidth);\n+  }\n+\n+  public VectorizedParquetValuesReader(\n+      int bw,\n+      boolean rl,\n+      int mdl) {\n+    this.fixedWidth = true;\n+    this.readLength = rl;\n+    this.maxDefLevel = mdl;\n+    init(bw);\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {\n+    this.inputStream = in;\n+    if (fixedWidth) {\n+      // initialize for repetition and definition levels\n+      if (readLength) {\n+        int length = readIntLittleEndian();\n+        this.inputStream = in.sliceStream(length);\n+      }\n+    } else {\n+      // initialize for values\n+      if (in.available() > 0) {\n+        init(in.read());\n+      }\n+    }\n+    if (bitWidth == 0) {\n+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.\n+      this.mode = MODE.RLE;\n+      this.currentCount = valueCount;\n+      this.currentValue = 0;\n+    } else {\n+      this.currentCount = 0;\n+    }\n+  }\n+\n+  /**\n+   * Initializes the internal state for decoding ints of `bitWidth`.\n+   */\n+  private void init(int bw) {\n+    Preconditions.checkArgument(bw >= 0 && bw <= 32, \"bitWidth must be >= 0 and <= 32\");\n+    this.bitWidth = bw;\n+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);\n+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);\n+  }\n+\n+  /**\n+   * Reads the next varint encoded int.\n+   */\n+  private int readUnsignedVarInt() throws IOException {\n+    int value = 0;\n+    int shift = 0;\n+    int byteRead;\n+    do {\n+      byteRead = inputStream.read();\n+      value |= (byteRead & 0x7F) << shift;\n+      shift += 7;\n+    } while ((byteRead & 0x80) != 0);\n+    return value;\n+  }\n+\n+  /**\n+   * Reads the next 4 byte little endian int.\n+   */\n+  private int readIntLittleEndian() throws IOException {\n+    int ch4 = inputStream.read();\n+    int ch3 = inputStream.read();\n+    int ch2 = inputStream.read();\n+    int ch1 = inputStream.read();\n+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);\n+  }\n+\n+  /**\n+   * Reads the next byteWidth little endian int.\n+   */\n+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n+    switch (bytesWidth) {\n+      case 0:\n+        return 0;\n+      case 1:\n+        return inputStream.read();\n+      case 2: {\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 8) + ch2;\n+      }\n+      case 3: {\n+        int ch3 = inputStream.read();\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n+      }\n+      case 4: {\n+        return readIntLittleEndian();\n+      }\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  /**\n+   * Reads the next group.\n+   */\n+  private void readNextGroup() {\n+    try {\n+      int header = readUnsignedVarInt();\n+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;\n+      switch (mode) {\n+        case RLE:\n+          this.currentCount = header >>> 1;\n+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();\n+          return;\n+        case PACKED:\n+          int numGroups = header >>> 1;\n+          this.currentCount = numGroups * 8;\n+\n+          if (this.packedValuesBuffer.length < this.currentCount) {\n+            this.packedValuesBuffer = new int[this.currentCount];\n+          }\n+          packedValuesBufferIdx = 0;\n+          int valueIndex = 0;\n+          while (valueIndex < this.currentCount) {\n+            // values are bit packed 8 at a time, so reading bitWidth will always work\n+            ByteBuffer buffer = inputStream.slice(bitWidth);\n+            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);\n+            valueIndex += 8;\n+          }\n+          return;\n+        default:\n+          throw new ParquetDecodingException(\"not a valid mode \" + this.mode);\n+      }\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"Failed to read from input stream\", e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean readBoolean() {\n+    return this.readInteger() != 0;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    this.readInteger();\n+  }\n+\n+  @Override\n+  public int readValueDictionaryId() {\n+    return readInteger();\n+  }\n+\n+  @Override\n+  public int readInteger() {\n+    if (this.currentCount == 0) {\n+      this.readNextGroup();\n+    }\n+\n+    this.currentCount--;\n+    switch (mode) {\n+      case RLE:\n+        return this.currentValue;\n+      case PACKED:\n+        return this.packedValuesBuffer[packedValuesBufferIdx++];\n+    }\n+    throw new RuntimeException(\"Unreachable\");\n+  }\n+\n+  public void readBatchOfDictionaryIds(\n+      final IntVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader dictionaryEncodedValuesReader) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readDictionaryIdsInternal(vector, idx, numValues);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, vector.getValidityBuffer());\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.set(idx, dictionaryEncodedValuesReader.readInteger());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  // Used for reading dictionary ids in a vectorized fashion. Unlike other methods, this doesn't\n+  // check definition level.\n+  private void readDictionaryIdsInternal(\n+      final IntVector intVector,\n+      final int numValsInVector,\n+      final int numValuesToRead) {\n+    int left = numValuesToRead;\n+    int idx = numValsInVector;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, currentValue);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, packedValuesBuffer[packedValuesBufferIdx]);\n+            packedValuesBufferIdx++;\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfLongs(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              numValues);\n+          bufferIdx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(\n+                  typeWidth,\n+                  valuesReader,\n+                  bufferIdx,\n+                  vector.getValidityBuffer(),\n+                  vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedLongs(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedLongsInternal(vector, typeWidth, idx, numValues, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, validityBuffer);\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setLong(idx, dict.decodeToLong(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedLongsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer().setLong(idx, dict.decodeToLong(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer()\n+                .setLong(idx, dict.decodeToLong(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfIntegers(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedIntegers(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedIntegersInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setInt(idx, dict.decodeToInt(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedIntegersInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf dataBuffer = vector.getDataBuffer();\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            dataBuffer.setInt(idx, dict.decodeToInt(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            dataBuffer.setInt(idx, dict.decodeToInt(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFloats(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void setValue(\n+      int typeWidth,\n+      ValuesAsBytesReader valuesReader,\n+      int bufferIdx,\n+      ArrowBuf validityBuffer,\n+      ArrowBuf dataBuffer) {\n+    dataBuffer.setBytes(bufferIdx * typeWidth, valuesReader.getBuffer(typeWidth));\n+    BitVectorHelper.setValidityBitToOne(validityBuffer, bufferIdx);\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFloats(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFloatsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, validityBuffer);\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFloatsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDoubles(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setValue(typeWidth, valuesReader, bufferIdx, vector.getValidityBuffer(), vector.getDataBuffer());\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedDoubles(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedDoublesInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedDoublesInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedWidthBinary(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFixedWidthBinary(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFixedWidthBinaryInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer()\n+                  .setBytes(idx * typeWidth, dict.decodeToBinary(valuesReader.readInteger()).getBytesUnsafe());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFixedWidthBinaryInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer().setBytes(idx * typeWidth, dict.decodeToBinary(currentValue).getBytesUnsafe());\n+            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            vector.getDataBuffer()\n+                .setBytes(\n+                    idx * typeWidth,\n+                    dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe());\n+            BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedLengthDecimals(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(bufferIdx, byteArray);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(bufferIdx, byteArray);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFixedLengthDecimals(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedFixedLengthDecimalsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              byte[] decimalBytes = dict.decodeToBinary(valuesReader.readInteger()).getBytesUnsafe();\n+              byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+              System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedFixedLengthDecimalsInternal(\n+      FieldVector vector,\n+      int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            byte[] decimalBytes = dict.decodeToBinary(currentValue).getBytesUnsafe();\n+            byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+            System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+            ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            byte[] decimalBytes = dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe();\n+            byte[] vectorBytes = new byte[DecimalVector.TYPE_WIDTH];\n+            System.arraycopy(decimalBytes, 0, vectorBytes, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+            ((DecimalVector) vector).setBigEndian(idx, vectorBytes);\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  /**\n+   * Method for reading a batch of non-decimal numeric data types (INT32, INT64, FLOAT, DOUBLE, DATE, TIMESTAMP) This\n+   * method reads batches of bytes from Parquet and writes them into the data buffer underneath the Arrow vector. It\n+   * appropriately sets the validity buffer in the Arrow vector.\n+   */\n+  public void readBatchVarWidth(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              setVarWidthBinaryValue(vector, valuesReader, bufferIdx);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setVarWidthBinaryValue(vector, valuesReader, bufferIdx);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void setVarWidthBinaryValue(FieldVector vector, ValuesAsBytesReader valuesReader, int bufferIdx) {\n+    int len = valuesReader.readInteger();\n+    ByteBuffer buffer = valuesReader.getBuffer(len);\n+    // Calling setValueLengthSafe takes care of allocating a larger buffer if\n+    // running out of space.\n+    ((BaseVariableWidthVector) vector).setValueLengthSafe(bufferIdx, len);\n+    // It is possible that the data buffer was reallocated. So it is important to\n+    // not cache the data buffer reference but instead use vector.getDataBuffer().\n+    vector.getDataBuffer().writeBytes(buffer.array(), buffer.position(), buffer.limit() - buffer.position());\n+    // Similarly, we need to get the latest reference to the validity buffer as well\n+    // since reallocation changes reference of the validity buffers as well.\n+    BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), bufferIdx);\n+  }\n+\n+  public void readBatchOfDictionaryEncodedVarWidth(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader dictionaryEncodedValuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedVarWidthBinaryInternal(vector, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              ((BaseVariableWidthVector) vector).setSafe(\n+                  idx,\n+                  dict.decodeToBinary(dictionaryEncodedValuesReader.readInteger()).getBytesUnsafe());\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedVarWidthBinaryInternal(\n+      FieldVector vector,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            ((BaseVariableWidthVector) vector).setSafe(idx, dict.decodeToBinary(currentValue).getBytesUnsafe());\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            ((BaseVariableWidthVector) vector).setSafe(\n+                idx,\n+                dict.decodeToBinary(packedValuesBuffer[packedValuesBufferIdx++]).getBytesUnsafe());\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfIntLongBackedDecimals(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, 0, typeWidth);\n+              vector.getDataBuffer().setBytes(bufferIdx * DecimalVector.TYPE_WIDTH, byteArray);\n+              BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), bufferIdx);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+              valuesReader.getBuffer(typeWidth).get(byteArray, 0, typeWidth);\n+              vector.getDataBuffer().setBytes(bufferIdx * DecimalVector.TYPE_WIDTH, byteArray);\n+              BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), bufferIdx);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedIntLongBackedDecimals(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedParquetValuesReader valuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            valuesReader.readBatchOfDictionaryEncodedIntLongBackedDecimalsInternal(vector, typeWidth, idx, num, dict);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              ((DecimalVector) vector).set(\n+                  idx,\n+                  typeWidth == Integer.BYTES ?\n+                      dict.decodeToInt(valuesReader.readInteger())\n+                      : dict.decodeToLong(valuesReader.readInteger()));\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void readBatchOfDictionaryEncodedIntLongBackedDecimalsInternal(\n+      FieldVector vector,\n+      final int typeWidth,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < num; i++) {\n+            ((DecimalVector) vector).set(\n+                idx,\n+                typeWidth == Integer.BYTES ? dict.decodeToInt(currentValue) : dict.decodeToLong(currentValue));\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            ((DecimalVector) vector).set(\n+                idx,\n+                typeWidth == Integer.BYTES ?\n+                    dict.decodeToInt(currentValue)\n+                    : dict.decodeToLong(packedValuesBuffer[packedValuesBufferIdx++]));\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfBooleans(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              ((BitVector) vector).setSafe(bufferIdx, valuesReader.readBoolean() ? 1 : 0);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              ((BitVector) vector).setSafe(bufferIdx, valuesReader.readBoolean() ? 1 : 0);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  private void setBinaryInVector(\n+      VarBinaryVector vector,\n+      int typeWidth,\n+      ValuesAsBytesReader valuesReader,\n+      int bufferIdx) {\n+    byte[] byteArray = new byte[typeWidth];\n+    valuesReader.getBuffer(typeWidth).get(byteArray);\n+    vector.setSafe(bufferIdx, byteArray);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 1342}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ0MzIzMDY1", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-344323065", "createdAt": "2020-01-17T01:44:35Z", "commit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMTo0NDozNVrOFeswFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xN1QwMTo0NDozNVrOFeswFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NzczNDgwNg==", "bodyText": "This class is really huge and there are two main modes it is used for: definition levels and dictionary indexes. Can we split this into two separate classes for those two uses? I don't think that would hurt performance, but it would be a bit easier to read. We could move the common things into a base class, too.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r367734806", "createdAt": "2020-01-17T01:44:35Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+public final class VectorizedParquetValuesReader extends ValuesReader {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60466f867fd9f7092b9d26b24a177c4e64735d5b"}, "originalPosition": 52}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU5OTg5MDIz", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-359989023", "createdAt": "2020-02-17T22:28:54Z", "commit": {"oid": "5ac46600fd5cdfd44d1b3411f855d94e3e983c90"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMjoyODo1NFrOFqw3yg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMjoyODo1NFrOFqw3yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDM4NTIyNg==", "bodyText": "It doesn't look like numNulls is correct, since it is incremented here as well as in setNotNull. Even if setNotNull didn't increment it, the value would also be incorrect if the same position were set twice. Do we need to track the number of nulls? It doesn't look like this is called anywhere in this set of changes.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380385226", "createdAt": "2020-02-17T22:28:54Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/NullabilityHolder.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+public class NullabilityHolder {\n+  private final byte[] isNull;\n+  private int numNulls;\n+\n+  public NullabilityHolder(int size) {\n+    this.isNull = new byte[size];\n+  }\n+\n+  public void setNull(int idx) {\n+    isNull[idx] = 1;\n+    numNulls++;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5ac46600fd5cdfd44d1b3411f855d94e3e983c90"}, "originalPosition": 32}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5ac46600fd5cdfd44d1b3411f855d94e3e983c90", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/5ac46600fd5cdfd44d1b3411f855d94e3e983c90", "committedDate": "2020-01-29T23:55:13Z", "message": "Address various code review comments"}, "afterCommit": {"oid": "644d695bf8f0627dec41b2f0bcdc0f57d5361e65", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/644d695bf8f0627dec41b2f0bcdc0f57d5361e65", "committedDate": "2020-02-17T23:09:03Z", "message": "Improve performance for null data. Address various code review comments. General code cleanup"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDA0NDk2", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360004496", "createdAt": "2020-02-17T23:37:24Z", "commit": {"oid": "644d695bf8f0627dec41b2f0bcdc0f57d5361e65"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMzozNzoyNFrOFqxq6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMzozNzoyNFrOFqxq6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDM5ODMxMw==", "bodyText": "Invalid Javadoc comment start?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380398313", "createdAt": "2020-02-17T23:37:24Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644d695bf8f0627dec41b2f0bcdc0f57d5361e65"}, "originalPosition": 50}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDA2MjE2", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360006216", "createdAt": "2020-02-17T23:47:39Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMzo0NzozOVrOFqxwrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMzo0NzozOVrOFqxwrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDM5OTc5MQ==", "bodyText": "Should 10 be a private static final variable instead of hard-coded here? Looks like this is supposed to be the default average value size for varchars.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380399791", "createdAt": "2020-02-17T23:47:39Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {\n+  public static final int DEFAULT_BATCH_SIZE = 5000;\n+  public static final int UNKNOWN_WIDTH = -1;\n+\n+  private final ColumnDescriptor columnDescriptor;\n+  private final int batchSize;\n+  private final VectorizedColumnIterator vectorizedColumnIterator;\n+  private final Types.NestedField icebergField;\n+  private final BufferAllocator rootAlloc;\n+  private FieldVector vec;\n+  private int typeWidth;\n+  private ReadType readType;\n+  private boolean reuseContainers = true;\n+  private NullabilityHolder nullabilityHolder;\n+\n+  // In cases when Parquet employs fall back to plain encoding, we eagerly decode the dictionary encoded pages\n+  // before storing the values in the Arrow vector. This means even if the dictionary is present, data\n+  // present in the vector may not necessarily be dictionary encoded.\n+  private Dictionary dictionary;\n+  private boolean allPagesDictEncoded;\n+\n+  // This value is copied from Arrow's BaseVariableWidthVector. We may need to change\n+  // this value if Arrow ends up changing this default.\n+  private static final int DEFAULT_RECORD_BYTE_COUNT = 8;\n+\n+  public VectorizedArrowReader(\n+      ColumnDescriptor desc,\n+      Types.NestedField icebergField,\n+      BufferAllocator ra,\n+      int batchSize,\n+      boolean setArrowValidityVector) {\n+    this.icebergField = icebergField;\n+    this.batchSize = (batchSize == 0) ? DEFAULT_BATCH_SIZE : batchSize;\n+    this.columnDescriptor = desc;\n+    this.rootAlloc = ra;\n+    this.vectorizedColumnIterator = new VectorizedColumnIterator(desc, \"\", batchSize, setArrowValidityVector);\n+  }\n+\n+  private VectorizedArrowReader() {\n+    this.icebergField = null;\n+    this.batchSize = DEFAULT_BATCH_SIZE;\n+    this.columnDescriptor = null;\n+    this.rootAlloc = null;\n+    this.vectorizedColumnIterator = null;\n+  }\n+\n+  private enum ReadType {\n+    FIXED_LENGTH_DECIMAL, INT_LONG_BACKED_DECIMAL, VARCHAR, VARBINARY, FIXED_WIDTH_BINARY,\n+    BOOLEAN, INT, LONG, FLOAT, DOUBLE\n+  }\n+\n+  @Override\n+  public VectorHolder read(int numValsToRead) {\n+    if (vec == null || !reuseContainers) {\n+      allocateFieldVector();\n+      nullabilityHolder = new NullabilityHolder(batchSize);\n+    } else {\n+      vec.setValueCount(0);\n+      nullabilityHolder.reset();\n+    }\n+    if (vectorizedColumnIterator.hasNext()) {\n+      if (allPagesDictEncoded) {\n+        vectorizedColumnIterator.nextBatchDictionaryIds((IntVector) vec, nullabilityHolder);\n+      } else {\n+        switch (readType) {\n+          case FIXED_LENGTH_DECIMAL:\n+            ((IcebergArrowVectors.DecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchFixedLengthDecimal(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case INT_LONG_BACKED_DECIMAL:\n+            ((IcebergArrowVectors.DecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchIntLongBackedDecimal(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case VARBINARY:\n+            ((IcebergArrowVectors.VarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchVarWidthType(vec, nullabilityHolder);\n+            break;\n+          case VARCHAR:\n+            ((IcebergArrowVectors.VarcharArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchVarWidthType(vec, nullabilityHolder);\n+            break;\n+          case FIXED_WIDTH_BINARY:\n+            ((IcebergArrowVectors.VarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchFixedWidthBinary(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case BOOLEAN:\n+            vectorizedColumnIterator.nextBatchBoolean(vec, nullabilityHolder);\n+            break;\n+          case INT:\n+            vectorizedColumnIterator.nextBatchIntegers(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case LONG:\n+            vectorizedColumnIterator.nextBatchLongs(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case FLOAT:\n+            vectorizedColumnIterator.nextBatchFloats(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case DOUBLE:\n+            vectorizedColumnIterator.nextBatchDoubles(vec, typeWidth, nullabilityHolder);\n+            break;\n+        }\n+      }\n+    }\n+    Preconditions.checkState(vec.getValueCount() == numValsToRead,\n+        \"Number of values read, %s, does not equal expected, %s\", vec.getValueCount(), numValsToRead);\n+    return new VectorHolder(columnDescriptor, vec, allPagesDictEncoded, dictionary, nullabilityHolder);\n+  }\n+\n+  private void allocateFieldVector() {\n+    if (allPagesDictEncoded) {\n+      Field field = new Field(\n+          icebergField.name(),\n+          new FieldType(icebergField.isOptional(), new ArrowType.Int(Integer.SIZE, true), null, null),\n+          null);\n+      this.vec = field.createVector(rootAlloc);\n+      ((IntVector) vec).allocateNew(batchSize);\n+      typeWidth = IntVector.TYPE_WIDTH;\n+    } else {\n+      PrimitiveType primitive = columnDescriptor.getPrimitiveType();\n+      if (primitive.getOriginalType() != null) {\n+        switch (columnDescriptor.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            this.vec = new IcebergArrowVectors.VarcharArrowVector(icebergField.name(), rootAlloc);\n+            //TODO: Possibly use the uncompressed page size info to set the initial capacity\n+            vec.setInitialCapacity(batchSize * 10);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 182}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDA2MzQ2", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360006346", "createdAt": "2020-02-17T23:48:25Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMzo0ODoyNVrOFqxxHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMzo0ODoyNVrOFqxxHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDM5OTkwMg==", "bodyText": "Minor: it helps readability to set instance fields using this.typeWidth = .... That way we can see it's a side-effect of the method call and not just a local variable.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380399902", "createdAt": "2020-02-17T23:48:25Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {\n+  public static final int DEFAULT_BATCH_SIZE = 5000;\n+  public static final int UNKNOWN_WIDTH = -1;\n+\n+  private final ColumnDescriptor columnDescriptor;\n+  private final int batchSize;\n+  private final VectorizedColumnIterator vectorizedColumnIterator;\n+  private final Types.NestedField icebergField;\n+  private final BufferAllocator rootAlloc;\n+  private FieldVector vec;\n+  private int typeWidth;\n+  private ReadType readType;\n+  private boolean reuseContainers = true;\n+  private NullabilityHolder nullabilityHolder;\n+\n+  // In cases when Parquet employs fall back to plain encoding, we eagerly decode the dictionary encoded pages\n+  // before storing the values in the Arrow vector. This means even if the dictionary is present, data\n+  // present in the vector may not necessarily be dictionary encoded.\n+  private Dictionary dictionary;\n+  private boolean allPagesDictEncoded;\n+\n+  // This value is copied from Arrow's BaseVariableWidthVector. We may need to change\n+  // this value if Arrow ends up changing this default.\n+  private static final int DEFAULT_RECORD_BYTE_COUNT = 8;\n+\n+  public VectorizedArrowReader(\n+      ColumnDescriptor desc,\n+      Types.NestedField icebergField,\n+      BufferAllocator ra,\n+      int batchSize,\n+      boolean setArrowValidityVector) {\n+    this.icebergField = icebergField;\n+    this.batchSize = (batchSize == 0) ? DEFAULT_BATCH_SIZE : batchSize;\n+    this.columnDescriptor = desc;\n+    this.rootAlloc = ra;\n+    this.vectorizedColumnIterator = new VectorizedColumnIterator(desc, \"\", batchSize, setArrowValidityVector);\n+  }\n+\n+  private VectorizedArrowReader() {\n+    this.icebergField = null;\n+    this.batchSize = DEFAULT_BATCH_SIZE;\n+    this.columnDescriptor = null;\n+    this.rootAlloc = null;\n+    this.vectorizedColumnIterator = null;\n+  }\n+\n+  private enum ReadType {\n+    FIXED_LENGTH_DECIMAL, INT_LONG_BACKED_DECIMAL, VARCHAR, VARBINARY, FIXED_WIDTH_BINARY,\n+    BOOLEAN, INT, LONG, FLOAT, DOUBLE\n+  }\n+\n+  @Override\n+  public VectorHolder read(int numValsToRead) {\n+    if (vec == null || !reuseContainers) {\n+      allocateFieldVector();\n+      nullabilityHolder = new NullabilityHolder(batchSize);\n+    } else {\n+      vec.setValueCount(0);\n+      nullabilityHolder.reset();\n+    }\n+    if (vectorizedColumnIterator.hasNext()) {\n+      if (allPagesDictEncoded) {\n+        vectorizedColumnIterator.nextBatchDictionaryIds((IntVector) vec, nullabilityHolder);\n+      } else {\n+        switch (readType) {\n+          case FIXED_LENGTH_DECIMAL:\n+            ((IcebergArrowVectors.DecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchFixedLengthDecimal(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case INT_LONG_BACKED_DECIMAL:\n+            ((IcebergArrowVectors.DecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchIntLongBackedDecimal(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case VARBINARY:\n+            ((IcebergArrowVectors.VarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchVarWidthType(vec, nullabilityHolder);\n+            break;\n+          case VARCHAR:\n+            ((IcebergArrowVectors.VarcharArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchVarWidthType(vec, nullabilityHolder);\n+            break;\n+          case FIXED_WIDTH_BINARY:\n+            ((IcebergArrowVectors.VarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchFixedWidthBinary(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case BOOLEAN:\n+            vectorizedColumnIterator.nextBatchBoolean(vec, nullabilityHolder);\n+            break;\n+          case INT:\n+            vectorizedColumnIterator.nextBatchIntegers(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case LONG:\n+            vectorizedColumnIterator.nextBatchLongs(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case FLOAT:\n+            vectorizedColumnIterator.nextBatchFloats(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case DOUBLE:\n+            vectorizedColumnIterator.nextBatchDoubles(vec, typeWidth, nullabilityHolder);\n+            break;\n+        }\n+      }\n+    }\n+    Preconditions.checkState(vec.getValueCount() == numValsToRead,\n+        \"Number of values read, %s, does not equal expected, %s\", vec.getValueCount(), numValsToRead);\n+    return new VectorHolder(columnDescriptor, vec, allPagesDictEncoded, dictionary, nullabilityHolder);\n+  }\n+\n+  private void allocateFieldVector() {\n+    if (allPagesDictEncoded) {\n+      Field field = new Field(\n+          icebergField.name(),\n+          new FieldType(icebergField.isOptional(), new ArrowType.Int(Integer.SIZE, true), null, null),\n+          null);\n+      this.vec = field.createVector(rootAlloc);\n+      ((IntVector) vec).allocateNew(batchSize);\n+      typeWidth = IntVector.TYPE_WIDTH;\n+    } else {\n+      PrimitiveType primitive = columnDescriptor.getPrimitiveType();\n+      if (primitive.getOriginalType() != null) {\n+        switch (columnDescriptor.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            this.vec = new IcebergArrowVectors.VarcharArrowVector(icebergField.name(), rootAlloc);\n+            //TODO: Possibly use the uncompressed page size info to set the initial capacity\n+            vec.setInitialCapacity(batchSize * 10);\n+            vec.allocateNewSafe();\n+            readType = ReadType.VARCHAR;\n+            typeWidth = UNKNOWN_WIDTH;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 185}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDA2NDg0", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360006484", "createdAt": "2020-02-17T23:49:12Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMzo0OToxM1rOFqxxmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMzo0OToxM1rOFqxxmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwMDAyNA==", "bodyText": "Minor: should this reader convert the field just once, or does this need to be done each time a new vector is allocated?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380400024", "createdAt": "2020-02-17T23:49:13Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {\n+  public static final int DEFAULT_BATCH_SIZE = 5000;\n+  public static final int UNKNOWN_WIDTH = -1;\n+\n+  private final ColumnDescriptor columnDescriptor;\n+  private final int batchSize;\n+  private final VectorizedColumnIterator vectorizedColumnIterator;\n+  private final Types.NestedField icebergField;\n+  private final BufferAllocator rootAlloc;\n+  private FieldVector vec;\n+  private int typeWidth;\n+  private ReadType readType;\n+  private boolean reuseContainers = true;\n+  private NullabilityHolder nullabilityHolder;\n+\n+  // In cases when Parquet employs fall back to plain encoding, we eagerly decode the dictionary encoded pages\n+  // before storing the values in the Arrow vector. This means even if the dictionary is present, data\n+  // present in the vector may not necessarily be dictionary encoded.\n+  private Dictionary dictionary;\n+  private boolean allPagesDictEncoded;\n+\n+  // This value is copied from Arrow's BaseVariableWidthVector. We may need to change\n+  // this value if Arrow ends up changing this default.\n+  private static final int DEFAULT_RECORD_BYTE_COUNT = 8;\n+\n+  public VectorizedArrowReader(\n+      ColumnDescriptor desc,\n+      Types.NestedField icebergField,\n+      BufferAllocator ra,\n+      int batchSize,\n+      boolean setArrowValidityVector) {\n+    this.icebergField = icebergField;\n+    this.batchSize = (batchSize == 0) ? DEFAULT_BATCH_SIZE : batchSize;\n+    this.columnDescriptor = desc;\n+    this.rootAlloc = ra;\n+    this.vectorizedColumnIterator = new VectorizedColumnIterator(desc, \"\", batchSize, setArrowValidityVector);\n+  }\n+\n+  private VectorizedArrowReader() {\n+    this.icebergField = null;\n+    this.batchSize = DEFAULT_BATCH_SIZE;\n+    this.columnDescriptor = null;\n+    this.rootAlloc = null;\n+    this.vectorizedColumnIterator = null;\n+  }\n+\n+  private enum ReadType {\n+    FIXED_LENGTH_DECIMAL, INT_LONG_BACKED_DECIMAL, VARCHAR, VARBINARY, FIXED_WIDTH_BINARY,\n+    BOOLEAN, INT, LONG, FLOAT, DOUBLE\n+  }\n+\n+  @Override\n+  public VectorHolder read(int numValsToRead) {\n+    if (vec == null || !reuseContainers) {\n+      allocateFieldVector();\n+      nullabilityHolder = new NullabilityHolder(batchSize);\n+    } else {\n+      vec.setValueCount(0);\n+      nullabilityHolder.reset();\n+    }\n+    if (vectorizedColumnIterator.hasNext()) {\n+      if (allPagesDictEncoded) {\n+        vectorizedColumnIterator.nextBatchDictionaryIds((IntVector) vec, nullabilityHolder);\n+      } else {\n+        switch (readType) {\n+          case FIXED_LENGTH_DECIMAL:\n+            ((IcebergArrowVectors.DecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchFixedLengthDecimal(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case INT_LONG_BACKED_DECIMAL:\n+            ((IcebergArrowVectors.DecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchIntLongBackedDecimal(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case VARBINARY:\n+            ((IcebergArrowVectors.VarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchVarWidthType(vec, nullabilityHolder);\n+            break;\n+          case VARCHAR:\n+            ((IcebergArrowVectors.VarcharArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchVarWidthType(vec, nullabilityHolder);\n+            break;\n+          case FIXED_WIDTH_BINARY:\n+            ((IcebergArrowVectors.VarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchFixedWidthBinary(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case BOOLEAN:\n+            vectorizedColumnIterator.nextBatchBoolean(vec, nullabilityHolder);\n+            break;\n+          case INT:\n+            vectorizedColumnIterator.nextBatchIntegers(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case LONG:\n+            vectorizedColumnIterator.nextBatchLongs(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case FLOAT:\n+            vectorizedColumnIterator.nextBatchFloats(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case DOUBLE:\n+            vectorizedColumnIterator.nextBatchDoubles(vec, typeWidth, nullabilityHolder);\n+            break;\n+        }\n+      }\n+    }\n+    Preconditions.checkState(vec.getValueCount() == numValsToRead,\n+        \"Number of values read, %s, does not equal expected, %s\", vec.getValueCount(), numValsToRead);\n+    return new VectorHolder(columnDescriptor, vec, allPagesDictEncoded, dictionary, nullabilityHolder);\n+  }\n+\n+  private void allocateFieldVector() {\n+    if (allPagesDictEncoded) {\n+      Field field = new Field(\n+          icebergField.name(),\n+          new FieldType(icebergField.isOptional(), new ArrowType.Int(Integer.SIZE, true), null, null),\n+          null);\n+      this.vec = field.createVector(rootAlloc);\n+      ((IntVector) vec).allocateNew(batchSize);\n+      typeWidth = IntVector.TYPE_WIDTH;\n+    } else {\n+      PrimitiveType primitive = columnDescriptor.getPrimitiveType();\n+      if (primitive.getOriginalType() != null) {\n+        switch (columnDescriptor.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            this.vec = new IcebergArrowVectors.VarcharArrowVector(icebergField.name(), rootAlloc);\n+            //TODO: Possibly use the uncompressed page size info to set the initial capacity\n+            vec.setInitialCapacity(batchSize * 10);\n+            vec.allocateNewSafe();\n+            readType = ReadType.VARCHAR;\n+            typeWidth = UNKNOWN_WIDTH;\n+            break;\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 190}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDA3NTIx", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360007521", "createdAt": "2020-02-17T23:55:02Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMzo1NTowMlrOFqx1LA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMzo1NTowMlrOFqx1LA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwMDk0MA==", "bodyText": "It isn't clear why this uses factor instead of len to determine the initial allocation size. I think it should be number of records per batch * bytes per value (len). That means we can also get rid of DEFAULT_RECORD_BYTE_COUNT because it isn't used anywhere else.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380400940", "createdAt": "2020-02-17T23:55:02Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {\n+  public static final int DEFAULT_BATCH_SIZE = 5000;\n+  public static final int UNKNOWN_WIDTH = -1;\n+\n+  private final ColumnDescriptor columnDescriptor;\n+  private final int batchSize;\n+  private final VectorizedColumnIterator vectorizedColumnIterator;\n+  private final Types.NestedField icebergField;\n+  private final BufferAllocator rootAlloc;\n+  private FieldVector vec;\n+  private int typeWidth;\n+  private ReadType readType;\n+  private boolean reuseContainers = true;\n+  private NullabilityHolder nullabilityHolder;\n+\n+  // In cases when Parquet employs fall back to plain encoding, we eagerly decode the dictionary encoded pages\n+  // before storing the values in the Arrow vector. This means even if the dictionary is present, data\n+  // present in the vector may not necessarily be dictionary encoded.\n+  private Dictionary dictionary;\n+  private boolean allPagesDictEncoded;\n+\n+  // This value is copied from Arrow's BaseVariableWidthVector. We may need to change\n+  // this value if Arrow ends up changing this default.\n+  private static final int DEFAULT_RECORD_BYTE_COUNT = 8;\n+\n+  public VectorizedArrowReader(\n+      ColumnDescriptor desc,\n+      Types.NestedField icebergField,\n+      BufferAllocator ra,\n+      int batchSize,\n+      boolean setArrowValidityVector) {\n+    this.icebergField = icebergField;\n+    this.batchSize = (batchSize == 0) ? DEFAULT_BATCH_SIZE : batchSize;\n+    this.columnDescriptor = desc;\n+    this.rootAlloc = ra;\n+    this.vectorizedColumnIterator = new VectorizedColumnIterator(desc, \"\", batchSize, setArrowValidityVector);\n+  }\n+\n+  private VectorizedArrowReader() {\n+    this.icebergField = null;\n+    this.batchSize = DEFAULT_BATCH_SIZE;\n+    this.columnDescriptor = null;\n+    this.rootAlloc = null;\n+    this.vectorizedColumnIterator = null;\n+  }\n+\n+  private enum ReadType {\n+    FIXED_LENGTH_DECIMAL, INT_LONG_BACKED_DECIMAL, VARCHAR, VARBINARY, FIXED_WIDTH_BINARY,\n+    BOOLEAN, INT, LONG, FLOAT, DOUBLE\n+  }\n+\n+  @Override\n+  public VectorHolder read(int numValsToRead) {\n+    if (vec == null || !reuseContainers) {\n+      allocateFieldVector();\n+      nullabilityHolder = new NullabilityHolder(batchSize);\n+    } else {\n+      vec.setValueCount(0);\n+      nullabilityHolder.reset();\n+    }\n+    if (vectorizedColumnIterator.hasNext()) {\n+      if (allPagesDictEncoded) {\n+        vectorizedColumnIterator.nextBatchDictionaryIds((IntVector) vec, nullabilityHolder);\n+      } else {\n+        switch (readType) {\n+          case FIXED_LENGTH_DECIMAL:\n+            ((IcebergArrowVectors.DecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchFixedLengthDecimal(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case INT_LONG_BACKED_DECIMAL:\n+            ((IcebergArrowVectors.DecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchIntLongBackedDecimal(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case VARBINARY:\n+            ((IcebergArrowVectors.VarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchVarWidthType(vec, nullabilityHolder);\n+            break;\n+          case VARCHAR:\n+            ((IcebergArrowVectors.VarcharArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchVarWidthType(vec, nullabilityHolder);\n+            break;\n+          case FIXED_WIDTH_BINARY:\n+            ((IcebergArrowVectors.VarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchFixedWidthBinary(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case BOOLEAN:\n+            vectorizedColumnIterator.nextBatchBoolean(vec, nullabilityHolder);\n+            break;\n+          case INT:\n+            vectorizedColumnIterator.nextBatchIntegers(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case LONG:\n+            vectorizedColumnIterator.nextBatchLongs(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case FLOAT:\n+            vectorizedColumnIterator.nextBatchFloats(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case DOUBLE:\n+            vectorizedColumnIterator.nextBatchDoubles(vec, typeWidth, nullabilityHolder);\n+            break;\n+        }\n+      }\n+    }\n+    Preconditions.checkState(vec.getValueCount() == numValsToRead,\n+        \"Number of values read, %s, does not equal expected, %s\", vec.getValueCount(), numValsToRead);\n+    return new VectorHolder(columnDescriptor, vec, allPagesDictEncoded, dictionary, nullabilityHolder);\n+  }\n+\n+  private void allocateFieldVector() {\n+    if (allPagesDictEncoded) {\n+      Field field = new Field(\n+          icebergField.name(),\n+          new FieldType(icebergField.isOptional(), new ArrowType.Int(Integer.SIZE, true), null, null),\n+          null);\n+      this.vec = field.createVector(rootAlloc);\n+      ((IntVector) vec).allocateNew(batchSize);\n+      typeWidth = IntVector.TYPE_WIDTH;\n+    } else {\n+      PrimitiveType primitive = columnDescriptor.getPrimitiveType();\n+      if (primitive.getOriginalType() != null) {\n+        switch (columnDescriptor.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            this.vec = new IcebergArrowVectors.VarcharArrowVector(icebergField.name(), rootAlloc);\n+            //TODO: Possibly use the uncompressed page size info to set the initial capacity\n+            vec.setInitialCapacity(batchSize * 10);\n+            vec.allocateNewSafe();\n+            readType = ReadType.VARCHAR;\n+            typeWidth = UNKNOWN_WIDTH;\n+            break;\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((IntVector) vec).allocateNew(batchSize);\n+            readType = ReadType.INT;\n+            typeWidth = IntVector.TYPE_WIDTH;\n+            break;\n+          case DATE:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((DateDayVector) vec).allocateNew(batchSize);\n+            readType = ReadType.INT;\n+            typeWidth = IntVector.TYPE_WIDTH;\n+            break;\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((BigIntVector) vec).allocateNew(batchSize);\n+            readType = ReadType.LONG;\n+            typeWidth = BigIntVector.TYPE_WIDTH;\n+            break;\n+          case TIMESTAMP_MICROS:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((TimeStampMicroTZVector) vec).allocateNew(batchSize);\n+            readType = ReadType.LONG;\n+            typeWidth = BigIntVector.TYPE_WIDTH;\n+            break;\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            this.vec = new IcebergArrowVectors.DecimalArrowVector(icebergField.name(), rootAlloc,\n+                decimal.getPrecision(), decimal.getScale());\n+            ((DecimalVector) vec).allocateNew(batchSize);\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                readType = ReadType.FIXED_LENGTH_DECIMAL;\n+                typeWidth = primitive.getTypeLength();\n+                break;\n+              case INT64:\n+                readType = ReadType.INT_LONG_BACKED_DECIMAL;\n+                typeWidth = BigIntVector.TYPE_WIDTH;\n+                break;\n+              case INT32:\n+                readType = ReadType.INT_LONG_BACKED_DECIMAL;\n+                typeWidth = IntVector.TYPE_WIDTH;\n+                break;\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+            break;\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+            int len = ((Types.FixedType) icebergField.type()).length();\n+            this.vec = new IcebergArrowVectors.VarBinaryArrowVector(icebergField.name(), rootAlloc);\n+            int factor = (len + DEFAULT_RECORD_BYTE_COUNT - 1) / DEFAULT_RECORD_BYTE_COUNT;\n+            vec.setInitialCapacity(batchSize * factor);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 248}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDA3NTc1", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360007575", "createdAt": "2020-02-17T23:55:22Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMzo1NToyMlrOFqx1Ww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMzo1NToyMlrOFqx1Ww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwMDk4Nw==", "bodyText": "Similar to the string case, I think this default size should be a constant somewhere.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380400987", "createdAt": "2020-02-17T23:55:22Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {\n+  public static final int DEFAULT_BATCH_SIZE = 5000;\n+  public static final int UNKNOWN_WIDTH = -1;\n+\n+  private final ColumnDescriptor columnDescriptor;\n+  private final int batchSize;\n+  private final VectorizedColumnIterator vectorizedColumnIterator;\n+  private final Types.NestedField icebergField;\n+  private final BufferAllocator rootAlloc;\n+  private FieldVector vec;\n+  private int typeWidth;\n+  private ReadType readType;\n+  private boolean reuseContainers = true;\n+  private NullabilityHolder nullabilityHolder;\n+\n+  // In cases when Parquet employs fall back to plain encoding, we eagerly decode the dictionary encoded pages\n+  // before storing the values in the Arrow vector. This means even if the dictionary is present, data\n+  // present in the vector may not necessarily be dictionary encoded.\n+  private Dictionary dictionary;\n+  private boolean allPagesDictEncoded;\n+\n+  // This value is copied from Arrow's BaseVariableWidthVector. We may need to change\n+  // this value if Arrow ends up changing this default.\n+  private static final int DEFAULT_RECORD_BYTE_COUNT = 8;\n+\n+  public VectorizedArrowReader(\n+      ColumnDescriptor desc,\n+      Types.NestedField icebergField,\n+      BufferAllocator ra,\n+      int batchSize,\n+      boolean setArrowValidityVector) {\n+    this.icebergField = icebergField;\n+    this.batchSize = (batchSize == 0) ? DEFAULT_BATCH_SIZE : batchSize;\n+    this.columnDescriptor = desc;\n+    this.rootAlloc = ra;\n+    this.vectorizedColumnIterator = new VectorizedColumnIterator(desc, \"\", batchSize, setArrowValidityVector);\n+  }\n+\n+  private VectorizedArrowReader() {\n+    this.icebergField = null;\n+    this.batchSize = DEFAULT_BATCH_SIZE;\n+    this.columnDescriptor = null;\n+    this.rootAlloc = null;\n+    this.vectorizedColumnIterator = null;\n+  }\n+\n+  private enum ReadType {\n+    FIXED_LENGTH_DECIMAL, INT_LONG_BACKED_DECIMAL, VARCHAR, VARBINARY, FIXED_WIDTH_BINARY,\n+    BOOLEAN, INT, LONG, FLOAT, DOUBLE\n+  }\n+\n+  @Override\n+  public VectorHolder read(int numValsToRead) {\n+    if (vec == null || !reuseContainers) {\n+      allocateFieldVector();\n+      nullabilityHolder = new NullabilityHolder(batchSize);\n+    } else {\n+      vec.setValueCount(0);\n+      nullabilityHolder.reset();\n+    }\n+    if (vectorizedColumnIterator.hasNext()) {\n+      if (allPagesDictEncoded) {\n+        vectorizedColumnIterator.nextBatchDictionaryIds((IntVector) vec, nullabilityHolder);\n+      } else {\n+        switch (readType) {\n+          case FIXED_LENGTH_DECIMAL:\n+            ((IcebergArrowVectors.DecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchFixedLengthDecimal(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case INT_LONG_BACKED_DECIMAL:\n+            ((IcebergArrowVectors.DecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchIntLongBackedDecimal(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case VARBINARY:\n+            ((IcebergArrowVectors.VarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchVarWidthType(vec, nullabilityHolder);\n+            break;\n+          case VARCHAR:\n+            ((IcebergArrowVectors.VarcharArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchVarWidthType(vec, nullabilityHolder);\n+            break;\n+          case FIXED_WIDTH_BINARY:\n+            ((IcebergArrowVectors.VarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchFixedWidthBinary(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case BOOLEAN:\n+            vectorizedColumnIterator.nextBatchBoolean(vec, nullabilityHolder);\n+            break;\n+          case INT:\n+            vectorizedColumnIterator.nextBatchIntegers(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case LONG:\n+            vectorizedColumnIterator.nextBatchLongs(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case FLOAT:\n+            vectorizedColumnIterator.nextBatchFloats(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case DOUBLE:\n+            vectorizedColumnIterator.nextBatchDoubles(vec, typeWidth, nullabilityHolder);\n+            break;\n+        }\n+      }\n+    }\n+    Preconditions.checkState(vec.getValueCount() == numValsToRead,\n+        \"Number of values read, %s, does not equal expected, %s\", vec.getValueCount(), numValsToRead);\n+    return new VectorHolder(columnDescriptor, vec, allPagesDictEncoded, dictionary, nullabilityHolder);\n+  }\n+\n+  private void allocateFieldVector() {\n+    if (allPagesDictEncoded) {\n+      Field field = new Field(\n+          icebergField.name(),\n+          new FieldType(icebergField.isOptional(), new ArrowType.Int(Integer.SIZE, true), null, null),\n+          null);\n+      this.vec = field.createVector(rootAlloc);\n+      ((IntVector) vec).allocateNew(batchSize);\n+      typeWidth = IntVector.TYPE_WIDTH;\n+    } else {\n+      PrimitiveType primitive = columnDescriptor.getPrimitiveType();\n+      if (primitive.getOriginalType() != null) {\n+        switch (columnDescriptor.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            this.vec = new IcebergArrowVectors.VarcharArrowVector(icebergField.name(), rootAlloc);\n+            //TODO: Possibly use the uncompressed page size info to set the initial capacity\n+            vec.setInitialCapacity(batchSize * 10);\n+            vec.allocateNewSafe();\n+            readType = ReadType.VARCHAR;\n+            typeWidth = UNKNOWN_WIDTH;\n+            break;\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((IntVector) vec).allocateNew(batchSize);\n+            readType = ReadType.INT;\n+            typeWidth = IntVector.TYPE_WIDTH;\n+            break;\n+          case DATE:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((DateDayVector) vec).allocateNew(batchSize);\n+            readType = ReadType.INT;\n+            typeWidth = IntVector.TYPE_WIDTH;\n+            break;\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((BigIntVector) vec).allocateNew(batchSize);\n+            readType = ReadType.LONG;\n+            typeWidth = BigIntVector.TYPE_WIDTH;\n+            break;\n+          case TIMESTAMP_MICROS:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((TimeStampMicroTZVector) vec).allocateNew(batchSize);\n+            readType = ReadType.LONG;\n+            typeWidth = BigIntVector.TYPE_WIDTH;\n+            break;\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            this.vec = new IcebergArrowVectors.DecimalArrowVector(icebergField.name(), rootAlloc,\n+                decimal.getPrecision(), decimal.getScale());\n+            ((DecimalVector) vec).allocateNew(batchSize);\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                readType = ReadType.FIXED_LENGTH_DECIMAL;\n+                typeWidth = primitive.getTypeLength();\n+                break;\n+              case INT64:\n+                readType = ReadType.INT_LONG_BACKED_DECIMAL;\n+                typeWidth = BigIntVector.TYPE_WIDTH;\n+                break;\n+              case INT32:\n+                readType = ReadType.INT_LONG_BACKED_DECIMAL;\n+                typeWidth = IntVector.TYPE_WIDTH;\n+                break;\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+            break;\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+            int len = ((Types.FixedType) icebergField.type()).length();\n+            this.vec = new IcebergArrowVectors.VarBinaryArrowVector(icebergField.name(), rootAlloc);\n+            int factor = (len + DEFAULT_RECORD_BYTE_COUNT - 1) / DEFAULT_RECORD_BYTE_COUNT;\n+            vec.setInitialCapacity(batchSize * factor);\n+            vec.allocateNew();\n+            readType = ReadType.FIXED_WIDTH_BINARY;\n+            typeWidth = len;\n+            break;\n+          case BINARY:\n+            this.vec = new IcebergArrowVectors.VarBinaryArrowVector(icebergField.name(), rootAlloc);\n+            //TODO: Possibly use the uncompressed page size info to set the initial capacity\n+            vec.setInitialCapacity(batchSize * 10);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 256}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDA3Nzk4", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360007798", "createdAt": "2020-02-17T23:56:30Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMzo1NjozMFrOFqx2Mw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMzo1NjozMFrOFqx2Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwMTIwMw==", "bodyText": "Do these need to be public?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380401203", "createdAt": "2020-02-17T23:56:30Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {\n+  public static final int DEFAULT_BATCH_SIZE = 5000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDA4MTk4", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360008198", "createdAt": "2020-02-17T23:58:46Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMzo1ODo0NlrOFqx3sA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xN1QyMzo1ODo0NlrOFqx3sA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwMTU4NA==", "bodyText": "This is used to initialize typeWidth, but it looks like all cases where typeWidth is passed during read expect it to be set correctly instead of -1. If that's the case, then let's make typeWidth an Integer and set this to null. That way, using it incorrectly at least results in a NullPointerException.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380401584", "createdAt": "2020-02-17T23:58:46Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {\n+  public static final int DEFAULT_BATCH_SIZE = 5000;\n+  public static final int UNKNOWN_WIDTH = -1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 57}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDA4NDM0", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360008434", "createdAt": "2020-02-18T00:00:05Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDowMDowNlrOFqx4ZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDowMDowNlrOFqx4ZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwMTc2NQ==", "bodyText": "Why are millisecond timestamps read as longs instead of being converted to microseconds?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380401765", "createdAt": "2020-02-18T00:00:06Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.arrow.vector.types.pojo.FieldType;\n+import org.apache.iceberg.arrow.ArrowSchemaUtil;\n+import org.apache.iceberg.arrow.vectorized.parquet.VectorizedColumnIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+\n+/***\n+ * {@link VectorizedReader VectorReader(s)} that read in a batch of values into Arrow vectors.\n+ * It also takes care of allocating the right kind of Arrow vectors depending on the corresponding\n+ * Iceberg/Parquet data types.\n+ */\n+public class VectorizedArrowReader implements VectorizedReader<VectorHolder> {\n+  public static final int DEFAULT_BATCH_SIZE = 5000;\n+  public static final int UNKNOWN_WIDTH = -1;\n+\n+  private final ColumnDescriptor columnDescriptor;\n+  private final int batchSize;\n+  private final VectorizedColumnIterator vectorizedColumnIterator;\n+  private final Types.NestedField icebergField;\n+  private final BufferAllocator rootAlloc;\n+  private FieldVector vec;\n+  private int typeWidth;\n+  private ReadType readType;\n+  private boolean reuseContainers = true;\n+  private NullabilityHolder nullabilityHolder;\n+\n+  // In cases when Parquet employs fall back to plain encoding, we eagerly decode the dictionary encoded pages\n+  // before storing the values in the Arrow vector. This means even if the dictionary is present, data\n+  // present in the vector may not necessarily be dictionary encoded.\n+  private Dictionary dictionary;\n+  private boolean allPagesDictEncoded;\n+\n+  // This value is copied from Arrow's BaseVariableWidthVector. We may need to change\n+  // this value if Arrow ends up changing this default.\n+  private static final int DEFAULT_RECORD_BYTE_COUNT = 8;\n+\n+  public VectorizedArrowReader(\n+      ColumnDescriptor desc,\n+      Types.NestedField icebergField,\n+      BufferAllocator ra,\n+      int batchSize,\n+      boolean setArrowValidityVector) {\n+    this.icebergField = icebergField;\n+    this.batchSize = (batchSize == 0) ? DEFAULT_BATCH_SIZE : batchSize;\n+    this.columnDescriptor = desc;\n+    this.rootAlloc = ra;\n+    this.vectorizedColumnIterator = new VectorizedColumnIterator(desc, \"\", batchSize, setArrowValidityVector);\n+  }\n+\n+  private VectorizedArrowReader() {\n+    this.icebergField = null;\n+    this.batchSize = DEFAULT_BATCH_SIZE;\n+    this.columnDescriptor = null;\n+    this.rootAlloc = null;\n+    this.vectorizedColumnIterator = null;\n+  }\n+\n+  private enum ReadType {\n+    FIXED_LENGTH_DECIMAL, INT_LONG_BACKED_DECIMAL, VARCHAR, VARBINARY, FIXED_WIDTH_BINARY,\n+    BOOLEAN, INT, LONG, FLOAT, DOUBLE\n+  }\n+\n+  @Override\n+  public VectorHolder read(int numValsToRead) {\n+    if (vec == null || !reuseContainers) {\n+      allocateFieldVector();\n+      nullabilityHolder = new NullabilityHolder(batchSize);\n+    } else {\n+      vec.setValueCount(0);\n+      nullabilityHolder.reset();\n+    }\n+    if (vectorizedColumnIterator.hasNext()) {\n+      if (allPagesDictEncoded) {\n+        vectorizedColumnIterator.nextBatchDictionaryIds((IntVector) vec, nullabilityHolder);\n+      } else {\n+        switch (readType) {\n+          case FIXED_LENGTH_DECIMAL:\n+            ((IcebergArrowVectors.DecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchFixedLengthDecimal(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case INT_LONG_BACKED_DECIMAL:\n+            ((IcebergArrowVectors.DecimalArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchIntLongBackedDecimal(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case VARBINARY:\n+            ((IcebergArrowVectors.VarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchVarWidthType(vec, nullabilityHolder);\n+            break;\n+          case VARCHAR:\n+            ((IcebergArrowVectors.VarcharArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchVarWidthType(vec, nullabilityHolder);\n+            break;\n+          case FIXED_WIDTH_BINARY:\n+            ((IcebergArrowVectors.VarBinaryArrowVector) vec).setNullabilityHolder(nullabilityHolder);\n+            vectorizedColumnIterator.nextBatchFixedWidthBinary(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case BOOLEAN:\n+            vectorizedColumnIterator.nextBatchBoolean(vec, nullabilityHolder);\n+            break;\n+          case INT:\n+            vectorizedColumnIterator.nextBatchIntegers(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case LONG:\n+            vectorizedColumnIterator.nextBatchLongs(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case FLOAT:\n+            vectorizedColumnIterator.nextBatchFloats(vec, typeWidth, nullabilityHolder);\n+            break;\n+          case DOUBLE:\n+            vectorizedColumnIterator.nextBatchDoubles(vec, typeWidth, nullabilityHolder);\n+            break;\n+        }\n+      }\n+    }\n+    Preconditions.checkState(vec.getValueCount() == numValsToRead,\n+        \"Number of values read, %s, does not equal expected, %s\", vec.getValueCount(), numValsToRead);\n+    return new VectorHolder(columnDescriptor, vec, allPagesDictEncoded, dictionary, nullabilityHolder);\n+  }\n+\n+  private void allocateFieldVector() {\n+    if (allPagesDictEncoded) {\n+      Field field = new Field(\n+          icebergField.name(),\n+          new FieldType(icebergField.isOptional(), new ArrowType.Int(Integer.SIZE, true), null, null),\n+          null);\n+      this.vec = field.createVector(rootAlloc);\n+      ((IntVector) vec).allocateNew(batchSize);\n+      typeWidth = IntVector.TYPE_WIDTH;\n+    } else {\n+      PrimitiveType primitive = columnDescriptor.getPrimitiveType();\n+      if (primitive.getOriginalType() != null) {\n+        switch (columnDescriptor.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            this.vec = new IcebergArrowVectors.VarcharArrowVector(icebergField.name(), rootAlloc);\n+            //TODO: Possibly use the uncompressed page size info to set the initial capacity\n+            vec.setInitialCapacity(batchSize * 10);\n+            vec.allocateNewSafe();\n+            readType = ReadType.VARCHAR;\n+            typeWidth = UNKNOWN_WIDTH;\n+            break;\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((IntVector) vec).allocateNew(batchSize);\n+            readType = ReadType.INT;\n+            typeWidth = IntVector.TYPE_WIDTH;\n+            break;\n+          case DATE:\n+            this.vec = ArrowSchemaUtil.convert(icebergField).createVector(rootAlloc);\n+            ((DateDayVector) vec).allocateNew(batchSize);\n+            readType = ReadType.INT;\n+            typeWidth = IntVector.TYPE_WIDTH;\n+            break;\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 202}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDA4ODgw", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360008880", "createdAt": "2020-02-18T00:02:49Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDowMjo1MFrOFqx5-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDowMjo1MFrOFqx5-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwMjE3MQ==", "bodyText": "What about using Mode instead of MODE? Usually symbols are all caps, while types are camel case.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380402171", "createdAt": "2020-02-18T00:02:50Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/BaseVectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+public class BaseVectorizedParquetValuesReader extends ValuesReader {\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  enum MODE {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDA4OTc4", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360008978", "createdAt": "2020-02-18T00:03:29Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDowMzozMFrOFqx6Xw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDowMzozMFrOFqx6Xw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwMjI3MQ==", "bodyText": "Are these accessed outside of subclasses? If not, then we should use protected instead of package-private.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380402271", "createdAt": "2020-02-18T00:03:30Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/BaseVectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+public class BaseVectorizedParquetValuesReader extends ValuesReader {\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  MODE mode;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 60}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDA5MzI0", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360009324", "createdAt": "2020-02-18T00:05:34Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDowNTozNFrOFqx7hQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDowNTozNFrOFqx7hQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwMjU2NQ==", "bodyText": "Is this readInteger call correct? If it isn't correct for all types, it would be better to throw UnsupportedOperationException and replace uses with a skipInteger() call.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380402565", "createdAt": "2020-02-18T00:05:34Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/BaseVectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the version in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>\n+ * <tr>If all pages of a column within the row group are not dictionary encoded, then\n+ * dictionary ids are eagerly decoded into actual values before writing them to the Arrow vectors</tr>\n+ * </p>\n+ */\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+public class BaseVectorizedParquetValuesReader extends ValuesReader {\n+  // Current decoding mode. The encoded data contains groups of either run length encoded data\n+  // (RLE) or bit packed data. Each group contains a header that indicates which group it is and\n+  // the number of values in the group.\n+  enum MODE {\n+    RLE,\n+    PACKED\n+  }\n+\n+  // Encoded data.\n+  private ByteBufferInputStream inputStream;\n+\n+  // bit/byte width of decoded data and utility to batch unpack them.\n+  private int bitWidth;\n+  private int bytesWidth;\n+  private BytePacker packer;\n+\n+  // Current decoding mode and values\n+  MODE mode;\n+  int currentCount;\n+  int currentValue;\n+\n+  // Buffer of decoded values if the values are PACKED.\n+  int[] packedValuesBuffer = new int[16];\n+  int packedValuesBufferIdx = 0;\n+\n+  // If true, the bit width is fixed. This decoder is used in different places and this also\n+  // controls if we need to read the bitwidth from the beginning of the data stream.\n+  private final boolean fixedWidth;\n+  private final boolean readLength;\n+  final int maxDefLevel;\n+\n+  final boolean setArrowValidityVector;\n+\n+  public BaseVectorizedParquetValuesReader(int maxDefLevel, boolean setValidityVector) {\n+    this.maxDefLevel = maxDefLevel;\n+    this.fixedWidth = false;\n+    this.readLength = false;\n+    this.setArrowValidityVector = setValidityVector;\n+  }\n+\n+  public BaseVectorizedParquetValuesReader(\n+      int bitWidth,\n+      int maxDefLevel,\n+      boolean setValidityVector) {\n+    this.fixedWidth = true;\n+    this.readLength = bitWidth != 0;\n+    this.maxDefLevel = maxDefLevel;\n+    this.setArrowValidityVector = setValidityVector;\n+    init(bitWidth);\n+  }\n+\n+  @Override\n+  public void initFromPage(int valueCount, ByteBufferInputStream in) throws IOException {\n+    this.inputStream = in;\n+    if (fixedWidth) {\n+      // initialize for repetition and definition levels\n+      if (readLength) {\n+        int length = readIntLittleEndian();\n+        this.inputStream = in.sliceStream(length);\n+      }\n+    } else {\n+      // initialize for values\n+      if (in.available() > 0) {\n+        init(in.read());\n+      }\n+    }\n+    if (bitWidth == 0) {\n+      // 0 bit width, treat this as an RLE run of valueCount number of 0's.\n+      this.mode = MODE.RLE;\n+      this.currentCount = valueCount;\n+      this.currentValue = 0;\n+    } else {\n+      this.currentCount = 0;\n+    }\n+  }\n+\n+  /**\n+   * Initializes the internal state for decoding ints of `bitWidth`.\n+   */\n+  private void init(int bw) {\n+    Preconditions.checkArgument(bw >= 0 && bw <= 32, \"bitWidth must be >= 0 and <= 32\");\n+    this.bitWidth = bw;\n+    this.bytesWidth = BytesUtils.paddedByteCountFromBits(bw);\n+    this.packer = Packer.LITTLE_ENDIAN.newBytePacker(bw);\n+  }\n+\n+  /**\n+   * Reads the next varint encoded int.\n+   */\n+  private int readUnsignedVarInt() throws IOException {\n+    int value = 0;\n+    int shift = 0;\n+    int byteRead;\n+    do {\n+      byteRead = inputStream.read();\n+      value |= (byteRead & 0x7F) << shift;\n+      shift += 7;\n+    } while ((byteRead & 0x80) != 0);\n+    return value;\n+  }\n+\n+  /**\n+   * Reads the next 4 byte little endian int.\n+   */\n+  private int readIntLittleEndian() throws IOException {\n+    int ch4 = inputStream.read();\n+    int ch3 = inputStream.read();\n+    int ch2 = inputStream.read();\n+    int ch1 = inputStream.read();\n+    return (ch1 << 24) + (ch2 << 16) + (ch3 << 8) + (ch4 << 0);\n+  }\n+\n+  /**\n+   * Reads the next byteWidth little endian int.\n+   */\n+  private int readIntLittleEndianPaddedOnBitWidth() throws IOException {\n+    switch (bytesWidth) {\n+      case 0:\n+        return 0;\n+      case 1:\n+        return inputStream.read();\n+      case 2: {\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 8) + ch2;\n+      }\n+      case 3: {\n+        int ch3 = inputStream.read();\n+        int ch2 = inputStream.read();\n+        int ch1 = inputStream.read();\n+        return (ch1 << 16) + (ch2 << 8) + (ch3 << 0);\n+      }\n+      case 4: {\n+        return readIntLittleEndian();\n+      }\n+    }\n+    throw new RuntimeException(\"Non-supported bytesWidth: \" + bytesWidth);\n+  }\n+\n+  /**\n+   * Reads the next group.\n+   */\n+  void readNextGroup() {\n+    try {\n+      int header = readUnsignedVarInt();\n+      this.mode = (header & 1) == 0 ? MODE.RLE : MODE.PACKED;\n+      switch (mode) {\n+        case RLE:\n+          this.currentCount = header >>> 1;\n+          this.currentValue = readIntLittleEndianPaddedOnBitWidth();\n+          return;\n+        case PACKED:\n+          int numGroups = header >>> 1;\n+          this.currentCount = numGroups * 8;\n+          if (this.packedValuesBuffer.length < this.currentCount) {\n+            this.packedValuesBuffer = new int[this.currentCount];\n+          }\n+          packedValuesBufferIdx = 0;\n+          int valueIndex = 0;\n+          while (valueIndex < this.currentCount) {\n+            // values are bit packed 8 at a time, so reading bitWidth will always work\n+            ByteBuffer buffer = inputStream.slice(bitWidth);\n+            this.packer.unpack8Values(buffer, buffer.position(), this.packedValuesBuffer, valueIndex);\n+            valueIndex += 8;\n+          }\n+          return;\n+        default:\n+          throw new ParquetDecodingException(\"not a valid mode \" + this.mode);\n+      }\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"Failed to read from input stream\", e);\n+    }\n+  }\n+\n+  @Override\n+  public boolean readBoolean() {\n+    return this.readInteger() != 0;\n+  }\n+\n+  @Override\n+  public void skip() {\n+    this.readInteger();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 224}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDEwMjAy", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360010202", "createdAt": "2020-02-18T00:10:37Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDoxMDozOFrOFqx-VA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDoxMDozOFrOFqx-VA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwMzI4NA==", "bodyText": "These comments are all the same?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380403284", "createdAt": "2020-02-18T00:10:38Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedColumnIterator.java", "diffHunk": "@@ -0,0 +1,262 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import java.io.IOException;\n+import org.apache.arrow.util.Preconditions;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.page.DataPage;\n+import org.apache.parquet.column.page.DictionaryPage;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.column.page.PageReader;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * Vectorized version of the ColumnIterator that reads column values in data pages of a column in a row group in a\n+ * batched fashion.\n+ */\n+public class VectorizedColumnIterator {\n+\n+  private final ColumnDescriptor desc;\n+  private final VectorizedPageIterator vectorizedPageIterator;\n+\n+  // state reset for each row group\n+  private PageReader columnPageReader = null;\n+  private long totalValuesCount = 0L;\n+  private long valuesRead = 0L;\n+  private long advanceNextPageCount = 0L;\n+  private final int batchSize;\n+\n+  public VectorizedColumnIterator(ColumnDescriptor desc, String writerVersion, int batchSize,\n+                                  boolean setArrowValidityVector) {\n+    Preconditions.checkArgument(desc.getMaxRepetitionLevel() == 0,\n+        \"Only non-nested columns are supported for vectorized reads\");\n+    this.desc = desc;\n+    this.batchSize = batchSize;\n+    this.vectorizedPageIterator = new VectorizedPageIterator(desc, writerVersion, setArrowValidityVector);\n+  }\n+\n+  public Dictionary setRowGroupInfo(PageReadStore store, boolean allPagesDictEncoded) {\n+    this.columnPageReader = store.getPageReader(desc);\n+    this.totalValuesCount = columnPageReader.getTotalValueCount();\n+    this.valuesRead = 0L;\n+    this.advanceNextPageCount = 0L;\n+    this.vectorizedPageIterator.reset();\n+    Dictionary dict = readDictionaryForColumn(store);\n+    this.vectorizedPageIterator.setDictionaryForColumn(dict, allPagesDictEncoded);\n+    advance();\n+    return dict;\n+  }\n+\n+  private void advance() {\n+    if (valuesRead >= advanceNextPageCount) {\n+      // A parquet page may be empty i.e. contains no values\n+      while (!vectorizedPageIterator.hasNext()) {\n+        DataPage page = columnPageReader.readPage();\n+        if (page != null) {\n+          vectorizedPageIterator.setPage(page);\n+          this.advanceNextPageCount += vectorizedPageIterator.currentPageCount();\n+        } else {\n+          return;\n+        }\n+      }\n+    }\n+  }\n+\n+  public boolean hasNext() {\n+    return valuesRead < totalValuesCount;\n+  }\n+\n+  /**\n+   * Method for reading a batch of non-decimal numeric data types (INT32, INT64, FLOAT, DOUBLE, DATE, TIMESTAMP)\n+   */\n+  public void nextBatchIntegers(FieldVector fieldVector, int typeWidth, NullabilityHolder holder) {\n+    int rowsReadSoFar = 0;\n+    while (rowsReadSoFar < batchSize && hasNext()) {\n+      advance();\n+      int rowsInThisBatch = vectorizedPageIterator.nextBatchIntegers(fieldVector, batchSize - rowsReadSoFar,\n+          rowsReadSoFar, typeWidth, holder);\n+      rowsReadSoFar += rowsInThisBatch;\n+      this.valuesRead += rowsInThisBatch;\n+      fieldVector.setValueCount(rowsReadSoFar);\n+    }\n+  }\n+\n+  /**\n+   * Method for reading a batch of non-decimal numeric data types (INT32, INT64, FLOAT, DOUBLE, DATE, TIMESTAMP)\n+   */\n+  public void nextBatchDictionaryIds(IntVector vector, NullabilityHolder holder) {\n+    int rowsReadSoFar = 0;\n+    while (rowsReadSoFar < batchSize && hasNext()) {\n+      advance();\n+      int rowsInThisBatch = vectorizedPageIterator.nextBatchDictionaryIds(vector, batchSize - rowsReadSoFar,\n+          rowsReadSoFar, holder);\n+      rowsReadSoFar += rowsInThisBatch;\n+      this.valuesRead += rowsInThisBatch;\n+      vector.setValueCount(rowsReadSoFar);\n+    }\n+  }\n+\n+  /**\n+   * Method for reading a batch of non-decimal numeric data types (INT32, INT64, FLOAT, DOUBLE, DATE, TIMESTAMP)\n+   */\n+  public void nextBatchLongs(FieldVector fieldVector, int typeWidth, NullabilityHolder holder) {\n+    int rowsReadSoFar = 0;\n+    while (rowsReadSoFar < batchSize && hasNext()) {\n+      advance();\n+      int rowsInThisBatch = vectorizedPageIterator.nextBatchLongs(fieldVector, batchSize - rowsReadSoFar,\n+          rowsReadSoFar, typeWidth, holder);\n+      rowsReadSoFar += rowsInThisBatch;\n+      this.valuesRead += rowsInThisBatch;\n+      fieldVector.setValueCount(rowsReadSoFar);\n+    }\n+  }\n+\n+  /**\n+   * Method for reading a batch of non-decimal numeric data types (INT32, INT64, FLOAT, DOUBLE, DATE, TIMESTAMP)\n+   */\n+  public void nextBatchFloats(FieldVector fieldVector, int typeWidth, NullabilityHolder holder) {\n+    int rowsReadSoFar = 0;\n+    while (rowsReadSoFar < batchSize && hasNext()) {\n+      advance();\n+      int rowsInThisBatch = vectorizedPageIterator.nextBatchFloats(fieldVector, batchSize - rowsReadSoFar,\n+          rowsReadSoFar, typeWidth, holder);\n+      rowsReadSoFar += rowsInThisBatch;\n+      this.valuesRead += rowsInThisBatch;\n+      fieldVector.setValueCount(rowsReadSoFar);\n+    }\n+  }\n+\n+  /**\n+   * Method for reading a batch of non-decimal numeric data types (INT32, INT64, FLOAT, DOUBLE, DATE, TIMESTAMP)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 152}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDEwNjIx", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360010621", "createdAt": "2020-02-18T00:13:08Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDoxMzowOFrOFqx_8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDoxMzowOFrOFqx_8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwMzY5OQ==", "bodyText": "Style: we don't typically use final for method arguments, or newlines between each one. We do treat method arguments as final, though.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380403699", "createdAt": "2020-02-18T00:13:08Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,408 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.parquet.column.Dictionary;\n+\n+public class VectorizedDictionaryEncodedParquetValuesReader extends BaseVectorizedParquetValuesReader {\n+\n+  public VectorizedDictionaryEncodedParquetValuesReader(int maxDefLevel, boolean setValidityVector) {\n+    super(maxDefLevel, setValidityVector);\n+  }\n+\n+  // Used for reading dictionary ids in a vectorized fashion. Unlike other methods, this doesn't\n+  // check definition level.\n+  void readBatchOfDictionaryIds(\n+      final IntVector intVector,\n+      final int numValsInVector,\n+      final int numValuesToRead,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDEyNTc1", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360012575", "createdAt": "2020-02-18T00:24:04Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDoyNDowNFrOFqyGUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDoyNDowNFrOFqyGUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwNTMzMQ==", "bodyText": "None of the methods in this class should be checking definition levels, right? Those are checked by the definition level reader that is used to call these methods. If that's right, then could you add that to class-level Javadoc? I think explaining how this class is used would be really useful for future maintenance.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380405331", "createdAt": "2020-02-18T00:24:04Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,408 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.parquet.column.Dictionary;\n+\n+public class VectorizedDictionaryEncodedParquetValuesReader extends BaseVectorizedParquetValuesReader {\n+\n+  public VectorizedDictionaryEncodedParquetValuesReader(int maxDefLevel, boolean setValidityVector) {\n+    super(maxDefLevel, setValidityVector);\n+  }\n+\n+  // Used for reading dictionary ids in a vectorized fashion. Unlike other methods, this doesn't\n+  // check definition level.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDE0MjEy", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360014212", "createdAt": "2020-02-18T00:32:28Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDozMjoyOFrOFqyL0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwMDozMjoyOFrOFqyL0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwNjczOQ==", "bodyText": "Does this if affect performance? Seems like we could get this done by using a different method, like readBatchOfDictionaryEncodedLongs(vec, idx, size, dict, nullabilityHolder) vs readBatchOfDictionaryEncodedLongs(vec, idx, size, dict)", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380406739", "createdAt": "2020-02-18T00:32:28Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,408 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.parquet.column.Dictionary;\n+\n+public class VectorizedDictionaryEncodedParquetValuesReader extends BaseVectorizedParquetValuesReader {\n+\n+  public VectorizedDictionaryEncodedParquetValuesReader(int maxDefLevel, boolean setValidityVector) {\n+    super(maxDefLevel, setValidityVector);\n+  }\n+\n+  // Used for reading dictionary ids in a vectorized fashion. Unlike other methods, this doesn't\n+  // check definition level.\n+  void readBatchOfDictionaryIds(\n+      final IntVector intVector,\n+      final int numValsInVector,\n+      final int numValuesToRead,\n+      NullabilityHolder nullabilityHolder) {\n+    int left = numValuesToRead;\n+    int idx = numValsInVector;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, currentValue);\n+            nullabilityHolder.setNotNull(idx);\n+            idx++;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            intVector.set(idx, packedValuesBuffer[packedValuesBufferIdx]);\n+            nullabilityHolder.setNotNull(idx);\n+            packedValuesBufferIdx++;\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  void readBatchOfDictionaryEncodedLongs(\n+      FieldVector vector,\n+      int index,\n+      int numValuesToRead,\n+      Dictionary dict,\n+      NullabilityHolder nullabilityHolder) {\n+    int left = numValuesToRead;\n+    int idx = index;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          for (int i = 0; i < numValues; i++) {\n+            vector.getDataBuffer().setLong(idx, dict.decodeToLong(currentValue));\n+            if (setArrowValidityVector) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 91}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwNTIxNzc2", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360521776", "createdAt": "2020-02-18T17:14:16Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxNzoxNDoxNlrOFrLH0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxNzoxNDoxNlrOFrLH0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxNTMxMw==", "bodyText": "It seems strange to me that this is always cast to a VectorizedParquetValuesReader in this class. Why not make the field that type?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380815313", "createdAt": "2020-02-18T17:14:16Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java", "diffHunk": "@@ -0,0 +1,444 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import java.io.IOException;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.BasePageIterator;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.CorruptDeltaByteArrays;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.Encoding;\n+import org.apache.parquet.column.values.RequiresPreviousReader;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+public class VectorizedPageIterator extends BasePageIterator {\n+  private final boolean setArrowValidityVector;\n+\n+  public VectorizedPageIterator(ColumnDescriptor desc, String writerVersion, boolean setValidityVector) {\n+    super(desc, writerVersion);\n+    this.setArrowValidityVector = setValidityVector;\n+  }\n+\n+  private boolean eagerDecodeDictionary;\n+  private ValuesAsBytesReader plainValuesReader = null;\n+  private VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader = null;\n+  private boolean allPagesDictEncoded;\n+\n+  // Dictionary is set per row group\n+  public void setDictionaryForColumn(Dictionary dict, boolean allDictEncoded) {\n+    this.dictionary = dict;\n+    this.allPagesDictEncoded = allDictEncoded;\n+  }\n+\n+  @Override\n+  protected void reset() {\n+    this.page = null;\n+    this.triplesCount = 0;\n+    this.triplesRead = 0;\n+    this.repetitionLevels = null;\n+    this.plainValuesReader = null;\n+    this.vectorizedDefinitionLevelReader = null;\n+    this.hasNext = false;\n+  }\n+\n+  public int currentPageCount() {\n+    return triplesCount;\n+  }\n+\n+  public boolean hasNext() {\n+    return hasNext;\n+  }\n+\n+  /**\n+   * Method for reading a batch of dictionary ids from the dicitonary encoded data pages. Like definition levels,\n+   * dictionary ids in Parquet are RLE/bin-packed encoded as well.\n+   */\n+  public int nextBatchDictionaryIds(\n+      final IntVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    ((VectorizedParquetValuesReader) vectorizedDefinitionLevelReader).readBatchOfDictionaryIds(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 90}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwNTI2OTE3", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360526917", "createdAt": "2020-02-18T17:21:27Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxNzoyMToyN1rOFrLX9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxNzoyMToyN1rOFrLX9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgxOTQ0NQ==", "bodyText": "This would work with a vectorized ValuesReader right?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380819445", "createdAt": "2020-02-18T17:21:27Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/BasePageIterator.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.parquet;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.IOException;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.Encoding;\n+import org.apache.parquet.column.ValuesType;\n+import org.apache.parquet.column.page.DataPage;\n+import org.apache.parquet.column.page.DataPageV1;\n+import org.apache.parquet.column.page.DataPageV2;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.rle.RunLengthBitPackingHybridDecoder;\n+import org.apache.parquet.io.ParquetDecodingException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+public abstract class BasePageIterator {\n+  private static final Logger LOG = LoggerFactory.getLogger(BasePageIterator.class);\n+\n+  protected final ColumnDescriptor desc;\n+  protected final String writerVersion;\n+\n+  // iterator state\n+  protected boolean hasNext = false;\n+  protected int triplesRead = 0;\n+  protected int currentDL = 0;\n+  protected int currentRL = 0;\n+\n+  // page bookkeeping\n+  protected Dictionary dictionary = null;\n+  protected DataPage page = null;\n+  protected int triplesCount = 0;\n+  protected Encoding valueEncoding = null;\n+  protected IntIterator definitionLevels = null;\n+  protected IntIterator repetitionLevels = null;\n+  protected ValuesReader vectorizedDefinitionLevelReader = null;\n+  protected ValuesReader values = null;\n+\n+  protected BasePageIterator(ColumnDescriptor descriptor, String writerVersion) {\n+    this.desc = descriptor;\n+    this.writerVersion = writerVersion;\n+  }\n+\n+  protected abstract void reset();\n+\n+  protected abstract boolean supportsVectorizedReads();\n+\n+  protected abstract IntIterator newNonVectorizedDefinitionLevelReader(ValuesReader dlReader);\n+\n+  protected abstract ValuesReader newVectorizedDefinitionLevelReader(ColumnDescriptor descriptor);\n+\n+  protected abstract void initDataReader(Encoding dataEncoding, ByteBufferInputStream in, int valueCount);\n+\n+  public void setPage(DataPage page) {\n+    Preconditions.checkNotNull(page, \"Cannot read from null page\");\n+    this.page = page;\n+    this.page.accept(new DataPage.Visitor<ValuesReader>() {\n+      @Override\n+      public ValuesReader visit(DataPageV1 dataPageV1) {\n+        initFromPage(dataPageV1);\n+        return null;\n+      }\n+\n+      @Override\n+      public ValuesReader visit(DataPageV2 dataPageV2) {\n+        initFromPage(dataPageV2);\n+        return null;\n+      }\n+    });\n+    this.triplesRead = 0;\n+    this.hasNext = triplesRead < triplesCount;\n+  }\n+\n+  protected void initFromPage(DataPageV1 initPage) {\n+    this.triplesCount = initPage.getValueCount();\n+    ValuesReader dlReader = null;\n+    if (supportsVectorizedReads()) {\n+      this.vectorizedDefinitionLevelReader = newVectorizedDefinitionLevelReader(desc);\n+    } else {\n+      dlReader = initPage.getDlEncoding().getValuesReader(desc, ValuesType.DEFINITION_LEVEL);\n+      this.definitionLevels = newNonVectorizedDefinitionLevelReader(dlReader);\n+    }\n+    ValuesReader rlReader = initPage.getRlEncoding().getValuesReader(desc, ValuesType.REPETITION_LEVEL);\n+    this.repetitionLevels = new PageIterator.ValuesReaderIntIterator(rlReader);\n+    try {\n+      BytesInput bytes = initPage.getBytes();\n+      LOG.debug(\"page size {} bytes and {} records\", bytes.size(), triplesCount);\n+      LOG.debug(\"reading repetition levels at 0\");\n+      ByteBufferInputStream in = bytes.toInputStream();\n+      rlReader.initFromPage(triplesCount, in);\n+      LOG.debug(\"reading definition levels at {}\", in.position());\n+      if (supportsVectorizedReads()) {\n+        this.vectorizedDefinitionLevelReader.initFromPage(triplesCount, in);\n+      } else {\n+        dlReader.initFromPage(triplesCount, in);\n+      }\n+      LOG.debug(\"reading data at {}\", in.position());\n+      initDataReader(initPage.getValueEncoding(), in, initPage.getValueCount());\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"could not read page \" + initPage + \" in col \" + desc, e);\n+    }\n+  }\n+\n+  protected void initFromPage(DataPageV2 initPage) {\n+    this.triplesCount = initPage.getValueCount();\n+    this.repetitionLevels = newRLEIterator(desc.getMaxRepetitionLevel(), initPage.getRepetitionLevels());\n+    if (supportsVectorizedReads()) {\n+      this.vectorizedDefinitionLevelReader = newVectorizedDefinitionLevelReader(desc);\n+    } else {\n+      this.definitionLevels = newRLEIterator(desc.getMaxDefinitionLevel(), initPage.getDefinitionLevels());\n+    }\n+    LOG.debug(\"page data size {} bytes and {} records\", initPage.getData().size(), triplesCount);\n+    try {\n+      initDataReader(initPage.getDataEncoding(), initPage.getData().toInputStream(), triplesCount);\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"could not read page \" + initPage + \" in col \" + desc, e);\n+    }\n+  }\n+\n+  private IntIterator newRLEIterator(int maxLevel, BytesInput bytes) {\n+    try {\n+      if (maxLevel == 0) {\n+        return new NullIntIterator();\n+      }\n+      return new RLEIntIterator(\n+          new RunLengthBitPackingHybridDecoder(\n+              BytesUtils.getWidthFromMaxInt(maxLevel),\n+              bytes.toInputStream()));\n+    } catch (IOException e) {\n+      throw new ParquetDecodingException(\"could not read levels in page for col \" + desc, e);\n+    }\n+  }\n+\n+  public void setDictionary(Dictionary dict) {\n+    this.dictionary = dict;\n+  }\n+\n+  protected abstract static class IntIterator {\n+    abstract int nextInt();\n+  }\n+\n+  static class ValuesReaderIntIterator extends IntIterator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 166}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwNTI4Mjk0", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360528294", "createdAt": "2020-02-18T17:23:32Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxNzoyMzozMlrOFrLcaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxNzoyMzozMlrOFrLcaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgyMDU4NA==", "bodyText": "Couldn't this create a vectorized definition level reader either way? That still exposes nextInteger(), which is all the non-vectorized version needs to call for an IntIterator. Then you could always use the same DL reader and the subclasses can choose how to wrap it (non-vectorized) or call its methods (vectorized).", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380820584", "createdAt": "2020-02-18T17:23:32Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/BasePageIterator.java", "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.parquet;\n+\n+import com.google.common.base.Preconditions;\n+import java.io.IOException;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.Encoding;\n+import org.apache.parquet.column.ValuesType;\n+import org.apache.parquet.column.page.DataPage;\n+import org.apache.parquet.column.page.DataPageV1;\n+import org.apache.parquet.column.page.DataPageV2;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.rle.RunLengthBitPackingHybridDecoder;\n+import org.apache.parquet.io.ParquetDecodingException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+public abstract class BasePageIterator {\n+  private static final Logger LOG = LoggerFactory.getLogger(BasePageIterator.class);\n+\n+  protected final ColumnDescriptor desc;\n+  protected final String writerVersion;\n+\n+  // iterator state\n+  protected boolean hasNext = false;\n+  protected int triplesRead = 0;\n+  protected int currentDL = 0;\n+  protected int currentRL = 0;\n+\n+  // page bookkeeping\n+  protected Dictionary dictionary = null;\n+  protected DataPage page = null;\n+  protected int triplesCount = 0;\n+  protected Encoding valueEncoding = null;\n+  protected IntIterator definitionLevels = null;\n+  protected IntIterator repetitionLevels = null;\n+  protected ValuesReader vectorizedDefinitionLevelReader = null;\n+  protected ValuesReader values = null;\n+\n+  protected BasePageIterator(ColumnDescriptor descriptor, String writerVersion) {\n+    this.desc = descriptor;\n+    this.writerVersion = writerVersion;\n+  }\n+\n+  protected abstract void reset();\n+\n+  protected abstract boolean supportsVectorizedReads();\n+\n+  protected abstract IntIterator newNonVectorizedDefinitionLevelReader(ValuesReader dlReader);\n+\n+  protected abstract ValuesReader newVectorizedDefinitionLevelReader(ColumnDescriptor descriptor);\n+\n+  protected abstract void initDataReader(Encoding dataEncoding, ByteBufferInputStream in, int valueCount);\n+\n+  public void setPage(DataPage page) {\n+    Preconditions.checkNotNull(page, \"Cannot read from null page\");\n+    this.page = page;\n+    this.page.accept(new DataPage.Visitor<ValuesReader>() {\n+      @Override\n+      public ValuesReader visit(DataPageV1 dataPageV1) {\n+        initFromPage(dataPageV1);\n+        return null;\n+      }\n+\n+      @Override\n+      public ValuesReader visit(DataPageV2 dataPageV2) {\n+        initFromPage(dataPageV2);\n+        return null;\n+      }\n+    });\n+    this.triplesRead = 0;\n+    this.hasNext = triplesRead < triplesCount;\n+  }\n+\n+  protected void initFromPage(DataPageV1 initPage) {\n+    this.triplesCount = initPage.getValueCount();\n+    ValuesReader dlReader = null;\n+    if (supportsVectorizedReads()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 101}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwNTI4NjM4", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360528638", "createdAt": "2020-02-18T17:24:00Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxNzoyNDowMFrOFrLdeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxNzoyNDowMFrOFrLdeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgyMDg1OA==", "bodyText": "I think we can remove these comment lines.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380820858", "createdAt": "2020-02-18T17:24:00Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java", "diffHunk": "@@ -255,4 +260,19 @@ public static boolean hasNonDictionaryPages(ColumnChunkMetaData meta) {\n       return true;\n     }\n   }\n+\n+  public static Dictionary readDictionary(ColumnDescriptor desc, PageReader pageSource) {\n+    DictionaryPage dictionaryPage = pageSource.readDictionaryPage();\n+    if (dictionaryPage != null) {\n+      try {\n+        return dictionaryPage.getEncoding().initDictionary(desc, dictionaryPage);\n+//        if (converter.hasDictionarySupport()) {\n+//          converter.setDictionary(dictionary);\n+//        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwNTMyNjAx", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360532601", "createdAt": "2020-02-18T17:29:36Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxNzoyOTozN1rOFrLp5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxNzoyOTozN1rOFrLp5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgyNDAzOQ==", "bodyText": "This was correct before; \"setup\" is a noun and \"set up\" is the verb form.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380824039", "createdAt": "2020-02-18T17:29:37Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/VectorizedReader.java", "diffHunk": "@@ -44,8 +44,13 @@\n   void setRowGroupInfo(PageReadStore pages, Map<ColumnPath, ColumnChunkMetaData> metadata);\n \n   /**\n-   * Set up the reader to reuse the underlying containers used for storing batches\n+   * Setup the reader to reuse the underlying containers used for storing batches", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwNTMzNjI5", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-360533629", "createdAt": "2020-02-18T17:31:08Z", "commit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxNzozMTowOFrOFrLtKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQxNzozMTowOFrOFrLtKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDgyNDg3Mw==", "bodyText": "Why does this class have both dictionary and non-dictionary methods?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r380824873", "createdAt": "2020-02-18T17:31:08Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,912 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.column.Dictionary;\n+\n+public final class VectorizedParquetValuesReader extends BaseVectorizedParquetValuesReader {\n+\n+  public VectorizedParquetValuesReader(int bitWidth, int maxDefLevel, boolean setArrowValidityVector) {\n+    super(bitWidth, maxDefLevel, setArrowValidityVector);\n+  }\n+\n+  public void readBatchOfDictionaryIds(\n+      final IntVector vector,\n+      final int numValsInVector,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readBatchOfDictionaryIds(vector, idx, numValues, nullabilityHolder);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, vector.getValidityBuffer());\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.set(idx, dictionaryEncodedValuesReader.readInteger());\n+              if (setArrowValidityVector) {\n+                BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+              } else {\n+                nullabilityHolder.setNotNull(idx);\n+              }\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfLongs(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              numValues);\n+          bufferIdx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setLong(bufferIdx * typeWidth, valuesReader.readLong());\n+              if (setArrowValidityVector) {\n+                BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), bufferIdx);\n+              } else {\n+                nullabilityHolder.setNotNull(bufferIdx);\n+              }\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedLongs(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int numValues = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedLongs(vector,\n+                idx, numValues, dict, nullabilityHolder);\n+          } else {\n+            setNulls(nullabilityHolder, idx, numValues, validityBuffer);\n+          }\n+          idx += numValues;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < numValues; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setLong(idx, dict.decodeToLong(dictionaryEncodedValuesReader.readInteger()));\n+              if (setArrowValidityVector) {\n+                BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+              } else {\n+                nullabilityHolder.setNotNull(idx);\n+              }\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= numValues;\n+      currentCount -= numValues;\n+    }\n+  }\n+\n+  public void readBatchOfIntegers(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setInt(bufferIdx * typeWidth, valuesReader.readInteger());\n+              if (setArrowValidityVector) {\n+                BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), bufferIdx);\n+              } else {\n+                nullabilityHolder.setNotNull(bufferIdx);\n+              }\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedIntegers(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedIntegers(vector, idx,\n+                num, dict, nullabilityHolder);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setInt(idx, dict.decodeToInt(dictionaryEncodedValuesReader.readInteger()));\n+              if (setArrowValidityVector) {\n+                BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+              } else {\n+                nullabilityHolder.setNotNull(idx);\n+              }\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFloats(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder, ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setFloat(bufferIdx * typeWidth, valuesReader.readFloat());\n+              if (setArrowValidityVector) {\n+                BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), bufferIdx);\n+              } else {\n+                nullabilityHolder.setNotNull(bufferIdx);\n+              }\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFloats(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      ArrowBuf validityBuffer = vector.getValidityBuffer();\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedFloats(vector, idx,\n+                num, dict, nullabilityHolder);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, validityBuffer);\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setFloat(idx, dict.decodeToFloat(dictionaryEncodedValuesReader.readInteger()));\n+              if (setArrowValidityVector) {\n+                BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+              } else {\n+                nullabilityHolder.setNotNull(idx);\n+              }\n+            } else {\n+              setNull(nullabilityHolder, idx, validityBuffer);\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDoubles(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          setNextNValuesInVector(\n+              typeWidth,\n+              nullabilityHolder,\n+              valuesReader,\n+              bufferIdx,\n+              vector,\n+              num);\n+          bufferIdx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setDouble(bufferIdx * typeWidth, valuesReader.readDouble());\n+              if (setArrowValidityVector) {\n+                BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(),  bufferIdx);\n+              } else {\n+                nullabilityHolder.setNotNull(bufferIdx);\n+              }\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedDoubles(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedDoubles(vector, idx,\n+                num, dict, nullabilityHolder);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              vector.getDataBuffer().setDouble(idx, dict.decodeToDouble(dictionaryEncodedValuesReader.readInteger()));\n+              if (setArrowValidityVector) {\n+                BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+              } else {\n+                nullabilityHolder.setNotNull(idx);\n+              }\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedWidthBinary(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx, nullabilityHolder);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              setBinaryInVector((VarBinaryVector) vector, typeWidth, valuesReader, bufferIdx, nullabilityHolder);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFixedWidthBinary(\n+      final FieldVector vector,\n+      final int numValsInVector,\n+      final int typeWidth,\n+      final int batchSize,\n+      NullabilityHolder nullabilityHolder,\n+      VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader,\n+      Dictionary dict) {\n+    int idx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedFixedWidthBinary(vector, typeWidth, idx,\n+                num, dict, nullabilityHolder);\n+          } else {\n+            setNulls(nullabilityHolder, idx, num, vector.getValidityBuffer());\n+          }\n+          idx += num;\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; i++) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              ByteBuffer buffer = dict.decodeToBinary(dictionaryEncodedValuesReader.readInteger()).toByteBuffer();\n+              vector.getDataBuffer().setBytes(idx * typeWidth, buffer.array(),\n+                  buffer.position() + buffer.arrayOffset(), buffer.limit() - buffer.position());\n+              if (setArrowValidityVector) {\n+                BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);\n+              } else {\n+                nullabilityHolder.setNotNull(idx);\n+              }\n+            } else {\n+              setNull(nullabilityHolder, idx, vector.getValidityBuffer());\n+            }\n+            idx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfFixedLengthDecimals(\n+      final FieldVector vector, final int numValsInVector,\n+      final int typeWidth, final int batchSize, NullabilityHolder nullabilityHolder,\n+      ValuesAsBytesReader valuesReader) {\n+    int bufferIdx = numValsInVector;\n+    int left = batchSize;\n+    while (left > 0) {\n+      if (this.currentCount == 0) {\n+        this.readNextGroup();\n+      }\n+      int num = Math.min(left, this.currentCount);\n+      byte[] byteArray = new byte[DecimalVector.TYPE_WIDTH];\n+      switch (mode) {\n+        case RLE:\n+          if (currentValue == maxDefLevel) {\n+            for (int i = 0; i < num; i++) {\n+              valuesReader.getBuffer(typeWidth).get(byteArray, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(bufferIdx, byteArray);\n+              nullabilityHolder.setNotNull(bufferIdx);\n+              bufferIdx++;\n+            }\n+          } else {\n+            setNulls(nullabilityHolder, bufferIdx, num, vector.getValidityBuffer());\n+            bufferIdx += num;\n+          }\n+          break;\n+        case PACKED:\n+          for (int i = 0; i < num; ++i) {\n+            if (packedValuesBuffer[packedValuesBufferIdx++] == maxDefLevel) {\n+              valuesReader.getBuffer(typeWidth).get(byteArray, DecimalVector.TYPE_WIDTH - typeWidth, typeWidth);\n+              ((DecimalVector) vector).setBigEndian(bufferIdx, byteArray);\n+              nullabilityHolder.setNotNull(bufferIdx);\n+            } else {\n+              setNull(nullabilityHolder, bufferIdx, vector.getValidityBuffer());\n+            }\n+            bufferIdx++;\n+          }\n+          break;\n+      }\n+      left -= num;\n+      currentCount -= num;\n+    }\n+  }\n+\n+  public void readBatchOfDictionaryEncodedFixedLengthDecimals(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60ef0aa1b619da693c820057a20ccf81a00e9ac1"}, "originalPosition": 570}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6aa61ad81ac4de8bd2a98c4ac8b5012df70166b4", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/6aa61ad81ac4de8bd2a98c4ac8b5012df70166b4", "committedDate": "2020-02-19T00:11:08Z", "message": "Resolve conflicts with master branch"}, "afterCommit": {"oid": "f5eb4767906ec4a3f6fc54b4dcdc053a9064e191", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/f5eb4767906ec4a3f6fc54b4dcdc053a9064e191", "committedDate": "2020-02-19T00:13:45Z", "message": "Resolve conflicts with master branch"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e2f15529933533b487ef9f48d68c366b65a0f76e", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/e2f15529933533b487ef9f48d68c366b65a0f76e", "committedDate": "2020-02-21T09:04:33Z", "message": "Arrow changes for supporting vectorized reads\n\nCo-authored-by: gautamkowshik@gmail.com\nCo-authored-by: anjalinorwood@gmail.com"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "96189d83b21f8be397fa73a3775896b606e14cd7", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/96189d83b21f8be397fa73a3775896b606e14cd7", "committedDate": "2020-02-21T09:04:33Z", "message": "Improve performance for null data. Address various code review comments. General code cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b5854068d066a50d14035ae45543ad39c7d6e3ef", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/b5854068d066a50d14035ae45543ad39c7d6e3ef", "committedDate": "2020-02-21T09:04:33Z", "message": "Add missing file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6d030e4292fb245d615d69b918736ba63885045a", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/6d030e4292fb245d615d69b918736ba63885045a", "committedDate": "2020-02-21T09:05:44Z", "message": "Refactoring and cleanup. Address code review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eabf5729190d6f7e37661381eddf38c3a7336d5a", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/eabf5729190d6f7e37661381eddf38c3a7336d5a", "committedDate": "2020-02-21T09:57:41Z", "message": "Add support for timestamp_millis.\nRebase with master\nRevert to Arrow 0.14.1 since 0.15.0 makes vectors non-extensible."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f5eb4767906ec4a3f6fc54b4dcdc053a9064e191", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/f5eb4767906ec4a3f6fc54b4dcdc053a9064e191", "committedDate": "2020-02-19T00:13:45Z", "message": "Resolve conflicts with master branch"}, "afterCommit": {"oid": "eabf5729190d6f7e37661381eddf38c3a7336d5a", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/eabf5729190d6f7e37661381eddf38c3a7336d5a", "committedDate": "2020-02-21T09:57:41Z", "message": "Add support for timestamp_millis.\nRebase with master\nRevert to Arrow 0.14.1 since 0.15.0 makes vectors non-extensible."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "43c97ed9d2cf83c1520230b058396df5fd585d03", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/43c97ed9d2cf83c1520230b058396df5fd585d03", "committedDate": "2020-02-21T18:44:23Z", "message": "Update LICENSE file to reflect code adapted from Apache Spark"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYyOTcwNDUy", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-362970452", "createdAt": "2020-02-21T23:10:55Z", "commit": {"oid": "43c97ed9d2cf83c1520230b058396df5fd585d03"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMzoxMDo1NVrOFtHOlw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMzoxMDo1NVrOFtHOlw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg0ODY2Mw==", "bodyText": "Nit: Using tr tags seems odd here, outside of a table.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r382848663", "createdAt": "2020-02-21T23:10:55Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/BaseVectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.column.values.bitpacking.BytePacker;\n+import org.apache.parquet.column.values.bitpacking.Packer;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+/**\n+ * A values reader for Parquet's run-length encoded data that reads column data in batches instead of one value at a\n+ * time. This is based off of the VectorizedRleValuesReader class in Apache Spark with these changes:\n+ * <p>\n+ * <tr>Writes batches of values retrieved to Arrow vectors</tr>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43c97ed9d2cf83c1520230b058396df5fd585d03"}, "originalPosition": 36}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYyOTcyMzMz", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-362972333", "createdAt": "2020-02-21T23:17:46Z", "commit": {"oid": "43c97ed9d2cf83c1520230b058396df5fd585d03"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMzoxNzo0NlrOFtHUlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yMVQyMzoxNzo0NlrOFtHUlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Mjg1MDE5Nw==", "bodyText": "Should this be in BasePageIterator and not here?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r382850197", "createdAt": "2020-02-21T23:17:46Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java", "diffHunk": "@@ -0,0 +1,484 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import java.io.IOException;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.BasePageIterator;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.CorruptDeltaByteArrays;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.column.Encoding;\n+import org.apache.parquet.column.page.DataPageV1;\n+import org.apache.parquet.column.page.DataPageV2;\n+import org.apache.parquet.column.values.RequiresPreviousReader;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+public class VectorizedPageIterator extends BasePageIterator {\n+  private final boolean setArrowValidityVector;\n+\n+  public VectorizedPageIterator(ColumnDescriptor desc, String writerVersion, boolean setValidityVector) {\n+    super(desc, writerVersion);\n+    this.setArrowValidityVector = setValidityVector;\n+  }\n+\n+  private boolean eagerDecodeDictionary;\n+  private ValuesAsBytesReader plainValuesReader = null;\n+  private VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader = null;\n+  private boolean allPagesDictEncoded;\n+  private VectorizedParquetValuesReader vectorizedDefinitionLevelReader;\n+\n+  // Dictionary is set per row group\n+  public void setDictionaryForColumn(Dictionary dict, boolean allDictEncoded) {\n+    this.dictionary = dict;\n+    this.allPagesDictEncoded = allDictEncoded;\n+  }\n+\n+  @Override\n+  protected void reset() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43c97ed9d2cf83c1520230b058396df5fd585d03"}, "originalPosition": 63}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4867edde618ccd05be9ec3ff0a76404b7fc2da36", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/4867edde618ccd05be9ec3ff0a76404b7fc2da36", "committedDate": "2020-02-22T08:35:52Z", "message": "Refactor ColumnIterator. Cleanup"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYzODU2NDMz", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-363856433", "createdAt": "2020-02-25T03:56:22Z", "commit": {"oid": "4867edde618ccd05be9ec3ff0a76404b7fc2da36"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQwMzo1NjoyM1rOFt37TQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNVQwMzo1NjoyM1rOFt37TQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MzY0NjU0MQ==", "bodyText": "maybe unnecessary empty line.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r383646541", "createdAt": "2020-02-25T03:56:23Z", "author": {"login": "XiaokunDing"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ValuesAsBytesReader.java", "diffHunk": "@@ -76,12 +91,29 @@ public final boolean readBoolean() {\n     return value;\n   }\n \n+  /**\n+   *\n+   * @return 1 if true, 0 otherwise\n+   */\n+  public final int readBooleanAsInt() {\n+    if (bitOffset == 0) {\n+      currentByte = getByte();\n+    }\n+    int value = (currentByte & (1 << bitOffset)) >> bitOffset;\n+    bitOffset += 1;\n+    if (bitOffset == 8) {\n+      bitOffset = 0;\n+    }\n+    return value;\n+  }\n+\n   private byte getByte() {\n     try {\n       return (byte) valuesInputStream.read();\n     } catch (IOException e) {\n       throw new ParquetDecodingException(\"Failed to read a byte\", e);\n     }\n   }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4867edde618ccd05be9ec3ff0a76404b7fc2da36"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY4MTg4ODE5", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-368188819", "createdAt": "2020-03-03T17:42:54Z", "commit": {"oid": "4867edde618ccd05be9ec3ff0a76404b7fc2da36"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QxNzo0Mjo1NFrOFxP8qg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QxNzo0Mjo1NFrOFxP8qg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzE4NTgzNA==", "bodyText": "Looks like this will only use eager decoding when not all pages are dictionary encoded.\nWhat about cases where there is no need to keep the dictionary around, like int columns?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r387185834", "createdAt": "2020-03-03T17:42:54Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java", "diffHunk": "@@ -0,0 +1,469 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import java.io.IOException;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.BasePageIterator;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.CorruptDeltaByteArrays;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Encoding;\n+import org.apache.parquet.column.page.DataPageV1;\n+import org.apache.parquet.column.page.DataPageV2;\n+import org.apache.parquet.column.values.RequiresPreviousReader;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+public class VectorizedPageIterator extends BasePageIterator {\n+  private final boolean setArrowValidityVector;\n+\n+  public VectorizedPageIterator(ColumnDescriptor desc, String writerVersion, boolean setValidityVector) {\n+    super(desc, writerVersion);\n+    this.setArrowValidityVector = setValidityVector;\n+  }\n+\n+  private boolean eagerDecodeDictionary;\n+  private ValuesAsBytesReader plainValuesReader = null;\n+  private VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader = null;\n+  private boolean allPagesDictEncoded;\n+  private VectorizedParquetValuesReader vectorizedDefinitionLevelReader;\n+\n+  public void setAllPagesDictEncoded(boolean allDictEncoded) {\n+    this.allPagesDictEncoded = allDictEncoded;\n+  }\n+\n+  @Override\n+  protected void reset() {\n+    super.reset();\n+    this.plainValuesReader = null;\n+    this.vectorizedDefinitionLevelReader = null;\n+  }\n+\n+  /**\n+   * Method for reading a batch of dictionary ids from the dicitonary encoded data pages. Like definition levels,\n+   * dictionary ids in Parquet are RLE/bin-packed encoded as well.\n+   */\n+  public int nextBatchDictionaryIds(\n+      final IntVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    vectorizedDefinitionLevelReader.readBatchOfDictionaryIds(\n+        vector,\n+        numValsInVector,\n+        actualBatchSize,\n+        holder,\n+        dictionaryEncodedValuesReader);\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of INT32 data type\n+   */\n+  public int nextBatchIntegers(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedIntegers(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchOfIntegers(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of INT64 data type\n+   */\n+  public int nextBatchLongs(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedLongs(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchOfLongs(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of TIMESTAMP_MILLIS data type. In iceberg, TIMESTAMP\n+   * is always represented in micro-seconds. So we multiply values stored in millis with 1000\n+   * before writing them to the vector.\n+   */\n+  public int nextBatchTimestampMillis(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedTimestampMillis(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchOfTimestampMillis(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of FLOAT data type.\n+   */\n+  public int nextBatchFloats(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedFloats(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchOfFloats(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of DOUBLE data type\n+   */\n+  public int nextBatchDoubles(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedDoubles(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchOfDoubles(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  private int getActualBatchSize(int expectedBatchSize) {\n+    return Math.min(expectedBatchSize, triplesCount - triplesRead);\n+  }\n+\n+  /**\n+   * Method for reading a batch of decimals backed by INT32 and INT64 parquet data types. Since Arrow stores all\n+   * decimals in 16 bytes, byte arrays are appropriately padded before being written to Arrow data buffers.\n+   */\n+  public int nextBatchIntLongBackedDecimal(\n+      final FieldVector vector, final int expectedBatchSize, final int numValsInVector,\n+      final int typeWidth, NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader\n+          .readBatchOfDictionaryEncodedIntLongBackedDecimals(\n+              vector,\n+              numValsInVector,\n+              typeWidth,\n+              actualBatchSize,\n+              nullabilityHolder,\n+              dictionaryEncodedValuesReader,\n+              dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchOfIntLongBackedDecimals(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of decimals backed by fixed length byte array parquet data type. Arrow stores all\n+   * decimals in 16 bytes. This method provides the necessary padding to the decimals read. Moreover, Arrow interprets\n+   * the decimals in Arrow buffer as little endian. Parquet stores fixed length decimals as big endian. So, this method\n+   * uses {@link DecimalVector#setBigEndian(int, byte[])} method so that the data in Arrow vector is indeed little\n+   * endian.\n+   */\n+  public int nextBatchFixedLengthDecimal(\n+      final FieldVector vector, final int expectedBatchSize, final int numValsInVector,\n+      final int typeWidth, NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedFixedLengthDecimals(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchOfFixedLengthDecimals(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of variable width data type (ENUM, JSON, UTF8, BSON).\n+   */\n+  public int nextBatchVarWidthType(\n+      final FieldVector vector,\n+      final int expectedBatchSize,\n+      final int numValsInVector,\n+      NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedVarWidth(\n+          vector,\n+          numValsInVector,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchVarWidth(\n+          vector,\n+          numValsInVector,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading batches of fixed width binary type (e.g. BYTE[7]). Spark does not support fixed width binary\n+   * data type. To work around this limitation, the data is read as fixed width binary from parquet and stored in a\n+   * {@link VarBinaryVector} in Arrow.\n+   */\n+  public int nextBatchFixedWidthBinary(\n+      final FieldVector vector, final int expectedBatchSize, final int numValsInVector,\n+      final int typeWidth, NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedFixedWidthBinary(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchOfFixedWidthBinary(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading batches of booleans.\n+   */\n+  public int nextBatchBoolean(\n+      final FieldVector vector,\n+      final int expectedBatchSize,\n+      final int numValsInVector,\n+      NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    vectorizedDefinitionLevelReader\n+        .readBatchOfBooleans(vector, numValsInVector, actualBatchSize,\n+            nullabilityHolder, plainValuesReader);\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  @Override\n+  protected void initDataReader(Encoding dataEncoding, ByteBufferInputStream in, int valueCount) {\n+    ValuesReader previousReader = plainValuesReader;\n+    this.eagerDecodeDictionary = dataEncoding.usesDictionary() && dictionary != null && !allPagesDictEncoded;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4867edde618ccd05be9ec3ff0a76404b7fc2da36"}, "originalPosition": 428}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY4MTg5OTQx", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-368189941", "createdAt": "2020-03-03T17:44:32Z", "commit": {"oid": "4867edde618ccd05be9ec3ff0a76404b7fc2da36"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QxNzo0NDozMlrOFxQANg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QxNzo0NDozMlrOFxQANg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzE4Njc0Mg==", "bodyText": "Can you rename this to VectorizedParquetDefinitionLevelReader? I'd like to capture that this class is a ValuesReader that reads definition levels and then uses another reader for values.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r387186742", "createdAt": "2020-03-03T17:44:32Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetValuesReader.java", "diffHunk": "@@ -0,0 +1,1005 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.nio.ByteBuffer;\n+import org.apache.arrow.vector.BaseVariableWidthVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.BitVectorHelper;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.column.Dictionary;\n+\n+public final class VectorizedParquetValuesReader extends BaseVectorizedParquetValuesReader {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4867edde618ccd05be9ec3ff0a76404b7fc2da36"}, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY4MTk3MjUx", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-368197251", "createdAt": "2020-03-03T17:54:57Z", "commit": {"oid": "4867edde618ccd05be9ec3ff0a76404b7fc2da36"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QxNzo1NDo1N1rOFxQXLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QxNzo1NDo1N1rOFxQXLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzE5MjYyMw==", "bodyText": "Why is this abstract?", "url": "https://github.com/apache/iceberg/pull/723#discussion_r387192623", "createdAt": "2020-03-03T17:54:57Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/PageIterator.java", "diffHunk": "@@ -23,26 +23,18 @@\n import java.io.IOException;\n import org.apache.parquet.CorruptDeltaByteArrays;\n import org.apache.parquet.bytes.ByteBufferInputStream;\n-import org.apache.parquet.bytes.BytesInput;\n-import org.apache.parquet.bytes.BytesUtils;\n import org.apache.parquet.column.ColumnDescriptor;\n-import org.apache.parquet.column.Dictionary;\n import org.apache.parquet.column.Encoding;\n import org.apache.parquet.column.ValuesType;\n import org.apache.parquet.column.page.DataPage;\n import org.apache.parquet.column.page.DataPageV1;\n import org.apache.parquet.column.page.DataPageV2;\n import org.apache.parquet.column.values.RequiresPreviousReader;\n import org.apache.parquet.column.values.ValuesReader;\n-import org.apache.parquet.column.values.rle.RunLengthBitPackingHybridDecoder;\n import org.apache.parquet.io.ParquetDecodingException;\n import org.apache.parquet.io.api.Binary;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-abstract class PageIterator<T> implements TripleIterator<T> {\n-  private static final Logger LOG = LoggerFactory.getLogger(PageIterator.class);\n \n+abstract class PageIterator<T> extends BasePageIterator implements TripleIterator<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4867edde618ccd05be9ec3ff0a76404b7fc2da36"}, "originalPosition": 24}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "178210b956979468a98682edcb223c322307a4a9", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/178210b956979468a98682edcb223c322307a4a9", "committedDate": "2020-03-03T22:49:05Z", "message": "Remove .patch files. Address code review comments."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "beae31dacdd04f727d5423193f71c08280441654", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/beae31dacdd04f727d5423193f71c08280441654", "committedDate": "2020-03-03T22:26:32Z", "message": "Remove .patch files. Address code review comments."}, "afterCommit": {"oid": "178210b956979468a98682edcb223c322307a4a9", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/178210b956979468a98682edcb223c322307a4a9", "committedDate": "2020-03-03T22:49:05Z", "message": "Remove .patch files. Address code review comments."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0321d3547e6f7d810d9f9a7fc993b2c65f6b17b7", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/0321d3547e6f7d810d9f9a7fc993b2c65f6b17b7", "committedDate": "2020-03-03T23:01:49Z", "message": "Fix checkstyle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "466e951a32d8ed8c3f6060220e230641bd95cd11", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/466e951a32d8ed8c3f6060220e230641bd95cd11", "committedDate": "2020-03-04T00:11:36Z", "message": "Remove files"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY4NDE3NTQx", "url": "https://github.com/apache/iceberg/pull/723#pullrequestreview-368417541", "createdAt": "2020-03-04T00:12:16Z", "commit": {"oid": "0321d3547e6f7d810d9f9a7fc993b2c65f6b17b7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMDoxMjoxNlrOFxbOVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMDoxMjoxNlrOFxbOVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM3MDU4Mw==", "bodyText": "@samarthjain, in a follow-up, let's update the logic here to something other than just eagerly decoding integers. We probably want to do this for 4-byte floats, for example.", "url": "https://github.com/apache/iceberg/pull/723#discussion_r387370583", "createdAt": "2020-03-04T00:12:16Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java", "diffHunk": "@@ -0,0 +1,471 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.arrow.vectorized.parquet;\n+\n+import java.io.IOException;\n+import org.apache.arrow.vector.DecimalVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.parquet.BasePageIterator;\n+import org.apache.iceberg.parquet.ParquetUtil;\n+import org.apache.iceberg.parquet.ValuesAsBytesReader;\n+import org.apache.parquet.CorruptDeltaByteArrays;\n+import org.apache.parquet.bytes.ByteBufferInputStream;\n+import org.apache.parquet.bytes.BytesUtils;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Encoding;\n+import org.apache.parquet.column.page.DataPageV1;\n+import org.apache.parquet.column.page.DataPageV2;\n+import org.apache.parquet.column.values.RequiresPreviousReader;\n+import org.apache.parquet.column.values.ValuesReader;\n+import org.apache.parquet.io.ParquetDecodingException;\n+\n+public class VectorizedPageIterator extends BasePageIterator {\n+  private final boolean setArrowValidityVector;\n+\n+  public VectorizedPageIterator(ColumnDescriptor desc, String writerVersion, boolean setValidityVector) {\n+    super(desc, writerVersion);\n+    this.setArrowValidityVector = setValidityVector;\n+  }\n+\n+  private boolean eagerDecodeDictionary;\n+  private ValuesAsBytesReader plainValuesReader = null;\n+  private VectorizedDictionaryEncodedParquetValuesReader dictionaryEncodedValuesReader = null;\n+  private boolean allPagesDictEncoded;\n+  private VectorizedParquetDefinitionLevelReader vectorizedDefinitionLevelReader;\n+\n+  public void setAllPagesDictEncoded(boolean allDictEncoded) {\n+    this.allPagesDictEncoded = allDictEncoded;\n+  }\n+\n+  @Override\n+  protected void reset() {\n+    super.reset();\n+    this.plainValuesReader = null;\n+    this.vectorizedDefinitionLevelReader = null;\n+  }\n+\n+  /**\n+   * Method for reading a batch of dictionary ids from the dicitonary encoded data pages. Like definition levels,\n+   * dictionary ids in Parquet are RLE/bin-packed encoded as well.\n+   */\n+  public int nextBatchDictionaryIds(\n+      final IntVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    vectorizedDefinitionLevelReader.readBatchOfDictionaryIds(\n+        vector,\n+        numValsInVector,\n+        actualBatchSize,\n+        holder,\n+        dictionaryEncodedValuesReader);\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of INT32 data type\n+   */\n+  public int nextBatchIntegers(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedIntegers(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchOfIntegers(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of INT64 data type\n+   */\n+  public int nextBatchLongs(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedLongs(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchOfLongs(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of TIMESTAMP_MILLIS data type. In iceberg, TIMESTAMP\n+   * is always represented in micro-seconds. So we multiply values stored in millis with 1000\n+   * before writing them to the vector.\n+   */\n+  public int nextBatchTimestampMillis(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedTimestampMillis(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchOfTimestampMillis(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of FLOAT data type.\n+   */\n+  public int nextBatchFloats(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedFloats(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchOfFloats(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of values of DOUBLE data type\n+   */\n+  public int nextBatchDoubles(\n+      final FieldVector vector, final int expectedBatchSize,\n+      final int numValsInVector,\n+      final int typeWidth, NullabilityHolder holder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedDoubles(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchOfDoubles(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          holder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  private int getActualBatchSize(int expectedBatchSize) {\n+    return Math.min(expectedBatchSize, triplesCount - triplesRead);\n+  }\n+\n+  /**\n+   * Method for reading a batch of decimals backed by INT32 and INT64 parquet data types. Since Arrow stores all\n+   * decimals in 16 bytes, byte arrays are appropriately padded before being written to Arrow data buffers.\n+   */\n+  public int nextBatchIntLongBackedDecimal(\n+      final FieldVector vector, final int expectedBatchSize, final int numValsInVector,\n+      final int typeWidth, NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader\n+          .readBatchOfDictionaryEncodedIntLongBackedDecimals(\n+              vector,\n+              numValsInVector,\n+              typeWidth,\n+              actualBatchSize,\n+              nullabilityHolder,\n+              dictionaryEncodedValuesReader,\n+              dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchOfIntLongBackedDecimals(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of decimals backed by fixed length byte array parquet data type. Arrow stores all\n+   * decimals in 16 bytes. This method provides the necessary padding to the decimals read. Moreover, Arrow interprets\n+   * the decimals in Arrow buffer as little endian. Parquet stores fixed length decimals as big endian. So, this method\n+   * uses {@link DecimalVector#setBigEndian(int, byte[])} method so that the data in Arrow vector is indeed little\n+   * endian.\n+   */\n+  public int nextBatchFixedLengthDecimal(\n+      final FieldVector vector, final int expectedBatchSize, final int numValsInVector,\n+      final int typeWidth, NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedFixedLengthDecimals(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchOfFixedLengthDecimals(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading a batch of variable width data type (ENUM, JSON, UTF8, BSON).\n+   */\n+  public int nextBatchVarWidthType(\n+      final FieldVector vector,\n+      final int expectedBatchSize,\n+      final int numValsInVector,\n+      NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedVarWidth(\n+          vector,\n+          numValsInVector,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchVarWidth(\n+          vector,\n+          numValsInVector,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading batches of fixed width binary type (e.g. BYTE[7]). Spark does not support fixed width binary\n+   * data type. To work around this limitation, the data is read as fixed width binary from parquet and stored in a\n+   * {@link VarBinaryVector} in Arrow.\n+   */\n+  public int nextBatchFixedWidthBinary(\n+      final FieldVector vector, final int expectedBatchSize, final int numValsInVector,\n+      final int typeWidth, NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    if (eagerDecodeDictionary) {\n+      vectorizedDefinitionLevelReader.readBatchOfDictionaryEncodedFixedWidthBinary(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          dictionaryEncodedValuesReader,\n+          dictionary);\n+    } else {\n+      vectorizedDefinitionLevelReader.readBatchOfFixedWidthBinary(\n+          vector,\n+          numValsInVector,\n+          typeWidth,\n+          actualBatchSize,\n+          nullabilityHolder,\n+          plainValuesReader);\n+    }\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  /**\n+   * Method for reading batches of booleans.\n+   */\n+  public int nextBatchBoolean(\n+      final FieldVector vector,\n+      final int expectedBatchSize,\n+      final int numValsInVector,\n+      NullabilityHolder nullabilityHolder) {\n+    final int actualBatchSize = getActualBatchSize(expectedBatchSize);\n+    if (actualBatchSize <= 0) {\n+      return 0;\n+    }\n+    vectorizedDefinitionLevelReader\n+        .readBatchOfBooleans(vector, numValsInVector, actualBatchSize,\n+            nullabilityHolder, plainValuesReader);\n+    triplesRead += actualBatchSize;\n+    this.hasNext = triplesRead < triplesCount;\n+    return actualBatchSize;\n+  }\n+\n+  @Override\n+  protected void initDataReader(Encoding dataEncoding, ByteBufferInputStream in, int valueCount) {\n+    ValuesReader previousReader = plainValuesReader;\n+    this.eagerDecodeDictionary = dataEncoding.usesDictionary() && dictionary != null &&\n+        (ParquetUtil.isIntType(desc.getPrimitiveType()) || !allPagesDictEncoded);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0321d3547e6f7d810d9f9a7fc993b2c65f6b17b7"}, "originalPosition": 430}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4870, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}