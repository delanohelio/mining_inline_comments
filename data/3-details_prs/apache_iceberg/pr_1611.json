{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAzMTQxOTg1", "number": 1611, "title": "DOCS: describe type compatibility between Spark and Iceberg", "bodyText": "I've found the type compatibility being asymmetric among the cases, and it required me to look into the code to find the conversions (TypeToSparkType, SparkTypeToType).\nGiven the conversions won't change frequently, it'd be nice to document the conversions so that end users don't feel surprised. This would be also helpful if end users consider using same Iceberg table across components - end users will have a clear view of the types when they create an Iceberg table from Spark, when they read an Iceberg table from Spark, when they write to the Iceberg table via Spark.\nProbably it might be ideal to have such type compatibility matrixes for all supported components, so that end users can look into these matrixes and check type issues on interop.", "createdAt": "2020-10-14T06:27:44Z", "url": "https://github.com/apache/iceberg/pull/1611", "merged": true, "mergeCommit": {"oid": "eee4f87e95bc4bb3c6ff720545f2a4ad8de96c33"}, "closed": true, "closedAt": "2020-11-03T20:19:27Z", "author": {"login": "HeartSaVioR"}, "timelineItems": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdSW_qNgH2gAyNTAzMTQxOTg1OmQ5NzE0ZmJmZjU2Mjk1NzM4NmRlOGVhNzIzZWM0OTJjNzMyZTlmYzE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdVdzcegFqTUxNjAzMDcwMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "d9714fbff562957386de8ea723ec492c732e9fc1", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/d9714fbff562957386de8ea723ec492c732e9fc1", "committedDate": "2020-10-14T06:15:19Z", "message": "DOCS: describe type compatibility between Spark and Iceberg"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA4MDMyMTc2", "url": "https://github.com/apache/iceberg/pull/1611#pullrequestreview-508032176", "createdAt": "2020-10-14T06:33:11Z", "commit": {"oid": "d9714fbff562957386de8ea723ec492c732e9fc1"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwNjozMzoxMlrOHhEI0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQwNjozMzoxMlrOHhEI0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQzMjg0OQ==", "bodyText": "While I simply put uuid -> string based on the implementation of TypeToSparkType, I failed to verify this as it looks to be really tricky to write UUID column.\nIt doesn't look possible to write UUID column from Spark. Even adding UUID type to SUPPORTED_PRIMITIVES in DataTest leads multiple tests failing.\nIs there any known way to write UUID column, or it'd be better to simply remove uuid here?", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r504432849", "createdAt": "2020-10-14T06:33:12Z", "author": {"login": "HeartSaVioR"}, "path": "site/docs/spark.md", "diffHunk": "@@ -728,3 +730,82 @@ spark.read.format(\"iceberg\").load(\"db.table.files\").show(truncate = false)\n // Hadoop path table\n spark.read.format(\"iceberg\").load(\"hdfs://nn:8020/path/to/table#files\").show(truncate = false)\n ```\n+\n+## Type compatibility\n+\n+Spark and Iceberg support different set of types. Iceberg does the type conversion automatically, but not for all combinations,\n+so you may want to understand the type conversion in Iceberg in prior to design the types of columns in your tables.\n+\n+### Spark type to Iceberg type on creating table\n+\n+This type conversion table describes how Spark types are converted to the Iceberg types. The conversion applies on creating Iceberg table via Spark without using Iceberg core API.\n+\n+| Spark           | Iceberg                 | Notes |\n+|-----------------|-------------------------|-------|\n+| boolean         | boolean                 |       |\n+| integer         | integer                 |       |\n+| short           | integer                 |       |\n+| byte            | integer                 |       |\n+| long            | long                    |       |\n+| float           | float                   |       |\n+| double          | double                  |       |\n+| date            | date                    |       |\n+| timestamp       | timestamp with timezone |       |\n+| string          | string                  |       |\n+| char            | string                  |       |\n+| varchar         | string                  |       |\n+| binary          | binary                  |       |\n+| decimal         | decimal                 |       |\n+| struct          | struct                  |       |\n+| array           | list                    |       |\n+| map             | map                     |       |\n+\n+The type conversion is asymmetric: this table doesn't represent the types of Iceberg Spark can \"read\" from, or \"write\" to.\n+The following sections describe the feasibility on read/write for Iceberg type from Spark.\n+\n+### Iceberg to Spark on reading from Iceberg table\n+\n+| Iceberg                    | Spark                   | Note  |\n+|----------------------------|-------------------------|-------|\n+| boolean                    | boolean                 |       |\n+| integer                    | integer                 |       |\n+| long                       | long                    |       |\n+| float                      | float                   |       |\n+| double                     | double                  |       |\n+| date                       | date                    |       |\n+| time                       | <N/A>                   |       |\n+| timestamp with timezone    | timestamp               |       |\n+| timestamp without timezone | <N/A>                   |       |\n+| string                     | string                  |       |\n+| uuid                       | string                  |       |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d9714fbff562957386de8ea723ec492c732e9fc1"}, "originalPosition": 60}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e276e08ec3e0ff76cec2598b5f27410bbc654f5b", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/e276e08ec3e0ff76cec2598b5f27410bbc654f5b", "committedDate": "2020-10-14T06:37:20Z", "message": "refine"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA4NDk5MjEy", "url": "https://github.com/apache/iceberg/pull/1611#pullrequestreview-508499212", "createdAt": "2020-10-14T15:58:37Z", "commit": {"oid": "e276e08ec3e0ff76cec2598b5f27410bbc654f5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNTo1ODozN1rOHhaKag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNTo1ODozN1rOHhaKag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc5MzcwNg==", "bodyText": "Binary can be written to a fixed column. It will be validated at write time.", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r504793706", "createdAt": "2020-10-14T15:58:37Z", "author": {"login": "rdblue"}, "path": "site/docs/spark.md", "diffHunk": "@@ -728,3 +730,82 @@ spark.read.format(\"iceberg\").load(\"db.table.files\").show(truncate = false)\n // Hadoop path table\n spark.read.format(\"iceberg\").load(\"hdfs://nn:8020/path/to/table#files\").show(truncate = false)\n ```\n+\n+## Type compatibility\n+\n+Spark and Iceberg support different set of types. Iceberg does the type conversion automatically, but not for all combinations,\n+so you may want to understand the type conversion in Iceberg in prior to design the types of columns in your tables.\n+\n+### Spark type to Iceberg type on creating table\n+\n+This type conversion table describes how Spark types are converted to the Iceberg types. The conversion applies on creating Iceberg table via Spark without using Iceberg core API.\n+\n+| Spark           | Iceberg                 | Notes |\n+|-----------------|-------------------------|-------|\n+| boolean         | boolean                 |       |\n+| integer         | integer                 |       |\n+| short           | integer                 |       |\n+| byte            | integer                 |       |\n+| long            | long                    |       |\n+| float           | float                   |       |\n+| double          | double                  |       |\n+| date            | date                    |       |\n+| timestamp       | timestamp with timezone |       |\n+| string          | string                  |       |\n+| char            | string                  |       |\n+| varchar         | string                  |       |\n+| binary          | binary                  |       |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e276e08ec3e0ff76cec2598b5f27410bbc654f5b"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA4NDk5NTIy", "url": "https://github.com/apache/iceberg/pull/1611#pullrequestreview-508499522", "createdAt": "2020-10-14T15:58:56Z", "commit": {"oid": "e276e08ec3e0ff76cec2598b5f27410bbc654f5b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNTo1ODo1NlrOHhaLVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNFQxNTo1ODo1NlrOHhaLVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc5Mzk0MA==", "bodyText": "Should we convert timestamp without zone to timestamp in Spark when reading?", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r504793940", "createdAt": "2020-10-14T15:58:56Z", "author": {"login": "rdblue"}, "path": "site/docs/spark.md", "diffHunk": "@@ -728,3 +730,82 @@ spark.read.format(\"iceberg\").load(\"db.table.files\").show(truncate = false)\n // Hadoop path table\n spark.read.format(\"iceberg\").load(\"hdfs://nn:8020/path/to/table#files\").show(truncate = false)\n ```\n+\n+## Type compatibility\n+\n+Spark and Iceberg support different set of types. Iceberg does the type conversion automatically, but not for all combinations,\n+so you may want to understand the type conversion in Iceberg in prior to design the types of columns in your tables.\n+\n+### Spark type to Iceberg type on creating table\n+\n+This type conversion table describes how Spark types are converted to the Iceberg types. The conversion applies on creating Iceberg table via Spark without using Iceberg core API.\n+\n+| Spark           | Iceberg                 | Notes |\n+|-----------------|-------------------------|-------|\n+| boolean         | boolean                 |       |\n+| integer         | integer                 |       |\n+| short           | integer                 |       |\n+| byte            | integer                 |       |\n+| long            | long                    |       |\n+| float           | float                   |       |\n+| double          | double                  |       |\n+| date            | date                    |       |\n+| timestamp       | timestamp with timezone |       |\n+| string          | string                  |       |\n+| char            | string                  |       |\n+| varchar         | string                  |       |\n+| binary          | binary                  |       |\n+| decimal         | decimal                 |       |\n+| struct          | struct                  |       |\n+| array           | list                    |       |\n+| map             | map                     |       |\n+\n+The type conversion is asymmetric: this table doesn't represent the types of Iceberg Spark can \"read\" from, or \"write\" to.\n+The following sections describe the feasibility on read/write for Iceberg type from Spark.\n+\n+### Iceberg to Spark on reading from Iceberg table\n+\n+| Iceberg                    | Spark                   | Note  |\n+|----------------------------|-------------------------|-------|\n+| boolean                    | boolean                 |       |\n+| integer                    | integer                 |       |\n+| long                       | long                    |       |\n+| float                      | float                   |       |\n+| double                     | double                  |       |\n+| date                       | date                    |       |\n+| time                       | N/A                     |       |\n+| timestamp with timezone    | timestamp               |       |\n+| timestamp without timezone | N/A                     |       |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e276e08ec3e0ff76cec2598b5f27410bbc654f5b"}, "originalPosition": 58}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bdc7cfabcfe9db39aec9e742ec795dce1508108c", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/bdc7cfabcfe9db39aec9e742ec795dce1508108c", "committedDate": "2020-10-16T02:31:24Z", "message": "Applied review comment"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE2MDMwNzAy", "url": "https://github.com/apache/iceberg/pull/1611#pullrequestreview-516030702", "createdAt": "2020-10-23T21:53:04Z", "commit": {"oid": "bdc7cfabcfe9db39aec9e742ec795dce1508108c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMTo1MzowNFrOHnfcTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QyMTo1MzowNFrOHnfcTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE3MTY2Mg==", "bodyText": "I think this could be merged with the table above by adding lines for time, timestamp without time zone, and fixed. What do you think about that? It would be a lot less documentation and the \"notes\" column above would be used. Right now, it's blank.", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r511171662", "createdAt": "2020-10-23T21:53:04Z", "author": {"login": "rdblue"}, "path": "site/docs/spark.md", "diffHunk": "@@ -728,3 +730,62 @@ spark.read.format(\"iceberg\").load(\"db.table.files\").show(truncate = false)\n // Hadoop path table\n spark.read.format(\"iceberg\").load(\"hdfs://nn:8020/path/to/table#files\").show(truncate = false)\n ```\n+\n+## Type compatibility\n+\n+Spark and Iceberg support different set of types. Iceberg does the type conversion automatically, but not for all combinations,\n+so you may want to understand the type conversion in Iceberg in prior to design the types of columns in your tables.\n+\n+### Spark type to Iceberg type\n+\n+This type conversion table describes how Spark types are converted to the Iceberg types. The conversion applies on both creating Iceberg table and writing to Iceberg table via Spark.\n+\n+| Spark           | Iceberg                 | Notes |\n+|-----------------|-------------------------|-------|\n+| boolean         | boolean                 |       |\n+| short           | integer                 |       |\n+| byte            | integer                 |       |\n+| integer         | integer                 |       |\n+| long            | long                    |       |\n+| float           | float                   |       |\n+| double          | double                  |       |\n+| date            | date                    |       |\n+| timestamp       | timestamp with timezone |       |\n+| char            | string                  |       |\n+| varchar         | string                  |       |\n+| string          | string                  |       |\n+| binary          | binary                  |       |\n+| decimal         | decimal                 |       |\n+| struct          | struct                  |       |\n+| array           | list                    |       |\n+| map             | map                     |       |\n+\n+!!! Note\n+    The table is based on representing conversion during creating table. In fact, broader supports are applied on write. Here're some points on write:\n+    \n+    * Iceberg numeric types (`integer`, `long`, `float`, `double`, `decimal`) support promotion during writes. e.g. You can write Spark types `short`, `byte`, `integer`, `long` to Iceberg type `long`.\n+    * You can write to Iceberg `fixed` type using Spark `binary` type. Note that assertion on the length will be performed.\n+\n+### Iceberg type to Spark type\n+\n+This type conversion table describes how Iceberg types are converted to the Spark types. The conversion applies on reading from Iceberg table via Spark.\n+\n+| Iceberg                    | Spark                   | Note          |\n+|----------------------------|-------------------------|---------------|\n+| boolean                    | boolean                 |               |\n+| integer                    | integer                 |               |\n+| long                       | long                    |               |\n+| float                      | float                   |               |\n+| double                     | double                  |               |\n+| date                       | date                    |               |\n+| time                       |                         | Not supported |\n+| timestamp with timezone    | timestamp               |               |\n+| timestamp without timezone |                         | Not supported |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bdc7cfabcfe9db39aec9e742ec795dce1508108c"}, "originalPosition": 63}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3955, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}