{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM2MjIwMDMy", "number": 1906, "title": "Migrate Procedures", "bodyText": "Adds Procedures for Migrate and Snapshot Actions", "createdAt": "2020-12-10T20:05:46Z", "url": "https://github.com/apache/iceberg/pull/1906", "merged": true, "mergeCommit": {"oid": "94bf40a50a2c796314993b028fabdafdb8b2a697"}, "closed": true, "closedAt": "2020-12-11T16:56:46Z", "author": {"login": "RussellSpitzer"}, "timelineItems": {"totalCount": 60, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdk5B8KgH2gAyNTM2MjIwMDMyOjllZTMxZmJiMmY3N2FiMmI4NmNmMzk4YTFkMGVhNWE2OThmYzY1Njc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdlK7LFgFqTU1MDI4NjczNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567", "committedDate": "2020-12-10T20:05:13Z", "message": "Adds Procedures for Migrate and And Snapshot\n\nAdds new procedures which take the same args as the Actions but can be accessed via\nSQL."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NTYxNDg2", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549561486", "createdAt": "2020-12-10T20:08:43Z", "commit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDowODo0M1rOIDbPfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDowODo0M1rOIDbPfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ2Mjk3Mg==", "bodyText": "I'll one line this", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540462972", "createdAt": "2020-12-10T20:08:43Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -70,21 +78,23 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   protected Identifier toIdentifier(String identifierAsString, String argName) {\n-    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n-        \"Cannot handle an empty identifier for argument %s\", argName);\n-\n-    CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n-        \"identifier for arg \" + argName, spark, identifierAsString, tableCatalog);\n-\n-    CatalogPlugin catalog = catalogAndIdentifier.catalog();\n-    Identifier identifier = catalogAndIdentifier.identifier();\n+    CatalogAndIdentifier catalogAndIdentifier = toCatalogAndIdentifer(identifierAsString, argName, tableCatalog);\n \n     Preconditions.checkArgument(\n-        catalog.equals(tableCatalog),\n+        catalogAndIdentifier.catalog().equals(tableCatalog),\n         \"Cannot run procedure in catalog '%s': '%s' is a table in catalog '%s'\",\n-        tableCatalog.name(), identifierAsString, catalog.name());\n+        tableCatalog.name(), identifierAsString, catalogAndIdentifier.catalog().name());\n+\n+    return catalogAndIdentifier.identifier();\n+  }\n+\n+  protected CatalogAndIdentifier toCatalogAndIdentifer(String identifierAsString, String argName,\n+                                                       CatalogPlugin catalog) {\n+    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n+        \"Cannot handle an empty identifier for argument %s\", argName);\n \n-    return identifier;\n+    return Spark3Util.catalogAndIdentifier(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "originalPosition": 45}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NTYxODky", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549561892", "createdAt": "2020-12-10T20:09:20Z", "commit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDowOToyMFrOIDbQwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDowOToyMFrOIDbQwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ2MzI5Nw==", "bodyText": "This is required to allow Map(string, string, *) expressions", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540463297", "createdAt": "2020-12-10T20:09:20Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/antlr/org.apache.spark.sql.catalyst.parser.extensions/IcebergSqlExtensions.g4", "diffHunk": "@@ -76,6 +76,7 @@ callArgument\n \n expression\n     : constant\n+    | stringMap", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NTYyODIy", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549562822", "createdAt": "2020-12-10T20:10:39Z", "commit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDoxMDozOVrOIDbTyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDoxMDozOVrOIDbTyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ2NDA3Mg==", "bodyText": "To keep the following to 1 line per entry", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540464072", "createdAt": "2020-12-10T20:10:39Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final DataType MAP = DataTypes.createMapType(DataTypes.StringType, DataTypes.StringType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "originalPosition": 38}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NTY1Nzk3", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549565797", "createdAt": "2020-12-10T20:14:43Z", "commit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDoxNDo0M1rOIDbdew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDoxNDo0M1rOIDbdew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ2NjU1NQ==", "bodyText": "Calling scala from java", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540466555", "createdAt": "2020-12-10T20:14:43Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final DataType MAP = DataTypes.createMapType(DataTypes.StringType, DataTypes.StringType);\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotProcedure>() {\n+      @Override\n+      protected SnapshotProcedure doBuild() {\n+        return new SnapshotProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    String dest = args.getString(1);\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(3)) {\n+      args.getMap(3).foreach(DataTypes.StringType, DataTypes.StringType,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "originalPosition": 83}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9de4fc6aee6f7f62697ffd15e41ed80a799878c6", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/9de4fc6aee6f7f62697ffd15e41ed80a799878c6", "committedDate": "2020-12-10T20:16:11Z", "message": "Minor Cleanup"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/3de93e32df8261a241e842d499e86fba9ad9fc9f", "committedDate": "2020-12-10T20:23:53Z", "message": "Make grammer consistent"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d5bc2b514291564833c6f40b05a2f4b62836b81b", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/d5bc2b514291564833c6f40b05a2f4b62836b81b", "committedDate": "2020-12-10T20:22:15Z", "message": "Make grammer consistent"}, "afterCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/3de93e32df8261a241e842d499e86fba9ad9fc9f", "committedDate": "2020-12-10T20:23:53Z", "message": "Make grammer consistent"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjIxNjM3", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549621637", "createdAt": "2020-12-10T21:35:22Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTozNToyMlrOIDeVxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTozNToyMlrOIDeVxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUxMzczNA==", "bodyText": "nit: how common is it to be in BaseProcedure? Not a strong opinion, just asking.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540513734", "createdAt": "2020-12-10T21:35:22Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -37,9 +37,13 @@\n import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n import org.apache.spark.sql.execution.CacheManager;\n import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n import scala.Option;\n \n abstract class BaseProcedure implements Procedure {\n+  protected static final DataType STRING_MAP = DataTypes.createMapType(DataTypes.StringType, DataTypes.StringType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 9}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjMxODcy", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549631872", "createdAt": "2020-12-10T21:50:52Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1MDo1MlrOIDe4Yw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1MDo1MlrOIDe4Yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyMjU5NQ==", "bodyText": "Should we make it migrated_files_count to match other procedures?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540522595", "createdAt": "2020-12-10T21:50:52Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjMyMzg4", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549632388", "createdAt": "2020-12-10T21:51:39Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1MTozOVrOIDe6Ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1MTozOVrOIDe6Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyMzAxMA==", "bodyText": "nit: let's import ProcedureBuilder directly to match other procedures.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540523010", "createdAt": "2020-12-10T21:51:39Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 50}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjMzNTcx", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549633571", "createdAt": "2020-12-10T21:53:34Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1MzozNFrOIDe99w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1MzozNFrOIDe99w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyNDAyMw==", "bodyText": "nit: MigrateTableProcedure?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540524023", "createdAt": "2020-12-10T21:53:34Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 36}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjMzNjg2", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549633686", "createdAt": "2020-12-10T21:53:47Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1Mzo0N1rOIDe-XA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1Mzo0N1rOIDe-XA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyNDEyNA==", "bodyText": "I am afraid this description will not show properly in the SQL plan. Let's make it MigrateTableProcedure as in other procedures.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540524124", "createdAt": "2020-12-10T21:53:47Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(1)) {\n+      args.getMap(1).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    String tableName = args.getString(0);\n+    CatalogAndIdentifier tableIdent = toCatalogAndIdentifer(tableName, PARAMETERS[0].name(), tableCatalog());\n+    CreateAction action =  new Spark3MigrateAction(spark(), tableIdent.catalog(), tableIdent.identifier());\n+\n+    long numFiles = action.withProperties(options).execute();\n+    return new InternalRow[] {newInternalRow(numFiles)};\n+  }\n+\n+  @Override\n+  public String description() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 89}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjMzOTU0", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549633954", "createdAt": "2020-12-10T21:54:11Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1NDoxMlrOIDe_Pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1NDoxMlrOIDe_Pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyNDM1MQ==", "bodyText": "nit: extra space", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540524351", "createdAt": "2020-12-10T21:54:12Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(1)) {\n+      args.getMap(1).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    String tableName = args.getString(0);\n+    CatalogAndIdentifier tableIdent = toCatalogAndIdentifer(tableName, PARAMETERS[0].name(), tableCatalog());\n+    CreateAction action =  new Spark3MigrateAction(spark(), tableIdent.catalog(), tableIdent.identifier());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjM5MzU4", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549639358", "createdAt": "2020-12-10T22:02:02Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjowMjowMlrOIDfRmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjowMjowMlrOIDfRmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyOTA0OA==", "bodyText": "typo? toCatalogAndIdentifer -> toCatalogAndIdentifier?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540529048", "createdAt": "2020-12-10T22:02:02Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -70,21 +82,22 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   protected Identifier toIdentifier(String identifierAsString, String argName) {\n-    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n-        \"Cannot handle an empty identifier for argument %s\", argName);\n-\n-    CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n-        \"identifier for arg \" + argName, spark, identifierAsString, tableCatalog);\n-\n-    CatalogPlugin catalog = catalogAndIdentifier.catalog();\n-    Identifier identifier = catalogAndIdentifier.identifier();\n+    CatalogAndIdentifier catalogAndIdentifier = toCatalogAndIdentifer(identifierAsString, argName, tableCatalog);\n \n     Preconditions.checkArgument(\n-        catalog.equals(tableCatalog),\n+        catalogAndIdentifier.catalog().equals(tableCatalog),\n         \"Cannot run procedure in catalog '%s': '%s' is a table in catalog '%s'\",\n-        tableCatalog.name(), identifierAsString, catalog.name());\n+        tableCatalog.name(), identifierAsString, catalogAndIdentifier.catalog().name());\n+\n+    return catalogAndIdentifier.identifier();\n+  }\n+\n+  protected CatalogAndIdentifier toCatalogAndIdentifer(String identifierAsString, String argName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjM5Njg3", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549639687", "createdAt": "2020-12-10T22:02:33Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjowMjozNFrOIDfS1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjowMjozNFrOIDfS1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyOTM2NQ==", "bodyText": "nit: Maps.newHashMap()?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540529365", "createdAt": "2020-12-10T22:02:34Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 71}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjQwOTY5", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549640969", "createdAt": "2020-12-10T22:04:31Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjowNDozMVrOIDfXGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjowNDozMVrOIDfXGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzMDQ1Nw==", "bodyText": "Should it be table_properties or just properties? There will be a difference between options and properties in Spark 3.1.0.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540530457", "createdAt": "2020-12-10T22:04:31Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjQ0NTM3", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549644537", "createdAt": "2020-12-10T22:09:59Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjowOTo1OVrOIDfjKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjowOTo1OVrOIDfjKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzMzU0Ng==", "bodyText": "I find it a bit misleading that we don't process arguments in the order they are defined.\nHow about this?\n    String identAsString = args.getString(0);\n    CatalogAndIdentifier catalogAndIdent = toCatalogAndIdentifer(identAsString, PARAMETERS[0].name(), tableCatalog());\n\n    Map<String, String> tableProps = Maps.newHashMap();\n    MapData providedProps = args.getMap(1);\n    if (providedProps != null) {\n      providedProps.foreach(DataTypes.StringType, DataTypes.StringType,\n          (k, v) -> {\n            tableProps.put(k.toString(), v.toString());\n            return BoxedUnit.UNIT;\n          });\n    }", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540533546", "createdAt": "2020-12-10T22:09:59Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(1)) {\n+      args.getMap(1).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    String tableName = args.getString(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 80}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjQ0NzEw", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549644710", "createdAt": "2020-12-10T22:10:15Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMDoxNVrOIDfj0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMDoxNVrOIDfj0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzMzcxNQ==", "bodyText": "nit: numMigratedFiles?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540533715", "createdAt": "2020-12-10T22:10:15Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(1)) {\n+      args.getMap(1).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    String tableName = args.getString(0);\n+    CatalogAndIdentifier tableIdent = toCatalogAndIdentifer(tableName, PARAMETERS[0].name(), tableCatalog());\n+    CreateAction action =  new Spark3MigrateAction(spark(), tableIdent.catalog(), tableIdent.identifier());\n+\n+    long numFiles = action.withProperties(options).execute();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 84}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjQ0OTgy", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549644982", "createdAt": "2020-12-10T22:10:41Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMDo0MVrOIDfkwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMDo0MVrOIDfkwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzMzk1Mg==", "bodyText": "nit: options -> tableProps?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540533952", "createdAt": "2020-12-10T22:10:41Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 71}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjQ1NzA0", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549645704", "createdAt": "2020-12-10T22:11:48Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMTo0OFrOIDfnbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMTo0OFrOIDfnbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNDYzNg==", "bodyText": "What about source_table, table, table_location, table_properties? cc @RussellSpitzer @rdblue", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540534636", "createdAt": "2020-12-10T22:11:48Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjQ2MjY0", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549646264", "createdAt": "2020-12-10T22:12:43Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMjo0M1rOIDfpuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMjo0M1rOIDfpuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNTIyNA==", "bodyText": "imported_data_files_count?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540535224", "createdAt": "2020-12-10T22:12:43Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjQ2NjIy", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549646622", "createdAt": "2020-12-10T22:13:13Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMzoxNFrOIDfq2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMzoxNFrOIDfq2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNTUxMg==", "bodyText": "Same comments as for migrate.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540535512", "createdAt": "2020-12-10T22:13:14Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotProcedure>() {\n+      @Override\n+      protected SnapshotProcedure doBuild() {\n+        return new SnapshotProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    String dest = args.getString(1);\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = Maps.newHashMap();\n+    if (!args.isNullAt(3)) {\n+      args.getMap(3).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    CatalogAndIdentifier sourceIdent = toCatalogAndIdentifer(source, PARAMETERS[0].name(), tableCatalog());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 88}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjQ2ODMx", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549646831", "createdAt": "2020-12-10T22:13:32Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMzozMlrOIDfrgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMzozMlrOIDfrgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNTY4Mw==", "bodyText": "same comments as for migrate.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540535683", "createdAt": "2020-12-10T22:13:32Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotProcedure>() {\n+      @Override\n+      protected SnapshotProcedure doBuild() {\n+        return new SnapshotProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    String dest = args.getString(1);\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = Maps.newHashMap();\n+    if (!args.isNullAt(3)) {\n+      args.getMap(3).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    CatalogAndIdentifier sourceIdent = toCatalogAndIdentifer(source, PARAMETERS[0].name(), tableCatalog());\n+    CatalogAndIdentifier destIdent = toCatalogAndIdentifer(dest, PARAMETERS[1].name(), tableCatalog());\n+\n+    Preconditions.checkArgument(sourceIdent != destIdent || sourceIdent.catalog() != destIdent.catalog(),\n+        \"Cannot create a snapshot with the same name as the source of the snapshot.\");\n+    SnapshotAction action =  new Spark3SnapshotAction(spark(), sourceIdent.catalog(), sourceIdent.identifier(),\n+        destIdent.catalog(), destIdent.identifier());\n+\n+    long numFiles;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 96}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjQ3MDIy", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549647022", "createdAt": "2020-12-10T22:13:53Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMzo1M1rOIDfsOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMzo1M1rOIDfsOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNTg2NQ==", "bodyText": "SnapshotTableProcedure?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540535865", "createdAt": "2020-12-10T22:13:53Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjQ3MjQy", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549647242", "createdAt": "2020-12-10T22:14:10Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxNDoxMVrOIDfs_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxNDoxMVrOIDfs_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNjA2MA==", "bodyText": "same here. SnapshotTableProcedure?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540536060", "createdAt": "2020-12-10T22:14:11Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotProcedure>() {\n+      @Override\n+      protected SnapshotProcedure doBuild() {\n+        return new SnapshotProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    String dest = args.getString(1);\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = Maps.newHashMap();\n+    if (!args.isNullAt(3)) {\n+      args.getMap(3).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    CatalogAndIdentifier sourceIdent = toCatalogAndIdentifer(source, PARAMETERS[0].name(), tableCatalog());\n+    CatalogAndIdentifier destIdent = toCatalogAndIdentifer(dest, PARAMETERS[1].name(), tableCatalog());\n+\n+    Preconditions.checkArgument(sourceIdent != destIdent || sourceIdent.catalog() != destIdent.catalog(),\n+        \"Cannot create a snapshot with the same name as the source of the snapshot.\");\n+    SnapshotAction action =  new Spark3SnapshotAction(spark(), sourceIdent.catalog(), sourceIdent.identifier(),\n+        destIdent.catalog(), destIdent.identifier());\n+\n+    long numFiles;\n+    if (snapshotLocation != null) {\n+      numFiles = action.withLocation(snapshotLocation).withProperties(options).execute();\n+    } else {\n+      numFiles = action.withProperties(options).execute();\n+    }\n+\n+    return new InternalRow[] {newInternalRow(numFiles)};\n+  }\n+\n+  @Override\n+  public String description() {\n+    return \"Creates an Iceberg table from a Spark Table. The Created table will be isolated from the original table\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 108}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjgxNzgy", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549681782", "createdAt": "2020-12-10T23:03:41Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowMzo0MVrOIDhdQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowMzo0MVrOIDhdQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2NDgwMw==", "bodyText": "There's a scalarSql method for when a SQL command produces one row with one value. That will assert that there is only one row and one column, which may be easier.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540564803", "createdAt": "2020-12-10T23:03:41Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjgzMDk3", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549683097", "createdAt": "2020-12-10T23:06:11Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowNjoxMlrOIDhkXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowNjoxMlrOIDhkXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2NjYyMA==", "bodyText": "Since the location is set, should this validate that the migrated table's location matches the one passed here?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540566620", "createdAt": "2020-12-10T23:06:12Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testMigrateWithOptions() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 74}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjgzMzM0", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549683334", "createdAt": "2020-12-10T23:06:42Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowNjo0MlrOIDhlRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowNjo0MlrOIDhlRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2Njg1Mw==", "bodyText": "Other tests drop the table if exists rather than using IF NOT EXISTS. I think that's a better pattern because if the table already exists, it probably violates the assumptions of this test.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540566853", "createdAt": "2020-12-10T23:06:42Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testMigrateWithOptions() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s', map('foo', 'bar'))\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 93}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjgzNjkz", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549683693", "createdAt": "2020-12-10T23:07:23Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowNzoyM1rOIDhm8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowNzoyM1rOIDhm8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2NzI4MQ==", "bodyText": "\"migrated\" -> \"added\"?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540567281", "createdAt": "2020-12-10T23:07:23Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testMigrateWithOptions() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s', map('foo', 'bar'))\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 98}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5Njg0MDE0", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549684014", "createdAt": "2020-12-10T23:08:05Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowODowNVrOIDhoXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowODowNVrOIDhoXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2NzY0NQ==", "bodyText": "Since the source table has a known location, I think this should validate that the snapshot table uses a different one.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540567645", "createdAt": "2020-12-10T23:08:05Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testMigrateWithOptions() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s', map('foo', 'bar'))\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 104}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5Njg3ODcx", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549687871", "createdAt": "2020-12-10T23:16:23Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoxNjoyNFrOIDh2Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoxNjoyNFrOIDh2Zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3MTIzOQ==", "bodyText": "Nit: I don't think we need to keep adding these checks since it tests the resolver, not the procedure.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540571239", "createdAt": "2020-12-10T23:16:24Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testMigrateWithOptions() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s', map('foo', 'bar'))\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot( snapshot_source => '%s', table => '%s', table_options => map('foo','bar'))\",\n+        catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithAlternateLocation() throws IOException {\n+    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n+    String location = temp.newFolder().toString();\n+    String snapshotLocation = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot( snapshot_source => '%s', table => '%s', table_location => '%s')\",\n+        catalogName, sourceName, tableName, snapshotLocation).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    String storageLocation = validationCatalog.loadTable(tableIdent).location();\n+    Assert.assertEquals(\"Snapshot should be made at specified location\", snapshotLocation, storageLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testInvalidSnapshotsCases() {\n+    AssertHelpers.assertThrows(\"Should not allow mixed args\",\n+        AnalysisException.class, \"Named and positional arguments cannot be mixed\",\n+        () -> sql(\"CALL %s.system.snapshot('n', table => 't')\", catalogName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 157}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5Njg4MDY5", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549688069", "createdAt": "2020-12-10T23:16:49Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoxNjo0OVrOIDh3Og==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoxNjo0OVrOIDh3Og==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3MTQ1MA==", "bodyText": "Similarly, this isn't the procedure name.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540571450", "createdAt": "2020-12-10T23:16:49Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testMigrateWithOptions() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s', map('foo', 'bar'))\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot( snapshot_source => '%s', table => '%s', table_options => map('foo','bar'))\",\n+        catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithAlternateLocation() throws IOException {\n+    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n+    String location = temp.newFolder().toString();\n+    String snapshotLocation = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot( snapshot_source => '%s', table => '%s', table_location => '%s')\",\n+        catalogName, sourceName, tableName, snapshotLocation).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    String storageLocation = validationCatalog.loadTable(tableIdent).location();\n+    Assert.assertEquals(\"Snapshot should be made at specified location\", snapshotLocation, storageLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testInvalidSnapshotsCases() {\n+    AssertHelpers.assertThrows(\"Should not allow mixed args\",\n+        AnalysisException.class, \"Named and positional arguments cannot be mixed\",\n+        () -> sql(\"CALL %s.system.snapshot('n', table => 't')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should not resolve procedures in arbitrary namespaces\",\n+        NoSuchProcedureException.class, \"not found\",\n+        () -> sql(\"CALL %s.custom.snapshot('n', 't')\", catalogName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 161}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5Njg4NTE5", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549688519", "createdAt": "2020-12-10T23:17:47Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoxNzo0N1rOIDh46w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoxNzo0N1rOIDh46w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3MTg4Mw==", "bodyText": "This can only validate one case, where either source or dest is empty. I think this should be split into empty source and empty dest cases.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540571883", "createdAt": "2020-12-10T23:17:47Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testMigrateWithOptions() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s', map('foo', 'bar'))\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot( snapshot_source => '%s', table => '%s', table_options => map('foo','bar'))\",\n+        catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithAlternateLocation() throws IOException {\n+    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n+    String location = temp.newFolder().toString();\n+    String snapshotLocation = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot( snapshot_source => '%s', table => '%s', table_location => '%s')\",\n+        catalogName, sourceName, tableName, snapshotLocation).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    String storageLocation = validationCatalog.loadTable(tableIdent).location();\n+    Assert.assertEquals(\"Snapshot should be made at specified location\", snapshotLocation, storageLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testInvalidSnapshotsCases() {\n+    AssertHelpers.assertThrows(\"Should not allow mixed args\",\n+        AnalysisException.class, \"Named and positional arguments cannot be mixed\",\n+        () -> sql(\"CALL %s.system.snapshot('n', table => 't')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should not resolve procedures in arbitrary namespaces\",\n+        NoSuchProcedureException.class, \"not found\",\n+        () -> sql(\"CALL %s.custom.snapshot('n', 't')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n+        AnalysisException.class, \"Missing required parameters\",\n+        () -> sql(\"CALL %s.system.snapshot('foo')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n+        AnalysisException.class, \"Wrong arg type\",\n+        () -> sql(\"CALL %s.system.snapshot('n', 't', map('foo', 'bar'))\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",\n+        IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n+        () -> sql(\"CALL %s.system.snapshot('', '')\", catalogName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 173}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5Njg5MjY1", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549689265", "createdAt": "2020-12-10T23:19:26Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoxOToyNlrOIDh8Gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoxOToyNlrOIDh8Gw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3MjY5OQ==", "bodyText": "I like how the other procedures are tested in a suite named after the procedure, like TestRemoveOrphanFilesProcedure. I don't see much value in a suite for both migrate and snapshot together and it isn't obvious where these tests live. Could you split this into TestMigrateProcedure and TestSnapshotProcedure?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540572699", "createdAt": "2020-12-10T23:19:26Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NjkyNDY5", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549692469", "createdAt": "2020-12-10T23:26:44Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoyNjo0NFrOIDiIeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoyNjo0NFrOIDiIeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3NTg2NQ==", "bodyText": "What is the rationale for creating actions directly instead of going through Actions?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540575865", "createdAt": "2020-12-10T23:26:44Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(1)) {\n+      args.getMap(1).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    String tableName = args.getString(0);\n+    CatalogAndIdentifier tableIdent = toCatalogAndIdentifer(tableName, PARAMETERS[0].name(), tableCatalog());\n+    CreateAction action =  new Spark3MigrateAction(spark(), tableIdent.catalog(), tableIdent.identifier());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5Njk1MzQ5", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549695349", "createdAt": "2020-12-10T23:33:14Z", "commit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzozMzoxNFrOIDiS8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzozMzoxNFrOIDiS8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3ODU0NQ==", "bodyText": "Hm. I would prefer not to make these public, but I see that this needs the identifier that has already been parsed to do the custom catalog validation. Should be fine for now, but we should keep this in mind for when we fix the Actions API.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540578545", "createdAt": "2020-12-10T23:33:14Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -46,11 +46,11 @@\n  * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n  * table.\n  */\n-class Spark3MigrateAction extends Spark3CreateAction {\n+public class Spark3MigrateAction extends Spark3CreateAction {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 5}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d265b1527cdbab753aa4deccf63d649a085a5b4e", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/d265b1527cdbab753aa4deccf63d649a085a5b4e", "committedDate": "2020-12-11T03:03:38Z", "message": "Reviewer comments/Migrate location fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "40b19eee15a0cacaa9e18233e398ff9f9deccd69", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/40b19eee15a0cacaa9e18233e398ff9f9deccd69", "committedDate": "2020-12-11T03:15:31Z", "message": "Split Migrate and Snapshot Procedure Tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/97c2251656df1eaebd06ec886ec31d979b20b576", "committedDate": "2020-12-11T04:36:33Z", "message": "Fix Table Creation Deletion in Tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTE2MjUx", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549916251", "createdAt": "2020-12-11T09:07:09Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowNzowOVrOIDvfKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowNzowOVrOIDvfKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5NDY2NA==", "bodyText": "nit: this could fit on one line, right?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540794664", "createdAt": "2020-12-11T09:07:09Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTE2NTU2", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549916556", "createdAt": "2020-12-11T09:07:33Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowNzozM1rOIDvgVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowNzozM1rOIDvgVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5NDk2NA==", "bodyText": "nit: scalarSql method should be used", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540794964", "createdAt": "2020-12-11T09:07:33Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 58}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTE3NDQ2", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549917446", "createdAt": "2020-12-11T09:08:45Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowODo0NVrOIDvjEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowODo0NVrOIDvjEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5NTY2NQ==", "bodyText": "nit: this should fit on one line", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540795665", "createdAt": "2020-12-11T09:08:45Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 77}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTE3NTkz", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549917593", "createdAt": "2020-12-11T09:08:59Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowODo1OVrOIDvjjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowODo1OVrOIDvjjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5NTc4OA==", "bodyText": "nit: withProperties?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540795788", "createdAt": "2020-12-11T09:08:59Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 74}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTE4NzMz", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549918733", "createdAt": "2020-12-11T09:10:33Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMDozM1rOIDvnbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMDozM1rOIDvnbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5Njc4MA==", "bodyText": "nit: scalarSql", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540796780", "createdAt": "2020-12-11T09:10:33Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object result = scalarSql(\n+        \"CALL %s.system.snapshot(source_table => '%s', table => '%s', properties => map('foo','bar'))\",\n+        catalogName, sourceName, tableName);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    Map<String, String> props = createdTable.properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithAlternateLocation() throws IOException {\n+    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n+    String location = temp.newFolder().toString();\n+    String snapshotLocation = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 108}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTE5MDEy", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549919012", "createdAt": "2020-12-11T09:10:57Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMDo1OFrOIDvoUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMDo1OFrOIDvoUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5NzAwOQ==", "bodyText": "nit: should fit on one line", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540797009", "createdAt": "2020-12-11T09:10:58Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object result = scalarSql(\n+        \"CALL %s.system.snapshot(source_table => '%s', table => '%s', properties => map('foo','bar'))\",\n+        catalogName, sourceName, tableName);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    Map<String, String> props = createdTable.properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithAlternateLocation() throws IOException {\n+    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n+    String location = temp.newFolder().toString();\n+    String snapshotLocation = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 105}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTE5ODc0", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549919874", "createdAt": "2020-12-11T09:12:08Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMjowOFrOIDvrbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMjowOFrOIDvrbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5NzgwNQ==", "bodyText": "Should we also add a test where we get a map where keys or values are not strings?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540797805", "createdAt": "2020-12-11T09:12:08Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object result = scalarSql(\n+        \"CALL %s.system.snapshot(source_table => '%s', table => '%s', properties => map('foo','bar'))\",\n+        catalogName, sourceName, tableName);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    Map<String, String> props = createdTable.properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithAlternateLocation() throws IOException {\n+    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n+    String location = temp.newFolder().toString();\n+    String snapshotLocation = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot(source_table => '%s', table => '%s', table_location => '%s')\",\n+        catalogName, sourceName, tableName, snapshotLocation).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    String storageLocation = validationCatalog.loadTable(tableIdent).location();\n+    Assert.assertEquals(\"Snapshot should be made at specified location\", snapshotLocation, storageLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testInvalidSnapshotsCases() {\n+    AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n+        AnalysisException.class, \"Missing required parameters\",\n+        () -> sql(\"CALL %s.system.snapshot('foo')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n+        AnalysisException.class, \"Wrong arg type\",\n+        () -> sql(\"CALL %s.system.snapshot('n', 't', map('foo', 'bar'))\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",\n+        IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n+        () -> sql(\"CALL %s.system.snapshot('', 'dest')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 138}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTIwOTk4", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549920998", "createdAt": "2020-12-11T09:13:38Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMzozOFrOIDvu-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMzozOFrOIDvu-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5ODcxMw==", "bodyText": "I think there is a typo: toCatalogAdnIdentifier ->  toCatalogAndIdentifier", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540798713", "createdAt": "2020-12-11T09:13:38Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -70,21 +82,22 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   protected Identifier toIdentifier(String identifierAsString, String argName) {\n-    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n-        \"Cannot handle an empty identifier for argument %s\", argName);\n-\n-    CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n-        \"identifier for arg \" + argName, spark, identifierAsString, tableCatalog);\n-\n-    CatalogPlugin catalog = catalogAndIdentifier.catalog();\n-    Identifier identifier = catalogAndIdentifier.identifier();\n+    CatalogAndIdentifier catalogAndIdentifier = toCatalogAdnIdentifier(identifierAsString, argName, tableCatalog);\n \n     Preconditions.checkArgument(\n-        catalog.equals(tableCatalog),\n+        catalogAndIdentifier.catalog().equals(tableCatalog),\n         \"Cannot run procedure in catalog '%s': '%s' is a table in catalog '%s'\",\n-        tableCatalog.name(), identifierAsString, catalog.name());\n+        tableCatalog.name(), identifierAsString, catalogAndIdentifier.catalog().name());\n+\n+    return catalogAndIdentifier.identifier();\n+  }\n+\n+  protected CatalogAndIdentifier toCatalogAdnIdentifier(String identifierAsString, String argName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTIxMjQ1", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549921245", "createdAt": "2020-12-11T09:13:59Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMzo1OVrOIDvvug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMzo1OVrOIDvvug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5ODkwNg==", "bodyText": "+1", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540798906", "createdAt": "2020-12-11T09:13:59Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -122,6 +122,8 @@ public Long execute() {\n     properties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n     properties.put(\"migrated\", \"true\");\n     properties.putAll(additionalProperties());\n+    properties.putIfAbsent(LOCATION, sourceTableLocation());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 18}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTI0NTI4", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549924528", "createdAt": "2020-12-11T09:18:21Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxODoyMlrOIDv6_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxODoyMlrOIDv6_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDgwMTc5MQ==", "bodyText": "I'd actually add some empty lines to separate logical blocks similar to what you have in snapshot.\n    Map<String, String> properties = Maps.newHashMap();\n\n    properties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n    EXCLUDED_PROPERTIES.forEach(properties::remove);\n\n    properties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n    properties.put(\"migrated\", \"true\");\n    properties.putAll(additionalProperties());\n    properties.putIfAbsent(LOCATION, sourceTableLocation());\n\n    return properties;", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540801791", "createdAt": "2020-12-11T09:18:22Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -122,6 +122,8 @@ public Long execute() {\n     properties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n     properties.put(\"migrated\", \"true\");\n     properties.putAll(additionalProperties());\n+    properties.putIfAbsent(LOCATION, sourceTableLocation());\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 19}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTg3MDY3", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549987067", "createdAt": "2020-12-11T10:42:42Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0Mjo0MlrOIDzKUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0Mjo0MlrOIDzKUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg1NDg2Ng==", "bodyText": "I find naming here a bit inconsistent with other procedures. What about this?\nString identAsString = args.getString(0);\nCatalogAndIdentifier catalogAndIdent = toCatalogAndIdentifer(identAsString, PARAMETERS[0].name(), tableCatalog());", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540854866", "createdAt": "2020-12-11T10:42:42Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateTableProcedure.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.iceberg.spark.procedures.SparkProcedures.ProcedureBuilder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateTableProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"properties\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"migrated_files_count\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateTableProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateTableProcedure>() {\n+      @Override\n+      protected MigrateTableProcedure doBuild() {\n+        return new MigrateTableProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String tableName = args.getString(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 72}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTg4OTU0", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549988954", "createdAt": "2020-12-11T10:45:20Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0NToyMFrOIDzQXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0NToyMFrOIDzQXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg1NjQxNA==", "bodyText": "Should it be just location as we just use properties?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540856414", "createdAt": "2020-12-11T10:45:20Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotTableProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"source_table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTg5NjI0", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549989624", "createdAt": "2020-12-11T10:46:17Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0NjoxOFrOIDzShA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0NjoxOFrOIDzShA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg1Njk2NA==", "bodyText": "I am okay without dest prefix but if anyone feels strongly, I am ok to add.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540856964", "createdAt": "2020-12-11T10:46:18Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotTableProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"source_table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTkwNDc0", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549990474", "createdAt": "2020-12-11T10:47:35Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0NzozNVrOIDzVeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0NzozNVrOIDzVeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg1NzcyMg==", "bodyText": "Think the name should match whatever we do in migrate. What about imported_files_count?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540857722", "createdAt": "2020-12-11T10:47:35Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotTableProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"source_table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"properties\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"imported_datafiles_count\", DataTypes.LongType, false, Metadata.empty())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 46}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTkwODA5", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549990809", "createdAt": "2020-12-11T10:48:02Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0ODowMlrOIDzWgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0ODowMlrOIDzWgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg1Nzk4NA==", "bodyText": "nit: extra space", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540857984", "createdAt": "2020-12-11T10:48:02Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotTableProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"source_table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"properties\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"imported_datafiles_count\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotTableProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotTableProcedure>() {\n+      @Override\n+      protected SnapshotTableProcedure doBuild() {\n+        return new SnapshotTableProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    CatalogAndIdentifier sourceIdent = toCatalogAdnIdentifier(source, PARAMETERS[0].name(), tableCatalog());\n+\n+    String dest = args.getString(1);\n+    CatalogAndIdentifier destIdent = toCatalogAdnIdentifier(dest, PARAMETERS[1].name(), tableCatalog());\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = Maps.newHashMap();\n+    if (!args.isNullAt(3)) {\n+      args.getMap(3).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    Preconditions.checkArgument(sourceIdent != destIdent || sourceIdent.catalog() != destIdent.catalog(),\n+        \"Cannot create a snapshot with the same name as the source of the snapshot.\");\n+    SnapshotAction action =  new Spark3SnapshotAction(spark(), sourceIdent.catalog(), sourceIdent.identifier(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 93}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTkyMDc5", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549992079", "createdAt": "2020-12-11T10:49:53Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0OTo1M1rOIDza4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0OTo1M1rOIDza4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg1OTEwNQ==", "bodyText": "nit: tableProps?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540859105", "createdAt": "2020-12-11T10:49:53Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotTableProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"source_table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"properties\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"imported_datafiles_count\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotTableProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotTableProcedure>() {\n+      @Override\n+      protected SnapshotTableProcedure doBuild() {\n+        return new SnapshotTableProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    CatalogAndIdentifier sourceIdent = toCatalogAdnIdentifier(source, PARAMETERS[0].name(), tableCatalog());\n+\n+    String dest = args.getString(1);\n+    CatalogAndIdentifier destIdent = toCatalogAdnIdentifier(dest, PARAMETERS[1].name(), tableCatalog());\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = Maps.newHashMap();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5OTkzMjk2", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-549993296", "createdAt": "2020-12-11T10:51:37Z", "commit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo1MTozN1rOIDze3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo1MTozN1rOIDze3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2MDEyNg==", "bodyText": "I'd separate assigning the snapshot location like this:\n    if (snapshotLocation != null) {\n      action.withLocation(snapshotLocation);\n    }\n\n    long numImportedFiles = action.withProperties(tableProps).execute();\n    return new InternalRow[] {newInternalRow(numImportedFiles)};", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540860126", "createdAt": "2020-12-11T10:51:37Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotTableProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"source_table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"properties\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"imported_datafiles_count\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotTableProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotTableProcedure>() {\n+      @Override\n+      protected SnapshotTableProcedure doBuild() {\n+        return new SnapshotTableProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    CatalogAndIdentifier sourceIdent = toCatalogAdnIdentifier(source, PARAMETERS[0].name(), tableCatalog());\n+\n+    String dest = args.getString(1);\n+    CatalogAndIdentifier destIdent = toCatalogAdnIdentifier(dest, PARAMETERS[1].name(), tableCatalog());\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = Maps.newHashMap();\n+    if (!args.isNullAt(3)) {\n+      args.getMap(3).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    Preconditions.checkArgument(sourceIdent != destIdent || sourceIdent.catalog() != destIdent.catalog(),\n+        \"Cannot create a snapshot with the same name as the source of the snapshot.\");\n+    SnapshotAction action =  new Spark3SnapshotAction(spark(), sourceIdent.catalog(), sourceIdent.identifier(),\n+        destIdent.catalog(), destIdent.identifier());\n+\n+    long importedDataFiles;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 96}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "38d7f6de7502cc0d1635d3d26633a67844a2bab8", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/38d7f6de7502cc0d1635d3d26633a67844a2bab8", "committedDate": "2020-12-11T14:40:25Z", "message": "More reviewer comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUwMTg2MTIy", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-550186122", "createdAt": "2020-12-11T15:14:26Z", "commit": {"oid": "38d7f6de7502cc0d1635d3d26633a67844a2bab8"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUwMjg2NzM3", "url": "https://github.com/apache/iceberg/pull/1906#pullrequestreview-550286737", "createdAt": "2020-12-11T16:56:07Z", "commit": {"oid": "38d7f6de7502cc0d1635d3d26633a67844a2bab8"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3589, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}