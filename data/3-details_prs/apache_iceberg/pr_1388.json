{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDc0MTUxNTQz", "number": 1388, "title": "[Parquet Vectorized Reads] Fix reading of files with mix of dictionary and non-dictionary encoded row groups", "bodyText": "\u2026", "createdAt": "2020-08-26T21:03:13Z", "url": "https://github.com/apache/iceberg/pull/1388", "merged": true, "mergeCommit": {"oid": "e815318a5378c126e05888c55c7e02955b0946f0"}, "closed": true, "closedAt": "2020-08-28T17:02:53Z", "author": {"login": "samarthjain"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdCykoggH2gAyNDc0MTUxNTQzOmZlZGNhOGZkZjNlN2Q2Yjk2MzAwNjcxNDAzOGIyMDkxNmNiZDEzYjA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdDYEhSAFqTQ3Nzg4OTM0Mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "fedca8fdf3e7d6b963006714038b20916cbd13b0", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/fedca8fdf3e7d6b963006714038b20916cbd13b0", "committedDate": "2020-08-26T21:20:21Z", "message": "[Parquet Vectorized Reads] Fix reading of files with mix of dictionary and non-dictionary encoded row groups"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "82cc088f08a0dda5bf91236b6487af3952fb6e7b", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/82cc088f08a0dda5bf91236b6487af3952fb6e7b", "committedDate": "2020-08-26T21:02:12Z", "message": "[Parquet Vectorized Reads] Fix reading of files with mix of dictionary and non-dictionary encoded row groups"}, "afterCommit": {"oid": "fedca8fdf3e7d6b963006714038b20916cbd13b0", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/fedca8fdf3e7d6b963006714038b20916cbd13b0", "committedDate": "2020-08-26T21:20:21Z", "message": "[Parquet Vectorized Reads] Fix reading of files with mix of dictionary and non-dictionary encoded row groups"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3MTY5NzMw", "url": "https://github.com/apache/iceberg/pull/1388#pullrequestreview-477169730", "createdAt": "2020-08-28T00:38:04Z", "commit": {"oid": "fedca8fdf3e7d6b963006714038b20916cbd13b0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMDozODowNFrOHIlsXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMDozODowNFrOHIlsXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc2ODIyMA==", "bodyText": "What about adding a Parquet.concat util method? I don't think it is a good idea to make ParquetIO public just for this test case. But it would be nice to have a concat method somewhere that could concatenate Parquet files.", "url": "https://github.com/apache/iceberg/pull/1388#discussion_r478768220", "createdAt": "2020-08-28T00:38:04Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryEncodedVectorizedReads.java", "diffHunk": "@@ -39,4 +51,48 @@\n   public void testVectorizedReadsWithNewContainers() throws IOException {\n \n   }\n+\n+  @Test\n+  public void testMixedDictionaryNonDictionaryReads() throws IOException {\n+    Schema schema = new Schema(SUPPORTED_PRIMITIVES.fields());\n+\n+    File dictionaryEncodedFile = temp.newFile();\n+    Assert.assertTrue(\"Delete should succeed\", dictionaryEncodedFile.delete());\n+    Iterable<GenericData.Record> dictionaryEncodableData = RandomData.generateDictionaryEncodableData(\n+        schema,\n+        10000,\n+        0L,\n+        RandomData.DEFAULT_NULL_PERCENTAGE);\n+    try (FileAppender<GenericData.Record> writer = getParquetWriter(schema, dictionaryEncodedFile)) {\n+      writer.addAll(dictionaryEncodableData);\n+    }\n+\n+    File plainEncodingFile = temp.newFile();\n+    Assert.assertTrue(\"Delete should succeed\", plainEncodingFile.delete());\n+    Iterable<GenericData.Record> nonDictionaryData = RandomData.generate(schema, 10000, 0L,\n+        RandomData.DEFAULT_NULL_PERCENTAGE);\n+    try (FileAppender<GenericData.Record> writer = getParquetWriter(schema, plainEncodingFile)) {\n+      writer.addAll(nonDictionaryData);\n+    }\n+\n+    File mixedFile = temp.newFile();\n+    Assert.assertTrue(\"Delete should succeed\", mixedFile.delete());\n+    OutputFile outputFile = Files.localOutput(mixedFile);\n+    int rowGroupSize = Integer.parseInt(PARQUET_ROW_GROUP_SIZE_BYTES_DEFAULT);\n+    ParquetFileWriter writer = new ParquetFileWriter(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fedca8fdf3e7d6b963006714038b20916cbd13b0"}, "originalPosition": 58}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "41c87bc9aff3d87f743c6d3b94e91e3c97523df3", "author": {"user": {"login": "samarthjain", "name": "Samarth Jain"}}, "url": "https://github.com/apache/iceberg/commit/41c87bc9aff3d87f743c6d3b94e91e3c97523df3", "committedDate": "2020-08-28T09:28:35Z", "message": "Add a utility to concatenate parquet files"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3ODg4NTk5", "url": "https://github.com/apache/iceberg/pull/1388#pullrequestreview-477888599", "createdAt": "2020-08-28T17:00:30Z", "commit": {"oid": "41c87bc9aff3d87f743c6d3b94e91e3c97523df3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNzowMDozMFrOHJN8Tg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNzowMDozMFrOHJN8Tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQyNzY2Mg==", "bodyText": "We can use the default row group size from table properties here. It will be ignored when appending files because row groups are appended directly and not rewritten.", "url": "https://github.com/apache/iceberg/pull/1388#discussion_r479427662", "createdAt": "2020-08-28T17:00:30Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -690,4 +692,25 @@ private ParquetReadBuilder(org.apache.parquet.io.InputFile file) {\n       return new ParquetReadSupport<>(schema, readSupport, callInit, nameMapping);\n     }\n   }\n+\n+  /**\n+   * @param inputFiles   an {@link Iterable} of parquet files. The order of iteration determines the order in which\n+   *                     content of files are read and written to the @param outputFile\n+   * @param outputFile   the output parquet file containing all the data from @param inputFiles\n+   * @param rowGroupSize the row group size to use when writing the @param outputFile\n+   * @param schema       the schema of the data\n+   * @param metadata     extraMetadata to write at the footer of the @param outputFile\n+   */\n+  public static void concat(Iterable<File> inputFiles, File outputFile, int rowGroupSize, Schema schema,\n+                            Map<String, String> metadata) throws IOException {\n+    OutputFile file = Files.localOutput(outputFile);\n+    ParquetFileWriter writer = new ParquetFileWriter(\n+            ParquetIO.file(file), ParquetSchemaUtil.convert(schema, \"table\"),\n+            ParquetFileWriter.Mode.CREATE, rowGroupSize, 0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41c87bc9aff3d87f743c6d3b94e91e3c97523df3"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc3ODg5MzQy", "url": "https://github.com/apache/iceberg/pull/1388#pullrequestreview-477889342", "createdAt": "2020-08-28T17:01:39Z", "commit": {"oid": "41c87bc9aff3d87f743c6d3b94e91e3c97523df3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNzowMTo0MFrOHJN-dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQxNzowMTo0MFrOHJN-dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTQyODIxNA==", "bodyText": "I think the input files and output file should use InputFile and OutputFile. That way this isn't limited to just the local FS.", "url": "https://github.com/apache/iceberg/pull/1388#discussion_r479428214", "createdAt": "2020-08-28T17:01:40Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -690,4 +692,25 @@ private ParquetReadBuilder(org.apache.parquet.io.InputFile file) {\n       return new ParquetReadSupport<>(schema, readSupport, callInit, nameMapping);\n     }\n   }\n+\n+  /**\n+   * @param inputFiles   an {@link Iterable} of parquet files. The order of iteration determines the order in which\n+   *                     content of files are read and written to the @param outputFile\n+   * @param outputFile   the output parquet file containing all the data from @param inputFiles\n+   * @param rowGroupSize the row group size to use when writing the @param outputFile\n+   * @param schema       the schema of the data\n+   * @param metadata     extraMetadata to write at the footer of the @param outputFile\n+   */\n+  public static void concat(Iterable<File> inputFiles, File outputFile, int rowGroupSize, Schema schema,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "41c87bc9aff3d87f743c6d3b94e91e3c97523df3"}, "originalPosition": 29}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4120, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}