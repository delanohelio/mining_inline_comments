{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIzNDUyNjE2", "number": 1784, "reviewThreads": {"totalCount": 22, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMDozMToyOVrOE6wQQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDozODo1MVrOE7tD0g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMDQzNDU2OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMDozMToyOVrOH2A_Pg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwNDoyMzoyNlrOH2MOCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQwMTM0Mg==", "bodyText": "One huge drawback to this is we can't actually use any Catalog Options that have been specified, so if we have options in the future than alter the way a Metadata table is read we won't have them here.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526401342", "createdAt": "2020-11-18T20:31:29Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -137,7 +138,19 @@\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      if (tableName.startsWith(\"spark_catalog\")) {\n+        // Do to the design of Spark, we cannot pass multi element namespaces to the session catalog\n+        // We also don't know whether the Catalog is Hive or Hadoop Based, so we will try to load it\n+        // in the hive manner first, then fall back and try the location if we have completely run out of options\n+        // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+        try {\n+          return noCatalogReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQxMTE0MA==", "bodyText": "If we could extend out into BaseSpark3Actions this would be a good place to diverge, since I think we could load the metadata table directly from the catalog, and bypass the name resolution code. But we can't do that without redoing the hierarchy or having the Rewrite Actions use reflection to determine their \"loadMetadataTable\" Method", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526411140", "createdAt": "2020-11-18T20:49:36Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -137,7 +138,19 @@\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      if (tableName.startsWith(\"spark_catalog\")) {\n+        // Do to the design of Spark, we cannot pass multi element namespaces to the session catalog\n+        // We also don't know whether the Catalog is Hive or Hadoop Based, so we will try to load it\n+        // in the hive manner first, then fall back and try the location if we have completely run out of options\n+        // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+        try {\n+          return noCatalogReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQwMTM0Mg=="}, "originalCommit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU4NTM1Mw==", "bodyText": "Still to figure out is how we do this, since the current method is static and it must be because the hierarchy is currently setup such that some spark actions do not extend from this class", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526585353", "createdAt": "2020-11-19T04:23:26Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -137,7 +138,19 @@\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      if (tableName.startsWith(\"spark_catalog\")) {\n+        // Do to the design of Spark, we cannot pass multi element namespaces to the session catalog\n+        // We also don't know whether the Catalog is Hive or Hadoop Based, so we will try to load it\n+        // in the hive manner first, then fall back and try the location if we have completely run out of options\n+        // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+        try {\n+          return noCatalogReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQwMTM0Mg=="}, "originalCommit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMDc0MzA2OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMTo1NjoyOVrOH2D5Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjozMjo0OFrOH2FDIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ0ODk5OQ==", "bodyText": "Would it make sense to call it dataFrameReader instead of noCatalogReader everywhere?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526448999", "createdAt": "2020-11-18T21:56:29Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -137,7 +138,19 @@\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      if (tableName.startsWith(\"spark_catalog\")) {\n+        // Do to the design of Spark, we cannot pass multi element namespaces to the session catalog\n+        // We also don't know whether the Catalog is Hive or Hadoop Based, so we will try to load it\n+        // in the hive manner first, then fall back and try the location if we have completely run out of options\n+        // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+        try {\n+          return noCatalogReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ2Nzg3Mg==", "bodyText": "\ud83e\udd37 It's a no catalog reader whenever it's used :) But sure I can rename it.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526467872", "createdAt": "2020-11-18T22:32:48Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -137,7 +138,19 @@\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      if (tableName.startsWith(\"spark_catalog\")) {\n+        // Do to the design of Spark, we cannot pass multi element namespaces to the session catalog\n+        // We also don't know whether the Catalog is Hive or Hadoop Based, so we will try to load it\n+        // in the hive manner first, then fall back and try the location if we have completely run out of options\n+        // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+        try {\n+          return noCatalogReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ0ODk5OQ=="}, "originalCommit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMDc0Nzc0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMTo1Nzo1MFrOH2D8Tw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjozODo0OVrOH2FN8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ0OTc0Mw==", "bodyText": "Do we want to put this logic into a separate method like loadMetadataUsingCatalog or something?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526449743", "createdAt": "2020-11-18T21:57:50Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -137,7 +138,19 @@\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      if (tableName.startsWith(\"spark_catalog\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3MDY0Mg==", "bodyText": "My only thought on why not to do this, is that hopefully in the future we get to remove the \"startsWith(spark_catalog)\" branch, and then the method is just spark.table(). But I can change it for now.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526470642", "createdAt": "2020-11-18T22:38:49Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -137,7 +138,19 @@\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      if (tableName.startsWith(\"spark_catalog\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ0OTc0Mw=="}, "originalCommit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMDc1Mjg0OnYy", "diffSide": "RIGHT", "path": "spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMTo1OToyM1rOH2D_kg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMTo1OToyM1rOH2D_kg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ1MDU3OA==", "bodyText": "nit: throws Exception?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526450578", "createdAt": "2020-11-18T21:59:23Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java", "diffHunk": "@@ -110,4 +112,56 @@ public void testSparkCatalogHiveTable() throws TableAlreadyExistsException, NoSu\n         results.contains(\"file:\" + location + \"/data/trashfile\"));\n   }\n \n+  @Test\n+  public void testSparkSessionCatalogHadoopTable()\n+      throws TableAlreadyExistsException, NoSuchTableException, IOException, NoSuchNamespaceException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMDc1MzE4OnYy", "diffSide": "RIGHT", "path": "spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMTo1OTozMVrOH2D_yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMTo1OTozMVrOH2D_yA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ1MDYzMg==", "bodyText": "nit: same here", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526450632", "createdAt": "2020-11-18T21:59:31Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java", "diffHunk": "@@ -110,4 +112,56 @@ public void testSparkCatalogHiveTable() throws TableAlreadyExistsException, NoSu\n         results.contains(\"file:\" + location + \"/data/trashfile\"));\n   }\n \n+  @Test\n+  public void testSparkSessionCatalogHadoopTable()\n+      throws TableAlreadyExistsException, NoSuchTableException, IOException, NoSuchNamespaceException {\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.warehouse\", tableLocation);\n+    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n+\n+    String[] database = {\"default\"};\n+    Identifier id = Identifier.of(database, \"table\");\n+    Map<String, String> options = Maps.newHashMap();\n+    Transform[] transforms = {};\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    SparkTable table = (SparkTable) cat.loadTable(id);\n+\n+    spark.sql(\"INSERT INTO default.table VALUES (1,1,1)\");\n+\n+    String location = table.table().location().replaceFirst(\"file:\", \"\");\n+    new File(location + \"/data/trashfile\").createNewFile();\n+\n+    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n+        .olderThan(System.currentTimeMillis() + 1000).execute();\n+    Assert.assertTrue(\"trash file should be removed\",\n+        results.contains(\"file:\" + location + \"/data/trashfile\"));\n+  }\n+\n+  @Test\n+  public void testSparkSessionCatalogHiveTable()\n+      throws TableAlreadyExistsException, NoSuchTableException, IOException, NoSuchNamespaceException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "703b68cf6a7cc20b7fefb75f7e7c4e7b18b62575"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMTEwNDU2OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDowNTo1NFrOH2HTEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDowNTo1NFrOH2HTEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwNDcyMw==", "bodyText": "Nit: typo: \"due\" not \"do\".", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526504723", "createdAt": "2020-11-19T00:05:54Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,16 +129,35 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMTEwNjQ2OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDowNjo0NVrOH2HUOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDowNjo0NVrOH2HUOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwNTAxOA==", "bodyText": "I think this is a bug in Spark. There isn't a work-around that I know of.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526505018", "createdAt": "2020-11-19T00:06:45Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,16 +129,35 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n+      // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n+      // hadoop location method if that fails\n+      // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMTExNDAyOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDoxMDoxMFrOH2HYww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwNDo0NzowNFrOH2MmZA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwNjE3OQ==", "bodyText": "If we know that the catalog is spark_catalog, then we should just try to load without removing the catalog name. If we remove the catalog name, then we don't know that the right table will be loaded because the Spark catalog may not be the session's current catalog.\nAnd, if the metadata table type works then so would using the prefix spark_catalog. Names like spark_catalog.db.table work, it is just spark_catalog.db.table.meta that does not. If meta is added and the current catalog is spark_catalog, then I think it will fail no matter what.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526506179", "createdAt": "2020-11-19T00:10:10Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,16 +129,35 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n+      // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n+      // hadoop location method if that fails\n+      // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+      try {\n+        return dataFrameReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU5MTU4OA==", "bodyText": "I don't think I follow.  Spark checks\ndef isSessionCatalog(catalog: CatalogPlugin): Boolean = {\n    catalog.name().equalsIgnoreCase(CatalogManager.SESSION_CATALOG_NAME)\n  }\nTo decide if the catalog is the session catalog and fail the parsing. If it does then lookup table matches this pattern\nobject SessionCatalogAndIdentifier {\n    import org.apache.spark.sql.connector.catalog.CatalogV2Implicits.MultipartIdentifierHelper\n\n    def unapply(parts: Seq[String]): Option[(CatalogPlugin, Identifier)] = parts match {\n      case CatalogAndIdentifier(catalog, ident) if CatalogV2Util.isSessionCatalog(catalog) =>\n        if (ident.namespace.length != 1) {\n          throw new AnalysisException(\n            s\"The namespace in session catalog must have exactly one name part: ${parts.quoted}\")\n        }\n        Some(catalog, ident)\n      case _ => None\n    }\n  }\nSo it doesn't matter if the spark-catalog is the current Catalog or not, we can never load a table by name with more than 3 pieces if it starts with spark-catalog.\nHere we are falling back to looking into the default hive catalog, which is all we can do without having direct access to Spark3 CatalogPlugins.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526591588", "createdAt": "2020-11-19T04:47:04Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,16 +129,35 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n+      // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n+      // hadoop location method if that fails\n+      // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+      try {\n+        return dataFrameReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwNjE3OQ=="}, "originalCommit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMTExNzEwOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDoxMTozMFrOH2HamA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwNDozOToxMVrOH2MeIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwNjY0OA==", "bodyText": "Missing return?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526506648", "createdAt": "2020-11-19T00:11:30Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,16 +129,35 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n+      // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n+      // hadoop location method if that fails\n+      // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+      try {\n+        return dataFrameReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);\n+      } catch (NoSuchTableException noSuchTableException) {\n+        return dataFrameReader.load(tableLocation + \"#\" + type);\n+      }\n+    } else {\n+      return spark.table(tableName + \".\" + type);\n+    }\n+  }\n+\n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n                                                   MetadataTableType type) {\n-    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n       // Hadoop Table or Metadata location passed, load without a catalog\n-      return noCatalogReader.load(tableName + \"#\" + type);\n+      return dataFrameReader.load(tableName + \"#\" + type);\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      loadMetadataTableFromCatalog(spark, tableName, tableLocation, type);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU4OTQ3NQ==", "bodyText": "yep sorry, when I renamed this I forgot to add the return", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526589475", "createdAt": "2020-11-19T04:39:11Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,16 +129,35 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static Dataset<Row> loadMetadataTableFromCatalog(SparkSession spark, String tableName, String tableLocation,\n+                                                           MetadataTableType type) {\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n+    if (tableName.startsWith(\"spark_catalog\")) {\n+      // Do to the design of Spark, we cannot pass multi-element namespaces to the session catalog.\n+      // We also don't know whether the Catalog is Hive or Hadoop Based so we can't just load one way or the other.\n+      // Instead we will try to load the metadata table in the hive manner first, then fall back and try the\n+      // hadoop location method if that fails\n+      // TODO remove this when we have Spark workaround for multipart identifiers in SparkSessionCatalog\n+      try {\n+        return dataFrameReader.load(tableName.replaceFirst(\"spark_catalog\\\\.\", \"\") + \".\" + type);\n+      } catch (NoSuchTableException noSuchTableException) {\n+        return dataFrameReader.load(tableLocation + \"#\" + type);\n+      }\n+    } else {\n+      return spark.table(tableName + \".\" + type);\n+    }\n+  }\n+\n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n                                                   MetadataTableType type) {\n-    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n       // Hadoop Table or Metadata location passed, load without a catalog\n-      return noCatalogReader.load(tableName + \"#\" + type);\n+      return dataFrameReader.load(tableName + \"#\" + type);\n     }\n     // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n+      loadMetadataTableFromCatalog(spark, tableName, tableLocation, type);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwNjY0OA=="}, "originalCommit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMTE1MjQ5OnYy", "diffSide": "RIGHT", "path": "spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwMDoyNzoxNlrOH2HvOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQxNTo0OToyM1rOH2lHwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUxMTkyOQ==", "bodyText": "I think this test case only works because HiveCatalogs uses the value of hive.metastore.uris from the environment's hive-site.xml. By removing spark_catalog and then using the DataFrameReader, the Hive catalog from HiveCatalogs is used, which has the same URI.\nI think this is actually the right thing to do, but I would do it more directly and obviously so that it is clear what is happening:\n\nGet the session catalog from the catalog manager\nIf the session catalog is a SparkSessionCatalog, get the underlying Iceberg catalog\nUse the Iceberg catalog to load the metadata table, because it accepts the full table identifier", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526511929", "createdAt": "2020-11-19T00:27:16Z", "author": {"login": "rdblue"}, "path": "spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java", "diffHunk": "@@ -110,4 +112,54 @@ public void testSparkCatalogHiveTable() throws TableAlreadyExistsException, NoSu\n         results.contains(\"file:\" + location + \"/data/trashfile\"));\n   }\n \n+  @Test\n+  public void testSparkSessionCatalogHadoopTable() throws Exception {\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.warehouse\", tableLocation);\n+    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n+\n+    String[] database = {\"default\"};\n+    Identifier id = Identifier.of(database, \"table\");\n+    Map<String, String> options = Maps.newHashMap();\n+    Transform[] transforms = {};\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    SparkTable table = (SparkTable) cat.loadTable(id);\n+\n+    spark.sql(\"INSERT INTO default.table VALUES (1,1,1)\");\n+\n+    String location = table.table().location().replaceFirst(\"file:\", \"\");\n+    new File(location + \"/data/trashfile\").createNewFile();\n+\n+    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n+        .olderThan(System.currentTimeMillis() + 1000).execute();\n+    Assert.assertTrue(\"trash file should be removed\",\n+        results.contains(\"file:\" + location + \"/data/trashfile\"));\n+  }\n+\n+  @Test\n+  public void testSparkSessionCatalogHiveTable() throws Exception {\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hive\");\n+    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n+\n+    String[] database = {\"default\"};\n+    Identifier id = Identifier.of(database, \"sessioncattest\");\n+    Map<String, String> options = Maps.newHashMap();\n+    Transform[] transforms = {};\n+    cat.dropTable(id);\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    SparkTable table = (SparkTable) cat.loadTable(id);\n+\n+    spark.sql(\"INSERT INTO default.sessioncattest VALUES (1,1,1)\");\n+\n+    String location = table.table().location().replaceFirst(\"file:\", \"\");\n+    new File(location + \"/data/trashfile\").createNewFile();\n+\n+    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n+        .olderThan(System.currentTimeMillis() + 1000).execute();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjU4NDY4Mg==", "bodyText": "Discussed on slack: Notes here\nMain reason I didn't do this originally is we then need to break the code in to a Spark3 and Spark2 versions. But if we could, we wouldn't have to do this at all since we could just in the Spark 3 mode load the table by getting the Catalog directly and loading from there, instead of trying to fall back to the non-catalog path. Thinking we are going to go down the path of attempting to get Spark Version specific code in here.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526584682", "createdAt": "2020-11-19T04:20:33Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java", "diffHunk": "@@ -110,4 +112,54 @@ public void testSparkCatalogHiveTable() throws TableAlreadyExistsException, NoSu\n         results.contains(\"file:\" + location + \"/data/trashfile\"));\n   }\n \n+  @Test\n+  public void testSparkSessionCatalogHadoopTable() throws Exception {\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.warehouse\", tableLocation);\n+    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n+\n+    String[] database = {\"default\"};\n+    Identifier id = Identifier.of(database, \"table\");\n+    Map<String, String> options = Maps.newHashMap();\n+    Transform[] transforms = {};\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    SparkTable table = (SparkTable) cat.loadTable(id);\n+\n+    spark.sql(\"INSERT INTO default.table VALUES (1,1,1)\");\n+\n+    String location = table.table().location().replaceFirst(\"file:\", \"\");\n+    new File(location + \"/data/trashfile\").createNewFile();\n+\n+    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n+        .olderThan(System.currentTimeMillis() + 1000).execute();\n+    Assert.assertTrue(\"trash file should be removed\",\n+        results.contains(\"file:\" + location + \"/data/trashfile\"));\n+  }\n+\n+  @Test\n+  public void testSparkSessionCatalogHiveTable() throws Exception {\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hive\");\n+    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n+\n+    String[] database = {\"default\"};\n+    Identifier id = Identifier.of(database, \"sessioncattest\");\n+    Map<String, String> options = Maps.newHashMap();\n+    Transform[] transforms = {};\n+    cat.dropTable(id);\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    SparkTable table = (SparkTable) cat.loadTable(id);\n+\n+    spark.sql(\"INSERT INTO default.sessioncattest VALUES (1,1,1)\");\n+\n+    String location = table.table().location().replaceFirst(\"file:\", \"\");\n+    new File(location + \"/data/trashfile\").createNewFile();\n+\n+    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n+        .olderThan(System.currentTimeMillis() + 1000).execute();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUxMTkyOQ=="}, "originalCommit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjk5MzM0Ng==", "bodyText": "More notes for myself, Once we have direct access to to catalog manager, let's just do\nString[] ns = {\"default\", \"sessioncattest\"};\nDataset.ofRows(spark, DataSourceV2Relation.create(cat.loadTable(Identifier.of(ns, \"entries\")), Option.apply(cat), Option.apply(id)));\nManually resolving the relation and ignoring the misfeature of the session catalog name resolution", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r526993346", "createdAt": "2020-11-19T15:49:23Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction3.java", "diffHunk": "@@ -110,4 +112,54 @@ public void testSparkCatalogHiveTable() throws TableAlreadyExistsException, NoSu\n         results.contains(\"file:\" + location + \"/data/trashfile\"));\n   }\n \n+  @Test\n+  public void testSparkSessionCatalogHadoopTable() throws Exception {\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.warehouse\", tableLocation);\n+    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n+\n+    String[] database = {\"default\"};\n+    Identifier id = Identifier.of(database, \"table\");\n+    Map<String, String> options = Maps.newHashMap();\n+    Transform[] transforms = {};\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    SparkTable table = (SparkTable) cat.loadTable(id);\n+\n+    spark.sql(\"INSERT INTO default.table VALUES (1,1,1)\");\n+\n+    String location = table.table().location().replaceFirst(\"file:\", \"\");\n+    new File(location + \"/data/trashfile\").createNewFile();\n+\n+    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n+        .olderThan(System.currentTimeMillis() + 1000).execute();\n+    Assert.assertTrue(\"trash file should be removed\",\n+        results.contains(\"file:\" + location + \"/data/trashfile\"));\n+  }\n+\n+  @Test\n+  public void testSparkSessionCatalogHiveTable() throws Exception {\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\");\n+    spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hive\");\n+    SparkSessionCatalog cat = (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n+\n+    String[] database = {\"default\"};\n+    Identifier id = Identifier.of(database, \"sessioncattest\");\n+    Map<String, String> options = Maps.newHashMap();\n+    Transform[] transforms = {};\n+    cat.dropTable(id);\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    SparkTable table = (SparkTable) cat.loadTable(id);\n+\n+    spark.sql(\"INSERT INTO default.sessioncattest VALUES (1,1,1)\");\n+\n+    String location = table.table().location().replaceFirst(\"file:\", \"\");\n+    new File(location + \"/data/trashfile\").createNewFile();\n+\n+    List<String> results = Actions.forTable(table.table()).removeOrphanFiles()\n+        .olderThan(System.currentTimeMillis() + 1000).execute();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUxMTkyOQ=="}, "originalCommit": {"oid": "6e621037c6d46acb3f96370e3fd752eff8be1ecb"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNTQ2ODI2OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMDo0Mjo0MFrOH2w1jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMDo0Mjo0MFrOH2w1jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NTI5NQ==", "bodyText": "Everything below here was pulled from the Create PR and is basically a copy of the Catalog And Identifier Resolution methods from Spark.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527185295", "createdAt": "2020-11-19T20:42:40Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNTQ3MDY1OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMDo0MzoyOVrOH2w3EA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMDo0MzoyOVrOH2w3EA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE4NTY4MA==", "bodyText": "This is how Spark would have made the relation from our metadata table if it didn't think multiple pieces in the Namespace was a dealbreaker.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527185680", "createdAt": "2020-11-19T20:43:29Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNTU4NDMzOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToxNToxMFrOH2x7RA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMTo1NTozOFrOH2zdcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzE0MA==", "bodyText": "How stable do we expect implicits to be?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527203140", "createdAt": "2020-11-19T21:15:10Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+    CatalogPlugin currentCatalog = spark.sessionState().catalogManager().currentCatalog();\n+    String[] currentNamespace = spark.sessionState().catalogManager().currentNamespace();\n+    if (namePartsSeq.length() == 1) {\n+      return new CatalogAndIdentifier(currentCatalog, Identifier.of(currentNamespace, namePartsSeq.head()));\n+    } else {\n+      try {\n+        CatalogPlugin namedCatalog = spark.sessionState().catalogManager().catalog(namePartsSeq.head());\n+        return new CatalogAndIdentifier(namedCatalog,\n+            CatalogV2Implicits.MultipartIdentifierHelper((Seq<String>) namePartsSeq.tail()).asIdentifier());\n+      } catch (Exception e) {\n+        return new CatalogAndIdentifier(currentCatalog,\n+            CatalogV2Implicits.MultipartIdentifierHelper(namePartsSeq).asIdentifier());\n+      }\n+    }\n+  }\n+\n+  public static TableIdentifier toTableIdentifier(Identifier table) {\n+    return new CatalogV2Implicits.IdentifierHelper(table).asTableIdentifier();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 99}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIyODI3Mg==", "bodyText": "This isn't used here, it's only in the Create Action. I'll remove it for now and we can decide whether we need it later.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527228272", "createdAt": "2020-11-19T21:55:38Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+    CatalogPlugin currentCatalog = spark.sessionState().catalogManager().currentCatalog();\n+    String[] currentNamespace = spark.sessionState().catalogManager().currentNamespace();\n+    if (namePartsSeq.length() == 1) {\n+      return new CatalogAndIdentifier(currentCatalog, Identifier.of(currentNamespace, namePartsSeq.head()));\n+    } else {\n+      try {\n+        CatalogPlugin namedCatalog = spark.sessionState().catalogManager().catalog(namePartsSeq.head());\n+        return new CatalogAndIdentifier(namedCatalog,\n+            CatalogV2Implicits.MultipartIdentifierHelper((Seq<String>) namePartsSeq.tail()).asIdentifier());\n+      } catch (Exception e) {\n+        return new CatalogAndIdentifier(currentCatalog,\n+            CatalogV2Implicits.MultipartIdentifierHelper(namePartsSeq).asIdentifier());\n+      }\n+    }\n+  }\n+\n+  public static TableIdentifier toTableIdentifier(Identifier table) {\n+    return new CatalogV2Implicits.IdentifierHelper(table).asTableIdentifier();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzE0MA=="}, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNTU4NjU3OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToxNTozN1rOH2x8dA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMTo1NToxMlrOH2zckA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzQ0NA==", "bodyText": "Can we factor this out into a separate method like toIdentifier?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527203444", "createdAt": "2020-11-19T21:15:37Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+    CatalogPlugin currentCatalog = spark.sessionState().catalogManager().currentCatalog();\n+    String[] currentNamespace = spark.sessionState().catalogManager().currentNamespace();\n+    if (namePartsSeq.length() == 1) {\n+      return new CatalogAndIdentifier(currentCatalog, Identifier.of(currentNamespace, namePartsSeq.head()));\n+    } else {\n+      try {\n+        CatalogPlugin namedCatalog = spark.sessionState().catalogManager().catalog(namePartsSeq.head());\n+        return new CatalogAndIdentifier(namedCatalog,\n+            CatalogV2Implicits.MultipartIdentifierHelper((Seq<String>) namePartsSeq.tail()).asIdentifier());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzYzOQ==", "bodyText": "I mean the construction of MultipartIdentifierHelper.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527203639", "createdAt": "2020-11-19T21:15:58Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+    CatalogPlugin currentCatalog = spark.sessionState().catalogManager().currentCatalog();\n+    String[] currentNamespace = spark.sessionState().catalogManager().currentNamespace();\n+    if (namePartsSeq.length() == 1) {\n+      return new CatalogAndIdentifier(currentCatalog, Identifier.of(currentNamespace, namePartsSeq.head()));\n+    } else {\n+      try {\n+        CatalogPlugin namedCatalog = spark.sessionState().catalogManager().catalog(namePartsSeq.head());\n+        return new CatalogAndIdentifier(namedCatalog,\n+            CatalogV2Implicits.MultipartIdentifierHelper((Seq<String>) namePartsSeq.tail()).asIdentifier());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzQ0NA=="}, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIyODA0OA==", "bodyText": "I am just dropping this whole section, so we just use Identifier.of directly. We won't use the implicit then.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527228048", "createdAt": "2020-11-19T21:55:12Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+    CatalogPlugin currentCatalog = spark.sessionState().catalogManager().currentCatalog();\n+    String[] currentNamespace = spark.sessionState().catalogManager().currentNamespace();\n+    if (namePartsSeq.length() == 1) {\n+      return new CatalogAndIdentifier(currentCatalog, Identifier.of(currentNamespace, namePartsSeq.head()));\n+    } else {\n+      try {\n+        CatalogPlugin namedCatalog = spark.sessionState().catalogManager().catalog(namePartsSeq.head());\n+        return new CatalogAndIdentifier(namedCatalog,\n+            CatalogV2Implicits.MultipartIdentifierHelper((Seq<String>) namePartsSeq.tail()).asIdentifier());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzQ0NA=="}, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNTU4ODk0OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToxNjoxOFrOH2x98g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToyMzo0N1rOH2yNsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzgyNg==", "bodyText": "Can we do the precondition first in the method?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527203826", "createdAt": "2020-11-19T21:16:18Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNzg1OA==", "bodyText": "Yeah I just like nonEmpty more than !(isEmpty)", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527207858", "createdAt": "2020-11-19T21:23:47Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwMzgyNg=="}, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNTU5MDk0OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToxNjo1NFrOH2x_Ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMTo0MDo0NVrOH2y8Qg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNDE1MA==", "bodyText": "Can we introduce CatalogManager catalogManager variable and reuse it in all lines below?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527204150", "createdAt": "2020-11-19T21:16:54Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+    CatalogPlugin currentCatalog = spark.sessionState().catalogManager().currentCatalog();\n+    String[] currentNamespace = spark.sessionState().catalogManager().currentNamespace();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIxOTc3OA==", "bodyText": "Yeah, mostly this was just a direct translation of the scala code with explicit implicit, we can change it to be more idiomatic java", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527219778", "createdAt": "2020-11-19T21:40:45Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts) {\n+    Seq<String> namePartsSeq = JavaConverters.asScalaIterator(nameParts.iterator()).toSeq();\n+    Preconditions.checkArgument(namePartsSeq.nonEmpty(),\n+        \"Cannot determine catalog and Identifier from empty name parts\");\n+    CatalogPlugin currentCatalog = spark.sessionState().catalogManager().currentCatalog();\n+    String[] currentNamespace = spark.sessionState().catalogManager().currentNamespace();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNDE1MA=="}, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNTU5ODU3OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToxOTowNFrOH2yDww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToxOTowNFrOH2yDww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNTMxNQ==", "bodyText": "Can we split it into multiple lines?\n... parser = spark.sessionState().sqlParser();\n... nameParts = parser.parseMultipartIdentifier(name);\nreturn catalogAndIdentifier(spark, JavaConverters.seqAsJavaList(nameParts));", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527205315", "createdAt": "2020-11-19T21:19:04Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +565,83 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /*\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type)\n+      throws CatalogNotFoundException, ParseException, NoSuchTableException {\n+\n+    CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+    if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+      BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+      Identifier baseIdent = catalogAndIdentifier.identifier;\n+      Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+      Table metaTable = catalog.loadTable(metaIdent);\n+      return Dataset.ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));\n+    } else {\n+      throw new CatalogNotFoundException(String.format(\"Cannot cast %s as an Iceberg catalog\",\n+          catalogAndIdentifier.catalog.name()));\n+    }\n+  }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) throws ParseException {\n+    return catalogAndIdentifier(spark,\n+          JavaConverters.seqAsJavaList(spark.sessionState().sqlParser().parseMultipartIdentifier(name)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 68}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNTYwMjQwOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToyMDowMVrOH2yF-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMToyNjoyNFrOH2yTQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNTg4Mg==", "bodyText": "Can we invert the condition and check if it starts with 3?", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527205882", "createdAt": "2020-11-19T21:20:01Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,33 +127,56 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static boolean isExpectedCatalogLookupException(Exception exception) {\n+    return exception.getMessage().contains(\"AnalysisException\") ||\n+        exception.getMessage().contains(\"SparkException\") ||\n+        exception.getMessage().contains(\"NoSuchTableException\") ||\n+        exception.getMessage().contains(\"CatalogNotFoundException\");\n+  }\n+\n+  // Attempt to use Spark3 Catalog resolution if available on the path\n+  private static DynMethods.StaticMethod loadCatalogImpl = null;\n+\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String tableName, MetadataTableType type)\n+      throws NoSuchMethodException {\n+    if (loadCatalogImpl == null) {\n+      loadCatalogImpl = DynMethods.builder(\"loadCatalogMetadataTable\")\n+          .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n+              SparkSession.class, String.class, MetadataTableType.class)\n+          .buildStaticChecked();\n+    }\n+    return loadCatalogImpl.invoke(spark, tableName, type);\n+  }\n+\n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n                                                   MetadataTableType type) {\n-    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n       // Hadoop Table or Metadata location passed, load without a catalog\n-      return noCatalogReader.load(tableName + \"#\" + type);\n+      return dataFrameReader.load(tableName + \"#\" + type);\n     }\n-    // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n-    } catch (Exception e) {\n-      if (!(e instanceof ParseException || e instanceof AnalysisException)) {\n-        // Rethrow unexpected exceptions\n-        throw e;\n+      // Try DSV2 catalog based name based resolution\n+      if (!spark.version().startsWith(\"2\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNzA3NA==", "bodyText": "but what about Spark 4 :) , let me change it to a numeral check > 2", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527207074", "createdAt": "2020-11-19T21:22:15Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,33 +127,56 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static boolean isExpectedCatalogLookupException(Exception exception) {\n+    return exception.getMessage().contains(\"AnalysisException\") ||\n+        exception.getMessage().contains(\"SparkException\") ||\n+        exception.getMessage().contains(\"NoSuchTableException\") ||\n+        exception.getMessage().contains(\"CatalogNotFoundException\");\n+  }\n+\n+  // Attempt to use Spark3 Catalog resolution if available on the path\n+  private static DynMethods.StaticMethod loadCatalogImpl = null;\n+\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String tableName, MetadataTableType type)\n+      throws NoSuchMethodException {\n+    if (loadCatalogImpl == null) {\n+      loadCatalogImpl = DynMethods.builder(\"loadCatalogMetadataTable\")\n+          .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n+              SparkSession.class, String.class, MetadataTableType.class)\n+          .buildStaticChecked();\n+    }\n+    return loadCatalogImpl.invoke(spark, tableName, type);\n+  }\n+\n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n                                                   MetadataTableType type) {\n-    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n       // Hadoop Table or Metadata location passed, load without a catalog\n-      return noCatalogReader.load(tableName + \"#\" + type);\n+      return dataFrameReader.load(tableName + \"#\" + type);\n     }\n-    // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n-    } catch (Exception e) {\n-      if (!(e instanceof ParseException || e instanceof AnalysisException)) {\n-        // Rethrow unexpected exceptions\n-        throw e;\n+      // Try DSV2 catalog based name based resolution\n+      if (!spark.version().startsWith(\"2\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNTg4Mg=="}, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwOTI4Mg==", "bodyText": "actually I guess since we have an explicit spark3 module we can check just for 3 ...", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527209282", "createdAt": "2020-11-19T21:26:24Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,33 +127,56 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static boolean isExpectedCatalogLookupException(Exception exception) {\n+    return exception.getMessage().contains(\"AnalysisException\") ||\n+        exception.getMessage().contains(\"SparkException\") ||\n+        exception.getMessage().contains(\"NoSuchTableException\") ||\n+        exception.getMessage().contains(\"CatalogNotFoundException\");\n+  }\n+\n+  // Attempt to use Spark3 Catalog resolution if available on the path\n+  private static DynMethods.StaticMethod loadCatalogImpl = null;\n+\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String tableName, MetadataTableType type)\n+      throws NoSuchMethodException {\n+    if (loadCatalogImpl == null) {\n+      loadCatalogImpl = DynMethods.builder(\"loadCatalogMetadataTable\")\n+          .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n+              SparkSession.class, String.class, MetadataTableType.class)\n+          .buildStaticChecked();\n+    }\n+    return loadCatalogImpl.invoke(spark, tableName, type);\n+  }\n+\n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n                                                   MetadataTableType type) {\n-    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n       // Hadoop Table or Metadata location passed, load without a catalog\n-      return noCatalogReader.load(tableName + \"#\" + type);\n+      return dataFrameReader.load(tableName + \"#\" + type);\n     }\n-    // Try catalog based name based resolution\n     try {\n-      return spark.table(tableName + \".\" + type);\n-    } catch (Exception e) {\n-      if (!(e instanceof ParseException || e instanceof AnalysisException)) {\n-        // Rethrow unexpected exceptions\n-        throw e;\n+      // Try DSV2 catalog based name based resolution\n+      if (!spark.version().startsWith(\"2\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzIwNTg4Mg=="}, "originalCommit": {"oid": "e2b762efb8ed72928fc62e873779bd1abe11bfcb"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNjAzNTAzOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMzozMDowM1rOH22MCQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQyMzozMDowM1rOH22MCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzI3Mjk2OQ==", "bodyText": "Just realized I can move all of this into the Spark3Util class and just return null if we can't find the table via that method", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527272969", "createdAt": "2020-11-19T23:30:03Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,33 +127,56 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  private static boolean isExpectedCatalogLookupException(Exception exception) {\n+    return exception.getMessage().contains(\"AnalysisException\") ||", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43c5c5ff1970d66c711e7fd8613924c4242b3fb3"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwNjY2NjQ1OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwMzozMzowMVrOH28ORQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQwMzozMzowMVrOH28ORQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzM3MTg0NQ==", "bodyText": "nit: I'd put the relation into a separate var to keep this on one line.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527371845", "createdAt": "2020-11-20T03:33:01Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -548,4 +564,97 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  /**\n+   * Returns a Metadata Table Dataset if it can be loaded from a Spark V2 Catalog\n+   *\n+   * Because Spark does not allow more than 1 piece in the namespace for a Session Catalog table, we circumvent\n+   * the entire resolution path for tables and instead look up the table directly ourselves. This lets us correctly\n+   * get metadata tables for the SessionCatalog, if we didn't have to work around this we could just use spark.table.\n+   *\n+   * @param spark SparkSession used for looking up catalog references and tables\n+   * @param name The multipart identifier of the base Iceberg table\n+   * @param type The type of metadata table to load\n+   * @return null if we cannot find the Metadata Table, a Dataset of rows otherwise\n+   */\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String name, MetadataTableType type) {\n+    try {\n+      CatalogAndIdentifier catalogAndIdentifier = catalogAndIdentifier(spark, name);\n+      if (catalogAndIdentifier.catalog instanceof BaseCatalog) {\n+        BaseCatalog catalog = (BaseCatalog) catalogAndIdentifier.catalog;\n+        Identifier baseIdent = catalogAndIdentifier.identifier;\n+        Identifier metaIdent = Identifier.of(ArrayUtils.add(baseIdent.namespace(), baseIdent.name()), type.name());\n+        Table metaTable = catalog.loadTable(metaIdent);\n+        return Dataset\n+            .ofRows(spark, DataSourceV2Relation.create(metaTable, Some.apply(catalog), Some.apply(metaIdent)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ea63004ed1a8ebb51bf42b442562141eb0d27cfb"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDM5NDI5OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDozNzo0MVrOH3f96A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDo0NjozOFrOH3gMxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1NzQ4MA==", "bodyText": "This doesn't need to load the method each time it is called. Usually, I would add orNoop and call build to construct a static field. Then in this method, you'd just need to check whether you have the method or noop:\n  Preconditions.checkArgument(!LOAD_CATALOG.isNoop(), \"Cannot find Spark3Util class ...\");\n  LOAD_CATALOG.invoke(spark, tableName, type);", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527957480", "createdAt": "2020-11-20T20:37:41Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,33 +127,48 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  // Attempt to use Spark3 Catalog resolution if available on the path\n+  private static DynMethods.StaticMethod loadCatalogImpl = null;\n+\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String tableName, MetadataTableType type) {\n+    if (loadCatalogImpl == null) {\n+      try {\n+        loadCatalogImpl = DynMethods.builder(\"loadCatalogMetadataTable\")\n+            .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n+                SparkSession.class, String.class, MetadataTableType.class)\n+            .buildStaticChecked();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "59b573c7dec4a0b3fdef881fcea640fc27c6f7e2"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk2MTI4NA==", "bodyText": "That's nifty, I originally had a second variable to check if it was failed to load but thought that was a bit of a waste. This seems pretty clean though", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527961284", "createdAt": "2020-11-20T20:46:38Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,33 +127,48 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  // Attempt to use Spark3 Catalog resolution if available on the path\n+  private static DynMethods.StaticMethod loadCatalogImpl = null;\n+\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String tableName, MetadataTableType type) {\n+    if (loadCatalogImpl == null) {\n+      try {\n+        loadCatalogImpl = DynMethods.builder(\"loadCatalogMetadataTable\")\n+            .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n+                SparkSession.class, String.class, MetadataTableType.class)\n+            .buildStaticChecked();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1NzQ4MA=="}, "originalCommit": {"oid": "59b573c7dec4a0b3fdef881fcea640fc27c6f7e2"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDM5Njk4OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDozODo1MVrOH3f_mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQyMDozODo1MVrOH3f_mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzk1NzkxNA==", "bodyText": "Nit: should have an empty line after the last block.", "url": "https://github.com/apache/iceberg/pull/1784#discussion_r527957914", "createdAt": "2020-11-20T20:38:51Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseSparkAction.java", "diffHunk": "@@ -128,33 +127,48 @@\n     return manifestDF.union(otherMetadataFileDF).union(manifestListDF);\n   }\n \n+  // Attempt to use Spark3 Catalog resolution if available on the path\n+  private static DynMethods.StaticMethod loadCatalogImpl = null;\n+\n+  private static Dataset<Row> loadCatalogMetadataTable(SparkSession spark, String tableName, MetadataTableType type) {\n+    if (loadCatalogImpl == null) {\n+      try {\n+        loadCatalogImpl = DynMethods.builder(\"loadCatalogMetadataTable\")\n+            .hiddenImpl(\"org.apache.iceberg.spark.Spark3Util\",\n+                SparkSession.class, String.class, MetadataTableType.class)\n+            .buildStaticChecked();\n+      } catch (NoSuchMethodException e) {\n+        throw new IllegalArgumentException(\"Cannot find Spark3Util class but Spark 3 is being used.\", e);\n+      }\n+    }\n+    return loadCatalogImpl.invoke(spark, tableName, type);\n+  }\n+\n   protected static Dataset<Row> loadMetadataTable(SparkSession spark, String tableName, String tableLocation,\n                                                   MetadataTableType type) {\n-    DataFrameReader noCatalogReader = spark.read().format(\"iceberg\");\n+    DataFrameReader dataFrameReader = spark.read().format(\"iceberg\");\n     if (tableName.contains(\"/\")) {\n       // Hadoop Table or Metadata location passed, load without a catalog\n-      return noCatalogReader.load(tableName + \"#\" + type);\n+      return dataFrameReader.load(tableName + \"#\" + type);\n     }\n-    // Try catalog based name based resolution\n-    try {\n-      return spark.table(tableName + \".\" + type);\n-    } catch (Exception e) {\n-      if (!(e instanceof ParseException || e instanceof AnalysisException)) {\n-        // Rethrow unexpected exceptions\n-        throw e;\n-      }\n-      // Catalog based resolution failed, our catalog may be a non-DatasourceV2 Catalog\n-      if (tableName.startsWith(\"hadoop.\")) {\n-        // Try loading by location as Hadoop table without Catalog\n-        return noCatalogReader.load(tableLocation + \"#\" + type);\n-      } else if (tableName.startsWith(\"hive\")) {\n-        // Try loading by name as a Hive table without Catalog\n-        return noCatalogReader.load(tableName.replaceFirst(\"hive\\\\.\", \"\") + \".\" + type);\n-      } else {\n-        throw new IllegalArgumentException(String.format(\n-            \"Cannot find the metadata table for %s of type %s\", tableName, type));\n+    // Try DSV2 catalog based name based resolution\n+    if (spark.version().startsWith(\"3\")) {\n+      Dataset<Row> catalogMetadataTable = loadCatalogMetadataTable(spark, tableName, type);\n+      if (catalogMetadataTable != null) {\n+        return catalogMetadataTable;\n       }\n     }\n+    // Catalog based resolution failed, our catalog may be a non-DatasourceV2 Catalog", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "59b573c7dec4a0b3fdef881fcea640fc27c6f7e2"}, "originalPosition": 77}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3475, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}