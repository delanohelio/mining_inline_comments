{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ1MDU2NDEz", "number": 1172, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQxOToxOToxMlrOEMn3DQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQxOTozMDowMlrOEMoE3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNjcxNDM3OnYy", "diffSide": "RIGHT", "path": "site/docs/spark.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQxOToxOToxMlrOGu19zA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQxOToxOToxMlrOGu19zA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTc3MTg1Mg==", "bodyText": "Did you mean spark.sql.catalog.hadoop_prod.type = hadoop", "url": "https://github.com/apache/iceberg/pull/1172#discussion_r451771852", "createdAt": "2020-07-08T19:19:12Z", "author": {"login": "rdsr"}, "path": "site/docs/spark.md", "diffHunk": "@@ -19,42 +19,272 @@\n \n Iceberg uses Apache Spark's DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API with different levels of support in Spark versions.\n \n-| Feature support                              | Spark 2.4 | Spark 3.0 (unreleased) | Notes                                          |\n-|----------------------------------------------|-----------|------------------------|------------------------------------------------|\n-| [DataFrame reads](#reading-an-iceberg-table) | \u2714\ufe0f        | \u2714\ufe0f                     |                                                |\n-| [DataFrame append](#appending-data)          | \u2714\ufe0f        | \u2714\ufe0f                     |                                                |\n-| [DataFrame overwrite](#overwriting-data)     | \u2714\ufe0f        | \u2714\ufe0f                     | Overwrite mode replaces partitions dynamically |\n-| [Metadata tables](#inspecting-tables)        | \u2714\ufe0f        | \u2714\ufe0f                     |                                                |\n-| SQL create table                             |           | \u2714\ufe0f                     |                                                |\n-| SQL alter table                              |           | \u2714\ufe0f                     |                                                |\n-| SQL drop table                               |           | \u2714\ufe0f                     |                                                |\n-| SQL select                                   |           | \u2714\ufe0f                     |                                                |\n-| SQL create table as                          |           | \u2714\ufe0f                     |                                                |\n-| SQL replace table as                         |           | \u2714\ufe0f                     |                                                |\n-| SQL insert into                              |           | \u2714\ufe0f                     |                                                |\n-| SQL insert overwrite                         |           | \u2714\ufe0f                     |                                                |\n+| Feature support                                  | Spark 3.0| Spark 2.4  | Notes                                          |\n+|--------------------------------------------------|----------|------------|------------------------------------------------|\n+| [SQL create table](#create-table)                | \u2714\ufe0f        |            |                                                |\n+| [SQL create table as](#create-table-as-select)   | \u2714\ufe0f        |            |                                                |\n+| [SQL replace table as](#replace-table-as-select) | \u2714\ufe0f        |            |                                                |\n+| [SQL alter table](#alter-table)                  | \u2714\ufe0f        |            |                                                |\n+| [SQL drop table](#drop-table)                    | \u2714\ufe0f        |            |                                                |\n+| [SQL select](#querying-with-sql)                 | \u2714\ufe0f        |            |                                                |\n+| [SQL insert into](#insert-into)                  | \u2714\ufe0f        |            |                                                |\n+| [SQL insert overwrite](#insert-overwrite)        | \u2714\ufe0f        |            |                                                |\n+| [DataFrame reads](#querying-with-dataframes)     | \u2714\ufe0f        | \u2714\ufe0f          |                                                |\n+| [DataFrame append](#appending-data)              | \u2714\ufe0f        | \u2714\ufe0f          |                                                |\n+| [DataFrame overwrite](#overwriting-data)         | \u2714\ufe0f        | \u2714\ufe0f          | \u26a0 Behavior changed in Spark 3.0                |\n+| [DataFrame CTAS and RTAS](#creating-tables)      | \u2714\ufe0f        |            |                                                |\n+| [Metadata tables](#inspecting-tables)            | \u2714\ufe0f        | \u2714\ufe0f          |                                                |\n+\n+## Configuring catalogs\n+\n+Spark 3.0 adds an API to plug in table catalogs that are used to load, create, and manage Iceberg tables. Spark catalogs are configured by setting Spark properties under `spark.sql.catalog`.\n+\n+This creates an Iceberg catalog named `hive_prod` that loads tables from a Hive metastore:\n+\n+```plain\n+spark.sql.catalog.hive_prod = org.apache.iceberg.spark.SparkCatalog\n+spark.sql.catalog.hive_prod.type = hive\n+spark.sql.catalog.hive_prod.uri = thrift://metastore-host:port\n+```\n+\n+Iceberg also supports a directory-based catalog in HDFS that can be configured using `type=hadoop`:\n+\n+```plain\n+spark.sql.catalog.hadoop_prod = org.apache.iceberg.spark.SparkCatalog\n+spark.sql.catalog.hadoop_prod.type = hive", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "336b145b299389c99c993565681b879de220c195"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNjcxOTA1OnYy", "diffSide": "RIGHT", "path": "site/docs/spark.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQxOToyMDozNVrOGu2AtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQxOToyMDozNVrOGu2AtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTc3MjU5Ng==", "bodyText": "nit: should this be  \"... track of the current catalog...\"", "url": "https://github.com/apache/iceberg/pull/1172#discussion_r451772596", "createdAt": "2020-07-08T19:20:35Z", "author": {"login": "rdsr"}, "path": "site/docs/spark.md", "diffHunk": "@@ -19,42 +19,272 @@\n \n Iceberg uses Apache Spark's DataSourceV2 API for data source and catalog implementations. Spark DSv2 is an evolving API with different levels of support in Spark versions.\n \n-| Feature support                              | Spark 2.4 | Spark 3.0 (unreleased) | Notes                                          |\n-|----------------------------------------------|-----------|------------------------|------------------------------------------------|\n-| [DataFrame reads](#reading-an-iceberg-table) | \u2714\ufe0f        | \u2714\ufe0f                     |                                                |\n-| [DataFrame append](#appending-data)          | \u2714\ufe0f        | \u2714\ufe0f                     |                                                |\n-| [DataFrame overwrite](#overwriting-data)     | \u2714\ufe0f        | \u2714\ufe0f                     | Overwrite mode replaces partitions dynamically |\n-| [Metadata tables](#inspecting-tables)        | \u2714\ufe0f        | \u2714\ufe0f                     |                                                |\n-| SQL create table                             |           | \u2714\ufe0f                     |                                                |\n-| SQL alter table                              |           | \u2714\ufe0f                     |                                                |\n-| SQL drop table                               |           | \u2714\ufe0f                     |                                                |\n-| SQL select                                   |           | \u2714\ufe0f                     |                                                |\n-| SQL create table as                          |           | \u2714\ufe0f                     |                                                |\n-| SQL replace table as                         |           | \u2714\ufe0f                     |                                                |\n-| SQL insert into                              |           | \u2714\ufe0f                     |                                                |\n-| SQL insert overwrite                         |           | \u2714\ufe0f                     |                                                |\n+| Feature support                                  | Spark 3.0| Spark 2.4  | Notes                                          |\n+|--------------------------------------------------|----------|------------|------------------------------------------------|\n+| [SQL create table](#create-table)                | \u2714\ufe0f        |            |                                                |\n+| [SQL create table as](#create-table-as-select)   | \u2714\ufe0f        |            |                                                |\n+| [SQL replace table as](#replace-table-as-select) | \u2714\ufe0f        |            |                                                |\n+| [SQL alter table](#alter-table)                  | \u2714\ufe0f        |            |                                                |\n+| [SQL drop table](#drop-table)                    | \u2714\ufe0f        |            |                                                |\n+| [SQL select](#querying-with-sql)                 | \u2714\ufe0f        |            |                                                |\n+| [SQL insert into](#insert-into)                  | \u2714\ufe0f        |            |                                                |\n+| [SQL insert overwrite](#insert-overwrite)        | \u2714\ufe0f        |            |                                                |\n+| [DataFrame reads](#querying-with-dataframes)     | \u2714\ufe0f        | \u2714\ufe0f          |                                                |\n+| [DataFrame append](#appending-data)              | \u2714\ufe0f        | \u2714\ufe0f          |                                                |\n+| [DataFrame overwrite](#overwriting-data)         | \u2714\ufe0f        | \u2714\ufe0f          | \u26a0 Behavior changed in Spark 3.0                |\n+| [DataFrame CTAS and RTAS](#creating-tables)      | \u2714\ufe0f        |            |                                                |\n+| [Metadata tables](#inspecting-tables)            | \u2714\ufe0f        | \u2714\ufe0f          |                                                |\n+\n+## Configuring catalogs\n+\n+Spark 3.0 adds an API to plug in table catalogs that are used to load, create, and manage Iceberg tables. Spark catalogs are configured by setting Spark properties under `spark.sql.catalog`.\n+\n+This creates an Iceberg catalog named `hive_prod` that loads tables from a Hive metastore:\n+\n+```plain\n+spark.sql.catalog.hive_prod = org.apache.iceberg.spark.SparkCatalog\n+spark.sql.catalog.hive_prod.type = hive\n+spark.sql.catalog.hive_prod.uri = thrift://metastore-host:port\n+```\n+\n+Iceberg also supports a directory-based catalog in HDFS that can be configured using `type=hadoop`:\n+\n+```plain\n+spark.sql.catalog.hadoop_prod = org.apache.iceberg.spark.SparkCatalog\n+spark.sql.catalog.hadoop_prod.type = hive\n+spark.sql.catalog.hadoop_prod.warehouse = hdfs://nn:8020/warehouse/path\n+```\n+\n+!!! Note\n+    The Hive-based catalog only loads Iceberg tables. To load non-Iceberg tables in the same Hive metastore, use a [session catalog](#replacing-the-session-catalog).\n+\n+### Using catalogs\n+\n+Catalog names are used in SQL queries to identify a table. In the examples above, `hive_prod` and `hadoop_prod` can be used to prefix database and table names that will be loaded from those catalogs.\n+\n+```sql\n+SELECT * FROM hive_prod.db.table -- load db.table from catalog hive_prod\n+```\n+\n+Spark 3 keeps track of a current catalog and namespace, which can be omitted from table names.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "336b145b299389c99c993565681b879de220c195"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgxNjc0OTczOnYy", "diffSide": "RIGHT", "path": "site/docs/spark.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQxOTozMDowMlrOGu2T5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQxOTozMDowMlrOGu2T5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTc3NzUxMQ==", "bodyText": "nit: restricted to 1st July", "url": "https://github.com/apache/iceberg/pull/1172#discussion_r451777511", "createdAt": "2020-07-08T19:30:02Z", "author": {"login": "rdsr"}, "path": "site/docs/spark.md", "diffHunk": "@@ -91,50 +332,188 @@ spark.sql(\"\"\"select count(1) from table\"\"\").show()\n ```\n \n \n+## Writing with SQL\n+\n+Spark 3 supports SQL `INSERT INTO` and `INSERT OVERWRITE`, as well as the new `DataFrameWriterV2` API.\n+\n+### `INSERT INTO`\n+\n+To append new data to a table, use `INSERT INTO`.\n+\n+```sql\n+INSERT INTO prod.db.table VALUES (1, 'a'), (2, 'b')\n+```\n+```sql\n+INSERT INTO prod.db.table SELECT ...\n+```\n+\n+### `INSERT OVERWRITE`\n+\n+To replace data in the table with the result of a query, use `INSERT OVERWRITE`. Overwrites are atomic operations for Iceberg tables.\n+\n+The partitions that will be replaced by `INSERT OVERWRITE` depends on Spark's partition overwrite mode and the partitioning of a table.\n+\n+#### Overwrite behavior\n+\n+Spark's default overwrite mode is **static**, but **dynamic overwrite mode is recommended when writing to Iceberg tables.** Static overwrite mode determines which partitions to overwrite in a table by converting the `PARTITION` clause to a filter, but the `PARTITION` clause can only reference table columns.\n+\n+Dynamic overwrite mode is configured by setting `spark.sql.sources.partitionOverwriteMode=dynamic`.\n+\n+To demonstrate the behavior of dynamic and static overwrites, consider a `logs` table defined by the following DDL:\n+\n+```sql\n+CREATE TABLE prod.my_app.logs (\n+    uuid string NOT NULL,\n+    level string NOT NULL,\n+    ts timestamp NOT NULL,\n+    message string)\n+USING iceberg\n+PARTITIONED BY (level, hours(ts))\n+```\n+\n+#### Dynamic overwrite\n+\n+When Spark's overwrite mode is dynamic, partitions that have rows produced by the `SELECT` query will be replaced.\n+\n+For example, this query removes duplicate log events from the example `logs` table.\n+\n+```sql\n+INSERT OVERWRITE prod.my_app.logs\n+SELECT uuid, first(level), first(ts), first(message)\n+FROM prod.my_app.logs\n+WHERE cast(ts as date) = '2020-07-01'\n+GROUP BY uuid\n+```\n+\n+In dynamic mode, this will replace any partition with rows in the `SELECT` result. Because the date of all rows is restricted 1 July, only hours of that day will be replaced.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "336b145b299389c99c993565681b879de220c195"}, "originalPosition": 390}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3951, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}