{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTM2MjIwMDMy", "number": 1906, "reviewThreads": {"totalCount": 50, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDowODo0M1rOFDqUXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo1MTozN1rOFD7cew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5MzgzMzg4OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDowODo0M1rOIDbPfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDowODo0M1rOIDbPfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ2Mjk3Mg==", "bodyText": "I'll one line this", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540462972", "createdAt": "2020-12-10T20:08:43Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -70,21 +78,23 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   protected Identifier toIdentifier(String identifierAsString, String argName) {\n-    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n-        \"Cannot handle an empty identifier for argument %s\", argName);\n-\n-    CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n-        \"identifier for arg \" + argName, spark, identifierAsString, tableCatalog);\n-\n-    CatalogPlugin catalog = catalogAndIdentifier.catalog();\n-    Identifier identifier = catalogAndIdentifier.identifier();\n+    CatalogAndIdentifier catalogAndIdentifier = toCatalogAndIdentifer(identifierAsString, argName, tableCatalog);\n \n     Preconditions.checkArgument(\n-        catalog.equals(tableCatalog),\n+        catalogAndIdentifier.catalog().equals(tableCatalog),\n         \"Cannot run procedure in catalog '%s': '%s' is a table in catalog '%s'\",\n-        tableCatalog.name(), identifierAsString, catalog.name());\n+        tableCatalog.name(), identifierAsString, catalogAndIdentifier.catalog().name());\n+\n+    return catalogAndIdentifier.identifier();\n+  }\n+\n+  protected CatalogAndIdentifier toCatalogAndIdentifer(String identifierAsString, String argName,\n+                                                       CatalogPlugin catalog) {\n+    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n+        \"Cannot handle an empty identifier for argument %s\", argName);\n \n-    return identifier;\n+    return Spark3Util.catalogAndIdentifier(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5MzgzNTk4OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/antlr/org.apache.spark.sql.catalyst.parser.extensions/IcebergSqlExtensions.g4", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDowOToyMFrOIDbQwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMDo0NDozNVrOIDkAAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ2MzI5Nw==", "bodyText": "This is required to allow Map(string, string, *) expressions", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540463297", "createdAt": "2020-12-10T20:09:20Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/antlr/org.apache.spark.sql.catalyst.parser.extensions/IcebergSqlExtensions.g4", "diffHunk": "@@ -76,6 +76,7 @@ callArgument\n \n expression\n     : constant\n+    | stringMap", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2MTk4Nw==", "bodyText": "Looks like this is intended to mimic the map function?\nIf so, should this allow any constant for keys and values? I would expect something like this:\nmapLiteral\n    : MAP '(' pairs+=keyValue, (',' pairs+=keyValue)* ')'\n    ;\n\nkeyValue\n    : key=constant ',' value=constant\n    ;", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540561987", "createdAt": "2020-12-10T23:00:07Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/antlr/org.apache.spark.sql.catalyst.parser.extensions/IcebergSqlExtensions.g4", "diffHunk": "@@ -76,6 +76,7 @@ callArgument\n \n expression\n     : constant\n+    | stringMap", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ2MzI5Nw=="}, "originalCommit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2MjgxNg==", "bodyText": "Actually, we may want to remove keyValue and just use constant so that a missing value doesn't result in a parse error. Instead we can give a better error message in the AST builder.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540562816", "createdAt": "2020-12-10T23:01:10Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/antlr/org.apache.spark.sql.catalyst.parser.extensions/IcebergSqlExtensions.g4", "diffHunk": "@@ -76,6 +76,7 @@ callArgument\n \n expression\n     : constant\n+    | stringMap", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ2MzI5Nw=="}, "originalCommit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3MTU1Nw==", "bodyText": "I considered this, but we really wanted to only support String maps and I was falling back on the expression level to tell us if it's a bad Map (unequal key values) but we can do that here if you like.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540571557", "createdAt": "2020-12-10T23:17:04Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/antlr/org.apache.spark.sql.catalyst.parser.extensions/IcebergSqlExtensions.g4", "diffHunk": "@@ -76,6 +76,7 @@ callArgument\n \n expression\n     : constant\n+    | stringMap", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ2MzI5Nw=="}, "originalCommit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDYwNjQ2NA==", "bodyText": "Yeah, I think the list works. The main thing is whether we should allow passing non-string values.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540606464", "createdAt": "2020-12-11T00:44:35Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/antlr/org.apache.spark.sql.catalyst.parser.extensions/IcebergSqlExtensions.g4", "diffHunk": "@@ -76,6 +76,7 @@ callArgument\n \n expression\n     : constant\n+    | stringMap", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ2MzI5Nw=="}, "originalCommit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5Mzg0MTA3OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDoxMDozOVrOIDbTyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDoxMDozOVrOIDbTyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ2NDA3Mg==", "bodyText": "To keep the following to 1 line per entry", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540464072", "createdAt": "2020-12-10T20:10:39Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final DataType MAP = DataTypes.createMapType(DataTypes.StringType, DataTypes.StringType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5Mzg1NzgyOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMDoxNDo0M1rOIDbdew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMDo0MzoxOFrOIDj-bA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ2NjU1NQ==", "bodyText": "Calling scala from java", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540466555", "createdAt": "2020-12-10T20:14:43Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final DataType MAP = DataTypes.createMapType(DataTypes.StringType, DataTypes.StringType);\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotProcedure>() {\n+      @Override\n+      protected SnapshotProcedure doBuild() {\n+        return new SnapshotProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    String dest = args.getString(1);\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(3)) {\n+      args.getMap(3).foreach(DataTypes.StringType, DataTypes.StringType,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3NzQwNw==", "bodyText": "Yeah, maybe we should rely on Scala conversions and then do all the logic in Java? I'm not sure what impact this may have on compatibility, but I'd rather not break because Scala changes something.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540577407", "createdAt": "2020-12-10T23:30:30Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final DataType MAP = DataTypes.createMapType(DataTypes.StringType, DataTypes.StringType);\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotProcedure>() {\n+      @Override\n+      protected SnapshotProcedure doBuild() {\n+        return new SnapshotProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    String dest = args.getString(1);\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(3)) {\n+      args.getMap(3).foreach(DataTypes.StringType, DataTypes.StringType,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ2NjU1NQ=="}, "originalCommit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU5NzMyOA==", "bodyText": "This is a little difficult because it's the Scala method on InternalRow which doesn't really return a map, it returns \"catalyst.util.MapData\", so it's not a real map and the method we are using is\n  def foreach(keyType: DataType, valueType: DataType, f: (Any, Any) => Unit): Unit = {\nI thought about writing another method to convert MapData to Java maps but that also seems brittle. I think the safest thing to do here is use the Scala Method for MapData.foreach", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540597328", "createdAt": "2020-12-11T00:19:07Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final DataType MAP = DataTypes.createMapType(DataTypes.StringType, DataTypes.StringType);\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotProcedure>() {\n+      @Override\n+      protected SnapshotProcedure doBuild() {\n+        return new SnapshotProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    String dest = args.getString(1);\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(3)) {\n+      args.getMap(3).foreach(DataTypes.StringType, DataTypes.StringType,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ2NjU1NQ=="}, "originalCommit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDYwNjA2MA==", "bodyText": "Sounds reasonable to me.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540606060", "createdAt": "2020-12-11T00:43:18Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final DataType MAP = DataTypes.createMapType(DataTypes.StringType, DataTypes.StringType);\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotProcedure>() {\n+      @Override\n+      protected SnapshotProcedure doBuild() {\n+        return new SnapshotProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    String dest = args.getString(1);\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(3)) {\n+      args.getMap(3).foreach(DataTypes.StringType, DataTypes.StringType,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQ2NjU1NQ=="}, "originalCommit": {"oid": "9ee31fbb2f77ab2b86cf398a1d0ea5a698fc6567"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDE3NTg2OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTozNToyMlrOIDeVxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxODo0OFrOIDv7-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUxMzczNA==", "bodyText": "nit: how common is it to be in BaseProcedure? Not a strong opinion, just asking.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540513734", "createdAt": "2020-12-10T21:35:22Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -37,9 +37,13 @@\n import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n import org.apache.spark.sql.execution.CacheManager;\n import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n import scala.Option;\n \n abstract class BaseProcedure implements Procedure {\n+  protected static final DataType STRING_MAP = DataTypes.createMapType(DataTypes.StringType, DataTypes.StringType);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyMzY0MQ==", "bodyText": "2 Procedures at the moment \ud83e\udd37 so not that common, but more than once", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540523641", "createdAt": "2020-12-10T21:52:48Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -37,9 +37,13 @@\n import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n import org.apache.spark.sql.execution.CacheManager;\n import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n import scala.Option;\n \n abstract class BaseProcedure implements Procedure {\n+  protected static final DataType STRING_MAP = DataTypes.createMapType(DataTypes.StringType, DataTypes.StringType);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUxMzczNA=="}, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDgwMjA0MQ==", "bodyText": "Let's keep it.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540802041", "createdAt": "2020-12-11T09:18:48Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -37,9 +37,13 @@\n import org.apache.spark.sql.connector.iceberg.catalog.Procedure;\n import org.apache.spark.sql.execution.CacheManager;\n import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n import scala.Option;\n \n abstract class BaseProcedure implements Procedure {\n+  protected static final DataType STRING_MAP = DataTypes.createMapType(DataTypes.StringType, DataTypes.StringType);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUxMzczNA=="}, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDIzNjQxOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1MDo1MlrOIDe4Yw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1MDo1MlrOIDe4Yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyMjU5NQ==", "bodyText": "Should we make it migrated_files_count to match other procedures?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540522595", "createdAt": "2020-12-10T21:50:52Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDIzOTEyOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1MTozOVrOIDe6Ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1MTozOVrOIDe6Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyMzAxMA==", "bodyText": "nit: let's import ProcedureBuilder directly to match other procedures.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540523010", "createdAt": "2020-12-10T21:51:39Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDI0NTk5OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1MzozNFrOIDe99w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1MzozNFrOIDe99w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyNDAyMw==", "bodyText": "nit: MigrateTableProcedure?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540524023", "createdAt": "2020-12-10T21:53:34Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDI0NjY3OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1Mzo0N1rOIDe-XA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDoyODoyMFrOIDyoVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyNDEyNA==", "bodyText": "I am afraid this description will not show properly in the SQL plan. Let's make it MigrateTableProcedure as in other procedures.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540524124", "createdAt": "2020-12-10T21:53:47Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(1)) {\n+      args.getMap(1).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    String tableName = args.getString(0);\n+    CatalogAndIdentifier tableIdent = toCatalogAndIdentifer(tableName, PARAMETERS[0].name(), tableCatalog());\n+    CreateAction action =  new Spark3MigrateAction(spark(), tableIdent.catalog(), tableIdent.identifier());\n+\n+    long numFiles = action.withProperties(options).execute();\n+    return new InternalRow[] {newInternalRow(numFiles)};\n+  }\n+\n+  @Override\n+  public String description() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzMjQyMQ==", "bodyText": "ah yeah, sorry", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540532421", "createdAt": "2020-12-10T22:08:00Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(1)) {\n+      args.getMap(1).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    String tableName = args.getString(0);\n+    CatalogAndIdentifier tableIdent = toCatalogAndIdentifer(tableName, PARAMETERS[0].name(), tableCatalog());\n+    CreateAction action =  new Spark3MigrateAction(spark(), tableIdent.catalog(), tableIdent.identifier());\n+\n+    long numFiles = action.withProperties(options).execute();\n+    return new InternalRow[] {newInternalRow(numFiles)};\n+  }\n+\n+  @Override\n+  public String description() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyNDEyNA=="}, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3NjQ0Mw==", "bodyText": "The SQL plan uses description? I would expect description to be documentation and for the plan to use a method like planString. Not something we need to fix here.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540576443", "createdAt": "2020-12-10T23:28:06Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(1)) {\n+      args.getMap(1).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    String tableName = args.getString(0);\n+    CatalogAndIdentifier tableIdent = toCatalogAndIdentifer(tableName, PARAMETERS[0].name(), tableCatalog());\n+    CreateAction action =  new Spark3MigrateAction(spark(), tableIdent.catalog(), tableIdent.identifier());\n+\n+    long numFiles = action.withProperties(options).execute();\n+    return new InternalRow[] {newInternalRow(numFiles)};\n+  }\n+\n+  @Override\n+  public String description() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyNDEyNA=="}, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg0NjE2Ng==", "bodyText": "Sounds like something we can address while adding support for describing procedures.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540846166", "createdAt": "2020-12-11T10:28:20Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(1)) {\n+      args.getMap(1).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    String tableName = args.getString(0);\n+    CatalogAndIdentifier tableIdent = toCatalogAndIdentifer(tableName, PARAMETERS[0].name(), tableCatalog());\n+    CreateAction action =  new Spark3MigrateAction(spark(), tableIdent.catalog(), tableIdent.identifier());\n+\n+    long numFiles = action.withProperties(options).execute();\n+    return new InternalRow[] {newInternalRow(numFiles)};\n+  }\n+\n+  @Override\n+  public String description() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyNDEyNA=="}, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDI0ODI4OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1NDoxMlrOIDe_Pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMTo1NDoxMlrOIDe_Pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyNDM1MQ==", "bodyText": "nit: extra space", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540524351", "createdAt": "2020-12-10T21:54:12Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(1)) {\n+      args.getMap(1).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    String tableName = args.getString(0);\n+    CatalogAndIdentifier tableIdent = toCatalogAndIdentifer(tableName, PARAMETERS[0].name(), tableCatalog());\n+    CreateAction action =  new Spark3MigrateAction(spark(), tableIdent.catalog(), tableIdent.identifier());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDI3OTQwOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjowMjowMlrOIDfRmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjowODozOVrOIDfgAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyOTA0OA==", "bodyText": "typo? toCatalogAndIdentifer -> toCatalogAndIdentifier?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540529048", "createdAt": "2020-12-10T22:02:02Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -70,21 +82,22 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   protected Identifier toIdentifier(String identifierAsString, String argName) {\n-    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n-        \"Cannot handle an empty identifier for argument %s\", argName);\n-\n-    CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n-        \"identifier for arg \" + argName, spark, identifierAsString, tableCatalog);\n-\n-    CatalogPlugin catalog = catalogAndIdentifier.catalog();\n-    Identifier identifier = catalogAndIdentifier.identifier();\n+    CatalogAndIdentifier catalogAndIdentifier = toCatalogAndIdentifer(identifierAsString, argName, tableCatalog);\n \n     Preconditions.checkArgument(\n-        catalog.equals(tableCatalog),\n+        catalogAndIdentifier.catalog().equals(tableCatalog),\n         \"Cannot run procedure in catalog '%s': '%s' is a table in catalog '%s'\",\n-        tableCatalog.name(), identifierAsString, catalog.name());\n+        tableCatalog.name(), identifierAsString, catalogAndIdentifier.catalog().name());\n+\n+    return catalogAndIdentifier.identifier();\n+  }\n+\n+  protected CatalogAndIdentifier toCatalogAndIdentifer(String identifierAsString, String argName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzMjczNw==", "bodyText": "ah yeah, let me fix that", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540532737", "createdAt": "2020-12-10T22:08:39Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -70,21 +82,22 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   protected Identifier toIdentifier(String identifierAsString, String argName) {\n-    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n-        \"Cannot handle an empty identifier for argument %s\", argName);\n-\n-    CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n-        \"identifier for arg \" + argName, spark, identifierAsString, tableCatalog);\n-\n-    CatalogPlugin catalog = catalogAndIdentifier.catalog();\n-    Identifier identifier = catalogAndIdentifier.identifier();\n+    CatalogAndIdentifier catalogAndIdentifier = toCatalogAndIdentifer(identifierAsString, argName, tableCatalog);\n \n     Preconditions.checkArgument(\n-        catalog.equals(tableCatalog),\n+        catalogAndIdentifier.catalog().equals(tableCatalog),\n         \"Cannot run procedure in catalog '%s': '%s' is a table in catalog '%s'\",\n-        tableCatalog.name(), identifierAsString, catalog.name());\n+        tableCatalog.name(), identifierAsString, catalogAndIdentifier.catalog().name());\n+\n+    return catalogAndIdentifier.identifier();\n+  }\n+\n+  protected CatalogAndIdentifier toCatalogAndIdentifer(String identifierAsString, String argName,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyOTA0OA=="}, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDI4MTY3OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjowMjozNFrOIDfS1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjowMjozNFrOIDfS1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUyOTM2NQ==", "bodyText": "nit: Maps.newHashMap()?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540529365", "createdAt": "2020-12-10T22:02:34Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDI4ODgwOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjowNDozMVrOIDfXGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoyMzo0MFrOIDiDSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzMDQ1Nw==", "bodyText": "Should it be table_properties or just properties? There will be a difference between options and properties in Spark 3.1.0.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540530457", "createdAt": "2020-12-10T22:04:31Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3NDUzOQ==", "bodyText": "I like simpler, as long as it is clear. And I agree that we should not use options because of the specific meaning with datasource tables. properties is fine with me.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540574539", "createdAt": "2020-12-10T23:23:40Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzMDQ1Nw=="}, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDMxMDA5OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjowOTo1OVrOIDfjKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjowOTo1OVrOIDfjKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzMzU0Ng==", "bodyText": "I find it a bit misleading that we don't process arguments in the order they are defined.\nHow about this?\n    String identAsString = args.getString(0);\n    CatalogAndIdentifier catalogAndIdent = toCatalogAndIdentifer(identAsString, PARAMETERS[0].name(), tableCatalog());\n\n    Map<String, String> tableProps = Maps.newHashMap();\n    MapData providedProps = args.getMap(1);\n    if (providedProps != null) {\n      providedProps.foreach(DataTypes.StringType, DataTypes.StringType,\n          (k, v) -> {\n            tableProps.put(k.toString(), v.toString());\n            return BoxedUnit.UNIT;\n          });\n    }", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540533546", "createdAt": "2020-12-10T22:09:59Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(1)) {\n+      args.getMap(1).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    String tableName = args.getString(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDMxMTI1OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMDoxNVrOIDfj0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMDoxNVrOIDfj0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzMzcxNQ==", "bodyText": "nit: numMigratedFiles?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540533715", "createdAt": "2020-12-10T22:10:15Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(1)) {\n+      args.getMap(1).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    String tableName = args.getString(0);\n+    CatalogAndIdentifier tableIdent = toCatalogAndIdentifer(tableName, PARAMETERS[0].name(), tableCatalog());\n+    CreateAction action =  new Spark3MigrateAction(spark(), tableIdent.catalog(), tableIdent.identifier());\n+\n+    long numFiles = action.withProperties(options).execute();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDMxMjczOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMDo0MVrOIDfkwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMDo0MVrOIDfkwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzMzk1Mg==", "bodyText": "nit: options -> tableProps?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540533952", "createdAt": "2020-12-10T22:10:41Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDMxNzM0OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMTo0OFrOIDfnbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMDo0MTo0NlrOIDj8Ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNDYzNg==", "bodyText": "What about source_table, table, table_location, table_properties? cc @RussellSpitzer @rdblue", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540534636", "createdAt": "2020-12-10T22:11:48Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNDg5Mg==", "bodyText": "We may add dest prefix if we want to but I am not sure.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540534892", "createdAt": "2020-12-10T22:12:12Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNDYzNg=="}, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3NTA3OQ==", "bodyText": "I really like source and dest here :/", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540575079", "createdAt": "2020-12-10T23:24:55Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNDYzNg=="}, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3NjkwNg==", "bodyText": "+1 to those argument names. I'd be okay with just properties and location, but I'm fine with the table_ prefix as well. (It should also match what we do for migrate.)", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540576906", "createdAt": "2020-12-10T23:29:13Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNDYzNg=="}, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDYwNTQ3NA==", "bodyText": "I think @RussellSpitzer and I commented at the same time so I didn't see his comment. I think it would be fine to use source_table and dest_table.\nI just don't think that including the procedure name in the argument names is helpful. So rather than snapshot_source I would use source_table.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540605474", "createdAt": "2020-12-11T00:41:46Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNDYzNg=="}, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDMyMTU0OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMjo0M1rOIDfpuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMjo0M1rOIDfpuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNTIyNA==", "bodyText": "imported_data_files_count?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540535224", "createdAt": "2020-12-10T22:12:43Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDMyMzQyOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMzoxNFrOIDfq2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMzoxNFrOIDfq2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNTUxMg==", "bodyText": "Same comments as for migrate.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540535512", "createdAt": "2020-12-10T22:13:14Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotProcedure>() {\n+      @Override\n+      protected SnapshotProcedure doBuild() {\n+        return new SnapshotProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    String dest = args.getString(1);\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = Maps.newHashMap();\n+    if (!args.isNullAt(3)) {\n+      args.getMap(3).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    CatalogAndIdentifier sourceIdent = toCatalogAndIdentifer(source, PARAMETERS[0].name(), tableCatalog());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDMyNDU5OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMzozMlrOIDfrgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMzozMlrOIDfrgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNTY4Mw==", "bodyText": "same comments as for migrate.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540535683", "createdAt": "2020-12-10T22:13:32Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotProcedure>() {\n+      @Override\n+      protected SnapshotProcedure doBuild() {\n+        return new SnapshotProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    String dest = args.getString(1);\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = Maps.newHashMap();\n+    if (!args.isNullAt(3)) {\n+      args.getMap(3).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    CatalogAndIdentifier sourceIdent = toCatalogAndIdentifer(source, PARAMETERS[0].name(), tableCatalog());\n+    CatalogAndIdentifier destIdent = toCatalogAndIdentifer(dest, PARAMETERS[1].name(), tableCatalog());\n+\n+    Preconditions.checkArgument(sourceIdent != destIdent || sourceIdent.catalog() != destIdent.catalog(),\n+        \"Cannot create a snapshot with the same name as the source of the snapshot.\");\n+    SnapshotAction action =  new Spark3SnapshotAction(spark(), sourceIdent.catalog(), sourceIdent.identifier(),\n+        destIdent.catalog(), destIdent.identifier());\n+\n+    long numFiles;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDMyNTg4OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMzo1M1rOIDfsOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxMzo1M1rOIDfsOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNTg2NQ==", "bodyText": "SnapshotTableProcedure?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540535865", "createdAt": "2020-12-10T22:13:53Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDMyNzMzOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxNDoxMVrOIDfs_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMjoxNDoxMVrOIDfs_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDUzNjA2MA==", "bodyText": "same here. SnapshotTableProcedure?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540536060", "createdAt": "2020-12-10T22:14:11Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotProcedure.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"snapshot_source\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotProcedure>() {\n+      @Override\n+      protected SnapshotProcedure doBuild() {\n+        return new SnapshotProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    String dest = args.getString(1);\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = Maps.newHashMap();\n+    if (!args.isNullAt(3)) {\n+      args.getMap(3).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    CatalogAndIdentifier sourceIdent = toCatalogAndIdentifer(source, PARAMETERS[0].name(), tableCatalog());\n+    CatalogAndIdentifier destIdent = toCatalogAndIdentifer(dest, PARAMETERS[1].name(), tableCatalog());\n+\n+    Preconditions.checkArgument(sourceIdent != destIdent || sourceIdent.catalog() != destIdent.catalog(),\n+        \"Cannot create a snapshot with the same name as the source of the snapshot.\");\n+    SnapshotAction action =  new Spark3SnapshotAction(spark(), sourceIdent.catalog(), sourceIdent.identifier(),\n+        destIdent.catalog(), destIdent.identifier());\n+\n+    long numFiles;\n+    if (snapshotLocation != null) {\n+      numFiles = action.withLocation(snapshotLocation).withProperties(options).execute();\n+    } else {\n+      numFiles = action.withProperties(options).execute();\n+    }\n+\n+    return new InternalRow[] {newInternalRow(numFiles)};\n+  }\n+\n+  @Override\n+  public String description() {\n+    return \"Creates an Iceberg table from a Spark Table. The Created table will be isolated from the original table\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDUyMjY4OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowMzo0MVrOIDhdQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowMzo0MVrOIDhdQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2NDgwMw==", "bodyText": "There's a scalarSql method for when a SQL command produces one row with one value. That will assert that there is only one row and one column, which may be easier.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540564803", "createdAt": "2020-12-10T23:03:41Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDUzNDYzOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowNjoxMlrOIDhkXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowNjoxMlrOIDhkXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2NjYyMA==", "bodyText": "Since the location is set, should this validate that the migrated table's location matches the one passed here?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540566620", "createdAt": "2020-12-10T23:06:12Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testMigrateWithOptions() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDUzNjM1OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowNjo0MlrOIDhlRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMTo1NjoyMlrOIDllvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2Njg1Mw==", "bodyText": "Other tests drop the table if exists rather than using IF NOT EXISTS. I think that's a better pattern because if the table already exists, it probably violates the assumptions of this test.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540566853", "createdAt": "2020-12-10T23:06:42Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testMigrateWithOptions() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s', map('foo', 'bar'))\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDYzMjUwOA==", "bodyText": "ah yeah I only had that because I ran a test and canceled while it was running, I should have removed it", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540632508", "createdAt": "2020-12-11T01:56:22Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testMigrateWithOptions() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s', map('foo', 'bar'))\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2Njg1Mw=="}, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDUzOTUxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowNzoyM1rOIDhm8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowNzoyM1rOIDhm8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2NzI4MQ==", "bodyText": "\"migrated\" -> \"added\"?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540567281", "createdAt": "2020-12-10T23:07:23Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testMigrateWithOptions() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s', map('foo', 'bar'))\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDU0MjM2OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowODowNVrOIDhoXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzowODowNVrOIDhoXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU2NzY0NQ==", "bodyText": "Since the source table has a known location, I think this should validate that the snapshot table uses a different one.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540567645", "createdAt": "2020-12-10T23:08:05Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testMigrateWithOptions() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s', map('foo', 'bar'))\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDU2ODEwOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoxNjoyNFrOIDh2Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMzoxMjo1M1rOIDnLwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3MTIzOQ==", "bodyText": "Nit: I don't think we need to keep adding these checks since it tests the resolver, not the procedure.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540571239", "createdAt": "2020-12-10T23:16:24Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testMigrateWithOptions() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s', map('foo', 'bar'))\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot( snapshot_source => '%s', table => '%s', table_options => map('foo','bar'))\",\n+        catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithAlternateLocation() throws IOException {\n+    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n+    String location = temp.newFolder().toString();\n+    String snapshotLocation = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot( snapshot_source => '%s', table => '%s', table_location => '%s')\",\n+        catalogName, sourceName, tableName, snapshotLocation).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    String storageLocation = validationCatalog.loadTable(tableIdent).location();\n+    Assert.assertEquals(\"Snapshot should be made at specified location\", snapshotLocation, storageLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testInvalidSnapshotsCases() {\n+    AssertHelpers.assertThrows(\"Should not allow mixed args\",\n+        AnalysisException.class, \"Named and positional arguments cannot be mixed\",\n+        () -> sql(\"CALL %s.system.snapshot('n', table => 't')\", catalogName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 157}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY1ODYyNg==", "bodyText": "Sure I can remove these", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540658626", "createdAt": "2020-12-11T03:12:53Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testMigrateWithOptions() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s', map('foo', 'bar'))\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot( snapshot_source => '%s', table => '%s', table_options => map('foo','bar'))\",\n+        catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithAlternateLocation() throws IOException {\n+    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n+    String location = temp.newFolder().toString();\n+    String snapshotLocation = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot( snapshot_source => '%s', table => '%s', table_location => '%s')\",\n+        catalogName, sourceName, tableName, snapshotLocation).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    String storageLocation = validationCatalog.loadTable(tableIdent).location();\n+    Assert.assertEquals(\"Snapshot should be made at specified location\", snapshotLocation, storageLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testInvalidSnapshotsCases() {\n+    AssertHelpers.assertThrows(\"Should not allow mixed args\",\n+        AnalysisException.class, \"Named and positional arguments cannot be mixed\",\n+        () -> sql(\"CALL %s.system.snapshot('n', table => 't')\", catalogName));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3MTIzOQ=="}, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 157}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDU2OTUyOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoxNjo0OVrOIDh3Og==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoxNjo0OVrOIDh3Og==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3MTQ1MA==", "bodyText": "Similarly, this isn't the procedure name.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540571450", "createdAt": "2020-12-10T23:16:49Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testMigrateWithOptions() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s', map('foo', 'bar'))\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot( snapshot_source => '%s', table => '%s', table_options => map('foo','bar'))\",\n+        catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithAlternateLocation() throws IOException {\n+    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n+    String location = temp.newFolder().toString();\n+    String snapshotLocation = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot( snapshot_source => '%s', table => '%s', table_location => '%s')\",\n+        catalogName, sourceName, tableName, snapshotLocation).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    String storageLocation = validationCatalog.loadTable(tableIdent).location();\n+    Assert.assertEquals(\"Snapshot should be made at specified location\", snapshotLocation, storageLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testInvalidSnapshotsCases() {\n+    AssertHelpers.assertThrows(\"Should not allow mixed args\",\n+        AnalysisException.class, \"Named and positional arguments cannot be mixed\",\n+        () -> sql(\"CALL %s.system.snapshot('n', table => 't')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should not resolve procedures in arbitrary namespaces\",\n+        NoSuchProcedureException.class, \"not found\",\n+        () -> sql(\"CALL %s.custom.snapshot('n', 't')\", catalogName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 161}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDU3MjcxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoxNzo0N1rOIDh46w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoxNzo0N1rOIDh46w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3MTg4Mw==", "bodyText": "This can only validate one case, where either source or dest is empty. I think this should be split into empty source and empty dest cases.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540571883", "createdAt": "2020-12-10T23:17:47Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestCreateProcedures(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testMigrate() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s')\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testMigrateWithOptions() throws IOException {\n+    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Object[] result = sql(\"CALL %s.system.migrate('%s', map('foo', 'bar'))\", catalogName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot( snapshot_source => '%s', table => '%s', table_options => map('foo','bar'))\",\n+        catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    Map<String, String> props = validationCatalog.loadTable(tableIdent).properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithAlternateLocation() throws IOException {\n+    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n+    String location = temp.newFolder().toString();\n+    String snapshotLocation = temp.newFolder().toString();\n+    sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot( snapshot_source => '%s', table => '%s', table_location => '%s')\",\n+        catalogName, sourceName, tableName, snapshotLocation).get(0);\n+\n+    Assert.assertEquals(\"Should have migrated one file\", 1L, result[0]);\n+\n+    String storageLocation = validationCatalog.loadTable(tableIdent).location();\n+    Assert.assertEquals(\"Snapshot should be made at specified location\", snapshotLocation, storageLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testInvalidSnapshotsCases() {\n+    AssertHelpers.assertThrows(\"Should not allow mixed args\",\n+        AnalysisException.class, \"Named and positional arguments cannot be mixed\",\n+        () -> sql(\"CALL %s.system.snapshot('n', table => 't')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should not resolve procedures in arbitrary namespaces\",\n+        NoSuchProcedureException.class, \"not found\",\n+        () -> sql(\"CALL %s.custom.snapshot('n', 't')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n+        AnalysisException.class, \"Missing required parameters\",\n+        () -> sql(\"CALL %s.system.snapshot('foo')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n+        AnalysisException.class, \"Wrong arg type\",\n+        () -> sql(\"CALL %s.system.snapshot('n', 't', map('foo', 'bar'))\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",\n+        IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n+        () -> sql(\"CALL %s.system.snapshot('', '')\", catalogName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 173}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDU3ODYxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoxOToyNlrOIDh8Gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoxOToyNlrOIDh8Gw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3MjY5OQ==", "bodyText": "I like how the other procedures are tested in a suite named after the procedure, like TestRemoveOrphanFilesProcedure. I don't see much value in a suite for both migrate and snapshot together and it isn't obvious where these tests live. Could you split this into TestMigrateProcedure and TestSnapshotProcedure?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540572699", "createdAt": "2020-12-10T23:19:26Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateProcedures.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchProcedureException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestCreateProcedures extends SparkExtensionsTestBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDYwMTczOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzoyNjo0NFrOIDiIeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwMzoyNjowOFrOIDndEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3NTg2NQ==", "bodyText": "What is the rationale for creating actions directly instead of going through Actions?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540575865", "createdAt": "2020-12-10T23:26:44Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(1)) {\n+      args.getMap(1).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    String tableName = args.getString(0);\n+    CatalogAndIdentifier tableIdent = toCatalogAndIdentifer(tableName, PARAMETERS[0].name(), tableCatalog());\n+    CreateAction action =  new Spark3MigrateAction(spark(), tableIdent.catalog(), tableIdent.identifier());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY2MzA1Nw==", "bodyText": "We have to do this for the default catalog switch. The Actions api uses our normal catalogAndIdentifier method which defaults to using the CatalogManager currentCatalog for the default catalog", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540663057", "createdAt": "2020-12-11T03:26:08Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateProcedure.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_options\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"num_datafiles_included\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateProcedure>() {\n+      @Override\n+      protected MigrateProcedure doBuild() {\n+        return new MigrateProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    Map<String, String> options = new HashMap<>();\n+    if (!args.isNullAt(1)) {\n+      args.getMap(1).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    String tableName = args.getString(0);\n+    CatalogAndIdentifier tableIdent = toCatalogAndIdentifer(tableName, PARAMETERS[0].name(), tableCatalog());\n+    CreateAction action =  new Spark3MigrateAction(spark(), tableIdent.catalog(), tableIdent.identifier());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3NTg2NQ=="}, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NDYyMDE3OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQyMzozMzoxNFrOIDiS8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowMzozM1rOIDvXPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3ODU0NQ==", "bodyText": "Hm. I would prefer not to make these public, but I see that this needs the identifier that has already been parsed to do the custom catalog validation. Should be fine for now, but we should keep this in mind for when we fix the Actions API.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540578545", "createdAt": "2020-12-10T23:33:14Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -46,11 +46,11 @@\n  * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n  * table.\n  */\n-class Spark3MigrateAction extends Spark3CreateAction {\n+public class Spark3MigrateAction extends Spark3CreateAction {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDY1NzE5OQ==", "bodyText": "I think we should really have all of these spark implementations of actions in a \"spark\" package so we don't have to public everything.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540657199", "createdAt": "2020-12-11T03:08:26Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -46,11 +46,11 @@\n  * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n  * table.\n  */\n-class Spark3MigrateAction extends Spark3CreateAction {\n+public class Spark3MigrateAction extends Spark3CreateAction {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3ODU0NQ=="}, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5MjYzNg==", "bodyText": "This is another point to fix in actions before we release 0.11.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540792636", "createdAt": "2020-12-11T09:03:33Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -46,11 +46,11 @@\n  * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n  * table.\n  */\n-class Spark3MigrateAction extends Spark3CreateAction {\n+public class Spark3MigrateAction extends Spark3CreateAction {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU3ODU0NQ=="}, "originalCommit": {"oid": "3de93e32df8261a241e842d499e86fba9ad9fc9f"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjE5MzI3OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowNzowOVrOIDvfKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowNzowOVrOIDvfKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5NDY2NA==", "bodyText": "nit: this could fit on one line, right?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540794664", "createdAt": "2020-12-11T09:07:09Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjE5NTQ0OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowNzozM1rOIDvgVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowNzozM1rOIDvgVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5NDk2NA==", "bodyText": "nit: scalarSql method should be used", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540794964", "createdAt": "2020-12-11T09:07:33Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjE5OTg4OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowODo0NVrOIDvjEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowODo0NVrOIDvjEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5NTY2NQ==", "bodyText": "nit: this should fit on one line", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540795665", "createdAt": "2020-12-11T09:08:45Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjIwMDc0OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowODo1OVrOIDvjjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOTowODo1OVrOIDvjjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5NTc4OA==", "bodyText": "nit: withProperties?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540795788", "createdAt": "2020-12-11T09:08:59Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjIwNzQwOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMDozM1rOIDvnbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMDozM1rOIDvnbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5Njc4MA==", "bodyText": "nit: scalarSql", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540796780", "createdAt": "2020-12-11T09:10:33Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object result = scalarSql(\n+        \"CALL %s.system.snapshot(source_table => '%s', table => '%s', properties => map('foo','bar'))\",\n+        catalogName, sourceName, tableName);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    Map<String, String> props = createdTable.properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithAlternateLocation() throws IOException {\n+    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n+    String location = temp.newFolder().toString();\n+    String snapshotLocation = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjIwODk4OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMDo1OFrOIDvoUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMDo1OFrOIDvoUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5NzAwOQ==", "bodyText": "nit: should fit on one line", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540797009", "createdAt": "2020-12-11T09:10:58Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object result = scalarSql(\n+        \"CALL %s.system.snapshot(source_table => '%s', table => '%s', properties => map('foo','bar'))\",\n+        catalogName, sourceName, tableName);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    Map<String, String> props = createdTable.properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithAlternateLocation() throws IOException {\n+    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n+    String location = temp.newFolder().toString();\n+    String snapshotLocation = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjIxNDQ2OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMjowOFrOIDvrbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxNTowODozNlrOID82tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5NzgwNQ==", "bodyText": "Should we also add a test where we get a map where keys or values are not strings?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540797805", "createdAt": "2020-12-11T09:12:08Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object result = scalarSql(\n+        \"CALL %s.system.snapshot(source_table => '%s', table => '%s', properties => map('foo','bar'))\",\n+        catalogName, sourceName, tableName);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    Map<String, String> props = createdTable.properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithAlternateLocation() throws IOException {\n+    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n+    String location = temp.newFolder().toString();\n+    String snapshotLocation = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot(source_table => '%s', table => '%s', table_location => '%s')\",\n+        catalogName, sourceName, tableName, snapshotLocation).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    String storageLocation = validationCatalog.loadTable(tableIdent).location();\n+    Assert.assertEquals(\"Snapshot should be made at specified location\", snapshotLocation, storageLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testInvalidSnapshotsCases() {\n+    AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n+        AnalysisException.class, \"Missing required parameters\",\n+        () -> sql(\"CALL %s.system.snapshot('foo')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n+        AnalysisException.class, \"Wrong arg type\",\n+        () -> sql(\"CALL %s.system.snapshot('n', 't', map('foo', 'bar'))\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",\n+        IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n+        () -> sql(\"CALL %s.system.snapshot('', 'dest')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5OTU3Mg==", "bodyText": "If I understand correctly, this should be possible now?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540799572", "createdAt": "2020-12-11T09:15:08Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object result = scalarSql(\n+        \"CALL %s.system.snapshot(source_table => '%s', table => '%s', properties => map('foo','bar'))\",\n+        catalogName, sourceName, tableName);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    Map<String, String> props = createdTable.properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithAlternateLocation() throws IOException {\n+    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n+    String location = temp.newFolder().toString();\n+    String snapshotLocation = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot(source_table => '%s', table => '%s', table_location => '%s')\",\n+        catalogName, sourceName, tableName, snapshotLocation).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    String storageLocation = validationCatalog.loadTable(tableIdent).location();\n+    Assert.assertEquals(\"Snapshot should be made at specified location\", snapshotLocation, storageLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testInvalidSnapshotsCases() {\n+    AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n+        AnalysisException.class, \"Missing required parameters\",\n+        () -> sql(\"CALL %s.system.snapshot('foo')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n+        AnalysisException.class, \"Wrong arg type\",\n+        () -> sql(\"CALL %s.system.snapshot('n', 't', map('foo', 'bar'))\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",\n+        IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n+        () -> sql(\"CALL %s.system.snapshot('', 'dest')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5NzgwNQ=="}, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk3MDQ2OA==", "bodyText": "Almost everything apparently validly casts to a string, at least everything our parser allows here. This would be easier I think if we changed the parser to accept all possible expressions. I think for now the best I can do is an unbalanced map?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540970468", "createdAt": "2020-12-11T14:06:16Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object result = scalarSql(\n+        \"CALL %s.system.snapshot(source_table => '%s', table => '%s', properties => map('foo','bar'))\",\n+        catalogName, sourceName, tableName);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    Map<String, String> props = createdTable.properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithAlternateLocation() throws IOException {\n+    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n+    String location = temp.newFolder().toString();\n+    String snapshotLocation = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot(source_table => '%s', table => '%s', table_location => '%s')\",\n+        catalogName, sourceName, tableName, snapshotLocation).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    String storageLocation = validationCatalog.loadTable(tableIdent).location();\n+    Assert.assertEquals(\"Snapshot should be made at specified location\", snapshotLocation, storageLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testInvalidSnapshotsCases() {\n+    AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n+        AnalysisException.class, \"Missing required parameters\",\n+        () -> sql(\"CALL %s.system.snapshot('foo')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n+        AnalysisException.class, \"Wrong arg type\",\n+        () -> sql(\"CALL %s.system.snapshot('n', 't', map('foo', 'bar'))\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",\n+        IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n+        () -> sql(\"CALL %s.system.snapshot('', 'dest')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5NzgwNQ=="}, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTAxMzY4Ng==", "bodyText": "Let's ignore this for now then.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r541013686", "createdAt": "2020-12-11T15:08:36Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+  private static final String sourceName = \"spark_catalog.default.source\";\n+  // Currently we can only Snapshot only out of the Spark Session Catalog\n+\n+  public TestSnapshotTableProcedure(String catalogName, String implementation, Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS %S\", sourceName);\n+  }\n+\n+  @Test\n+  public void testSnapshot() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, sourceName, tableName).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithOptions() throws IOException {\n+    String location = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object result = scalarSql(\n+        \"CALL %s.system.snapshot(source_table => '%s', table => '%s', properties => map('foo','bar'))\",\n+        catalogName, sourceName, tableName);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+\n+    Table createdTable = validationCatalog.loadTable(tableIdent);\n+\n+    String tableLocation = createdTable.location();\n+    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+\n+    Map<String, String> props = createdTable.properties();\n+    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testSnapshotWithAlternateLocation() throws IOException {\n+    Assume.assumeTrue(\"No Snapshoting with Alternate locations with Hadoop Catalogs\", !catalogName.contains(\"hadoop\"));\n+    String location = temp.newFolder().toString();\n+    String snapshotLocation = temp.newFolder().toString();\n+    sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\", sourceName,\n+        location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", sourceName);\n+    Object[] result = sql(\n+        \"CALL %s.system.snapshot(source_table => '%s', table => '%s', table_location => '%s')\",\n+        catalogName, sourceName, tableName, snapshotLocation).get(0);\n+\n+    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+\n+    String storageLocation = validationCatalog.loadTable(tableIdent).location();\n+    Assert.assertEquals(\"Snapshot should be made at specified location\", snapshotLocation, storageLocation);\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+\n+    assertEquals(\"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(1L, \"a\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @Test\n+  public void testInvalidSnapshotsCases() {\n+    AssertHelpers.assertThrows(\"Should reject calls without all required args\",\n+        AnalysisException.class, \"Missing required parameters\",\n+        () -> sql(\"CALL %s.system.snapshot('foo')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with invalid arg types\",\n+        AnalysisException.class, \"Wrong arg type\",\n+        () -> sql(\"CALL %s.system.snapshot('n', 't', map('foo', 'bar'))\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",\n+        IllegalArgumentException.class, \"Cannot handle an empty identifier\",\n+        () -> sql(\"CALL %s.system.snapshot('', 'dest')\", catalogName));\n+\n+    AssertHelpers.assertThrows(\"Should reject calls with empty table identifier\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5NzgwNQ=="}, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 138}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjIyMDcyOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMzozOFrOIDvu-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxNDowODoxNFrOID6TEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5ODcxMw==", "bodyText": "I think there is a typo: toCatalogAdnIdentifier ->  toCatalogAndIdentifier", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540798713", "createdAt": "2020-12-11T09:13:38Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -70,21 +82,22 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   protected Identifier toIdentifier(String identifierAsString, String argName) {\n-    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n-        \"Cannot handle an empty identifier for argument %s\", argName);\n-\n-    CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n-        \"identifier for arg \" + argName, spark, identifierAsString, tableCatalog);\n-\n-    CatalogPlugin catalog = catalogAndIdentifier.catalog();\n-    Identifier identifier = catalogAndIdentifier.identifier();\n+    CatalogAndIdentifier catalogAndIdentifier = toCatalogAdnIdentifier(identifierAsString, argName, tableCatalog);\n \n     Preconditions.checkArgument(\n-        catalog.equals(tableCatalog),\n+        catalogAndIdentifier.catalog().equals(tableCatalog),\n         \"Cannot run procedure in catalog '%s': '%s' is a table in catalog '%s'\",\n-        tableCatalog.name(), identifierAsString, catalog.name());\n+        tableCatalog.name(), identifierAsString, catalogAndIdentifier.catalog().name());\n+\n+    return catalogAndIdentifier.identifier();\n+  }\n+\n+  protected CatalogAndIdentifier toCatalogAdnIdentifier(String identifierAsString, String argName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk3MTc5NA==", "bodyText": "fix!", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540971794", "createdAt": "2020-12-11T14:08:14Z", "author": {"login": "RussellSpitzer"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java", "diffHunk": "@@ -70,21 +82,22 @@ protected BaseProcedure(TableCatalog tableCatalog) {\n   }\n \n   protected Identifier toIdentifier(String identifierAsString, String argName) {\n-    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),\n-        \"Cannot handle an empty identifier for argument %s\", argName);\n-\n-    CatalogAndIdentifier catalogAndIdentifier = Spark3Util.catalogAndIdentifier(\n-        \"identifier for arg \" + argName, spark, identifierAsString, tableCatalog);\n-\n-    CatalogPlugin catalog = catalogAndIdentifier.catalog();\n-    Identifier identifier = catalogAndIdentifier.identifier();\n+    CatalogAndIdentifier catalogAndIdentifier = toCatalogAdnIdentifier(identifierAsString, argName, tableCatalog);\n \n     Preconditions.checkArgument(\n-        catalog.equals(tableCatalog),\n+        catalogAndIdentifier.catalog().equals(tableCatalog),\n         \"Cannot run procedure in catalog '%s': '%s' is a table in catalog '%s'\",\n-        tableCatalog.name(), identifierAsString, catalog.name());\n+        tableCatalog.name(), identifierAsString, catalogAndIdentifier.catalog().name());\n+\n+    return catalogAndIdentifier.identifier();\n+  }\n+\n+  protected CatalogAndIdentifier toCatalogAdnIdentifier(String identifierAsString, String argName,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5ODcxMw=="}, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjIyMjE1OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMzo1OVrOIDvvug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxMzo1OVrOIDvvug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDc5ODkwNg==", "bodyText": "+1", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540798906", "createdAt": "2020-12-11T09:13:59Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -122,6 +122,8 @@ public Long execute() {\n     properties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n     properties.put(\"migrated\", \"true\");\n     properties.putAll(additionalProperties());\n+    properties.putIfAbsent(LOCATION, sourceTableLocation());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjI0MjQ2OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxODoyMlrOIDv6_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQwOToxODoyMlrOIDv6_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDgwMTc5MQ==", "bodyText": "I'd actually add some empty lines to separate logical blocks similar to what you have in snapshot.\n    Map<String, String> properties = Maps.newHashMap();\n\n    properties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n    EXCLUDED_PROPERTIES.forEach(properties::remove);\n\n    properties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n    properties.put(\"migrated\", \"true\");\n    properties.putAll(additionalProperties());\n    properties.putIfAbsent(LOCATION, sourceTableLocation());\n\n    return properties;", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540801791", "createdAt": "2020-12-11T09:18:22Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -122,6 +122,8 @@ public Long execute() {\n     properties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n     properties.put(\"migrated\", \"true\");\n     properties.putAll(additionalProperties());\n+    properties.putIfAbsent(LOCATION, sourceTableLocation());\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjYwMzAxOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateTableProcedure.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0Mjo0MlrOIDzKUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0Mjo0MlrOIDzKUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg1NDg2Ng==", "bodyText": "I find naming here a bit inconsistent with other procedures. What about this?\nString identAsString = args.getString(0);\nCatalogAndIdentifier catalogAndIdent = toCatalogAndIdentifer(identAsString, PARAMETERS[0].name(), tableCatalog());", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540854866", "createdAt": "2020-12-11T10:42:42Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/MigrateTableProcedure.java", "diffHunk": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.CreateAction;\n+import org.apache.iceberg.actions.Spark3MigrateAction;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.iceberg.spark.procedures.SparkProcedures.ProcedureBuilder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class MigrateTableProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"properties\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"migrated_files_count\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private MigrateTableProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<MigrateTableProcedure>() {\n+      @Override\n+      protected MigrateTableProcedure doBuild() {\n+        return new MigrateTableProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String tableName = args.getString(0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjYxMzY1OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0NToyMFrOIDzQXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0NToyMFrOIDzQXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg1NjQxNA==", "bodyText": "Should it be just location as we just use properties?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540856414", "createdAt": "2020-12-11T10:45:20Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotTableProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"source_table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjYxNzQ3OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0NjoxOFrOIDzShA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0NjoxOFrOIDzShA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg1Njk2NA==", "bodyText": "I am okay without dest prefix but if anyone feels strongly, I am ok to add.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540856964", "createdAt": "2020-12-11T10:46:18Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotTableProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"source_table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjYyMzE4OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0NzozNVrOIDzVeg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0NzozNVrOIDzVeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg1NzcyMg==", "bodyText": "Think the name should match whatever we do in migrate. What about imported_files_count?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540857722", "createdAt": "2020-12-11T10:47:35Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotTableProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"source_table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"properties\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"imported_datafiles_count\", DataTypes.LongType, false, Metadata.empty())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjYyNDg2OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0ODowMlrOIDzWgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0ODowMlrOIDzWgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg1Nzk4NA==", "bodyText": "nit: extra space", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540857984", "createdAt": "2020-12-11T10:48:02Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotTableProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"source_table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"properties\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"imported_datafiles_count\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotTableProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotTableProcedure>() {\n+      @Override\n+      protected SnapshotTableProcedure doBuild() {\n+        return new SnapshotTableProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    CatalogAndIdentifier sourceIdent = toCatalogAdnIdentifier(source, PARAMETERS[0].name(), tableCatalog());\n+\n+    String dest = args.getString(1);\n+    CatalogAndIdentifier destIdent = toCatalogAdnIdentifier(dest, PARAMETERS[1].name(), tableCatalog());\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = Maps.newHashMap();\n+    if (!args.isNullAt(3)) {\n+      args.getMap(3).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    Preconditions.checkArgument(sourceIdent != destIdent || sourceIdent.catalog() != destIdent.catalog(),\n+        \"Cannot create a snapshot with the same name as the source of the snapshot.\");\n+    SnapshotAction action =  new Spark3SnapshotAction(spark(), sourceIdent.catalog(), sourceIdent.identifier(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjYzMjc4OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0OTo1M1rOIDza4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo0OTo1M1rOIDza4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg1OTEwNQ==", "bodyText": "nit: tableProps?", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540859105", "createdAt": "2020-12-11T10:49:53Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotTableProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"source_table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"properties\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"imported_datafiles_count\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotTableProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotTableProcedure>() {\n+      @Override\n+      protected SnapshotTableProcedure doBuild() {\n+        return new SnapshotTableProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    CatalogAndIdentifier sourceIdent = toCatalogAdnIdentifier(source, PARAMETERS[0].name(), tableCatalog());\n+\n+    String dest = args.getString(1);\n+    CatalogAndIdentifier destIdent = toCatalogAdnIdentifier(dest, PARAMETERS[1].name(), tableCatalog());\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = Maps.newHashMap();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5NjYzOTk1OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo1MTozN1rOIDze3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxMDo1MTo1MVrOIDzfhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2MDEyNg==", "bodyText": "I'd separate assigning the snapshot location like this:\n    if (snapshotLocation != null) {\n      action.withLocation(snapshotLocation);\n    }\n\n    long numImportedFiles = action.withProperties(tableProps).execute();\n    return new InternalRow[] {newInternalRow(numImportedFiles)};", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540860126", "createdAt": "2020-12-11T10:51:37Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotTableProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"source_table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"properties\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"imported_datafiles_count\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotTableProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotTableProcedure>() {\n+      @Override\n+      protected SnapshotTableProcedure doBuild() {\n+        return new SnapshotTableProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    CatalogAndIdentifier sourceIdent = toCatalogAdnIdentifier(source, PARAMETERS[0].name(), tableCatalog());\n+\n+    String dest = args.getString(1);\n+    CatalogAndIdentifier destIdent = toCatalogAdnIdentifier(dest, PARAMETERS[1].name(), tableCatalog());\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = Maps.newHashMap();\n+    if (!args.isNullAt(3)) {\n+      args.getMap(3).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    Preconditions.checkArgument(sourceIdent != destIdent || sourceIdent.catalog() != destIdent.catalog(),\n+        \"Cannot create a snapshot with the same name as the source of the snapshot.\");\n+    SnapshotAction action =  new Spark3SnapshotAction(spark(), sourceIdent.catalog(), sourceIdent.identifier(),\n+        destIdent.catalog(), destIdent.identifier());\n+\n+    long importedDataFiles;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2MDI5Mg==", "bodyText": "I think this will match the other procedures more.", "url": "https://github.com/apache/iceberg/pull/1906#discussion_r540860292", "createdAt": "2020-12-11T10:51:51Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/procedures/SnapshotTableProcedure.java", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.procedures;\n+\n+import java.util.Map;\n+import org.apache.iceberg.actions.SnapshotAction;\n+import org.apache.iceberg.actions.Spark3SnapshotAction;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util.CatalogAndIdentifier;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.iceberg.catalog.ProcedureParameter;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import scala.runtime.BoxedUnit;\n+\n+class SnapshotTableProcedure extends BaseProcedure {\n+  private static final ProcedureParameter[] PARAMETERS = new ProcedureParameter[]{\n+      ProcedureParameter.required(\"source_table\", DataTypes.StringType),\n+      ProcedureParameter.required(\"table\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"table_location\", DataTypes.StringType),\n+      ProcedureParameter.optional(\"properties\", STRING_MAP)\n+  };\n+\n+  private static final StructType OUTPUT_TYPE = new StructType(new StructField[]{\n+      new StructField(\"imported_datafiles_count\", DataTypes.LongType, false, Metadata.empty())\n+  });\n+\n+  private SnapshotTableProcedure(TableCatalog tableCatalog) {\n+    super(tableCatalog);\n+  }\n+\n+  public static SparkProcedures.ProcedureBuilder builder() {\n+    return new BaseProcedure.Builder<SnapshotTableProcedure>() {\n+      @Override\n+      protected SnapshotTableProcedure doBuild() {\n+        return new SnapshotTableProcedure(tableCatalog());\n+      }\n+    };\n+  }\n+\n+  @Override\n+  public ProcedureParameter[] parameters() {\n+    return PARAMETERS;\n+  }\n+\n+  @Override\n+  public StructType outputType() {\n+    return OUTPUT_TYPE;\n+  }\n+\n+  @Override\n+  public InternalRow[] call(InternalRow args) {\n+    String source = args.getString(0);\n+    CatalogAndIdentifier sourceIdent = toCatalogAdnIdentifier(source, PARAMETERS[0].name(), tableCatalog());\n+\n+    String dest = args.getString(1);\n+    CatalogAndIdentifier destIdent = toCatalogAdnIdentifier(dest, PARAMETERS[1].name(), tableCatalog());\n+\n+    String snapshotLocation = args.isNullAt(2) ? null : args.getString(2);\n+\n+    Map<String, String> options = Maps.newHashMap();\n+    if (!args.isNullAt(3)) {\n+      args.getMap(3).foreach(DataTypes.StringType, DataTypes.StringType,\n+          (k, v) -> {\n+            options.put(k.toString(), v.toString());\n+            return BoxedUnit.UNIT;\n+          });\n+    }\n+\n+    Preconditions.checkArgument(sourceIdent != destIdent || sourceIdent.catalog() != destIdent.catalog(),\n+        \"Cannot create a snapshot with the same name as the source of the snapshot.\");\n+    SnapshotAction action =  new Spark3SnapshotAction(spark(), sourceIdent.catalog(), sourceIdent.identifier(),\n+        destIdent.catalog(), destIdent.identifier());\n+\n+    long importedDataFiles;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDg2MDEyNg=="}, "originalCommit": {"oid": "97c2251656df1eaebd06ec886ec31d979b20b576"}, "originalPosition": 96}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3320, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}