{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg4ODk5MTEz", "number": 843, "title": "InputFormat support for Iceberg", "bodyText": "This addresses #170\nThe current code is pretty rough. I'm sending to get early feedback on the approach.\nMost of the scaffolding is  there.  There's support to allow for parameterized in memory records. All read value functions for data formats like Avro, Parquet and ORC are supported through the ReadSupport interface. The same interface also allows to add identity partition columns to the input row.\nI supported parameterized input records because much of the use of the InputFormat api is for supporting  MR based engines like Pig and Hive.\nI need feedback on the ReadSupport API and see whether folks are OK with it.  Detailed code feedback can be omitted for now. Once people are ok the high level approach I will clean up the code and add tests. Feedback welcome on testing the input format as well.\ncc @rdblue @teabot @jerryshao @massdosage\nThis patch is based on the work of @guilload", "createdAt": "2020-03-16T00:51:25Z", "url": "https://github.com/apache/iceberg/pull/843", "merged": true, "mergeCommit": {"oid": "71a7ab986a003fab614d076cffb5a4580062806e"}, "closed": true, "closedAt": "2020-04-06T23:02:57Z", "author": {"login": "rdsr"}, "timelineItems": {"totalCount": 73, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcOJIVKAFqTM3NDk3MTg0MQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcU0M5VgBqjMyMDMyOTAzNTU=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0OTcxODQx", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-374971841", "createdAt": "2020-03-16T07:39:15Z", "commit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQwNzozOToxNVrOF2ohGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQwNzozOToxNVrOF2ohGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgzMTI1Nw==", "bodyText": "Why do we need to depend on iceberg-hive module? Not sure if this will potentially introduce circular dependency.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r392831257", "createdAt": "2020-03-16T07:39:15Z", "author": {"login": "jerryshao"}, "path": "build.gradle", "diffHunk": "@@ -193,6 +193,24 @@ project(':iceberg-hive') {\n   }\n }\n \n+project(':iceberg-mr') {\n+  dependencies {\n+    compile project(':iceberg-api')\n+    compile project(':iceberg-core')\n+    compile project(':iceberg-hive')\n+    compile project(':iceberg-orc')\n+    compile project(':iceberg-parquet')\n+\n+    compileOnly(\"org.apache.hadoop:hadoop-client\") {\n+      exclude group: 'org.apache.avro', module: 'avro'\n+    }\n+\n+    testCompile project(path: ':iceberg-hive', configuration: 'testArtifacts')", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "originalPosition": 16}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1MDEwMDQy", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-375010042", "createdAt": "2020-03-16T08:49:19Z", "commit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQwODo0OToyMFrOF2qYRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQwODo0OToyMFrOF2qYRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjg2MTc2NQ==", "bodyText": "I read the pig & spark source reader, seems we could abstract the FileScanTask open(..) method with readSchema,  so that all the compute engine can share that part of code,  maybe we can defined it as:\nIterator<T> open(FileScanTask currentTask, Schema readSchema, ReadSupport<T> readSupport).\n\nI think it could be a file-level abstraction ..", "url": "https://github.com/apache/iceberg/pull/843#discussion_r392861765", "createdAt": "2020-03-16T08:49:20Z", "author": {"login": "openinx"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    final String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      // Not sure if this is having any effect?\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader();\n+  }\n+\n+  public static ConfBuilder updateConf(\n+      Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+    return new ConfBuilder(conf, path, readSupportClass);\n+  }\n+\n+  public static class ConfBuilder {\n+    private final Configuration conf;\n+\n+    public ConfBuilder(Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+      this.conf = conf;\n+      conf.set(TABLE_PATH, path);\n+      conf.set(READ_SUPPORT, readSupportClass.getName());\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+    }\n+\n+    public ConfBuilder filterExpression(Expression expression) throws IOException {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    //TODO: other options split-size, snapshotid etc..\n+    public Configuration updatedConf() {\n+      return conf;\n+    }\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private ReadSupport<T> readSupport;\n+    private Closeable currentCloseable;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext context) {\n+      Configuration conf = context.getConfiguration();\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = context;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.readSupport = readSupport(conf);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private ReadSupport<T> readSupport(Configuration conf) {\n+      String readSupportClassName = conf.get(READ_SUPPORT);\n+      try {\n+        return DynClasses\n+            .builder()\n+            .impl(readSupportClassName)\n+            .<ReadSupport<T>>buildChecked()\n+            .newInstance();\n+      } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+        throw new RuntimeException(String.format(\"Unable to instantiate read support %s\", readSupportClassName), e);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      if (hasJoinedPartitionColumns) {\n+        readSchema = TypeUtil.selectNot(tableSchema, idColumns);\n+        Schema partitionSchema = TypeUtil.select(tableSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readSchema),\n+            row -> readSupport.withPartitionColumns(row, partitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "originalPosition": 241}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1NDU3Mjky", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-375457292", "createdAt": "2020-03-16T17:57:55Z", "commit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxNzo1Nzo1NVrOF2_sQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxNzo1Nzo1NVrOF2_sQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIxMDk0NQ==", "bodyText": "Looks like this serialization (java serializable/gzip/base64) is only used for the filter expression. We should consider (in a separate commit) using a parser for Iceberg expressions so that we can convert them to infix strings instead and parse them when needed. That makes the job configuration readable, which is always a good thing.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393210945", "createdAt": "2020-03-16T17:57:55Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "originalPosition": 91}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1NDY4MTA5", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-375468109", "createdAt": "2020-03-16T18:09:40Z", "commit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODowOTo0MFrOF3AOvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODowOTo0MFrOF3AOvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIxOTc3Mw==", "bodyText": "Rather than allowing a user to pass in ReadSupport, I think it makes more sense to configure at a higher level. There are good built-in options that don't require exposing classes from Parquet, Avro, and ORC here.\nI think the config builder could expose methods to control the in-memory format. By default, it would use Iceberg generic records. Optionally, we could expose Pig's in-memory types and Hive's in-memory types. Maybe Avro as well. Those seem like reasonable options for an InputFormat given that users will probably not be customizing their in-memory classes if they are still using the MR read interface. At this point, I would expect the input format to be primarily used for integration with Pig and Hive.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393219773", "createdAt": "2020-03-16T18:09:40Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/ReadSupport.java", "diffHunk": "@@ -0,0 +1,50 @@\n+package org.apache.iceberg.mr;\n+\n+import java.util.function.BiFunction;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.orc.OrcValueReader;\n+import org.apache.iceberg.parquet.ParquetValueReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.util.function.Function;\n+\n+\n+/**\n+ * ReadSupport for MR InputFormat, providing value readers\n+ * for different data formats and appending identity partition columns\n+ * to the input row\n+ * @param <T>\n+ */\n+public interface ReadSupport<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "originalPosition": 23}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1NDY5ODE3", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-375469817", "createdAt": "2020-03-16T18:12:07Z", "commit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODoxMjowN1rOF3AU-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODoxMjowN1rOF3AU-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyMTM3MA==", "bodyText": "No need for \"Expression\" in this API. How about just filter?", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393221370", "createdAt": "2020-03-16T18:12:07Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    final String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      // Not sure if this is having any effect?\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader();\n+  }\n+\n+  public static ConfBuilder updateConf(\n+      Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+    return new ConfBuilder(conf, path, readSupportClass);\n+  }\n+\n+  public static class ConfBuilder {\n+    private final Configuration conf;\n+\n+    public ConfBuilder(Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+      this.conf = conf;\n+      conf.set(TABLE_PATH, path);\n+      conf.set(READ_SUPPORT, readSupportClass.getName());\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+    }\n+\n+    public ConfBuilder filterExpression(Expression expression) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "originalPosition": 133}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1NDcwMzE5", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-375470319", "createdAt": "2020-03-16T18:12:52Z", "commit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODoxMjo1MlrOF3AWiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODoxMjo1MlrOF3AWiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyMTc2OQ==", "bodyText": "I'd rather not throw IOException here. Can we update serializeToBase64 to not throw it?", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393221769", "createdAt": "2020-03-16T18:12:52Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    final String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      // Not sure if this is having any effect?\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader();\n+  }\n+\n+  public static ConfBuilder updateConf(\n+      Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+    return new ConfBuilder(conf, path, readSupportClass);\n+  }\n+\n+  public static class ConfBuilder {\n+    private final Configuration conf;\n+\n+    public ConfBuilder(Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+      this.conf = conf;\n+      conf.set(TABLE_PATH, path);\n+      conf.set(READ_SUPPORT, readSupportClass.getName());\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+    }\n+\n+    public ConfBuilder filterExpression(Expression expression) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "originalPosition": 133}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1NDcxNzYx", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-375471761", "createdAt": "2020-03-16T18:14:48Z", "commit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODoxNDo0OFrOF3AbDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODoxNDo0OFrOF3AbDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyMjkyNA==", "bodyText": "If we are going with the builder pattern, then let's set the table/path and in-memory data model in builder methods instead of here. I prefer not to minimize constructor args with builders by removing anything that might not need to be set (e.g. use Iceberg generics by default).", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393222924", "createdAt": "2020-03-16T18:14:48Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    final String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      // Not sure if this is having any effect?\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader();\n+  }\n+\n+  public static ConfBuilder updateConf(\n+      Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+    return new ConfBuilder(conf, path, readSupportClass);\n+  }\n+\n+  public static class ConfBuilder {\n+    private final Configuration conf;\n+\n+    public ConfBuilder(Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "originalPosition": 125}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1NDcyNTQ5", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-375472549", "createdAt": "2020-03-16T18:15:49Z", "commit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODoxNTo0OVrOF3Adog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODoxNTo0OVrOF3Adog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyMzU4Ng==", "bodyText": "Can we add documentation?", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393223586", "createdAt": "2020-03-16T18:15:49Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "originalPosition": 64}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1NDc0MjMw", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-375474230", "createdAt": "2020-03-16T18:18:14Z", "commit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODoxODoxNFrOF3Aikw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODoxODoxNFrOF3Aikw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyNDg1MQ==", "bodyText": "I'm not sure that this is needed. Unless this builder makes a copy of the Configuration, it is configuring the one passed in as methods are called. I think that's probably fine.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393224851", "createdAt": "2020-03-16T18:18:14Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    final String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      // Not sure if this is having any effect?\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader();\n+  }\n+\n+  public static ConfBuilder updateConf(\n+      Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+    return new ConfBuilder(conf, path, readSupportClass);\n+  }\n+\n+  public static class ConfBuilder {\n+    private final Configuration conf;\n+\n+    public ConfBuilder(Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+      this.conf = conf;\n+      conf.set(TABLE_PATH, path);\n+      conf.set(READ_SUPPORT, readSupportClass.getName());\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+    }\n+\n+    public ConfBuilder filterExpression(Expression expression) throws IOException {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    //TODO: other options split-size, snapshotid etc..\n+    public Configuration updatedConf() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "originalPosition": 144}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1NDc2OTQ2", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-375476946", "createdAt": "2020-03-16T18:22:16Z", "commit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODoyMjoxNlrOF3ArcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODoyMjoxNlrOF3ArcA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyNzEyMA==", "bodyText": "I like the builder pattern here. I used something similar for Kite's InputFormat.\nYou might consider how that builder worked. The static methods there would accept a Job when creating the builder. That way, there was no need for the user to also set the input format class. It was also not necessary to build the resulting Configuration and do something with it:\n  /**\n   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n   * returns a helper to add further configuration.\n   *\n   * @param job the {@code Job} to configure\n   */\n  public static ConfigBuilder configure(Job job) {\n    job.setInputFormatClass(IcebergInputFormat.class);\n    return new ConfigBuilder(job.getConfiguration());\n  }", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393227120", "createdAt": "2020-03-16T18:22:16Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    final String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      // Not sure if this is having any effect?\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader();\n+  }\n+\n+  public static ConfBuilder updateConf(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "originalPosition": 117}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1NDc5MTQ3", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-375479147", "createdAt": "2020-03-16T18:25:28Z", "commit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODoyNToyOFrOF3AyRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODoyNToyOFrOF3AyRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyODg3MQ==", "bodyText": "The base scan will combine the requested columns with the ones used by row filters to produce the required schema. If you don't intend to use that, then it isn't needed. But you could add the schema to the splits to know what to project.\nSee https://github.com/apache/incubator-iceberg/blob/master/core/src/main/java/org/apache/iceberg/BaseTableScan.java#L298", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393228871", "createdAt": "2020-03-16T18:25:28Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    final String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      // Not sure if this is having any effect?\n+      scan.project(SchemaParser.fromJson(schemaStr));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "originalPosition": 99}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc1NDg0MTIw", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-375484120", "createdAt": "2020-03-16T18:32:42Z", "commit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODozMjo0MlrOF3BB5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xNlQxODozMjo0MlrOF3BB5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIzMjg2OQ==", "bodyText": "You might want to use DynConstructors and build instead of buildChecked. That will get you better error messages and some of this error handling for free.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393232869", "createdAt": "2020-03-16T18:32:42Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    final String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      // Not sure if this is having any effect?\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader();\n+  }\n+\n+  public static ConfBuilder updateConf(\n+      Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+    return new ConfBuilder(conf, path, readSupportClass);\n+  }\n+\n+  public static class ConfBuilder {\n+    private final Configuration conf;\n+\n+    public ConfBuilder(Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+      this.conf = conf;\n+      conf.set(TABLE_PATH, path);\n+      conf.set(READ_SUPPORT, readSupportClass.getName());\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+    }\n+\n+    public ConfBuilder filterExpression(Expression expression) throws IOException {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    //TODO: other options split-size, snapshotid etc..\n+    public Configuration updatedConf() {\n+      return conf;\n+    }\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private ReadSupport<T> readSupport;\n+    private Closeable currentCloseable;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext context) {\n+      Configuration conf = context.getConfiguration();\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = context;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.readSupport = readSupport(conf);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private ReadSupport<T> readSupport(Configuration conf) {\n+      String readSupportClassName = conf.get(READ_SUPPORT);\n+      try {\n+        return DynClasses\n+            .builder()\n+            .impl(readSupportClassName)\n+            .<ReadSupport<T>>buildChecked()\n+            .newInstance();\n+      } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "originalPosition": 217}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "254b1d8f683844d531c03182af5409955df77432", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/254b1d8f683844d531c03182af5409955df77432", "committedDate": "2020-03-18T06:46:43Z", "message": "Address review comments"}, "afterCommit": {"oid": "5f5ffbd30f0ae7881225d886a27bdcb87c69078c", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/5f5ffbd30f0ae7881225d886a27bdcb87c69078c", "committedDate": "2020-03-18T06:51:19Z", "message": "Address review comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5f5ffbd30f0ae7881225d886a27bdcb87c69078c", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/5f5ffbd30f0ae7881225d886a27bdcb87c69078c", "committedDate": "2020-03-18T06:51:19Z", "message": "Address review comments"}, "afterCommit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/1d07d8c8a88ef9c9ee56c750c519fc13e455d697", "committedDate": "2020-03-18T13:17:58Z", "message": "Address review comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc3MDIyMzMz", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-377022333", "createdAt": "2020-03-18T16:15:21Z", "commit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNjoxNToyMVrOF4MkyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNjoxNToyMVrOF4MkyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3MDYwMA==", "bodyText": "We should note in documentation that this InputFormat must perform its own split planning and doesn't accept FileSplit instances. I think it's fairly common to create and pass file splits, so we may just want to accept them and treat them as single-file tasks.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394470600", "createdAt": "2020-03-18T16:15:21Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  private static final String LOCALITY = \"iceberg.mr.locality\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  public enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder inMemoryDataModel(InMemoryDataModel inMemoryDataModel) {\n+      conf.set(IN_MEMORY_DATA_MODEL, inMemoryDataModel.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = getTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(conf, task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      CombinedScanTask task = ((IcebergSplit) split).task;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "originalPosition": 235}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc3MDI0ODMz", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-377024833", "createdAt": "2020-03-18T16:18:12Z", "commit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNjoxODoxMlrOF4MsiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNjoxODoxMlrOF4MsiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3MjU4NA==", "bodyText": "I just realized that we also need to assert that the residual for each file in the task is either null or Expressions.alwaysTrue. That will guarantee that the filter expression is completely satisfied and no additional rows will be returned.\nAlternatively, we could run the residual to filter every row in the reader, but it's probably simpler for now to open that as a follow-up issue.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394472584", "createdAt": "2020-03-18T16:18:12Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  private static final String LOCALITY = \"iceberg.mr.locality\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  public enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder inMemoryDataModel(InMemoryDataModel inMemoryDataModel) {\n+      conf.set(IN_MEMORY_DATA_MODEL, inMemoryDataModel.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = getTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(conf, task)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "originalPosition": 207}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc3MDI3MjU0", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-377027254", "createdAt": "2020-03-18T16:20:57Z", "commit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNjoyMDo1N1rOF4M0OQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNjoyMDo1N1rOF4M0OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3NDU1Mw==", "bodyText": "I think we need to update how we handle partition columns in general. #585 points out that the current row-join approach doesn't work for nested columns. We'll need to update row materialization to fix it. I can work on that sometime soon because we also need it for metadata columns in Spark (in particular, row ordinal in a file).\nFYI @aokolnychyi.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394474553", "createdAt": "2020-03-18T16:20:57Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  private static final String LOCALITY = \"iceberg.mr.locality\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  public enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder inMemoryDataModel(InMemoryDataModel inMemoryDataModel) {\n+      conf.set(IN_MEMORY_DATA_MODEL, inMemoryDataModel.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = getTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(conf, task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+      if (hasJoinedPartitionColumns) {\n+        readSchema = TypeUtil.selectNot(tableSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(tableSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      return iterable.iterator();\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "originalPosition": 330}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc3MDI5Mzg2", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-377029386", "createdAt": "2020-03-18T16:23:28Z", "commit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNjoyMzoyOFrOF4M7Ag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNjoyMzoyOFrOF4M7Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3NjI5MA==", "bodyText": "What about changing this to usePigTuples() and useHiveRows()? That eliminates the need for a public enum and I think is easy to read.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394476290", "createdAt": "2020-03-18T16:23:28Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  private static final String LOCALITY = \"iceberg.mr.locality\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  public enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder inMemoryDataModel(InMemoryDataModel inMemoryDataModel) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "originalPosition": 165}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc3MDM0ODg0", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-377034884", "createdAt": "2020-03-18T16:29:37Z", "commit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNjoyOTozN1rOF4NMJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNjoyOTozN1rOF4NMJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ4MDY3OA==", "bodyText": "In Spark, this is an instance method so that user can override it and use their own catalog. I think that would be a good idea here as well:\npublic class CustomCatalogIcebergInputFormat extends IcebergInputFormat {\n  @Override\n  public Table getTable(String tablePath) {\n    CustomCatalog.get().loadTable(conf.get(tablePath));\n  }\n}\nIn that example, I'm assuming that we'd have a final getTable(Configuration) method that pulls TABLE_PATH out of the config and passes it to getTable.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394480678", "createdAt": "2020-03-18T16:29:37Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  private static final String LOCALITY = \"iceberg.mr.locality\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  public enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder inMemoryDataModel(InMemoryDataModel inMemoryDataModel) {\n+      conf.set(IN_MEMORY_DATA_MODEL, inMemoryDataModel.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = getTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(conf, task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+      if (hasJoinedPartitionColumns) {\n+        readSchema = TypeUtil.selectNot(tableSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(tableSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      return iterable.iterator();\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          return (T) icebergRecordWithPartitionsColumns((Record) row, identityPartitionSchema, spec, partition);\n+      }\n+      return row;\n+    }\n+\n+    private static Record icebergRecordWithPartitionsColumns(\n+        Record record, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      List<Types.NestedField> fields = Lists.newArrayList(record.struct().fields());\n+      fields.addAll(identityPartitionSchema.asStruct().fields());\n+      GenericRecord row = GenericRecord.create(Types.StructType.of(fields));\n+      int size = record.struct().fields().size();\n+      for (int i = 0; i < size; i++) {\n+        row.set(i, record.get(i));\n+      }\n+      List<PartitionField> partitionFields = spec.fields();\n+      List<Types.NestedField> identityColumns = identityPartitionSchema.columns();\n+      for (int i = 0; i < identityColumns.size(); i++) {\n+        Types.NestedField identityColumn = identityColumns.get(i);\n+\n+        for (int j = 0; j < partitionFields.size(); j++) {\n+          PartitionField partitionField = partitionFields.get(j);\n+          if (identityColumn.fieldId() == partitionField.sourceId() &&\n+              \"identity\".equals(partitionField.transform().toString())) {\n+            row.set(size + i, partition.get(j, spec.javaClasses()[i]));\n+          }\n+        }\n+      }\n+      return row;\n+    }\n+\n+    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n+                                             .project(readSchema)\n+                                             .split(task.start(), task.length());\n+\n+      if (reuseContainers) {\n+        avroReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          avroReadBuilder.createReaderFunc(DataReader::create);\n+      }\n+      return avroReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n+                                                      .project(readSchema)\n+                                                      .filter(task.residual())\n+                                                      .caseSensitive(caseSensitive)\n+                                                      .split(task.start(), task.length());\n+      if (reuseContainers) {\n+        parquetReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          parquetReadBuilder.createReaderFunc(\n+              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n+      }\n+      return parquetReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newOrcIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      ORC.ReadBuilder orcReadBuilder = ORC.read(inputFile)\n+                                          .schema(readSchema)\n+                                          .caseSensitive(caseSensitive)\n+                                          .split(task.start(), task.length());\n+      // ORC does not support reuse containers yet\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          //TODO: We do not have support for Iceberg generics for ORC\n+          throw new UnsupportedOperationException();\n+      }\n+\n+      return orcReadBuilder.build();\n+    }\n+  }\n+\n+  private static Table getTable(Configuration conf) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "originalPosition": 425}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc3MDM1NTEy", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-377035512", "createdAt": "2020-03-18T16:30:21Z", "commit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNjozMDoyMVrOF4NN9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNjozMDoyMVrOF4NN9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ4MTE0MA==", "bodyText": "Is this duplicated? Maybe we should have a locality util class.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394481140", "createdAt": "2020-03-18T16:30:21Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  private static final String LOCALITY = \"iceberg.mr.locality\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  public enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder inMemoryDataModel(InMemoryDataModel inMemoryDataModel) {\n+      conf.set(IN_MEMORY_DATA_MODEL, inMemoryDataModel.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = getTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(conf, task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+      if (hasJoinedPartitionColumns) {\n+        readSchema = TypeUtil.selectNot(tableSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(tableSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      return iterable.iterator();\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          return (T) icebergRecordWithPartitionsColumns((Record) row, identityPartitionSchema, spec, partition);\n+      }\n+      return row;\n+    }\n+\n+    private static Record icebergRecordWithPartitionsColumns(\n+        Record record, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      List<Types.NestedField> fields = Lists.newArrayList(record.struct().fields());\n+      fields.addAll(identityPartitionSchema.asStruct().fields());\n+      GenericRecord row = GenericRecord.create(Types.StructType.of(fields));\n+      int size = record.struct().fields().size();\n+      for (int i = 0; i < size; i++) {\n+        row.set(i, record.get(i));\n+      }\n+      List<PartitionField> partitionFields = spec.fields();\n+      List<Types.NestedField> identityColumns = identityPartitionSchema.columns();\n+      for (int i = 0; i < identityColumns.size(); i++) {\n+        Types.NestedField identityColumn = identityColumns.get(i);\n+\n+        for (int j = 0; j < partitionFields.size(); j++) {\n+          PartitionField partitionField = partitionFields.get(j);\n+          if (identityColumn.fieldId() == partitionField.sourceId() &&\n+              \"identity\".equals(partitionField.transform().toString())) {\n+            row.set(size + i, partition.get(j, spec.javaClasses()[i]));\n+          }\n+        }\n+      }\n+      return row;\n+    }\n+\n+    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n+                                             .project(readSchema)\n+                                             .split(task.start(), task.length());\n+\n+      if (reuseContainers) {\n+        avroReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          avroReadBuilder.createReaderFunc(DataReader::create);\n+      }\n+      return avroReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n+                                                      .project(readSchema)\n+                                                      .filter(task.residual())\n+                                                      .caseSensitive(caseSensitive)\n+                                                      .split(task.start(), task.length());\n+      if (reuseContainers) {\n+        parquetReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          parquetReadBuilder.createReaderFunc(\n+              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n+      }\n+      return parquetReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newOrcIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      ORC.ReadBuilder orcReadBuilder = ORC.read(inputFile)\n+                                          .schema(readSchema)\n+                                          .caseSensitive(caseSensitive)\n+                                          .split(task.start(), task.length());\n+      // ORC does not support reuse containers yet\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          //TODO: We do not have support for Iceberg generics for ORC\n+          throw new UnsupportedOperationException();\n+      }\n+\n+      return orcReadBuilder.build();\n+    }\n+  }\n+\n+  private static Table getTable(Configuration conf) {\n+    String path = conf.get(TABLE_PATH);\n+    if (path.contains(\"/\")) {\n+      HadoopTables tables = new HadoopTables(conf);\n+      return tables.load(path);\n+    } else {\n+      Catalog catalog = HiveCatalogs.loadCatalog(conf);\n+      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n+      return catalog.loadTable(tableIdentifier);\n+    }\n+  }\n+\n+  private static class IcebergSplit extends InputSplit implements Writable {\n+    private static final String[] ANYWHERE = new String[]{\"*\"};\n+    private CombinedScanTask task;\n+    private transient String[] locations;\n+    private transient Configuration conf;\n+\n+    IcebergSplit(Configuration conf, CombinedScanTask task) {\n+      this.task = task;\n+      this.conf = conf;\n+    }\n+\n+    @Override\n+    public long getLength() {\n+      return task.files().stream().mapToLong(FileScanTask::length).sum();\n+    }\n+\n+    @Override\n+    public String[] getLocations() {\n+      boolean localityPreferred = conf.getBoolean(LOCALITY, false);\n+      if (!localityPreferred) {\n+        return ANYWHERE;\n+      }\n+      if (locations != null) {\n+        return locations;\n+      }\n+\n+      Set<String> locationSets = Sets.newHashSet();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "originalPosition": 463}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc3MDM3NDA5", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-377037409", "createdAt": "2020-03-18T16:32:34Z", "commit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNjozMjozNFrOF4NTqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNjozMjozNFrOF4NTqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ4MjYwMQ==", "bodyText": "We should make this an assertion before committing this PR.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394482601", "createdAt": "2020-03-18T16:32:34Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/TestIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.avro.RandomAvroData;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+\n+public class TestIcebergInputFormat {\n+  private static final Configuration CONF = new Configuration();\n+  private static final HadoopTables TABLES = new HadoopTables(CONF);\n+\n+  private File tableLocation;\n+\n+  private static final Schema SCHEMA = new Schema(\n+      required(1, \"id\", Types.LongType.get()),\n+      optional(2, \"data\", Types.StringType.get()),\n+      required(3, \"date\", Types.StringType.get()));\n+\n+  private static final PartitionSpec PARTITION_BY_DATE = PartitionSpec\n+      .builderFor(SCHEMA)\n+      .identity(\"date\")\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+  IcebergInputFormat<GenericData.Record> icebergInputFormat;\n+\n+  @Test\n+  public void test() throws IOException, InterruptedException {\n+    tableLocation = new File(temp.newFolder(), \"table\");\n+    Table table = TABLES.create(SCHEMA, PARTITION_BY_DATE, tableLocation.toString());\n+    List<GenericData.Record> records = RandomAvroData.generate(SCHEMA, 5, 0L);\n+    File file = temp.newFile();\n+    Assert.assertTrue(file.delete());\n+    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(file))\n+                                                         .schema(SCHEMA)\n+                                                         .named(\"avro\")\n+                                                         .build()) {\n+      appender.addAll(records);\n+    }\n+\n+    DataFile dataFile = DataFiles.builder(PARTITION_BY_DATE)\n+                                 .withPartition(partitionData(\"2020-03-15\"))\n+                                 .withRecordCount(records.size())\n+                                 .withFileSizeInBytes(file.length())\n+                                 .withPath(file.toString())\n+                                 .withFormat(\"avro\")\n+                                 .build();\n+\n+    table.newAppend().appendFile(dataFile).commit();\n+\n+    Job job = Job.getInstance(new Configuration());\n+    IcebergInputFormat\n+        .configure(job)\n+        .readFrom(tableLocation.getAbsolutePath());\n+\n+    TaskAttemptContext context = new TaskAttemptContextImpl(new JobConf(job.getConfiguration()), new TaskAttemptID());\n+    icebergInputFormat = new IcebergInputFormat<>();\n+    List<InputSplit> splits = icebergInputFormat.getSplits(context);\n+    final RecordReader<Void, GenericData.Record> recordReader =\n+        icebergInputFormat.createRecordReader(splits.get(0), context);\n+    recordReader.initialize(splits.get(0), context);\n+    while (recordReader.nextKeyValue()) {\n+      System.out.println(recordReader.getCurrentValue());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "originalPosition": 111}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc3Mzk4MTkw", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-377398190", "createdAt": "2020-03-19T03:23:47Z", "commit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQwMzoyMzo0N1rOF4e1Pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQwMzoyMzo0N1rOF4e1Pw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDc2OTcyNw==", "bodyText": "Maybe we'd also consider adding incremental support here, like what current Spark Reader did.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394769727", "createdAt": "2020-03-19T03:23:47Z", "author": {"login": "jerryshao"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  private static final String LOCALITY = \"iceberg.mr.locality\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "originalPosition": 90}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc3NDAzNzE3", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-377403717", "createdAt": "2020-03-19T03:39:52Z", "commit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQwMzozOTo1MlrOF4fF1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQwMzozOTo1MlrOF4fF1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDc3Mzk3NA==", "bodyText": "For a simple MR application, do we need to involve HiveCatalog?\nIt seems strange to me that a simple MR application requires to communicate with Hive. I was thinking that if we could have an abstract catalog layer here, or only support Hadoop tables in MR as default implementation. Besides let Hive, Pig and others to choose a specific implementation, because each engine may have their own preference, and we don't have to bind to Hive or others here.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394773974", "createdAt": "2020-03-19T03:39:52Z", "author": {"login": "jerryshao"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  private static final String LOCALITY = \"iceberg.mr.locality\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  public enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder inMemoryDataModel(InMemoryDataModel inMemoryDataModel) {\n+      conf.set(IN_MEMORY_DATA_MODEL, inMemoryDataModel.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = getTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(conf, task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+      if (hasJoinedPartitionColumns) {\n+        readSchema = TypeUtil.selectNot(tableSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(tableSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      return iterable.iterator();\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          return (T) icebergRecordWithPartitionsColumns((Record) row, identityPartitionSchema, spec, partition);\n+      }\n+      return row;\n+    }\n+\n+    private static Record icebergRecordWithPartitionsColumns(\n+        Record record, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      List<Types.NestedField> fields = Lists.newArrayList(record.struct().fields());\n+      fields.addAll(identityPartitionSchema.asStruct().fields());\n+      GenericRecord row = GenericRecord.create(Types.StructType.of(fields));\n+      int size = record.struct().fields().size();\n+      for (int i = 0; i < size; i++) {\n+        row.set(i, record.get(i));\n+      }\n+      List<PartitionField> partitionFields = spec.fields();\n+      List<Types.NestedField> identityColumns = identityPartitionSchema.columns();\n+      for (int i = 0; i < identityColumns.size(); i++) {\n+        Types.NestedField identityColumn = identityColumns.get(i);\n+\n+        for (int j = 0; j < partitionFields.size(); j++) {\n+          PartitionField partitionField = partitionFields.get(j);\n+          if (identityColumn.fieldId() == partitionField.sourceId() &&\n+              \"identity\".equals(partitionField.transform().toString())) {\n+            row.set(size + i, partition.get(j, spec.javaClasses()[i]));\n+          }\n+        }\n+      }\n+      return row;\n+    }\n+\n+    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n+                                             .project(readSchema)\n+                                             .split(task.start(), task.length());\n+\n+      if (reuseContainers) {\n+        avroReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          avroReadBuilder.createReaderFunc(DataReader::create);\n+      }\n+      return avroReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n+                                                      .project(readSchema)\n+                                                      .filter(task.residual())\n+                                                      .caseSensitive(caseSensitive)\n+                                                      .split(task.start(), task.length());\n+      if (reuseContainers) {\n+        parquetReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          parquetReadBuilder.createReaderFunc(\n+              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n+      }\n+      return parquetReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newOrcIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      ORC.ReadBuilder orcReadBuilder = ORC.read(inputFile)\n+                                          .schema(readSchema)\n+                                          .caseSensitive(caseSensitive)\n+                                          .split(task.start(), task.length());\n+      // ORC does not support reuse containers yet\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          //TODO: We do not have support for Iceberg generics for ORC\n+          throw new UnsupportedOperationException();\n+      }\n+\n+      return orcReadBuilder.build();\n+    }\n+  }\n+\n+  private static Table getTable(Configuration conf) {\n+    String path = conf.get(TABLE_PATH);\n+    if (path.contains(\"/\")) {\n+      HadoopTables tables = new HadoopTables(conf);\n+      return tables.load(path);\n+    } else {\n+      Catalog catalog = HiveCatalogs.loadCatalog(conf);\n+      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n+      return catalog.loadTable(tableIdentifier);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "originalPosition": 433}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4NjcwNDc4", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-378670478", "createdAt": "2020-03-20T17:16:37Z", "commit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNzoxNjozN1rOF5ch4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMFQxNzoxODowMVrOF5ck7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc4MDU3Nw==", "bodyText": "I can see a future where we have Hive SerDes and StorageHandlers in the hive subproject which in turn will need to depend on the input format(s) here so we need to avoid a circular dependency right? In our InputFormat up to now we've just use HadoopTables and the JobConf to get everything we need.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r395780577", "createdAt": "2020-03-20T17:16:37Z", "author": {"login": "massdosage"}, "path": "build.gradle", "diffHunk": "@@ -193,6 +193,24 @@ project(':iceberg-hive') {\n   }\n }\n \n+project(':iceberg-mr') {\n+  dependencies {\n+    compile project(':iceberg-api')\n+    compile project(':iceberg-core')\n+    compile project(':iceberg-hive')\n+    compile project(':iceberg-orc')\n+    compile project(':iceberg-parquet')\n+\n+    compileOnly(\"org.apache.hadoop:hadoop-client\") {\n+      exclude group: 'org.apache.avro', module: 'avro'\n+    }\n+\n+    testCompile project(path: ':iceberg-hive', configuration: 'testArtifacts')", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgzMTI1Nw=="}, "originalCommit": {"oid": "f8dc2b09feac246310e6c8139bfa734330cf49a0"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc4MTM1Ng==", "bodyText": "Can I suggest putting this and the related classes into a org.apache.iceberg.mr.mapreduce package? So we can then add the org.apache.iceberg.mr.mapred implementation alongside it and the package names match the Hadoop API package names?", "url": "https://github.com/apache/iceberg/pull/843#discussion_r395781356", "createdAt": "2020-03-20T17:18:01Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "originalPosition": 1}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a13d758ab058feb7491478338db2ae667a6ae93d", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/a13d758ab058feb7491478338db2ae667a6ae93d", "committedDate": "2020-03-21T17:28:16Z", "message": "Address review comments [Take 2]"}, "afterCommit": {"oid": "8d0c77791a77118abdeb1f170efbcea71993a9ea", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/8d0c77791a77118abdeb1f170efbcea71993a9ea", "committedDate": "2020-03-21T17:38:34Z", "message": "Address review comments [Take 2]"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8d0c77791a77118abdeb1f170efbcea71993a9ea", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/8d0c77791a77118abdeb1f170efbcea71993a9ea", "committedDate": "2020-03-21T17:38:34Z", "message": "Address review comments [Take 2]"}, "afterCommit": {"oid": "0bf3319633f76d8c5cea5b6462a7c19b6928c03a", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/0bf3319633f76d8c5cea5b6462a7c19b6928c03a", "committedDate": "2020-03-21T18:20:43Z", "message": "Address review comments [Take 2]"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0bf3319633f76d8c5cea5b6462a7c19b6928c03a", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/0bf3319633f76d8c5cea5b6462a7c19b6928c03a", "committedDate": "2020-03-21T18:20:43Z", "message": "Address review comments [Take 2]"}, "afterCommit": {"oid": "88a9c1d9e8db1413549afb254788d6b59db0fb42", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/88a9c1d9e8db1413549afb254788d6b59db0fb42", "committedDate": "2020-03-21T18:28:30Z", "message": "Address review comments [Take 2]"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4OTQwNDM0", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-378940434", "createdAt": "2020-03-21T18:30:35Z", "commit": {"oid": "88a9c1d9e8db1413549afb254788d6b59db0fb42"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMVQxODozMDozNVrOF5q8nw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMVQxODozMDozNVrOF5q8nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAxNjc5OQ==", "bodyText": "Will add orc once ORC patches are committed which read/writer Iceberg generics", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396016799", "createdAt": "2020-03-21T18:30:35Z", "author": {"login": "rdsr"}, "path": "mr/src/test/java/org/apache/iceberg/mr/TestIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.FluentIterable;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.api.Database;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TestHelpers.Row;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataWriter;\n+import org.apache.iceberg.data.parquet.GenericParquetWriter;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.hive.HiveClientPool;\n+import org.apache.iceberg.hive.TestHiveMetastore;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+\n+@RunWith(Parameterized.class)\n+public class TestIcebergInputFormat {\n+  private static final Schema SCHEMA = new Schema(\n+      required(1, \"data\", Types.StringType.get()),\n+      required(3, \"id\", Types.LongType.get()),\n+      required(2, \"date\", Types.StringType.get()));\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+                                                         .identity(\"date\")\n+                                                         .bucket(\"id\", 1)\n+                                                         .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+  private HadoopTables tables;\n+  private Configuration conf;\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][]{\n+        new Object[]{\"parquet\"},\n+        new Object[]{\"avro\"}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88a9c1d9e8db1413549afb254788d6b59db0fb42"}, "originalPosition": 97}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "88a9c1d9e8db1413549afb254788d6b59db0fb42", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/88a9c1d9e8db1413549afb254788d6b59db0fb42", "committedDate": "2020-03-21T18:28:30Z", "message": "Address review comments [Take 2]"}, "afterCommit": {"oid": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3", "committedDate": "2020-03-21T19:32:56Z", "message": "Address review comments [Take 2]"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc5MDY2NzAy", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-379066702", "createdAt": "2020-03-22T23:26:30Z", "commit": {"oid": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMlQyMzoyNjozMFrOF5zoPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMlQyMzoyNjozMFrOF5zoPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTAzNw==", "bodyText": "Yes. FileIO is Serializable and should be used to create input files. We should also use the encryption manager to be correct (could open a follow-up).", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396159037", "createdAt": "2020-03-22T23:26:30Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,525 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(conf, task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+      if (hasJoinedPartitionColumns) {\n+        readSchema = TypeUtil.selectNot(tableSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(tableSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3"}, "originalPosition": 323}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc5MDY2Nzk3", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-379066797", "createdAt": "2020-03-22T23:27:27Z", "commit": {"oid": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMlQyMzoyNzoyN1rOF5zolg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMlQyMzoyNzoyN1rOF5zolg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTEyNg==", "bodyText": "Is it possible to throw this in getSplits so it isn't an error that may happen 95% of the way through a job?", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396159126", "createdAt": "2020-03-22T23:27:27Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,525 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(conf, task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+      if (hasJoinedPartitionColumns) {\n+        readSchema = TypeUtil.selectNot(tableSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(tableSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      if (inMemoryDataModel == InMemoryDataModel.DEFAULT) {\n+        Expression filter = currentTask.residual();\n+        return applyResidualsOnGenericRecords(iterable, filter).iterator();\n+      } else {\n+        return iterable.iterator();\n+      }\n+    }\n+\n+    private Iterable<T> applyResidualsOnGenericRecords(CloseableIterable<T> iterable, Expression filter) {\n+      if (filter == null || filter.equals(Expressions.alwaysTrue())) {\n+        return iterable;\n+      } else {\n+        throw new UnsupportedOperationException(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3"}, "originalPosition": 353}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc5MDY3MDQ5", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-379067049", "createdAt": "2020-03-22T23:30:17Z", "commit": {"oid": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMlQyMzozMDoxN1rOF5zpgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMlQyMzozMDoxN1rOF5zpgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTM2MQ==", "bodyText": "Minor: could we use > 0 instead of != -1?", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396159361", "createdAt": "2020-03-22T23:30:17Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,525 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3"}, "originalPosition": 208}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc5MDY3NDQ2", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-379067446", "createdAt": "2020-03-22T23:33:52Z", "commit": {"oid": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMlQyMzozMzo1MlrOF5zrDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMlQyMzozMzo1MlrOF5zrDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTc1Nw==", "bodyText": "Are Hive dependencies still needed for testCompile?\nTo test this with HMS, we could make it a test dependency of iceberg-hive and test out the catalog function.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396159757", "createdAt": "2020-03-22T23:33:52Z", "author": {"login": "rdblue"}, "path": "build.gradle", "diffHunk": "@@ -219,6 +219,58 @@ project(':iceberg-hive') {\n   }\n }\n \n+project(':iceberg-mr') {\n+  dependencies {\n+    compile project(':iceberg-api')\n+    compile project(':iceberg-core')\n+    compile project(':iceberg-orc')\n+    compile project(':iceberg-parquet')\n+    compile project(':iceberg-data')\n+\n+    compileOnly(\"org.apache.hadoop:hadoop-client\") {\n+      exclude group: 'org.apache.avro', module: 'avro'\n+    }\n+\n+    testCompile project(':iceberg-hive')\n+    testCompile project(path: ':iceberg-data', configuration: 'testArtifacts')\n+    testCompile project(path: ':iceberg-hive', configuration: 'testArtifacts')\n+    testCompile project(path: ':iceberg-api', configuration: 'testArtifacts')\n+    testCompile project(path: ':iceberg-core', configuration: 'testArtifacts')\n+\n+    // By default, hive-exec is a fat/uber jar and it exports a guava library\n+    // that's really old. We use the core classifier to be able to override our guava\n+    // version. Luckily, hive-exec seems to work okay so far with this version of guava\n+    // See: https://github.com/apache/hive/blob/master/ql/pom.xml#L911 for more context.\n+    testCompile(\"org.apache.hive:hive-exec::core\") {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3"}, "originalPosition": 26}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgwNTQ4Njg1", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-380548685", "createdAt": "2020-03-24T17:39:21Z", "commit": {"oid": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQxNzozOToyMlrOF67zug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQxNzo0MDozM1rOF672pQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzM0MTYyNg==", "bodyText": "Is it OK if this happens? We don't want to rethrow this?", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397341626", "createdAt": "2020-03-24T17:39:22Z", "author": {"login": "massdosage"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/Util.java", "diffHunk": "@@ -36,4 +47,21 @@ public static FileSystem getFs(Path path, Configuration conf) {\n       throw new RuntimeIOException(e, \"Failed to get file system for path: %s\", path);\n     }\n   }\n+\n+  public static String[] blockLocations(CombinedScanTask task, Configuration conf) {\n+    Set<String> locationSets = Sets.newHashSet();\n+    for (FileScanTask f : task.files()) {\n+      Path path = new Path(f.file().path().toString());\n+      try {\n+        FileSystem fs = path.getFileSystem(conf);\n+        for (BlockLocation b : fs.getFileBlockLocations(path, f.start(), f.length())) {\n+          locationSets.addAll(Arrays.asList(b.getHosts()));\n+        }\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Failed to get block locations for path {}\", path, ioe);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzM0MjM3Mw==", "bodyText": "OK, sure.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397342373", "createdAt": "2020-03-24T17:40:33Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc4MTM1Ng=="}, "originalCommit": {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697"}, "originalPosition": 1}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgwODczMDA3", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-380873007", "createdAt": "2020-03-25T05:52:26Z", "commit": {"oid": "046ee4ad19c2fbd42596013d2eba5c8b7c760820"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwNTo1MjoyNlrOF7M85A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwNTo1MjoyNlrOF7M85A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYyMjUwMA==", "bodyText": "Does MR treat \"*\" as ANYWHERE?", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397622500", "createdAt": "2020-03-25T05:52:26Z", "author": {"login": "jerryshao"}, "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,567 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note. This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(\n+              String.format(\n+                  \"Filter expression %s is not completely satisfied . Additional rows \" +\n+                      \"can be returned not satisfied by the filter expression\", residual));\n+        }\n+      });\n+    }\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+      if (hasJoinedPartitionColumns) {\n+        readSchema = TypeUtil.selectNot(tableSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(tableSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      if (inMemoryDataModel == InMemoryDataModel.DEFAULT) {\n+        Expression filter = currentTask.residual();\n+        return applyResidualsOnGenericRecords(iterable, filter).iterator();\n+      } else {\n+        return iterable.iterator();\n+      }\n+    }\n+\n+    private Iterable<T> applyResidualsOnGenericRecords(CloseableIterable<T> iterable, Expression filter) {\n+      if (filter == null || filter.equals(Expressions.alwaysTrue())) {\n+        return iterable;\n+      } else {\n+        throw new UnsupportedOperationException(\n+            String.format(\"Filter expression %s is not completely satisfied. Additional rows can be returned\" +\n+                              \" not satisfied by the filter expression\", filter));\n+      }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          return (T) genericRecordWithPartitionsColumns((Record) row, identityPartitionSchema, spec, partition);\n+      }\n+      return row;\n+    }\n+\n+    private static Record genericRecordWithPartitionsColumns(\n+        Record record, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      List<Types.NestedField> fields = Lists.newArrayList(record.struct().fields());\n+      fields.addAll(identityPartitionSchema.asStruct().fields());\n+      GenericRecord row = GenericRecord.create(Types.StructType.of(fields));\n+      int size = record.struct().fields().size();\n+      for (int i = 0; i < size; i++) {\n+        row.set(i, record.get(i));\n+      }\n+      List<PartitionField> partitionFields = spec.fields();\n+      List<Types.NestedField> identityColumns = identityPartitionSchema.columns();\n+      for (int i = 0; i < identityColumns.size(); i++) {\n+        Types.NestedField identityColumn = identityColumns.get(i);\n+\n+        for (int j = 0; j < partitionFields.size(); j++) {\n+          PartitionField partitionField = partitionFields.get(j);\n+          if (identityColumn.fieldId() == partitionField.sourceId() &&\n+              \"identity\".equals(partitionField.transform().toString())) {\n+            //TODO: identity partitions are being added to the end, this changes\n+            // the position of the column. This seems like a potential bug\n+            row.set(size + i, partition.get(j, spec.javaClasses()[i]));\n+          }\n+        }\n+      }\n+      return row;\n+    }\n+\n+    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n+                                             .project(readSchema)\n+                                             .split(task.start(), task.length());\n+\n+      if (reuseContainers) {\n+        avroReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          avroReadBuilder.createReaderFunc(DataReader::create);\n+      }\n+      return avroReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n+                                                      .project(readSchema)\n+                                                      .filter(task.residual())\n+                                                      .caseSensitive(caseSensitive)\n+                                                      .split(task.start(), task.length());\n+      if (reuseContainers) {\n+        parquetReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          parquetReadBuilder.createReaderFunc(\n+              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n+      }\n+      return parquetReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newOrcIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      ORC.ReadBuilder orcReadBuilder = ORC.read(inputFile)\n+                                          .schema(readSchema)\n+                                          .caseSensitive(caseSensitive)\n+                                          .split(task.start(), task.length());\n+      // ORC does not support reuse containers yet\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          //TODO: We do not have support for Iceberg generics for ORC\n+          throw new UnsupportedOperationException();\n+      }\n+\n+      return orcReadBuilder.build();\n+    }\n+  }\n+\n+  private static Table findTable(Configuration conf) {\n+    String path = conf.get(TABLE_PATH);\n+    String catalogFuncClass = conf.get(CATALOG);\n+    if (catalogFuncClass != null) {\n+      Function<Configuration, Catalog> catalogFunc\n+          = (Function<Configuration, Catalog>)\n+          DynConstructors.builder(Function.class)\n+                         .impl(catalogFuncClass)\n+                         .build()\n+                         .newInstance();\n+      Catalog catalog = catalogFunc.apply(conf);\n+      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n+      return catalog.loadTable(tableIdentifier);\n+    } else if (path.contains(\"/\")) {\n+      HadoopTables tables = new HadoopTables(conf);\n+      return tables.load(path);\n+    } else {\n+      throw new IllegalArgumentException(\"No custom catalog specified to load table \" + path);\n+    }\n+  }\n+\n+  private static class IcebergSplit extends InputSplit implements Writable {\n+    private static final String[] ANYWHERE = new String[]{\"*\"};\n+    private CombinedScanTask task;\n+    private transient String[] locations;\n+    private transient Configuration conf;\n+\n+    IcebergSplit(Configuration conf, CombinedScanTask task) {\n+      this.task = task;\n+      this.conf = conf;\n+    }\n+\n+    @Override\n+    public long getLength() {\n+      return task.files().stream().mapToLong(FileScanTask::length).sum();\n+    }\n+\n+    @Override\n+    public String[] getLocations() {\n+      boolean localityPreferred = conf.getBoolean(LOCALITY, false);\n+      if (!localityPreferred) {\n+        return ANYWHERE;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "046ee4ad19c2fbd42596013d2eba5c8b7c760820"}, "originalPosition": 544}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgwODc3MjE0", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-380877214", "createdAt": "2020-03-25T06:06:57Z", "commit": {"oid": "046ee4ad19c2fbd42596013d2eba5c8b7c760820"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwNjowNjo1N1rOF7NLhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwNjowNjo1N1rOF7NLhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYyNjI0Nw==", "bodyText": "Maybe we should add more tests to cover some of the configurations defined above.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397626247", "createdAt": "2020-03-25T06:06:57Z", "author": {"login": "jerryshao"}, "path": "mr/src/test/java/org/apache/iceberg/mr/TestIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.FluentIterable;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TestHelpers.Row;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataWriter;\n+import org.apache.iceberg.data.parquet.GenericParquetWriter;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+\n+@RunWith(Parameterized.class)\n+public class TestIcebergInputFormat {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "046ee4ad19c2fbd42596013d2eba5c8b7c760820"}, "originalPosition": 71}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgwODc4MjU4", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-380878258", "createdAt": "2020-03-25T06:10:22Z", "commit": {"oid": "046ee4ad19c2fbd42596013d2eba5c8b7c760820"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwNjoxMDoyM1rOF7NPXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwNjoxMDoyM1rOF7NPXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYyNzIyOQ==", "bodyText": "one more empty line.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397627229", "createdAt": "2020-03-25T06:10:23Z", "author": {"login": "jerryshao"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/Util.java", "diffHunk": "@@ -19,13 +19,24 @@\n \n package org.apache.iceberg.hadoop;\n \n+import com.google.common.collect.Sets;\n import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Set;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "046ee4ad19c2fbd42596013d2eba5c8b7c760820"}, "originalPosition": 18}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzgxMzkzMzk3", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-381393397", "createdAt": "2020-03-25T17:49:40Z", "commit": {"oid": "046ee4ad19c2fbd42596013d2eba5c8b7c760820"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxNzo0OTo0MFrOF7nM0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxNzo0OTo0MFrOF7nM0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA1MjU2MA==", "bodyText": "Nit: extra newline.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r398052560", "createdAt": "2020-03-25T17:49:40Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/TestIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.FluentIterable;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TestHelpers.Row;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataWriter;\n+import org.apache.iceberg.data.parquet.GenericParquetWriter;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "046ee4ad19c2fbd42596013d2eba5c8b7c760820"}, "originalPosition": 68}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7f3394bcdde5b1dcac3a37ed9a4625aa24c942f8", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/7f3394bcdde5b1dcac3a37ed9a4625aa24c942f8", "committedDate": "2020-04-02T06:08:58Z", "message": "Added more test cases"}, "afterCommit": {"oid": "637ed5ce6958b884e3437ec1a1d2bc394632e820", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/637ed5ce6958b884e3437ec1a1d2bc394632e820", "committedDate": "2020-04-02T06:12:47Z", "message": "Added more test cases"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "637ed5ce6958b884e3437ec1a1d2bc394632e820", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/637ed5ce6958b884e3437ec1a1d2bc394632e820", "committedDate": "2020-04-02T06:12:47Z", "message": "Added more test cases"}, "afterCommit": {"oid": "da8f03e0d78d2057e21bc0daa84c1baac9ba2d68", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/da8f03e0d78d2057e21bc0daa84c1baac9ba2d68", "committedDate": "2020-04-02T06:16:53Z", "message": "Added more test cases"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "da8f03e0d78d2057e21bc0daa84c1baac9ba2d68", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/da8f03e0d78d2057e21bc0daa84c1baac9ba2d68", "committedDate": "2020-04-02T06:16:53Z", "message": "Added more test cases"}, "afterCommit": {"oid": "0e250f954dc6388aea3bf3bce695ba9b593b5463", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/0e250f954dc6388aea3bf3bce695ba9b593b5463", "committedDate": "2020-04-02T07:09:02Z", "message": "Added more test cases"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0e250f954dc6388aea3bf3bce695ba9b593b5463", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/0e250f954dc6388aea3bf3bce695ba9b593b5463", "committedDate": "2020-04-02T07:09:02Z", "message": "Added more test cases"}, "afterCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/306f3f845fcb896b753364a24a352aeddc618483", "committedDate": "2020-04-02T07:14:42Z", "message": "Added more test cases"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg2NTU5NjAz", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-386559603", "createdAt": "2020-04-02T15:48:12Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQxNTo0ODoxM1rOF_xsQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wMlQxNTo0ODoxM1rOF_xsQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjQxODc1NQ==", "bodyText": "Had to change this logic slightly so that whatever schema the user projected, that is what is returned [we drop identity partition columns not projected by the user]", "url": "https://github.com/apache/iceberg/pull/843#discussion_r402418755", "createdAt": "2020-04-02T15:48:13Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -346,14 +348,15 @@ public void close() throws IOException {\n       DataFile file = currentTask.file();\n       // schema of rows returned by readers\n       PartitionSpec spec = currentTask.spec();\n-      Set<Integer> idColumns = spec.identitySourceIds();\n       Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(readSchema));\n       boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+\n       if (hasJoinedPartitionColumns) {\n-        readSchema = TypeUtil.selectNot(tableSchema, idColumns);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg2OTQ5OTMy", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-386949932", "createdAt": "2020-04-03T05:44:18Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwNTo0NDoxOFrOGAFzmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwNTo0NDoxOFrOGAFzmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc0ODMxNA==", "bodyText": "additional line.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r402748314", "createdAt": "2020-04-03T05:44:18Z", "author": {"login": "jerryshao"}, "path": "mr/src/main/java/org/apache/iceberg/mr/SerializationUtil.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Base64;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 31}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg2OTUxODc0", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-386951874", "createdAt": "2020-04-03T05:50:39Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwNTo1MDozOVrOGAF6Wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QwNTo1MDozOVrOGAF6Wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc1MDA0Mg==", "bodyText": "Also here.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r402750042", "createdAt": "2020-04-03T05:50:39Z", "author": {"login": "jerryshao"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 77}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NDE0NzI3", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-387414727", "createdAt": "2020-04-03T16:40:13Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo0MDoxM1rOGAdbzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo0MDoxM1rOGAdbzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzNTQzOA==", "bodyText": "Why is this necessary? Can't the work be done in either method when there is enough information to load the table?", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403135438", "createdAt": "2020-04-03T16:40:13Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 179}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NDE5MTI0", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-387419124", "createdAt": "2020-04-03T16:46:29Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo0NjoyOVrOGAdprw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo0NjoyOVrOGAdprw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzODk5MQ==", "bodyText": "I think that if the job configured plaformAppliesFilter then it is okay to return records even if the model is generics.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403138991", "createdAt": "2020-04-03T16:46:29Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 263}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NDE5ODYx", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-387419861", "createdAt": "2020-04-03T16:47:32Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo0NzozMlrOGAdr2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo0NzozMlrOGAdr2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzOTU0NQ==", "bodyText": "I think this should be GENERIC instead of DEFAULT. Defaults depend on context, but this enum doesn't need to encode what some other part of the code will do by default. It just needs to encode the data model.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403139545", "createdAt": "2020-04-03T16:47:32Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 104}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NDIzMTMz", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-387423133", "createdAt": "2020-04-03T16:52:23Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo1MjoyNFrOGAd1-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo1MjoyNFrOGAd1-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0MjEzOQ==", "bodyText": "I find this a little hard to reason about and I think a different name could make it more straight-forward. Rather than configuring \"someone will do task\" it is easier for the caller and this code to phrase it as \"do task\" or \"skip task\". That also avoids the need to refer to the \"platform\" because users won't know what that is and we have to explain it.\nWhat about changing this to skipResidualFiltering()? That is clear that the InputFormat is not responsible for filtering, but doesn't define who or what is responsible. It is also clearly dangerous, even if you don't know what a residual is.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403142139", "createdAt": "2020-04-03T16:52:24Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 205}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NDIzNDEw", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-387423410", "createdAt": "2020-04-03T16:52:47Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo1Mjo0OFrOGAd29w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo1Mjo0OFrOGAd29w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0MjM5MQ==", "bodyText": "Nit: the line wrapping here is strange.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403142391", "createdAt": "2020-04-03T16:52:48Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 203}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NDIzOTE2", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-387423916", "createdAt": "2020-04-03T16:53:32Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo1MzozMlrOGAd4hw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo1MzozMlrOGAd4hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0Mjc5MQ==", "bodyText": "Minor: this calls getBoolean and getEnum for every split. It would be nice to get those outside the loop and pass them in.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403142791", "createdAt": "2020-04-03T16:53:32Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 259}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NDI0Mzcw", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-387424370", "createdAt": "2020-04-03T16:54:09Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo1NDoxMFrOGAd6OA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo1NDoxMFrOGAd6OA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0MzIyNA==", "bodyText": "How about UnsupportedOperationException since this will eventually be implemented?", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403143224", "createdAt": "2020-04-03T16:54:10Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 267}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NDI1NDI1", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-387425425", "createdAt": "2020-04-03T16:55:40Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo1NTo0MFrOGAd9zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo1NTo0MFrOGAd9zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0NDE0Mg==", "bodyText": "Nit: I prefer separating state that won't change (caseSensitive, reuseContainers) from state that does change (currentRow, currentCloseable), but those are mixed together here.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403144142", "createdAt": "2020-04-03T16:55:40Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(\n+              String.format(\n+                  \"Filter expression %s is not completely satisfied. Additional rows \" +\n+                      \"can be returned not satisfied by the filter expression\", residual));\n+        }\n+      });\n+    }\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 291}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NDI2MDY0", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-387426064", "createdAt": "2020-04-03T16:56:35Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo1NjozNlrOGAeA8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNjo1NjozNlrOGAeA8g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0NDk0Ng==", "bodyText": "Maybe leave a TODO item to improve this.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403144946", "createdAt": "2020-04-03T16:56:36Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(\n+              String.format(\n+                  \"Filter expression %s is not completely satisfied. Additional rows \" +\n+                      \"can be returned not satisfied by the filter expression\", residual));\n+        }\n+      });\n+    }\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 339}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NDI4OTcz", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-387428973", "createdAt": "2020-04-03T17:00:38Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNzowMDozOVrOGAePUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNzowMDozOVrOGAePUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0ODYyNQ==", "bodyText": "In the future, we should update HadoopFileIO to use a transient Configuration and to keep a set of properties that it extracts from its original configuration. Then we can serialize HadoopFileIO more easily, and rebuild a working IO config on tasks.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403148625", "createdAt": "2020-04-03T17:00:39Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(\n+              String.format(\n+                  \"Filter expression %s is not completely satisfied. Additional rows \" +\n+                      \"can be returned not satisfied by the filter expression\", residual));\n+        }\n+      });\n+    }\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(readSchema));\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+\n+      if (hasJoinedPartitionColumns) {\n+        Schema readDataSchema = TypeUtil.selectNot(readSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(readSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readDataSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 369}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NDM1NjY1", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-387435665", "createdAt": "2020-04-03T17:10:44Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNzoxMDo0NFrOGAewtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNzoxMDo0NFrOGAewtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE1NzE3Mg==", "bodyText": "Nit: = should be on the previous line.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403157172", "createdAt": "2020-04-03T17:10:44Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(\n+              String.format(\n+                  \"Filter expression %s is not completely satisfied. Additional rows \" +\n+                      \"can be returned not satisfied by the filter expression\", residual));\n+        }\n+      });\n+    }\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(readSchema));\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+\n+      if (hasJoinedPartitionColumns) {\n+        Schema readDataSchema = TypeUtil.selectNot(readSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(readSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readDataSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      if (inMemoryDataModel == InMemoryDataModel.DEFAULT) {\n+        Expression filter = currentTask.residual();\n+        return applyResidualsOnGenericRecords(iterable, filter).iterator();\n+      } else {\n+        return iterable.iterator();\n+      }\n+    }\n+\n+    private Iterable<T> applyResidualsOnGenericRecords(CloseableIterable<T> iterable, Expression filter) {\n+      if (filter == null || filter.equals(Expressions.alwaysTrue())) {\n+        return iterable;\n+      } else {\n+        throw new UnsupportedOperationException(\n+            String.format(\"Filter expression %s is not completely satisfied. Additional rows can be returned\" +\n+                              \" not satisfied by the filter expression\", filter));\n+      }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          return (T) genericRecordWithPartitionsColumns((Record) row, identityPartitionSchema, spec, partition);\n+      }\n+      return row;\n+    }\n+\n+    private static Record genericRecordWithPartitionsColumns(\n+        Record record, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      List<Types.NestedField> fields = Lists.newArrayList(record.struct().fields());\n+      fields.addAll(identityPartitionSchema.asStruct().fields());\n+      GenericRecord row = GenericRecord.create(Types.StructType.of(fields));\n+      int size = record.struct().fields().size();\n+      for (int i = 0; i < size; i++) {\n+        row.set(i, record.get(i));\n+      }\n+      List<PartitionField> partitionFields = spec.fields();\n+      List<Types.NestedField> identityColumns = identityPartitionSchema.columns();\n+      for (int i = 0; i < identityColumns.size(); i++) {\n+        Types.NestedField identityColumn = identityColumns.get(i);\n+\n+        for (int j = 0; j < partitionFields.size(); j++) {\n+          PartitionField partitionField = partitionFields.get(j);\n+          if (identityColumn.fieldId() == partitionField.sourceId() &&\n+              \"identity\".equals(partitionField.transform().toString())) {\n+            //TODO: identity partitions are being added to the end, this changes\n+            // the position of the column. This seems like a potential bug\n+            row.set(size + i, partition.get(j, spec.javaClasses()[i]));\n+          }\n+        }\n+      }\n+      return row;\n+    }\n+\n+    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n+                                             .project(readSchema)\n+                                             .split(task.start(), task.length());\n+\n+      if (reuseContainers) {\n+        avroReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          avroReadBuilder.createReaderFunc(DataReader::create);\n+      }\n+      return avroReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n+                                                      .project(readSchema)\n+                                                      .filter(task.residual())\n+                                                      .caseSensitive(caseSensitive)\n+                                                      .split(task.start(), task.length());\n+      if (reuseContainers) {\n+        parquetReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          parquetReadBuilder.createReaderFunc(\n+              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n+      }\n+      return parquetReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newOrcIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      ORC.ReadBuilder orcReadBuilder = ORC.read(inputFile)\n+                                          .project(readSchema)\n+                                          .caseSensitive(caseSensitive)\n+                                          .split(task.start(), task.length());\n+      // ORC does not support reuse containers yet\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO: implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException(\"In memory representation not yet supported for Pig and Hive\");\n+        case DEFAULT:\n+          //TODO: We do not have support for Iceberg generics for ORC\n+          throw new UnsupportedOperationException();\n+      }\n+\n+      return orcReadBuilder.build();\n+    }\n+  }\n+\n+  private static Table findTable(Configuration conf) {\n+    String path = conf.get(TABLE_PATH);\n+    Preconditions.checkArgument(path != null, \"Table path should not be null\");\n+    String catalogFuncClass = conf.get(CATALOG);\n+    if (catalogFuncClass != null) {\n+      Function<Configuration, Catalog> catalogFunc\n+          = (Function<Configuration, Catalog>)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 512}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NDM2MTg0", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-387436184", "createdAt": "2020-04-03T17:11:35Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNzoxMTozNVrOGAezXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNzoxMTozNVrOGAezXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE1Nzg1Mw==", "bodyText": "I think this should be the first branch of the if because it is the easiest and is less likely to fail.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403157853", "createdAt": "2020-04-03T17:11:35Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(\n+              String.format(\n+                  \"Filter expression %s is not completely satisfied. Additional rows \" +\n+                      \"can be returned not satisfied by the filter expression\", residual));\n+        }\n+      });\n+    }\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(readSchema));\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+\n+      if (hasJoinedPartitionColumns) {\n+        Schema readDataSchema = TypeUtil.selectNot(readSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(readSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readDataSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      if (inMemoryDataModel == InMemoryDataModel.DEFAULT) {\n+        Expression filter = currentTask.residual();\n+        return applyResidualsOnGenericRecords(iterable, filter).iterator();\n+      } else {\n+        return iterable.iterator();\n+      }\n+    }\n+\n+    private Iterable<T> applyResidualsOnGenericRecords(CloseableIterable<T> iterable, Expression filter) {\n+      if (filter == null || filter.equals(Expressions.alwaysTrue())) {\n+        return iterable;\n+      } else {\n+        throw new UnsupportedOperationException(\n+            String.format(\"Filter expression %s is not completely satisfied. Additional rows can be returned\" +\n+                              \" not satisfied by the filter expression\", filter));\n+      }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          return (T) genericRecordWithPartitionsColumns((Record) row, identityPartitionSchema, spec, partition);\n+      }\n+      return row;\n+    }\n+\n+    private static Record genericRecordWithPartitionsColumns(\n+        Record record, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      List<Types.NestedField> fields = Lists.newArrayList(record.struct().fields());\n+      fields.addAll(identityPartitionSchema.asStruct().fields());\n+      GenericRecord row = GenericRecord.create(Types.StructType.of(fields));\n+      int size = record.struct().fields().size();\n+      for (int i = 0; i < size; i++) {\n+        row.set(i, record.get(i));\n+      }\n+      List<PartitionField> partitionFields = spec.fields();\n+      List<Types.NestedField> identityColumns = identityPartitionSchema.columns();\n+      for (int i = 0; i < identityColumns.size(); i++) {\n+        Types.NestedField identityColumn = identityColumns.get(i);\n+\n+        for (int j = 0; j < partitionFields.size(); j++) {\n+          PartitionField partitionField = partitionFields.get(j);\n+          if (identityColumn.fieldId() == partitionField.sourceId() &&\n+              \"identity\".equals(partitionField.transform().toString())) {\n+            //TODO: identity partitions are being added to the end, this changes\n+            // the position of the column. This seems like a potential bug\n+            row.set(size + i, partition.get(j, spec.javaClasses()[i]));\n+          }\n+        }\n+      }\n+      return row;\n+    }\n+\n+    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n+                                             .project(readSchema)\n+                                             .split(task.start(), task.length());\n+\n+      if (reuseContainers) {\n+        avroReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          avroReadBuilder.createReaderFunc(DataReader::create);\n+      }\n+      return avroReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n+                                                      .project(readSchema)\n+                                                      .filter(task.residual())\n+                                                      .caseSensitive(caseSensitive)\n+                                                      .split(task.start(), task.length());\n+      if (reuseContainers) {\n+        parquetReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          parquetReadBuilder.createReaderFunc(\n+              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n+      }\n+      return parquetReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newOrcIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      ORC.ReadBuilder orcReadBuilder = ORC.read(inputFile)\n+                                          .project(readSchema)\n+                                          .caseSensitive(caseSensitive)\n+                                          .split(task.start(), task.length());\n+      // ORC does not support reuse containers yet\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO: implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException(\"In memory representation not yet supported for Pig and Hive\");\n+        case DEFAULT:\n+          //TODO: We do not have support for Iceberg generics for ORC\n+          throw new UnsupportedOperationException();\n+      }\n+\n+      return orcReadBuilder.build();\n+    }\n+  }\n+\n+  private static Table findTable(Configuration conf) {\n+    String path = conf.get(TABLE_PATH);\n+    Preconditions.checkArgument(path != null, \"Table path should not be null\");\n+    String catalogFuncClass = conf.get(CATALOG);\n+    if (catalogFuncClass != null) {\n+      Function<Configuration, Catalog> catalogFunc\n+          = (Function<Configuration, Catalog>)\n+          DynConstructors.builder(Function.class)\n+                         .impl(catalogFuncClass)\n+                         .build()\n+                         .newInstance();\n+      Catalog catalog = catalogFunc.apply(conf);\n+      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n+      return catalog.loadTable(tableIdentifier);\n+    } else if (path.contains(\"/\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 520}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NDM2ODYx", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-387436861", "createdAt": "2020-04-03T17:12:39Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNzoxMjozOVrOGAe2_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNzoxMjozOVrOGAe2_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE1ODc4MQ==", "bodyText": "Nit: typical style is to use 2 indents / 4 spaces from the next line, not to indent to the same level as the last function call.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403158781", "createdAt": "2020-04-03T17:12:39Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(\n+              String.format(\n+                  \"Filter expression %s is not completely satisfied. Additional rows \" +\n+                      \"can be returned not satisfied by the filter expression\", residual));\n+        }\n+      });\n+    }\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(readSchema));\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+\n+      if (hasJoinedPartitionColumns) {\n+        Schema readDataSchema = TypeUtil.selectNot(readSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(readSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readDataSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      if (inMemoryDataModel == InMemoryDataModel.DEFAULT) {\n+        Expression filter = currentTask.residual();\n+        return applyResidualsOnGenericRecords(iterable, filter).iterator();\n+      } else {\n+        return iterable.iterator();\n+      }\n+    }\n+\n+    private Iterable<T> applyResidualsOnGenericRecords(CloseableIterable<T> iterable, Expression filter) {\n+      if (filter == null || filter.equals(Expressions.alwaysTrue())) {\n+        return iterable;\n+      } else {\n+        throw new UnsupportedOperationException(\n+            String.format(\"Filter expression %s is not completely satisfied. Additional rows can be returned\" +\n+                              \" not satisfied by the filter expression\", filter));\n+      }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          return (T) genericRecordWithPartitionsColumns((Record) row, identityPartitionSchema, spec, partition);\n+      }\n+      return row;\n+    }\n+\n+    private static Record genericRecordWithPartitionsColumns(\n+        Record record, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      List<Types.NestedField> fields = Lists.newArrayList(record.struct().fields());\n+      fields.addAll(identityPartitionSchema.asStruct().fields());\n+      GenericRecord row = GenericRecord.create(Types.StructType.of(fields));\n+      int size = record.struct().fields().size();\n+      for (int i = 0; i < size; i++) {\n+        row.set(i, record.get(i));\n+      }\n+      List<PartitionField> partitionFields = spec.fields();\n+      List<Types.NestedField> identityColumns = identityPartitionSchema.columns();\n+      for (int i = 0; i < identityColumns.size(); i++) {\n+        Types.NestedField identityColumn = identityColumns.get(i);\n+\n+        for (int j = 0; j < partitionFields.size(); j++) {\n+          PartitionField partitionField = partitionFields.get(j);\n+          if (identityColumn.fieldId() == partitionField.sourceId() &&\n+              \"identity\".equals(partitionField.transform().toString())) {\n+            //TODO: identity partitions are being added to the end, this changes\n+            // the position of the column. This seems like a potential bug\n+            row.set(size + i, partition.get(j, spec.javaClasses()[i]));\n+          }\n+        }\n+      }\n+      return row;\n+    }\n+\n+    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n+                                             .project(readSchema)\n+                                             .split(task.start(), task.length());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 447}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NDM3NjAz", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-387437603", "createdAt": "2020-04-03T17:13:48Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNzoxMzo0OVrOGAe7AQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNzoxMzo0OVrOGAe7AQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE1OTgwOQ==", "bodyText": "Why not ORC?", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403159809", "createdAt": "2020-04-03T17:13:49Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/mapreduce/TestIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.collect.FluentIterable;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TestHelpers.Row;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataWriter;\n+import org.apache.iceberg.data.parquet.GenericParquetWriter;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+\n+@RunWith(Parameterized.class)\n+public class TestIcebergInputFormat {\n+  static final Schema SCHEMA = new Schema(\n+      required(1, \"data\", Types.StringType.get()),\n+      required(2, \"id\", Types.LongType.get()),\n+      required(3, \"date\", Types.StringType.get()));\n+\n+  static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+                                                 .identity(\"date\")\n+                                                 .bucket(\"id\", 1)\n+                                                 .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+  private HadoopTables tables;\n+  private Configuration conf;\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][]{\n+        new Object[]{\"parquet\"},\n+        new Object[]{\"avro\"}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 95}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg3NDM5MjY1", "url": "https://github.com/apache/iceberg/pull/843#pullrequestreview-387439265", "createdAt": "2020-04-03T17:16:16Z", "commit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNzoxNjoxNlrOGAfDjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wM1QxNzoxNjoxNlrOGAfDjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE2MTk5OA==", "bodyText": "There are tests for this in Spark here: 6cafdab\nYou might consider adding some similar tests.", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403161998", "createdAt": "2020-04-03T17:16:16Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(\n+              String.format(\n+                  \"Filter expression %s is not completely satisfied. Additional rows \" +\n+                      \"can be returned not satisfied by the filter expression\", residual));\n+        }\n+      });\n+    }\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(readSchema));\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+\n+      if (hasJoinedPartitionColumns) {\n+        Schema readDataSchema = TypeUtil.selectNot(readSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(readSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readDataSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      if (inMemoryDataModel == InMemoryDataModel.DEFAULT) {\n+        Expression filter = currentTask.residual();\n+        return applyResidualsOnGenericRecords(iterable, filter).iterator();\n+      } else {\n+        return iterable.iterator();\n+      }\n+    }\n+\n+    private Iterable<T> applyResidualsOnGenericRecords(CloseableIterable<T> iterable, Expression filter) {\n+      if (filter == null || filter.equals(Expressions.alwaysTrue())) {\n+        return iterable;\n+      } else {\n+        throw new UnsupportedOperationException(\n+            String.format(\"Filter expression %s is not completely satisfied. Additional rows can be returned\" +\n+                              \" not satisfied by the filter expression\", filter));\n+      }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          return (T) genericRecordWithPartitionsColumns((Record) row, identityPartitionSchema, spec, partition);\n+      }\n+      return row;\n+    }\n+\n+    private static Record genericRecordWithPartitionsColumns(\n+        Record record, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "306f3f845fcb896b753364a24a352aeddc618483"}, "originalPosition": 418}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "15de263bee96fd49c4390f7bdec5f450d052a6dd", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/15de263bee96fd49c4390f7bdec5f450d052a6dd", "committedDate": "2020-04-05T20:55:22Z", "message": "InputFormat support for Iceberg"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a287b1e1c629d3d851372d69aebc3bcade3c546", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/7a287b1e1c629d3d851372d69aebc3bcade3c546", "committedDate": "2020-04-05T20:55:22Z", "message": "Address review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a38b8e326fab5946b0039013d7bd2ed4a3cc8cd4", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/a38b8e326fab5946b0039013d7bd2ed4a3cc8cd4", "committedDate": "2020-04-05T20:55:22Z", "message": "Address review comments [Take 2]"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3567b02d47ccf9b102b78c273fd4ceac8fce469f", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/3567b02d47ccf9b102b78c273fd4ceac8fce469f", "committedDate": "2020-04-05T20:55:22Z", "message": "c"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "25bb2cd77283fedec001a2fcfd32212c3f2e7083", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/25bb2cd77283fedec001a2fcfd32212c3f2e7083", "committedDate": "2020-04-05T20:55:22Z", "message": "[Address comments - take 3] Using Hadoop catalog to test custom catalog"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "406802109c232d2aeb5912f1e25ef6141d26390c", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/406802109c232d2aeb5912f1e25ef6141d26390c", "committedDate": "2020-04-05T20:55:22Z", "message": "Better name for filter residuals option"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a1c8e6eff60e2bc00c2211b6eb98fecdd8ef021c", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/a1c8e6eff60e2bc00c2211b6eb98fecdd8ef021c", "committedDate": "2020-04-05T20:55:22Z", "message": "Added more test cases"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c3eac6c103b91da530c090dc6600e68d232415d4", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/c3eac6c103b91da530c090dc6600e68d232415d4", "committedDate": "2020-04-05T20:38:25Z", "message": "Address review comments. Added more test cases"}, "afterCommit": {"oid": "54b6bc56606c41b477b298eb1a065025d7a25e68", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/54b6bc56606c41b477b298eb1a065025d7a25e68", "committedDate": "2020-04-05T20:55:22Z", "message": "Address review comments. Added more test cases"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "54b6bc56606c41b477b298eb1a065025d7a25e68", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/54b6bc56606c41b477b298eb1a065025d7a25e68", "committedDate": "2020-04-05T20:55:22Z", "message": "Address review comments. Added more test cases"}, "afterCommit": {"oid": "d08465340556d3bc0d3abfba08517b9e28ebdaa0", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/d08465340556d3bc0d3abfba08517b9e28ebdaa0", "committedDate": "2020-04-05T23:41:39Z", "message": "Address review comments. Added more test cases"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d08465340556d3bc0d3abfba08517b9e28ebdaa0", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/d08465340556d3bc0d3abfba08517b9e28ebdaa0", "committedDate": "2020-04-05T23:41:39Z", "message": "Address review comments. Added more test cases"}, "afterCommit": {"oid": "1aef1bd601d76742b6c60b4b075943466b57bf86", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/1aef1bd601d76742b6c60b4b075943466b57bf86", "committedDate": "2020-04-05T23:55:56Z", "message": "Address review comments. Added more test cases"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1aef1bd601d76742b6c60b4b075943466b57bf86", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/1aef1bd601d76742b6c60b4b075943466b57bf86", "committedDate": "2020-04-05T23:55:56Z", "message": "Address review comments. Added more test cases"}, "afterCommit": {"oid": "7cb3dd0f9ef257f9928a283393fae0c733817411", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/7cb3dd0f9ef257f9928a283393fae0c733817411", "committedDate": "2020-04-06T00:14:49Z", "message": "Address review comments. Added more test cases"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e327bd7e6798a00b4860659708da031f60c96058", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/e327bd7e6798a00b4860659708da031f60c96058", "committedDate": "2020-04-06T01:12:01Z", "message": "Address review comments. Added more test cases"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7cb3dd0f9ef257f9928a283393fae0c733817411", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/7cb3dd0f9ef257f9928a283393fae0c733817411", "committedDate": "2020-04-06T00:14:49Z", "message": "Address review comments. Added more test cases"}, "afterCommit": {"oid": "e327bd7e6798a00b4860659708da031f60c96058", "author": {"user": {"login": "rdsr", "name": "Ratandeep Ratti"}}, "url": "https://github.com/apache/iceberg/commit/e327bd7e6798a00b4860659708da031f60c96058", "committedDate": "2020-04-06T01:12:01Z", "message": "Address review comments. Added more test cases"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4723, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}