{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDUzODE1MjYy", "number": 1221, "reviewThreads": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMTo0OTo1MlrOEQYjNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMToyMDozNFrOERReiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1NjE0OTAxOnYy", "diffSide": "RIGHT", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMTo0OTo1MlrOG0gfsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMzo1NTo0MlrOG0jN3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcxMTUzOQ==", "bodyText": "I just had an idea for an alternative solution to this. What about detecting that there are no filters and instead returning a value based on the total-records value in snapshot metadata?\nUsually, estimating stats based on the number of rows and a guess for the size of a row is much better than using the actual size anyway. So if you can get the number of rows and come up with an estimate for the size of each row based on the table schema, then you wouldn't need to disable stats at all.", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r457711539", "createdAt": "2020-07-20T21:49:52Z", "author": {"login": "rdblue"}, "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -276,6 +280,9 @@ public void pruneColumns(StructType newRequestedSchema) {\n \n   @Override\n   public Statistics estimateStatistics() {\n+    if(disableEstimateStatistics) {\n+      return new Stats(Long.MAX_VALUE, Long.MAX_VALUE);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac5fc0fb393e68ab31cfd8d48d0bc3688e7ba0c8"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzc1NjEyNw==", "bodyText": "thats really good idea, I thought about this too but I was under impression that we had to look data file metadata to get this information.\nI try to update PR using  total-records", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r457756127", "createdAt": "2020-07-20T23:55:42Z", "author": {"login": "sudssf"}, "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -276,6 +280,9 @@ public void pruneColumns(StructType newRequestedSchema) {\n \n   @Override\n   public Statistics estimateStatistics() {\n+    if(disableEstimateStatistics) {\n+      return new Stats(Long.MAX_VALUE, Long.MAX_VALUE);\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcxMTUzOQ=="}, "originalCommit": {"oid": "ac5fc0fb393e68ab31cfd8d48d0bc3688e7ba0c8"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTU0NDE5OnYy", "diffSide": "RIGHT", "path": "site/docs/configuration.md", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNjo0MTozMFrOG1ApdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMzo1ODozMFrOG1N63Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzODMyNA==", "bodyText": "I don't think we need a table option for this. If we were going to return incorrect stats, then I would want a flag to enable or disable it. But because we are going to use table-level stats, we can detect when to do it based on whether or not there are filters. No filter, then use table level stats.", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458238324", "createdAt": "2020-07-21T16:41:30Z", "author": {"login": "rdblue"}, "path": "site/docs/configuration.md", "diffHunk": "@@ -109,14 +110,14 @@ spark.read\n     .table(\"catalog.db.table\")\n ```\n \n-| Spark option    | Default               | Description                                                                               |\n-| --------------- | --------------------- | ----------------------------------------------------------------------------------------- |\n-| snapshot-id     | (latest)              | Snapshot ID of the table snapshot to read                                                 |\n-| as-of-timestamp | (latest)              | A timestamp in milliseconds; the snapshot used will be the snapshot current at this time. |\n-| split-size      | As per table property | Overrides this table's read.split.target-size and read.split.metadata-target-size         |\n-| lookback        | As per table property | Overrides this table's read.split.planning-lookback                                       |\n-| file-open-cost  | As per table property | Overrides this table's read.split.open-file-cost                                          |\n-\n+| Spark option               | Default               | Description                                                                               |\n+| -------------------------- | --------------------- | ----------------------------------------------------------------------------------------- |\n+| snapshot-id                | (latest)              | Snapshot ID of the table snapshot to read                                                 |\n+| as-of-timestamp            | (latest)              | A timestamp in milliseconds; the snapshot used will be the snapshot current at this time. |\n+| split-size                 | As per table property | Overrides this table's read.split.target-size and read.split.metadata-target-size         |\n+| lookback                   | As per table property | Overrides this table's read.split.planning-lookback                                       |\n+| file-open-cost             | As per table property | Overrides this table's read.split.open-file-cost                                          |\n+| use-approximate-statistics | As per table property | Overrides this table's read.spark.read.spark.use-approximate-statistics                   |", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b73d189b2192cf2a47051af6550fc98d6a32c629"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI2NjQ2OQ==", "bodyText": "what if manifest size is still large after filters are applied?  user may want to disable scanning entire manifest if not trying to broadcast table", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458266469", "createdAt": "2020-07-21T17:26:38Z", "author": {"login": "sudssf"}, "path": "site/docs/configuration.md", "diffHunk": "@@ -109,14 +110,14 @@ spark.read\n     .table(\"catalog.db.table\")\n ```\n \n-| Spark option    | Default               | Description                                                                               |\n-| --------------- | --------------------- | ----------------------------------------------------------------------------------------- |\n-| snapshot-id     | (latest)              | Snapshot ID of the table snapshot to read                                                 |\n-| as-of-timestamp | (latest)              | A timestamp in milliseconds; the snapshot used will be the snapshot current at this time. |\n-| split-size      | As per table property | Overrides this table's read.split.target-size and read.split.metadata-target-size         |\n-| lookback        | As per table property | Overrides this table's read.split.planning-lookback                                       |\n-| file-open-cost  | As per table property | Overrides this table's read.split.open-file-cost                                          |\n-\n+| Spark option               | Default               | Description                                                                               |\n+| -------------------------- | --------------------- | ----------------------------------------------------------------------------------------- |\n+| snapshot-id                | (latest)              | Snapshot ID of the table snapshot to read                                                 |\n+| as-of-timestamp            | (latest)              | A timestamp in milliseconds; the snapshot used will be the snapshot current at this time. |\n+| split-size                 | As per table property | Overrides this table's read.split.target-size and read.split.metadata-target-size         |\n+| lookback                   | As per table property | Overrides this table's read.split.planning-lookback                                       |\n+| file-open-cost             | As per table property | Overrides this table's read.split.open-file-cost                                          |\n+| use-approximate-statistics | As per table property | Overrides this table's read.spark.read.spark.use-approximate-statistics                   |", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzODMyNA=="}, "originalCommit": {"oid": "b73d189b2192cf2a47051af6550fc98d6a32c629"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI3NjgyNA==", "bodyText": "I'm not sure I understand the case you're talking about. What I'm saying is that because we are using reliable stats that are maintained in the table metadata, we can always use them if there are no filters. That takes care of the bad case in Spark 2.4. Since Spark 2.4 doesn't ever push filters before calling estimateStatistics, it will always use metadata stats and will avoid the issue entirely.", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458276824", "createdAt": "2020-07-21T17:43:33Z", "author": {"login": "rdblue"}, "path": "site/docs/configuration.md", "diffHunk": "@@ -109,14 +110,14 @@ spark.read\n     .table(\"catalog.db.table\")\n ```\n \n-| Spark option    | Default               | Description                                                                               |\n-| --------------- | --------------------- | ----------------------------------------------------------------------------------------- |\n-| snapshot-id     | (latest)              | Snapshot ID of the table snapshot to read                                                 |\n-| as-of-timestamp | (latest)              | A timestamp in milliseconds; the snapshot used will be the snapshot current at this time. |\n-| split-size      | As per table property | Overrides this table's read.split.target-size and read.split.metadata-target-size         |\n-| lookback        | As per table property | Overrides this table's read.split.planning-lookback                                       |\n-| file-open-cost  | As per table property | Overrides this table's read.split.open-file-cost                                          |\n-\n+| Spark option               | Default               | Description                                                                               |\n+| -------------------------- | --------------------- | ----------------------------------------------------------------------------------------- |\n+| snapshot-id                | (latest)              | Snapshot ID of the table snapshot to read                                                 |\n+| as-of-timestamp            | (latest)              | A timestamp in milliseconds; the snapshot used will be the snapshot current at this time. |\n+| split-size                 | As per table property | Overrides this table's read.split.target-size and read.split.metadata-target-size         |\n+| lookback                   | As per table property | Overrides this table's read.split.planning-lookback                                       |\n+| file-open-cost             | As per table property | Overrides this table's read.split.open-file-cost                                          |\n+| use-approximate-statistics | As per table property | Overrides this table's read.spark.read.spark.use-approximate-statistics                   |", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzODMyNA=="}, "originalCommit": {"oid": "b73d189b2192cf2a47051af6550fc98d6a32c629"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ1NTU5NA==", "bodyText": "thanks for reply. to clarify I usecase is while using spark3 we still want to disable estimating statistics by reading manifest ( assuming client code is reading large portion of the table)", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458455594", "createdAt": "2020-07-21T23:57:50Z", "author": {"login": "sudssf"}, "path": "site/docs/configuration.md", "diffHunk": "@@ -109,14 +110,14 @@ spark.read\n     .table(\"catalog.db.table\")\n ```\n \n-| Spark option    | Default               | Description                                                                               |\n-| --------------- | --------------------- | ----------------------------------------------------------------------------------------- |\n-| snapshot-id     | (latest)              | Snapshot ID of the table snapshot to read                                                 |\n-| as-of-timestamp | (latest)              | A timestamp in milliseconds; the snapshot used will be the snapshot current at this time. |\n-| split-size      | As per table property | Overrides this table's read.split.target-size and read.split.metadata-target-size         |\n-| lookback        | As per table property | Overrides this table's read.split.planning-lookback                                       |\n-| file-open-cost  | As per table property | Overrides this table's read.split.open-file-cost                                          |\n-\n+| Spark option               | Default               | Description                                                                               |\n+| -------------------------- | --------------------- | ----------------------------------------------------------------------------------------- |\n+| snapshot-id                | (latest)              | Snapshot ID of the table snapshot to read                                                 |\n+| as-of-timestamp            | (latest)              | A timestamp in milliseconds; the snapshot used will be the snapshot current at this time. |\n+| split-size                 | As per table property | Overrides this table's read.split.target-size and read.split.metadata-target-size         |\n+| lookback                   | As per table property | Overrides this table's read.split.planning-lookback                                       |\n+| file-open-cost             | As per table property | Overrides this table's read.split.open-file-cost                                          |\n+| use-approximate-statistics | As per table property | Overrides this table's read.spark.read.spark.use-approximate-statistics                   |", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzODMyNA=="}, "originalCommit": {"oid": "b73d189b2192cf2a47051af6550fc98d6a32c629"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ1NTc3Mw==", "bodyText": "we can discuss this later , I will cleanup PR to remove parameter and always look at filter to decide estimateStatistics behaviour. thanks", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458455773", "createdAt": "2020-07-21T23:58:30Z", "author": {"login": "sudssf"}, "path": "site/docs/configuration.md", "diffHunk": "@@ -109,14 +110,14 @@ spark.read\n     .table(\"catalog.db.table\")\n ```\n \n-| Spark option    | Default               | Description                                                                               |\n-| --------------- | --------------------- | ----------------------------------------------------------------------------------------- |\n-| snapshot-id     | (latest)              | Snapshot ID of the table snapshot to read                                                 |\n-| as-of-timestamp | (latest)              | A timestamp in milliseconds; the snapshot used will be the snapshot current at this time. |\n-| split-size      | As per table property | Overrides this table's read.split.target-size and read.split.metadata-target-size         |\n-| lookback        | As per table property | Overrides this table's read.split.planning-lookback                                       |\n-| file-open-cost  | As per table property | Overrides this table's read.split.open-file-cost                                          |\n-\n+| Spark option               | Default               | Description                                                                               |\n+| -------------------------- | --------------------- | ----------------------------------------------------------------------------------------- |\n+| snapshot-id                | (latest)              | Snapshot ID of the table snapshot to read                                                 |\n+| as-of-timestamp            | (latest)              | A timestamp in milliseconds; the snapshot used will be the snapshot current at this time. |\n+| split-size                 | As per table property | Overrides this table's read.split.target-size and read.split.metadata-target-size         |\n+| lookback                   | As per table property | Overrides this table's read.split.planning-lookback                                       |\n+| file-open-cost             | As per table property | Overrides this table's read.split.open-file-cost                                          |\n+| use-approximate-statistics | As per table property | Overrides this table's read.spark.read.spark.use-approximate-statistics                   |", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzODMyNA=="}, "originalCommit": {"oid": "b73d189b2192cf2a47051af6550fc98d6a32c629"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTU0NTc3OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNjo0MTo1MlrOG1Aqaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNjo0MTo1MlrOG1Aqaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzODU3MQ==", "bodyText": "Nit: no need for a newline here.", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458238571", "createdAt": "2020-07-21T16:41:52Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * common reader code between spark2 and spark3 module\n+ */\n+public class ReaderUtils {\n+  private ReaderUtils() {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b73d189b2192cf2a47051af6550fc98d6a32c629"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTU0NjM2OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNjo0MjowMlrOG1Aq0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNjo0MjowMlrOG1Aq0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzODY3Mg==", "bodyText": "Nit: newlines should be added between methods.", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458238672", "createdAt": "2020-07-21T16:42:02Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * common reader code between spark2 and spark3 module\n+ */\n+public class ReaderUtils {\n+  private ReaderUtils() {\n+\n+  }\n+  /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b73d189b2192cf2a47051af6550fc98d6a32c629"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTU0NzY5OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNjo0MjoxOFrOG1Arlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNjo0MjoxOFrOG1Arlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzODg3MA==", "bodyText": "And no need for a newline between Javadoc and the documented method.", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458238870", "createdAt": "2020-07-21T16:42:18Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * common reader code between spark2 and spark3 module\n+ */\n+public class ReaderUtils {\n+  private ReaderUtils() {\n+\n+  }\n+  /**\n+   * guess table size based on table schema and total records.\n+   *\n+   * @param table iceberg table\n+   * @param totalRecords total records in the table\n+   * @return approxiate size based on table schema\n+   */\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b73d189b2192cf2a47051af6550fc98d6a32c629"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTU0OTE2OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNjo0MjozNlrOG1AseA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNjo0MjozNlrOG1AseA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzOTA5Ng==", "bodyText": "Please add newlines after control flow, like if/else or loops.", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458239096", "createdAt": "2020-07-21T16:42:36Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * common reader code between spark2 and spark3 module\n+ */\n+public class ReaderUtils {\n+  private ReaderUtils() {\n+\n+  }\n+  /**\n+   * guess table size based on table schema and total records.\n+   *\n+   * @param table iceberg table\n+   * @param totalRecords total records in the table\n+   * @return approxiate size based on table schema\n+   */\n+\n+  public static long approximateTableSize(Table table, long totalRecords) {\n+    if (totalRecords == Long.MAX_VALUE) {\n+      return totalRecords;\n+    }\n+    StructType type = SparkSchemaUtil.convert(table.schema());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b73d189b2192cf2a47051af6550fc98d6a32c629"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTU1NjQ0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNjo0NDoyOFrOG1AxAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNjo0NDoyOFrOG1AxAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI0MDI1Nw==", "bodyText": "I think this should pass in a StructType or Schema rather than the table. The table isn't really used.\nIt would also make sense to pass the value of lazyType() from the reader because that will account for column projection.", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458240257", "createdAt": "2020-07-21T16:44:28Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * common reader code between spark2 and spark3 module\n+ */\n+public class ReaderUtils {\n+  private ReaderUtils() {\n+\n+  }\n+  /**\n+   * guess table size based on table schema and total records.\n+   *\n+   * @param table iceberg table\n+   * @param totalRecords total records in the table\n+   * @return approxiate size based on table schema\n+   */\n+\n+  public static long approximateTableSize(Table table, long totalRecords) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b73d189b2192cf2a47051af6550fc98d6a32c629"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1OTU2MTYzOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNjo0NTozNVrOG1A0Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQxNjo0NTozNVrOG1A0Ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI0MTA1OA==", "bodyText": "I think it would be easier to use PropertyUtil.propertyAsLong instead of adding this method.", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r458241058", "createdAt": "2020-07-21T16:45:35Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * common reader code between spark2 and spark3 module\n+ */\n+public class ReaderUtils {\n+  private ReaderUtils() {\n+\n+  }\n+  /**\n+   * guess table size based on table schema and total records.\n+   *\n+   * @param table iceberg table\n+   * @param totalRecords total records in the table\n+   * @return approxiate size based on table schema\n+   */\n+\n+  public static long approximateTableSize(Table table, long totalRecords) {\n+    if (totalRecords == Long.MAX_VALUE) {\n+      return totalRecords;\n+    }\n+    StructType type = SparkSchemaUtil.convert(table.schema());\n+    long approximateSize = 0;\n+    for (StructField sparkField : type.fields()) {\n+      approximateSize += sparkField.dataType().defaultSize();\n+    }\n+    return approximateSize * totalRecords;\n+  }\n+\n+  /**\n+   * get total records from table metadata using {@link SnapshotSummary.TOTAL_RECORDS_PROP}.\n+   *\n+   * @param table iceberg table\n+   * @return total records from table metadata\n+   */\n+\n+  public static long totalRecordsFromMetadata(Table table) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b73d189b2192cf2a47051af6550fc98d6a32c629"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTQ1Mjg4OnYy", "diffSide": "RIGHT", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMTowNzo1NlrOG15RkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNTo0MToxOFrOG3oRGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NjA5Nw==", "bodyText": "Looks like this is a typo.", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r459166097", "createdAt": "2020-07-23T01:07:56Z", "author": {"login": "rdblue"}, "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -174,7 +175,7 @@ private Schema lazySchema() {\n   }\n \n   private Expression filterExpression() {\n-    if (filterExpressions != null) {\n+    if (filterExpressions != null ) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34c335975be32f3588fc93a6ae4de94acce7c9e3"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk4NDYwMA==", "bodyText": "ack", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r460984600", "createdAt": "2020-07-27T15:41:18Z", "author": {"login": "sudssf"}, "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -174,7 +175,7 @@ private Schema lazySchema() {\n   }\n \n   private Expression filterExpression() {\n-    if (filterExpressions != null) {\n+    if (filterExpressions != null ) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NjA5Nw=="}, "originalCommit": {"oid": "34c335975be32f3588fc93a6ae4de94acce7c9e3"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTQ1NDMzOnYy", "diffSide": "RIGHT", "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema24.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMTowODo1NVrOG15SbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNTo0MTo0OFrOG3oSXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NjMxNw==", "bodyText": "I don't think this test needs to be here. Tests for utility classes should be in their own files, and preferably in the same module as the utility class.", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r459166317", "createdAt": "2020-07-23T01:08:55Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema24.java", "diffHunk": "@@ -19,5 +19,43 @@\n \n package org.apache.iceberg.spark.source;\n \n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n public class TestSparkSchema24 extends TestSparkSchema {\n+\n+  @Test\n+  public void testReaderUtils() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34c335975be32f3588fc93a6ae4de94acce7c9e3"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk4NDkyNQ==", "bodyText": "ack", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r460984925", "createdAt": "2020-07-27T15:41:48Z", "author": {"login": "sudssf"}, "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema24.java", "diffHunk": "@@ -19,5 +19,43 @@\n \n package org.apache.iceberg.spark.source;\n \n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n public class TestSparkSchema24 extends TestSparkSchema {\n+\n+  @Test\n+  public void testReaderUtils() throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NjMxNw=="}, "originalCommit": {"oid": "34c335975be32f3588fc93a6ae4de94acce7c9e3"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTQ1NTQ4OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMTowOTozN1rOG15TEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNTo0MToxM1rOG3oQ3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NjQ4Mw==", "bodyText": "This doesn't need to change since the tests should be in the spark module instead of in a subclass of TestSparkSchema.", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r459166483", "createdAt": "2020-07-23T01:09:37Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema.java", "diffHunk": "@@ -46,12 +46,12 @@\n \n public abstract class TestSparkSchema {\n \n-  private static final Configuration CONF = new Configuration();\n-  private static final Schema SCHEMA = new Schema(\n+  protected static final Configuration CONF = new Configuration();\n+  protected static final Schema SCHEMA = new Schema(\n       optional(1, \"id\", Types.IntegerType.get()),\n       optional(2, \"data\", Types.StringType.get())\n   );\n-  private static SparkSession spark = null;\n+  protected static SparkSession spark = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34c335975be32f3588fc93a6ae4de94acce7c9e3"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk4NDU0MA==", "bodyText": "ack", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r460984540", "createdAt": "2020-07-27T15:41:13Z", "author": {"login": "sudssf"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema.java", "diffHunk": "@@ -46,12 +46,12 @@\n \n public abstract class TestSparkSchema {\n \n-  private static final Configuration CONF = new Configuration();\n-  private static final Schema SCHEMA = new Schema(\n+  protected static final Configuration CONF = new Configuration();\n+  protected static final Schema SCHEMA = new Schema(\n       optional(1, \"id\", Types.IntegerType.get()),\n       optional(2, \"data\", Types.StringType.get())\n   );\n-  private static SparkSession spark = null;\n+  protected static SparkSession spark = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NjQ4Mw=="}, "originalCommit": {"oid": "34c335975be32f3588fc93a6ae4de94acce7c9e3"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTQ2MjYzOnYy", "diffSide": "RIGHT", "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema24.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMToxMjo1OVrOG15XDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNTo0MTo1OFrOG3oSyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NzUwMA==", "bodyText": "I don't think the method being tested warrants a test that creates a table, writes data, etc. The method has 2 cases:\n\nnumRows is Long.MAX_VALUE -> return Long.MAX_VALUE\nnumRows is not -> multiply numRows by sizeEstimate(dataType)\n\nAll you need is 2 test cases: one for each possibility for numRows. The second case should make sure the estimate is sane.", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r459167500", "createdAt": "2020-07-23T01:12:59Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema24.java", "diffHunk": "@@ -19,5 +19,43 @@\n \n package org.apache.iceberg.spark.source;\n \n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n public class TestSparkSchema24 extends TestSparkSchema {\n+\n+  @Test\n+  public void testReaderUtils() throws IOException {\n+    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    tables.create(SCHEMA, spec, null, tableLocation);\n+    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\")\n+    );\n+    Dataset<Row> originalDf = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n+    originalDf.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .mode(\"append\")\n+        .save(tableLocation);\n+\n+    Table table = tables.load(tableLocation);\n+    long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n+              SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n+    Assert.assertEquals(\"totalRecords match\", 1, totalRecords);\n+    long tableSize = ReaderUtils.approximateTableSize(SparkSchemaUtil.convert(table.schema()), totalRecords);\n+    Assert.assertEquals(\"table size matches with expected approximation\", 24, tableSize);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34c335975be32f3588fc93a6ae4de94acce7c9e3"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk4NTAzNQ==", "bodyText": "added", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r460985035", "createdAt": "2020-07-27T15:41:58Z", "author": {"login": "sudssf"}, "path": "spark2/src/test/java/org/apache/iceberg/spark/source/TestSparkSchema24.java", "diffHunk": "@@ -19,5 +19,43 @@\n \n package org.apache.iceberg.spark.source;\n \n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n public class TestSparkSchema24 extends TestSparkSchema {\n+\n+  @Test\n+  public void testReaderUtils() throws IOException {\n+    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    tables.create(SCHEMA, spec, null, tableLocation);\n+    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\")\n+    );\n+    Dataset<Row> originalDf = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n+    originalDf.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .mode(\"append\")\n+        .save(tableLocation);\n+\n+    Table table = tables.load(tableLocation);\n+    long totalRecords = PropertyUtil.propertyAsLong(table.currentSnapshot().summary(),\n+              SnapshotSummary.TOTAL_RECORDS_PROP, Long.MAX_VALUE);\n+    Assert.assertEquals(\"totalRecords match\", 1, totalRecords);\n+    long tableSize = ReaderUtils.approximateTableSize(SparkSchemaUtil.convert(table.schema()), totalRecords);\n+    Assert.assertEquals(\"table size matches with expected approximation\", 24, tableSize);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2NzUwMA=="}, "originalCommit": {"oid": "34c335975be32f3588fc93a6ae4de94acce7c9e3"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTQ2NzA0OnYy", "diffSide": "RIGHT", "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMToxNTozNVrOG15Zag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNTo0MTo0MFrOG3oSBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2ODEwNg==", "bodyText": "Minor: I don't see much value in having this debug statement.\nIf you were trying to debug estimateStatistics, I think you'd want to have a message for every call, and you would want it to show the expression as well as the resulting estimate. I think Spark already has those logs, though.", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r459168106", "createdAt": "2020-07-23T01:15:35Z", "author": {"login": "rdblue"}, "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -276,6 +277,13 @@ public void pruneColumns(StructType newRequestedSchema) {\n \n   @Override\n   public Statistics estimateStatistics() {\n+    if (filterExpressions == null || filterExpressions == Expressions.alwaysTrue()) {\n+      LOG.debug(\"using table metadata to estimate table statistics\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34c335975be32f3588fc93a6ae4de94acce7c9e3"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk4NDgzOA==", "bodyText": "ack", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r460984838", "createdAt": "2020-07-27T15:41:40Z", "author": {"login": "sudssf"}, "path": "spark2/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -276,6 +277,13 @@ public void pruneColumns(StructType newRequestedSchema) {\n \n   @Override\n   public Statistics estimateStatistics() {\n+    if (filterExpressions == null || filterExpressions == Expressions.alwaysTrue()) {\n+      LOG.debug(\"using table metadata to estimate table statistics\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2ODEwNg=="}, "originalCommit": {"oid": "34c335975be32f3588fc93a6ae4de94acce7c9e3"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTQ3NTk1OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwMToyMDozNFrOG15eIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNTo0MTowNlrOG3oQkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2OTMxMg==", "bodyText": "Could you move this method to SparkSchemaUtil.estimateSize? I think it makes sense to put it in the schema utility methods, since it relates to a Spark schema. Looks like we don't have a test suite for that, so you could start TestSparkSchemaUtil with the tests for this.", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r459169312", "createdAt": "2020-07-23T01:20:34Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * common reader code between spark2 and spark3 module\n+ */\n+public class ReaderUtils {\n+  private ReaderUtils() {}\n+\n+  /**\n+   * guess table size based on table schema and total records.\n+   *\n+   * @param table        iceberg table\n+   * @param totalRecords total records in the table\n+   * @return approxiate size based on table schema\n+   */\n+  public static long approximateTableSize(StructType tableSchema, long totalRecords) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "34c335975be32f3588fc93a6ae4de94acce7c9e3"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDk4NDQ2NA==", "bodyText": "ack this makes more sense thanks for suggestion", "url": "https://github.com/apache/iceberg/pull/1221#discussion_r460984464", "createdAt": "2020-07-27T15:41:06Z", "author": {"login": "sudssf"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/ReaderUtils.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+/**\n+ * common reader code between spark2 and spark3 module\n+ */\n+public class ReaderUtils {\n+  private ReaderUtils() {}\n+\n+  /**\n+   * guess table size based on table schema and total records.\n+   *\n+   * @param table        iceberg table\n+   * @param totalRecords total records in the table\n+   * @return approxiate size based on table schema\n+   */\n+  public static long approximateTableSize(StructType tableSchema, long totalRecords) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTE2OTMxMg=="}, "originalCommit": {"oid": "34c335975be32f3588fc93a6ae4de94acce7c9e3"}, "originalPosition": 42}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3779, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}