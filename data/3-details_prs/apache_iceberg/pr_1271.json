{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU5MDIyNDAw", "number": 1271, "title": "Align the records written by GenericOrcWriter and SparkOrcWriter", "bodyText": "This PR addressed the bug in #1269,  it mainly fixed the two sub-issues:\n\n\nwhen writing a Decimal (precision<=18) into hive orc file,  the orc writer will scale down the decimal. for example,  we have a value  10.100  for type Decimal(10, 3),  the hive orc will remove all the trailing zero and store it as 101*10^(-1), mean precision is 3 and scale is 1.  Here the scale of decimal read from hive orc file, is not strictly equal to 3.  so for both spark orc reader and generic orc reader we need to transform it to the given scale =3 .  Otherwise, the unit test will be broken.\n\n\nThe long value of zoned timestamp can be negative,  while we spark orc reader/writer did not consider this case, and just use the  /  and % to do the arithmetic computation,  while actually we should use Math.floorDiv and Math.floorMod.", "createdAt": "2020-07-30T09:41:13Z", "url": "https://github.com/apache/iceberg/pull/1271", "merged": true, "mergeCommit": {"oid": "6f96b36a39f26cfbc6f66dc762148577e5697534"}, "closed": true, "closedAt": "2020-08-07T15:58:25Z", "author": {"login": "openinx"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc7eEUagFqTQ2MDQxODMyNA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc8at5UABqjM2MzE0NjQ2MTE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYwNDE4MzI0", "url": "https://github.com/apache/iceberg/pull/1271#pullrequestreview-460418324", "createdAt": "2020-08-04T00:23:19Z", "commit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMDoyMzoxOVrOG7M11g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQwMzoyOToyNFrOG7Pw3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcyOTU1OA==", "bodyText": "data.getNano() always returns positive integer, so is this change required?", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464729558", "createdAt": "2020-08-04T00:23:19Z", "author": {"login": "shardulm94"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -288,8 +289,10 @@ public void nonNullWrite(int rowId, LocalDate data, ColumnVector output) {\n     @Override\n     public void nonNullWrite(int rowId, OffsetDateTime data, ColumnVector output) {\n       TimestampColumnVector cv = (TimestampColumnVector) output;\n-      cv.time[rowId] = data.toInstant().toEpochMilli(); // millis\n-      cv.nanos[rowId] = (data.getNano() / 1_000) * 1_000; // truncate nanos to only keep microsecond precision\n+      // millis\n+      cv.time[rowId] = data.toInstant().toEpochMilli();\n+      // truncate nanos to only keep microsecond precision\n+      cv.nanos[rowId] = Math.floorDiv(data.getNano(), 1_000) * 1_000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2MzM1Nw==", "bodyText": "Nit: precision <= 18 check can be moved into the constructor", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464763357", "createdAt": "2020-08-04T02:33:47Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -195,7 +196,12 @@ public Long nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      return new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\n+      BigDecimal decimal = new BigDecimal(BigInteger.valueOf(value.serialize64(value.scale())), value.scale());\n+\n+      Preconditions.checkArgument(value.precision() <= precision && precision <= 18,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc2MzM4NA==", "bodyText": "Nit: precision <= 38 check can be moved into the constructor", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464763384", "createdAt": "2020-08-04T02:33:54Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -212,6 +218,10 @@ public Decimal nonNullRead(ColumnVector vector, int row) {\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       BigDecimal value = ((DecimalColumnVector) vector).vector[row]\n           .getHiveDecimal().bigDecimalValue();\n+\n+      Preconditions.checkArgument(value.precision() <= precision && precision <= 38,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NDAwMg==", "bodyText": "value.serialize64() will take in an expected scale as a parameter, so I think the only change required to the original code is to pass our expected reader scale into value.serialize64() instead of passing value.scale() and passing expected precision and scale to Decimal.set.\nSo this would look like return new Decimal().set(value.serialize64(scale), precision, scale);", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464774002", "createdAt": "2020-08-04T03:15:13Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -195,7 +196,12 @@ public Long nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      return new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\n+      BigDecimal decimal = new BigDecimal(BigInteger.valueOf(value.serialize64(value.scale())), value.scale());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NTU0Nw==", "bodyText": "Nit: precision <= 18 check can be moved into the constructor", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464775547", "createdAt": "2020-08-04T03:21:43Z", "author": {"login": "shardulm94"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -324,14 +329,24 @@ public void nonNullWrite(int rowId, LocalDateTime data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n-      // TODO: validate precision and scale from schema\n+      Preconditions.checkArgument(data.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, data);\n+      Preconditions.checkArgument(data.precision() <= precision && precision <= 18,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NTU5Ng==", "bodyText": "Nit: precision <= 38 check can be moved into the constructor", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464775596", "createdAt": "2020-08-04T03:21:53Z", "author": {"login": "shardulm94"}, "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -340,7 +355,11 @@ public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n-      // TODO: validate precision and scale from schema\n+      Preconditions.checkArgument(data.scale() == scale,\n+          \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, data);\n+      Preconditions.checkArgument(data.precision() <= precision && precision <= 38,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95c361026979dcd63758e80878e3d6917bcae505"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NzM5MQ==", "bodyText": "This check seems redundant to me. If we are already passing our expected precision and scale to data.getDecimal(), wont the scale and precision of the returned decimal always match?", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464777391", "createdAt": "2020-08-04T03:29:14Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java", "diffHunk": "@@ -237,9 +239,14 @@ public void addValue(int rowId, int column, SpecializedGetters data,\n         output.noNulls = false;\n         output.isNull[rowId] = true;\n       } else {\n+        Decimal decimal = data.getDecimal(column, precision, scale);\n+        Preconditions.checkArgument(scale == decimal.scale(),\n+            \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+        Preconditions.checkArgument(decimal.precision() <= precision && precision <= 18,\n+            \"Cannot write value as decimal(%s,%s), invalid precision: %s\", decimal);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "042670ffd50e8b8a111bba93a2875ee357509137"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc3NzQzOQ==", "bodyText": "This check seems redundant to me. If we are already passing our expected precision and scale to data.getDecimal(), wont the scale and precision of the returned decimal always match?", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r464777439", "createdAt": "2020-08-04T03:29:24Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java", "diffHunk": "@@ -261,9 +268,14 @@ public void addValue(int rowId, int column, SpecializedGetters data,\n         output.isNull[rowId] = true;\n       } else {\n         output.isNull[rowId] = false;\n-        ((DecimalColumnVector) output).vector[rowId].set(\n-            HiveDecimal.create(data.getDecimal(column, precision, scale)\n-                .toJavaBigDecimal()));\n+\n+        Decimal decimal = data.getDecimal(column, precision, scale);\n+        Preconditions.checkArgument(scale == decimal.scale(),\n+            \"Cannot write value as decimal(%s,%s), wrong scale: %s\", precision, scale, decimal);\n+        Preconditions.checkArgument(decimal.precision() <= precision && precision <= 38,\n+            \"Cannot write value as decimal(%s,%s), invalid precision: %s\", precision, scale, decimal);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "042670ffd50e8b8a111bba93a2875ee357509137"}, "originalPosition": 58}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyMDI4MDYy", "url": "https://github.com/apache/iceberg/pull/1271#pullrequestreview-462028062", "createdAt": "2020-08-05T21:13:14Z", "commit": {"oid": "d1f7a489efa28ae39dc367230fa81882f78d2cc3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMToxMzoxNFrOG8a1EA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMToxMzoxNFrOG8a1EA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAwNzMxMg==", "bodyText": "I'm not sure we need to check the precision either. If we read a value, then we should return it, right?", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r466007312", "createdAt": "2020-08-05T21:13:14Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -195,7 +197,15 @@ public Long nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public Decimal nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n-      return new Decimal().set(value.serialize64(value.scale()), value.precision(), value.scale());\n+\n+      // The scale of decimal read from hive ORC file may be not equals to the expected scale. For data type\n+      // decimal(10,3) and the value 10.100, the hive ORC writer will remove its trailing zero and store it\n+      // as 101*10^(-1), its scale will adjust from 3 to 1. So here we could not assert that value.scale() == scale.\n+      // we also need to convert the hive orc decimal to a decimal with expected precision and scale.\n+      Preconditions.checkArgument(value.precision() <= precision,\n+          \"Cannot read value as decimal(%s,%s), too large: %s\", precision, scale, value);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1f7a489efa28ae39dc367230fa81882f78d2cc3"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyMDM2OTk3", "url": "https://github.com/apache/iceberg/pull/1271#pullrequestreview-462036997", "createdAt": "2020-08-05T21:28:22Z", "commit": {"oid": "d1f7a489efa28ae39dc367230fa81882f78d2cc3"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMToyODoyM1rOG8bRug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMToyODoyM1rOG8bRug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAxNDY1MA==", "bodyText": "Validation should be done against this data, not data that has been read from a file. That way the test won't be broken by a problem with the reader or writer that produces the expected rows. To validate against these, use the GenericsHelpers.assertEqualsUnsafe methods.", "url": "https://github.com/apache/iceberg/pull/1271#discussion_r466014650", "createdAt": "2020-08-05T21:28:23Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkRecordOrcReaderWriter.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.orc.GenericOrcReader;\n+import org.apache.iceberg.data.orc.GenericOrcWriter;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.junit.Assert;\n+\n+public class TestSparkRecordOrcReaderWriter extends AvroDataTest {\n+  private static final int NUM_RECORDS = 200;\n+\n+  @Override\n+  protected void writeAndValidate(Schema schema) throws IOException {\n+    List<Record> records = RandomGenericData.generate(schema, NUM_RECORDS, 1992L);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1f7a489efa28ae39dc367230fa81882f78d2cc3"}, "originalPosition": 44}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "committedDate": "2020-08-07T02:05:54Z", "message": "Align the records between GenericOrcWriter and SparkOrcWriter"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "53e543bb3952240775c8f326b311ea81a5a95bd1", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/53e543bb3952240775c8f326b311ea81a5a95bd1", "committedDate": "2020-08-06T08:02:00Z", "message": "Add unit tests for decimal with trailing zero."}, "afterCommit": {"oid": "b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/b7fe76f2df2ea9b33741388b4c50fe3e1bd80181", "committedDate": "2020-08-07T02:05:54Z", "message": "Align the records between GenericOrcWriter and SparkOrcWriter"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4368, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}