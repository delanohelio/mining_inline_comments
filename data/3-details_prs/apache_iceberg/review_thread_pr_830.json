{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg1MjQ3NzMy", "number": 830, "reviewThreads": {"totalCount": 54, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOFQxOTozMDoxM1rODmHe6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzozODozMVrOEGCp7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMjk1MDgxOnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOFQxOTozMDoxM1rOFzW-cQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQwMzowODowNFrOF0AG0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTM5ODEyOQ==", "bodyText": "I think the nameMapping should be taken as a parameter in Parquet as part of the builder and passed in from there", "url": "https://github.com/apache/iceberg/pull/830#discussion_r389398129", "createdAt": "2020-03-08T19:30:13Z", "author": {"login": "rdsr"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -72,29 +74,31 @@\n     this.options = options;\n     this.reader = newReader(file, options);\n     MessageType fileSchema = reader.getFileMetaData().getSchema();\n-\n     boolean hasIds = ParquetSchemaUtil.hasIds(fileSchema);\n-    MessageType typeWithIds = hasIds ? fileSchema : ParquetSchemaUtil.addFallbackIds(fileSchema);\n \n     this.projection = hasIds ?\n-        ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n-        ParquetSchemaUtil.pruneColumnsFallback(fileSchema, expectedSchema);\n+            ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n+            ParquetSchemaUtil.pruneColumnsByName(fileSchema, expectedSchema);\n+\n     this.rowGroups = reader.getRowGroups();\n     this.shouldSkip = new boolean[rowGroups.size()];\n \n     ParquetMetricsRowGroupFilter statsFilter = null;\n     ParquetDictionaryRowGroupFilter dictFilter = null;\n     if (filter != null) {\n-      statsFilter = new ParquetMetricsRowGroupFilter(expectedSchema, filter, caseSensitive);\n-      dictFilter = new ParquetDictionaryRowGroupFilter(expectedSchema, filter, caseSensitive);\n+      NameMapping nameMapping = MappingUtil.create(ParquetSchemaUtil.convert(fileSchema));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3d2f4e17d66ce5a0a55cac7e1918fd6fc29ef44"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTM5ODkwMg==", "bodyText": "Also, if the nameMapping is not passed in and file schema does not have ids. We can infer the nameMapping, similar to what we've done here.  We had done this for Avro #580", "url": "https://github.com/apache/iceberg/pull/830#discussion_r389398902", "createdAt": "2020-03-08T19:40:38Z", "author": {"login": "rdsr"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -72,29 +74,31 @@\n     this.options = options;\n     this.reader = newReader(file, options);\n     MessageType fileSchema = reader.getFileMetaData().getSchema();\n-\n     boolean hasIds = ParquetSchemaUtil.hasIds(fileSchema);\n-    MessageType typeWithIds = hasIds ? fileSchema : ParquetSchemaUtil.addFallbackIds(fileSchema);\n \n     this.projection = hasIds ?\n-        ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n-        ParquetSchemaUtil.pruneColumnsFallback(fileSchema, expectedSchema);\n+            ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n+            ParquetSchemaUtil.pruneColumnsByName(fileSchema, expectedSchema);\n+\n     this.rowGroups = reader.getRowGroups();\n     this.shouldSkip = new boolean[rowGroups.size()];\n \n     ParquetMetricsRowGroupFilter statsFilter = null;\n     ParquetDictionaryRowGroupFilter dictFilter = null;\n     if (filter != null) {\n-      statsFilter = new ParquetMetricsRowGroupFilter(expectedSchema, filter, caseSensitive);\n-      dictFilter = new ParquetDictionaryRowGroupFilter(expectedSchema, filter, caseSensitive);\n+      NameMapping nameMapping = MappingUtil.create(ParquetSchemaUtil.convert(fileSchema));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTM5ODEyOQ=="}, "originalCommit": {"oid": "b3d2f4e17d66ce5a0a55cac7e1918fd6fc29ef44"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDA3MjAxOQ==", "bodyText": "It makes sense if we build name mapping from the expected schema, while here I need to build it from file schema so that we can use the metrics correctly.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r390072019", "createdAt": "2020-03-10T03:08:04Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -72,29 +74,31 @@\n     this.options = options;\n     this.reader = newReader(file, options);\n     MessageType fileSchema = reader.getFileMetaData().getSchema();\n-\n     boolean hasIds = ParquetSchemaUtil.hasIds(fileSchema);\n-    MessageType typeWithIds = hasIds ? fileSchema : ParquetSchemaUtil.addFallbackIds(fileSchema);\n \n     this.projection = hasIds ?\n-        ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n-        ParquetSchemaUtil.pruneColumnsFallback(fileSchema, expectedSchema);\n+            ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n+            ParquetSchemaUtil.pruneColumnsByName(fileSchema, expectedSchema);\n+\n     this.rowGroups = reader.getRowGroups();\n     this.shouldSkip = new boolean[rowGroups.size()];\n \n     ParquetMetricsRowGroupFilter statsFilter = null;\n     ParquetDictionaryRowGroupFilter dictFilter = null;\n     if (filter != null) {\n-      statsFilter = new ParquetMetricsRowGroupFilter(expectedSchema, filter, caseSensitive);\n-      dictFilter = new ParquetDictionaryRowGroupFilter(expectedSchema, filter, caseSensitive);\n+      NameMapping nameMapping = MappingUtil.create(ParquetSchemaUtil.convert(fileSchema));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTM5ODEyOQ=="}, "originalCommit": {"oid": "b3d2f4e17d66ce5a0a55cac7e1918fd6fc29ef44"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMjk1MjE2OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOFQxOTozMjozOVrOFzW_KQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQwMzowMjozOVrOF0AB1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTM5ODMxMw==", "bodyText": "For Avro  we assigned ids to fileSchema based on the name mapping  and projected ids, which we derived from expected schema. [Set<Integer> projectedIds = TypeUtil.getProjectedIds(expectedSchema)] . This was piggy backed on column pruning code for Avro see - AvroSchemaUtil.pruneColumns\nI'm unsure whether the same general approach can be applied here as well. @rdblue Thoughts?", "url": "https://github.com/apache/iceberg/pull/830#discussion_r389398313", "createdAt": "2020-03-08T19:32:39Z", "author": {"login": "rdsr"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -72,29 +74,31 @@\n     this.options = options;\n     this.reader = newReader(file, options);\n     MessageType fileSchema = reader.getFileMetaData().getSchema();\n-\n     boolean hasIds = ParquetSchemaUtil.hasIds(fileSchema);\n-    MessageType typeWithIds = hasIds ? fileSchema : ParquetSchemaUtil.addFallbackIds(fileSchema);\n \n     this.projection = hasIds ?\n-        ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n-        ParquetSchemaUtil.pruneColumnsFallback(fileSchema, expectedSchema);\n+            ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n+            ParquetSchemaUtil.pruneColumnsByName(fileSchema, expectedSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3d2f4e17d66ce5a0a55cac7e1918fd6fc29ef44"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDA3MDc0Mg==", "bodyText": "Here is a reversed way. I build name mapping based on fileSchema, and then project ids for expected schema according to names. The reason is the metrics are built from parquet footer so we have to bind the expression to fileSchema instead of expected schema in ParquetMetricsRowGroupFilter and ParquetDictionaryRowGroupFilter.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r390070742", "createdAt": "2020-03-10T03:02:39Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -72,29 +74,31 @@\n     this.options = options;\n     this.reader = newReader(file, options);\n     MessageType fileSchema = reader.getFileMetaData().getSchema();\n-\n     boolean hasIds = ParquetSchemaUtil.hasIds(fileSchema);\n-    MessageType typeWithIds = hasIds ? fileSchema : ParquetSchemaUtil.addFallbackIds(fileSchema);\n \n     this.projection = hasIds ?\n-        ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n-        ParquetSchemaUtil.pruneColumnsFallback(fileSchema, expectedSchema);\n+            ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n+            ParquetSchemaUtil.pruneColumnsByName(fileSchema, expectedSchema);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTM5ODMxMw=="}, "originalCommit": {"oid": "b3d2f4e17d66ce5a0a55cac7e1918fd6fc29ef44"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyMzc2ODM0OnYy", "diffSide": "RIGHT", "path": "api/src/main/java/org/apache/iceberg/types/TypeUtil.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNjoyMToxNlrOF0-kwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNjoyMToxNlrOF0-kwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTA5NTQ5MA==", "bodyText": "This is only used to pass the build, will fix in another pr.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r391095490", "createdAt": "2020-03-11T16:21:16Z", "author": {"login": "chenjunjiedada"}, "path": "api/src/main/java/org/apache/iceberg/types/TypeUtil.java", "diffHunk": "@@ -190,7 +191,7 @@ public static boolean isPromotionAllowed(Type from, Type.PrimitiveType to) {\n   }\n \n   public static class SchemaVisitor<T> {\n-    private final Deque<Integer> fieldIds = Lists.newLinkedList();\n+    private final Deque<Integer> fieldIds = new ConcurrentLinkedDeque<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eeb8455304d64ac93db55b14e23fcb0be54c6574"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwOTcyMzM3OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wNlQyMzo0ODozOFrOGBt-Zg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMDozNToyN1rOGB9Zzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1NTAxNA==", "bodyText": "This is called nameMapping in the Avro API. I think I prefer this name, but we should be consistent and either change that one or use the same name here.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r404455014", "createdAt": "2020-04-06T23:48:38Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -393,6 +395,11 @@ public ReadBuilder recordsPerBatch(int numRowsPerBatch) {\n       return this;\n     }\n \n+    public ReadBuilder withNameMapping(NameMapping newNameMapping) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcwNzc5MQ==", "bodyText": "Will change Avro side to use this one as well.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r404707791", "createdAt": "2020-04-07T10:35:27Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -393,6 +395,11 @@ public ReadBuilder recordsPerBatch(int numRowsPerBatch) {\n       return this;\n     }\n \n+    public ReadBuilder withNameMapping(NameMapping newNameMapping) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1NTAxNA=="}, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwOTc0ODE3OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwMDowMDoyMFrOGBuNaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNzoxNjozNVrOGEAcqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1ODg1OQ==", "bodyText": "This class doesn't need to know about the name mapping. The mapping should be used to add IDs to the file schema so that most classes don't need to add specific support.\nLook at how the other fallback happens using ParquetSchemaUtil.addFallbackIds. I think this should mimic that fallback in all cases.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r404458859", "createdAt": "2020-04-07T00:00:20Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "diffHunk": "@@ -75,6 +86,16 @@ public ParquetDictionaryRowGroupFilter(Schema schema, Expression unbound, boolea\n    */\n   public boolean shouldRead(MessageType fileSchema, BlockMetaData rowGroup,\n                             DictionaryPageReadStore dictionaries) {\n+    StructType struct;\n+\n+    if (nameMapping != null) {\n+      MessageType project = ParquetSchemaUtil.pruneColumnsByName(fileSchema, schema, nameMapping);\n+      struct = ParquetSchemaUtil.convert(project).asStruct();\n+    } else {\n+      struct = schema.asStruct();\n+    }\n+\n+    this.expr = Binder.bind(struct, Expressions.rewriteNot(expr), caseSensitive);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTIxNTg2MQ==", "bodyText": "The added unit test failed if we only use the ID fallback strategy. How about we keep both the original logic and the name mapping logic?", "url": "https://github.com/apache/iceberg/pull/830#discussion_r405215861", "createdAt": "2020-04-08T02:03:40Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "diffHunk": "@@ -75,6 +86,16 @@ public ParquetDictionaryRowGroupFilter(Schema schema, Expression unbound, boolea\n    */\n   public boolean shouldRead(MessageType fileSchema, BlockMetaData rowGroup,\n                             DictionaryPageReadStore dictionaries) {\n+    StructType struct;\n+\n+    if (nameMapping != null) {\n+      MessageType project = ParquetSchemaUtil.pruneColumnsByName(fileSchema, schema, nameMapping);\n+      struct = ParquetSchemaUtil.convert(project).asStruct();\n+    } else {\n+      struct = schema.asStruct();\n+    }\n+\n+    this.expr = Binder.bind(struct, Expressions.rewriteNot(expr), caseSensitive);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1ODg1OQ=="}, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTYzOTUyOA==", "bodyText": "We need to keep both ID assignment strategies, but we don't need to change these filters. Just pass the file schema with assigned IDs into this like before.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r405639528", "createdAt": "2020-04-08T16:05:46Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "diffHunk": "@@ -75,6 +86,16 @@ public ParquetDictionaryRowGroupFilter(Schema schema, Expression unbound, boolea\n    */\n   public boolean shouldRead(MessageType fileSchema, BlockMetaData rowGroup,\n                             DictionaryPageReadStore dictionaries) {\n+    StructType struct;\n+\n+    if (nameMapping != null) {\n+      MessageType project = ParquetSchemaUtil.pruneColumnsByName(fileSchema, schema, nameMapping);\n+      struct = ParquetSchemaUtil.convert(project).asStruct();\n+    } else {\n+      struct = schema.asStruct();\n+    }\n+\n+    this.expr = Binder.bind(struct, Expressions.rewriteNot(expr), caseSensitive);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1ODg1OQ=="}, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjA1Nzg2Ng==", "bodyText": "I changed to pass typeWithIds.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406057866", "createdAt": "2020-04-09T08:57:39Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "diffHunk": "@@ -75,6 +86,16 @@ public ParquetDictionaryRowGroupFilter(Schema schema, Expression unbound, boolea\n    */\n   public boolean shouldRead(MessageType fileSchema, BlockMetaData rowGroup,\n                             DictionaryPageReadStore dictionaries) {\n+    StructType struct;\n+\n+    if (nameMapping != null) {\n+      MessageType project = ParquetSchemaUtil.pruneColumnsByName(fileSchema, schema, nameMapping);\n+      struct = ParquetSchemaUtil.convert(project).asStruct();\n+    } else {\n+      struct = schema.asStruct();\n+    }\n+\n+    this.expr = Binder.bind(struct, Expressions.rewriteNot(expr), caseSensitive);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1ODg1OQ=="}, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM1MjYxNQ==", "bodyText": "Why is the name mapping still passed in here?", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406352615", "createdAt": "2020-04-09T17:12:16Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "diffHunk": "@@ -75,6 +86,16 @@ public ParquetDictionaryRowGroupFilter(Schema schema, Expression unbound, boolea\n    */\n   public boolean shouldRead(MessageType fileSchema, BlockMetaData rowGroup,\n                             DictionaryPageReadStore dictionaries) {\n+    StructType struct;\n+\n+    if (nameMapping != null) {\n+      MessageType project = ParquetSchemaUtil.pruneColumnsByName(fileSchema, schema, nameMapping);\n+      struct = ParquetSchemaUtil.convert(project).asStruct();\n+    } else {\n+      struct = schema.asStruct();\n+    }\n+\n+    this.expr = Binder.bind(struct, Expressions.rewriteNot(expr), caseSensitive);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1ODg1OQ=="}, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjY5MjU1MA==", "bodyText": "The evaluation build metadata mapping with BlockMetadata read from the footer which may contain columns that not in the file schema passed in, so I use name mapping here to filter out them.  The new commit changed to use a try/catch block.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406692550", "createdAt": "2020-04-10T10:01:37Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "diffHunk": "@@ -75,6 +86,16 @@ public ParquetDictionaryRowGroupFilter(Schema schema, Expression unbound, boolea\n    */\n   public boolean shouldRead(MessageType fileSchema, BlockMetaData rowGroup,\n                             DictionaryPageReadStore dictionaries) {\n+    StructType struct;\n+\n+    if (nameMapping != null) {\n+      MessageType project = ParquetSchemaUtil.pruneColumnsByName(fileSchema, schema, nameMapping);\n+      struct = ParquetSchemaUtil.convert(project).asStruct();\n+    } else {\n+      struct = schema.asStruct();\n+    }\n+\n+    this.expr = Binder.bind(struct, Expressions.rewriteNot(expr), caseSensitive);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1ODg1OQ=="}, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1NDgyNw==", "bodyText": "The Parquet schema will match exactly because the IDs were added to the file schema. Adding IDs doesn't change the names, so meta.getPath() will be the same one used by fileSchema.getType(...).", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406854827", "createdAt": "2020-04-10T17:16:35Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "diffHunk": "@@ -75,6 +86,16 @@ public ParquetDictionaryRowGroupFilter(Schema schema, Expression unbound, boolea\n    */\n   public boolean shouldRead(MessageType fileSchema, BlockMetaData rowGroup,\n                             DictionaryPageReadStore dictionaries) {\n+    StructType struct;\n+\n+    if (nameMapping != null) {\n+      MessageType project = ParquetSchemaUtil.pruneColumnsByName(fileSchema, schema, nameMapping);\n+      struct = ParquetSchemaUtil.convert(project).asStruct();\n+    } else {\n+      struct = schema.asStruct();\n+    }\n+\n+    this.expr = Binder.bind(struct, Expressions.rewriteNot(expr), caseSensitive);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1ODg1OQ=="}, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwOTc0ODYwOnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetMetricsRowGroupFilter.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwMDowMDozMlrOGBuNqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwMDowMDozMlrOGBuNqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1ODkyMw==", "bodyText": "This file also should not be modified.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r404458923", "createdAt": "2020-04-07T00:00:32Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetMetricsRowGroupFilter.java", "diffHunk": "@@ -64,8 +68,14 @@ public ParquetMetricsRowGroupFilter(Schema schema, Expression unbound) {\n \n   public ParquetMetricsRowGroupFilter(Schema schema, Expression unbound, boolean caseSensitive) {\n     this.schema = schema;\n-    StructType struct = schema.asStruct();\n-    this.expr = Binder.bind(struct, Expressions.rewriteNot(unbound), caseSensitive);\n+    this.expr = unbound;\n+    this.caseSensitive = caseSensitive;\n+  }\n+\n+\n+  public ParquetMetricsRowGroupFilter withNameMapping(NameMapping newNameMapping) {\n+    this.nameMapping = newNameMapping;\n+    return this;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwOTc0OTk2OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetReadSupport.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwMDowMToyM1rOGBuOhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQwMTo0ODowMlrOGCcKEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1OTE0MA==", "bodyText": "This should not replace the existing fallback strategy. If the mapping is present, it should be used. Otherwise, the existing position-based fallback strategy should be used.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r404459140", "createdAt": "2020-04-07T00:01:23Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetReadSupport.java", "diffHunk": "@@ -57,7 +58,7 @@ public ReadContext init(Configuration configuration, Map<String, String> keyValu\n \n     MessageType projection = ParquetSchemaUtil.hasIds(fileSchema) ?\n         ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n-        ParquetSchemaUtil.pruneColumnsFallback(fileSchema, expectedSchema);\n+        ParquetSchemaUtil.pruneColumnsByName(fileSchema, expectedSchema, MappingUtil.create(expectedSchema));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTIxMTY2Nw==", "bodyText": "Agreed.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r405211667", "createdAt": "2020-04-08T01:48:02Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetReadSupport.java", "diffHunk": "@@ -57,7 +58,7 @@ public ReadContext init(Configuration configuration, Map<String, String> keyValu\n \n     MessageType projection = ParquetSchemaUtil.hasIds(fileSchema) ?\n         ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n-        ParquetSchemaUtil.pruneColumnsFallback(fileSchema, expectedSchema);\n+        ParquetSchemaUtil.pruneColumnsByName(fileSchema, expectedSchema, MappingUtil.create(expectedSchema));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1OTE0MA=="}, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwOTc1MTY2OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwMDowMjoxMlrOGBuPbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxMDozNjoxMVrOGB9bSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1OTM3NQ==", "bodyText": "No need for this annotation. Looks like we should have removed it in the other PR as well.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r404459375", "createdAt": "2020-04-07T00:02:12Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -59,42 +61,46 @@\n   private final boolean reuseContainers;\n   @Nullable\n   private final Integer batchSize;\n+  @Nullable", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcwODE2OQ==", "bodyText": "OK.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r404708169", "createdAt": "2020-04-07T10:36:11Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -59,42 +61,46 @@\n   private final boolean reuseContainers;\n   @Nullable\n   private final Integer batchSize;\n+  @Nullable", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1OTM3NQ=="}, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwOTc1NDY2OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwMDowMzo0OFrOGBuRRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQwMTo0Nzo0OFrOGCcJ3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1OTg0NA==", "bodyText": "This is not the right place to infer a name mapping. This class should apply the name mapping if it exists, and use a position-based fallback otherwise.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r404459844", "createdAt": "2020-04-07T00:03:48Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -59,42 +61,46 @@\n   private final boolean reuseContainers;\n   @Nullable\n   private final Integer batchSize;\n+  @Nullable\n+  private final NameMapping nameMapping;\n \n   // List of column chunk metadata for each row group\n   private final List<Map<ColumnPath, ColumnChunkMetaData>> columnChunkMetaDataForRowGroups;\n \n   @SuppressWarnings(\"unchecked\")\n   ReadConf(InputFile file, ParquetReadOptions options, Schema expectedSchema, Expression filter,\n            Function<MessageType, ParquetValueReader<?>> readerFunc, Function<MessageType,\n-           VectorizedReader<?>> batchedReaderFunc, boolean reuseContainers,\n+           VectorizedReader<?>> batchedReaderFunc, NameMapping nameMapping, boolean reuseContainers,\n            boolean caseSensitive, Integer bSize) {\n     this.file = file;\n     this.options = options;\n     this.reader = newReader(file, options);\n     MessageType fileSchema = reader.getFileMetaData().getSchema();\n-\n     boolean hasIds = ParquetSchemaUtil.hasIds(fileSchema);\n-    MessageType typeWithIds = hasIds ? fileSchema : ParquetSchemaUtil.addFallbackIds(fileSchema);\n+    this.nameMapping = nameMapping == null ? MappingUtil.create(expectedSchema) : nameMapping;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTIxMTYxMg==", "bodyText": "Agreed.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r405211612", "createdAt": "2020-04-08T01:47:48Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -59,42 +61,46 @@\n   private final boolean reuseContainers;\n   @Nullable\n   private final Integer batchSize;\n+  @Nullable\n+  private final NameMapping nameMapping;\n \n   // List of column chunk metadata for each row group\n   private final List<Map<ColumnPath, ColumnChunkMetaData>> columnChunkMetaDataForRowGroups;\n \n   @SuppressWarnings(\"unchecked\")\n   ReadConf(InputFile file, ParquetReadOptions options, Schema expectedSchema, Expression filter,\n            Function<MessageType, ParquetValueReader<?>> readerFunc, Function<MessageType,\n-           VectorizedReader<?>> batchedReaderFunc, boolean reuseContainers,\n+           VectorizedReader<?>> batchedReaderFunc, NameMapping nameMapping, boolean reuseContainers,\n            boolean caseSensitive, Integer bSize) {\n     this.file = file;\n     this.options = options;\n     this.reader = newReader(file, options);\n     MessageType fileSchema = reader.getFileMetaData().getSchema();\n-\n     boolean hasIds = ParquetSchemaUtil.hasIds(fileSchema);\n-    MessageType typeWithIds = hasIds ? fileSchema : ParquetSchemaUtil.addFallbackIds(fileSchema);\n+    this.nameMapping = nameMapping == null ? MappingUtil.create(expectedSchema) : nameMapping;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ1OTg0NA=="}, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwOTc1NzI4OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwMDowNTowMVrOGBuSxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxMjozNToxN1rOGDWx6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2MDIyOA==", "bodyText": "What was the purpose of this change?", "url": "https://github.com/apache/iceberg/pull/830#discussion_r404460228", "createdAt": "2020-04-07T00:05:01Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -59,42 +61,46 @@\n   private final boolean reuseContainers;\n   @Nullable\n   private final Integer batchSize;\n+  @Nullable\n+  private final NameMapping nameMapping;\n \n   // List of column chunk metadata for each row group\n   private final List<Map<ColumnPath, ColumnChunkMetaData>> columnChunkMetaDataForRowGroups;\n \n   @SuppressWarnings(\"unchecked\")\n   ReadConf(InputFile file, ParquetReadOptions options, Schema expectedSchema, Expression filter,\n            Function<MessageType, ParquetValueReader<?>> readerFunc, Function<MessageType,\n-           VectorizedReader<?>> batchedReaderFunc, boolean reuseContainers,\n+           VectorizedReader<?>> batchedReaderFunc, NameMapping nameMapping, boolean reuseContainers,\n            boolean caseSensitive, Integer bSize) {\n     this.file = file;\n     this.options = options;\n     this.reader = newReader(file, options);\n     MessageType fileSchema = reader.getFileMetaData().getSchema();\n-\n     boolean hasIds = ParquetSchemaUtil.hasIds(fileSchema);\n-    MessageType typeWithIds = hasIds ? fileSchema : ParquetSchemaUtil.addFallbackIds(fileSchema);\n+    this.nameMapping = nameMapping == null ? MappingUtil.create(expectedSchema) : nameMapping;\n \n     this.projection = hasIds ?\n-        ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n-        ParquetSchemaUtil.pruneColumnsFallback(fileSchema, expectedSchema);\n+            ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n+            ParquetSchemaUtil.pruneColumnsByName(fileSchema, expectedSchema, this.nameMapping);\n+\n     this.rowGroups = reader.getRowGroups();\n     this.shouldSkip = new boolean[rowGroups.size()];\n \n     ParquetMetricsRowGroupFilter statsFilter = null;\n     ParquetDictionaryRowGroupFilter dictFilter = null;\n     if (filter != null) {\n-      statsFilter = new ParquetMetricsRowGroupFilter(expectedSchema, filter, caseSensitive);\n-      dictFilter = new ParquetDictionaryRowGroupFilter(expectedSchema, filter, caseSensitive);\n+      statsFilter = new ParquetMetricsRowGroupFilter(expectedSchema, filter, caseSensitive)\n+              .withNameMapping(this.nameMapping);\n+      dictFilter = new ParquetDictionaryRowGroupFilter(expectedSchema, filter, caseSensitive)\n+              .withNameMapping(this.nameMapping);\n     }\n \n     long computedTotalValues = 0L;\n     for (int i = 0; i < shouldSkip.length; i += 1) {\n       BlockMetaData rowGroup = rowGroups.get(i);\n       boolean shouldRead = filter == null || (\n-          statsFilter.shouldRead(typeWithIds, rowGroup) &&\n-              dictFilter.shouldRead(typeWithIds, rowGroup, reader.getDictionaryReader(rowGroup)));\n+          statsFilter.shouldRead(fileSchema, rowGroup) &&\n+              dictFilter.shouldRead(fileSchema, rowGroup, reader.getDictionaryReader(rowGroup)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjE3MjEzNg==", "bodyText": "I reverted this back to typeWithIds", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406172136", "createdAt": "2020-04-09T12:35:17Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -59,42 +61,46 @@\n   private final boolean reuseContainers;\n   @Nullable\n   private final Integer batchSize;\n+  @Nullable\n+  private final NameMapping nameMapping;\n \n   // List of column chunk metadata for each row group\n   private final List<Map<ColumnPath, ColumnChunkMetaData>> columnChunkMetaDataForRowGroups;\n \n   @SuppressWarnings(\"unchecked\")\n   ReadConf(InputFile file, ParquetReadOptions options, Schema expectedSchema, Expression filter,\n            Function<MessageType, ParquetValueReader<?>> readerFunc, Function<MessageType,\n-           VectorizedReader<?>> batchedReaderFunc, boolean reuseContainers,\n+           VectorizedReader<?>> batchedReaderFunc, NameMapping nameMapping, boolean reuseContainers,\n            boolean caseSensitive, Integer bSize) {\n     this.file = file;\n     this.options = options;\n     this.reader = newReader(file, options);\n     MessageType fileSchema = reader.getFileMetaData().getSchema();\n-\n     boolean hasIds = ParquetSchemaUtil.hasIds(fileSchema);\n-    MessageType typeWithIds = hasIds ? fileSchema : ParquetSchemaUtil.addFallbackIds(fileSchema);\n+    this.nameMapping = nameMapping == null ? MappingUtil.create(expectedSchema) : nameMapping;\n \n     this.projection = hasIds ?\n-        ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n-        ParquetSchemaUtil.pruneColumnsFallback(fileSchema, expectedSchema);\n+            ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n+            ParquetSchemaUtil.pruneColumnsByName(fileSchema, expectedSchema, this.nameMapping);\n+\n     this.rowGroups = reader.getRowGroups();\n     this.shouldSkip = new boolean[rowGroups.size()];\n \n     ParquetMetricsRowGroupFilter statsFilter = null;\n     ParquetDictionaryRowGroupFilter dictFilter = null;\n     if (filter != null) {\n-      statsFilter = new ParquetMetricsRowGroupFilter(expectedSchema, filter, caseSensitive);\n-      dictFilter = new ParquetDictionaryRowGroupFilter(expectedSchema, filter, caseSensitive);\n+      statsFilter = new ParquetMetricsRowGroupFilter(expectedSchema, filter, caseSensitive)\n+              .withNameMapping(this.nameMapping);\n+      dictFilter = new ParquetDictionaryRowGroupFilter(expectedSchema, filter, caseSensitive)\n+              .withNameMapping(this.nameMapping);\n     }\n \n     long computedTotalValues = 0L;\n     for (int i = 0; i < shouldSkip.length; i += 1) {\n       BlockMetaData rowGroup = rowGroups.get(i);\n       boolean shouldRead = filter == null || (\n-          statsFilter.shouldRead(typeWithIds, rowGroup) &&\n-              dictFilter.shouldRead(typeWithIds, rowGroup, reader.getDictionaryReader(rowGroup)));\n+          statsFilter.shouldRead(fileSchema, rowGroup) &&\n+              dictFilter.shouldRead(fileSchema, rowGroup, reader.getDictionaryReader(rowGroup)));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2MDIyOA=="}, "originalCommit": {"oid": "e7a0ecbf699e23a39c26d5658a256391883fa403"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzE3NTYyOnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/MessageTypeToType.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNzo1Mjo0MFrOGCPU1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxMjozNjowOFrOGDWzhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAwMTQyOQ==", "bodyText": "Why was this changed?", "url": "https://github.com/apache/iceberg/pull/830#discussion_r405001429", "createdAt": "2020-04-07T17:52:40Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/MessageTypeToType.java", "diffHunk": "@@ -200,7 +200,7 @@ protected int nextId() {\n     return current;\n   }\n \n-  private int getId(org.apache.parquet.schema.Type type) {\n+  protected int getId(org.apache.parquet.schema.Type type) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88637dc34b90dbb11ac94bf1f19059909ec4f533"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTIwNzI5Ng==", "bodyText": "This is changed so that we can override getid and use that to detect whether a schema has ID or not.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r405207296", "createdAt": "2020-04-08T01:31:05Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/MessageTypeToType.java", "diffHunk": "@@ -200,7 +200,7 @@ protected int nextId() {\n     return current;\n   }\n \n-  private int getId(org.apache.parquet.schema.Type type) {\n+  protected int getId(org.apache.parquet.schema.Type type) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAwMTQyOQ=="}, "originalCommit": {"oid": "88637dc34b90dbb11ac94bf1f19059909ec4f533"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTYzNjIzMg==", "bodyText": "I think it might be better to make a new visitor for this, rather than reusing MessageTypeToType. That maintains independence between classes that don't have the same purpose. It was easy before to use MessageTypeToType without modification, but there's no need to do that if it is no longer easy.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r405636232", "createdAt": "2020-04-08T16:01:03Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/MessageTypeToType.java", "diffHunk": "@@ -200,7 +200,7 @@ protected int nextId() {\n     return current;\n   }\n \n-  private int getId(org.apache.parquet.schema.Type type) {\n+  protected int getId(org.apache.parquet.schema.Type type) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAwMTQyOQ=="}, "originalCommit": {"oid": "88637dc34b90dbb11ac94bf1f19059909ec4f533"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjE3MjU0OQ==", "bodyText": "OK, I will add a new one.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406172549", "createdAt": "2020-04-09T12:36:08Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/MessageTypeToType.java", "diffHunk": "@@ -200,7 +200,7 @@ protected int nextId() {\n     return current;\n   }\n \n-  private int getId(org.apache.parquet.schema.Type type) {\n+  protected int getId(org.apache.parquet.schema.Type type) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAwMTQyOQ=="}, "originalCommit": {"oid": "88637dc34b90dbb11ac94bf1f19059909ec4f533"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUxMzE4MDgxOnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNzo1NDowNFrOGCPYQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOFQxNTo1ODo1OVrOGC1_NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAwMjMwNQ==", "bodyText": "It is fine if not everything has an ID. Columns without IDs will be ignored and should not cause a read to fail.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r405002305", "createdAt": "2020-04-07T17:54:04Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -87,29 +82,21 @@ public static boolean hasIds(MessageType fileSchema) {\n       // Try to convert the type to Iceberg. If an ID assignment is needed, return false.\n       ParquetTypeVisitor.visit(fileSchema, new MessageTypeToType(fileSchema) {\n         @Override\n-        protected int nextId() {\n-          throw new IllegalStateException(\"Needed to assign ID\");\n+        protected int getId(org.apache.parquet.schema.Type type) {\n+          org.apache.parquet.schema.Type.ID id = type.getId();\n+          if (id != null) {\n+            throw new IllegalStateException(\"at least one ID exists\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "88637dc34b90dbb11ac94bf1f19059909ec4f533"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTIwODY5OQ==", "bodyText": "Did I misunderstand the comment?   I thought you meant file schema either has all columns with IDs or without ID. Do you mean that a file schema can have some of the columns with IDs and some without?\n\nThis also needs to update ParquetSchemaUtil.hasIds so that any ID causes it to return true, instead of any missing ID. This should be okay since files either have IDs or do not, so it is not a behavior change.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r405208699", "createdAt": "2020-04-08T01:36:30Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -87,29 +82,21 @@ public static boolean hasIds(MessageType fileSchema) {\n       // Try to convert the type to Iceberg. If an ID assignment is needed, return false.\n       ParquetTypeVisitor.visit(fileSchema, new MessageTypeToType(fileSchema) {\n         @Override\n-        protected int nextId() {\n-          throw new IllegalStateException(\"Needed to assign ID\");\n+        protected int getId(org.apache.parquet.schema.Type type) {\n+          org.apache.parquet.schema.Type.ID id = type.getId();\n+          if (id != null) {\n+            throw new IllegalStateException(\"at least one ID exists\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAwMjMwNQ=="}, "originalCommit": {"oid": "88637dc34b90dbb11ac94bf1f19059909ec4f533"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTYzNDg2OA==", "bodyText": "Yes, we expect schemas to either have all IDs or none, but we don't need to fail if that assumption is violated. If we detect any ID in the schema, then we should trust the file's IDs. If there are no IDs, then we should infer them using a name mapping or position-based fallback, whatever was configured.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r405634868", "createdAt": "2020-04-08T15:58:59Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -87,29 +82,21 @@ public static boolean hasIds(MessageType fileSchema) {\n       // Try to convert the type to Iceberg. If an ID assignment is needed, return false.\n       ParquetTypeVisitor.visit(fileSchema, new MessageTypeToType(fileSchema) {\n         @Override\n-        protected int nextId() {\n-          throw new IllegalStateException(\"Needed to assign ID\");\n+        protected int getId(org.apache.parquet.schema.Type type) {\n+          org.apache.parquet.schema.Type.ID id = type.getId();\n+          if (id != null) {\n+            throw new IllegalStateException(\"at least one ID exists\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAwMjMwNQ=="}, "originalCommit": {"oid": "88637dc34b90dbb11ac94bf1f19059909ec4f533"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyMTczODAzOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/avro/Avro.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNzoxMToxNFrOGDhwwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQxNzoxMToxNFrOGDhwwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjM1MjA2NA==", "bodyText": "@rdsr, just want to confirm that you're okay with this change? If it is going to cause you problems because you've already deployed it, we can go with the original.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406352064", "createdAt": "2020-04-09T17:11:14Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/avro/Avro.java", "diffHunk": "@@ -235,7 +235,7 @@ public ReadBuilder rename(String fullName, String newName) {\n       return this;\n     }\n \n-    public ReadBuilder nameMapping(NameMapping newNameMapping) {\n+    public ReadBuilder withNameMapping(NameMapping newNameMapping) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "35e22ff55ac1a743d006c206fa719b311075e1c4"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDk5NTY0OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "isResolved": true, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1ODo1OVrOGEAANA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwMjoxMzo1NVrOGcMmbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0NzU0MA==", "bodyText": "Do you have a test case where this is thrown?", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406847540", "createdAt": "2020-04-10T16:58:59Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "diffHunk": "@@ -108,11 +118,15 @@ private boolean eval(MessageType fileSchema, BlockMetaData rowGroup,\n       }\n \n       for (ColumnChunkMetaData meta : rowGroup.getColumns()) {\n-        PrimitiveType colType = fileSchema.getType(meta.getPath().toArray()).asPrimitiveType();\n-        if (colType.getId() != null) {\n-          int id = colType.getId().intValue();\n-          isFallback.put(id, ParquetUtil.hasNonDictionaryPages(meta));\n-          mayContainNulls.put(id, mayContainNull(meta));\n+        try {\n+          PrimitiveType colType = fileSchema.getType(meta.getPath().toArray()).asPrimitiveType();\n+          if (colType.getId() != null) {\n+            int id = colType.getId().intValue();\n+            isFallback.put(id, ParquetUtil.hasNonDictionaryPages(meta));\n+            mayContainNulls.put(id, mayContainNull(meta));\n+          }\n+        } catch (org.apache.parquet.io.InvalidRecordException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk5NTQ3Mg==", "bodyText": "Yes. The testImportWithIncompatibleSchema in TestSparkTableUtil.java. See:\n[Executor task launch worker for task 10] WARN org.apache.iceberg.parquet.ParquetMetricsRowGroupFilter - Column id not found in given schema.\norg.apache.parquet.io.InvalidRecordException: id not found in message spark_schema {\n  optional binary data (STRING) = 1;\n}\n\n\tat org.apache.parquet.schema.GroupType.getFieldIndex(GroupType.java:175)\n\tat org.apache.parquet.schema.GroupType.getType(GroupType.java:207)\n\tat org.apache.parquet.schema.GroupType.getType(GroupType.java:311)\n\tat org.apache.parquet.schema.MessageType.getType(MessageType.java:90)\n\tat org.apache.iceberg.parquet.ParquetMetricsRowGroupFilter$MetricsEvalVisitor.eval(ParquetMetricsRowGroupFilter.java:104)\n\tat org.apache.iceberg.parquet.ParquetMetricsRowGroupFilter$MetricsEvalVisitor.access$000(ParquetMetricsRowGroupFilter.java:89)\n\tat org.apache.iceberg.parquet.ParquetMetricsRowGroupFilter.shouldRead(ParquetMetricsRowGroupFilter.java:83)\n\tat org.apache.iceberg.parquet.ReadConf.<init>(ReadConf.java:95)\n\tat org.apache.iceberg.parquet.ParquetReader.init(ParquetReader.java:67)\n\tat org.apache.iceberg.parquet.ParquetReader.iterator(ParquetReader.java:77)\n\tat org.apache.iceberg.spark.source.RowDataReader.open(RowDataReader.java:157)\n\tat org.apache.iceberg.spark.source.RowDataReader.open(RowDataReader.java:119)\n\tat org.apache.iceberg.spark.source.BaseDataReader.next(BaseDataReader.java:79)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:49)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406995472", "createdAt": "2020-04-11T00:49:53Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "diffHunk": "@@ -108,11 +118,15 @@ private boolean eval(MessageType fileSchema, BlockMetaData rowGroup,\n       }\n \n       for (ColumnChunkMetaData meta : rowGroup.getColumns()) {\n-        PrimitiveType colType = fileSchema.getType(meta.getPath().toArray()).asPrimitiveType();\n-        if (colType.getId() != null) {\n-          int id = colType.getId().intValue();\n-          isFallback.put(id, ParquetUtil.hasNonDictionaryPages(meta));\n-          mayContainNulls.put(id, mayContainNull(meta));\n+        try {\n+          PrimitiveType colType = fileSchema.getType(meta.getPath().toArray()).asPrimitiveType();\n+          if (colType.getId() != null) {\n+            int id = colType.getId().intValue();\n+            isFallback.put(id, ParquetUtil.hasNonDictionaryPages(meta));\n+            mayContainNulls.put(id, mayContainNull(meta));\n+          }\n+        } catch (org.apache.parquet.io.InvalidRecordException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0NzU0MA=="}, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTcyMzk4OA==", "bodyText": "The problem here is that ApplyNameMapping is omitting fields that are in the file from the schema with field IDs. Instead of changing these filters, ApplyNameMapping should return all fields of the file schema, even if they are not assigned field IDs.\nBy doing that, we can assume that the file schema always matches the file metadata. That's important to avoid unexpected exceptions like this one: we're using the file's column metadata to look up a column in the schema and that should always work.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r421723988", "createdAt": "2020-05-07T18:56:26Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "diffHunk": "@@ -108,11 +118,15 @@ private boolean eval(MessageType fileSchema, BlockMetaData rowGroup,\n       }\n \n       for (ColumnChunkMetaData meta : rowGroup.getColumns()) {\n-        PrimitiveType colType = fileSchema.getType(meta.getPath().toArray()).asPrimitiveType();\n-        if (colType.getId() != null) {\n-          int id = colType.getId().intValue();\n-          isFallback.put(id, ParquetUtil.hasNonDictionaryPages(meta));\n-          mayContainNulls.put(id, mayContainNull(meta));\n+        try {\n+          PrimitiveType colType = fileSchema.getType(meta.getPath().toArray()).asPrimitiveType();\n+          if (colType.getId() != null) {\n+            int id = colType.getId().intValue();\n+            isFallback.put(id, ParquetUtil.hasNonDictionaryPages(meta));\n+            mayContainNulls.put(id, mayContainNull(meta));\n+          }\n+        } catch (org.apache.parquet.io.InvalidRecordException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0NzU0MA=="}, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODkwMTAzMQ==", "bodyText": "@chenjunjiedada, this needs to be fixed so that there are no changes to the filter implementations.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r428901031", "createdAt": "2020-05-21T20:38:50Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "diffHunk": "@@ -108,11 +118,15 @@ private boolean eval(MessageType fileSchema, BlockMetaData rowGroup,\n       }\n \n       for (ColumnChunkMetaData meta : rowGroup.getColumns()) {\n-        PrimitiveType colType = fileSchema.getType(meta.getPath().toArray()).asPrimitiveType();\n-        if (colType.getId() != null) {\n-          int id = colType.getId().intValue();\n-          isFallback.put(id, ParquetUtil.hasNonDictionaryPages(meta));\n-          mayContainNulls.put(id, mayContainNull(meta));\n+        try {\n+          PrimitiveType colType = fileSchema.getType(meta.getPath().toArray()).asPrimitiveType();\n+          if (colType.getId() != null) {\n+            int id = colType.getId().intValue();\n+            isFallback.put(id, ParquetUtil.hasNonDictionaryPages(meta));\n+            mayContainNulls.put(id, mayContainNull(meta));\n+          }\n+        } catch (org.apache.parquet.io.InvalidRecordException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0NzU0MA=="}, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTAzMDM0NQ==", "bodyText": "I tried to remove the try...catch... here and let ApplyNameMapping to return the schema with partial IDs assigned. That causes problems when we visit the parquet type.\nFirstly, the PruneColumns visitor assumes the typeWithIds is fully assigned with IDs. Its getId has a precondition check. A file schema with partially assigned IDs cannot pass the condition check.\nSecondly, the HasId logic also has the same assumption, which makes SparkParquetReader throws NPE when it calls filedType.getId().intValue().", "url": "https://github.com/apache/iceberg/pull/830#discussion_r429030345", "createdAt": "2020-05-22T03:58:36Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "diffHunk": "@@ -108,11 +118,15 @@ private boolean eval(MessageType fileSchema, BlockMetaData rowGroup,\n       }\n \n       for (ColumnChunkMetaData meta : rowGroup.getColumns()) {\n-        PrimitiveType colType = fileSchema.getType(meta.getPath().toArray()).asPrimitiveType();\n-        if (colType.getId() != null) {\n-          int id = colType.getId().intValue();\n-          isFallback.put(id, ParquetUtil.hasNonDictionaryPages(meta));\n-          mayContainNulls.put(id, mayContainNull(meta));\n+        try {\n+          PrimitiveType colType = fileSchema.getType(meta.getPath().toArray()).asPrimitiveType();\n+          if (colType.getId() != null) {\n+            int id = colType.getId().intValue();\n+            isFallback.put(id, ParquetUtil.hasNonDictionaryPages(meta));\n+            mayContainNulls.put(id, mayContainNull(meta));\n+          }\n+        } catch (org.apache.parquet.io.InvalidRecordException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0NzU0MA=="}, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQyNjM5Mg==", "bodyText": "HasId should definitely succeed if there are fields without them. Its purpose is to check and we expect it to encounter schemas without IDs. Why is HasId being run on a schema that has had a name mapping applied?\nIf PruneColumns assumes all IDs are there, then I think we should fix that assumption.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r429426392", "createdAt": "2020-05-22T19:44:27Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "diffHunk": "@@ -108,11 +118,15 @@ private boolean eval(MessageType fileSchema, BlockMetaData rowGroup,\n       }\n \n       for (ColumnChunkMetaData meta : rowGroup.getColumns()) {\n-        PrimitiveType colType = fileSchema.getType(meta.getPath().toArray()).asPrimitiveType();\n-        if (colType.getId() != null) {\n-          int id = colType.getId().intValue();\n-          isFallback.put(id, ParquetUtil.hasNonDictionaryPages(meta));\n-          mayContainNulls.put(id, mayContainNull(meta));\n+        try {\n+          PrimitiveType colType = fileSchema.getType(meta.getPath().toArray()).asPrimitiveType();\n+          if (colType.getId() != null) {\n+            int id = colType.getId().intValue();\n+            isFallback.put(id, ParquetUtil.hasNonDictionaryPages(meta));\n+            mayContainNulls.put(id, mayContainNull(meta));\n+          }\n+        } catch (org.apache.parquet.io.InvalidRecordException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0NzU0MA=="}, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUwOTIzMg==", "bodyText": "SparkParquetReader build parquet reader according to typeWithIds passed from ReadConf.java. The HasId is used in ReadConf.java to assign IDs to file schema, and then it is used in SparkParquetReader to determine whether passing files schema has assigned IDs.\nBesides PruneColumns, The ReadBuilder also assumes all IDs assigned.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r429509232", "createdAt": "2020-05-23T03:19:15Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "diffHunk": "@@ -108,11 +118,15 @@ private boolean eval(MessageType fileSchema, BlockMetaData rowGroup,\n       }\n \n       for (ColumnChunkMetaData meta : rowGroup.getColumns()) {\n-        PrimitiveType colType = fileSchema.getType(meta.getPath().toArray()).asPrimitiveType();\n-        if (colType.getId() != null) {\n-          int id = colType.getId().intValue();\n-          isFallback.put(id, ParquetUtil.hasNonDictionaryPages(meta));\n-          mayContainNulls.put(id, mayContainNull(meta));\n+        try {\n+          PrimitiveType colType = fileSchema.getType(meta.getPath().toArray()).asPrimitiveType();\n+          if (colType.getId() != null) {\n+            int id = colType.getId().intValue();\n+            isFallback.put(id, ParquetUtil.hasNonDictionaryPages(meta));\n+            mayContainNulls.put(id, mayContainNull(meta));\n+          }\n+        } catch (org.apache.parquet.io.InvalidRecordException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0NzU0MA=="}, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjIxOTc1Ng==", "bodyText": "OK, Just fixed this.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r432219756", "createdAt": "2020-05-29T02:13:55Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetDictionaryRowGroupFilter.java", "diffHunk": "@@ -108,11 +118,15 @@ private boolean eval(MessageType fileSchema, BlockMetaData rowGroup,\n       }\n \n       for (ColumnChunkMetaData meta : rowGroup.getColumns()) {\n-        PrimitiveType colType = fileSchema.getType(meta.getPath().toArray()).asPrimitiveType();\n-        if (colType.getId() != null) {\n-          int id = colType.getId().intValue();\n-          isFallback.put(id, ParquetUtil.hasNonDictionaryPages(meta));\n-          mayContainNulls.put(id, mayContainNull(meta));\n+        try {\n+          PrimitiveType colType = fileSchema.getType(meta.getPath().toArray()).asPrimitiveType();\n+          if (colType.getId() != null) {\n+            int id = colType.getId().intValue();\n+            isFallback.put(id, ParquetUtil.hasNonDictionaryPages(meta));\n+            mayContainNulls.put(id, mayContainNull(meta));\n+          }\n+        } catch (org.apache.parquet.io.InvalidRecordException e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0NzU0MA=="}, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNDk5NzY2OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetMetricsRowGroupFilter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNjo1OTo0M1rOGEABew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQwMDo0ODoxNVrOGEJBHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0Nzg2Nw==", "bodyText": "Please revert unnecessary changes and move expression binding back into the constructor.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406847867", "createdAt": "2020-04-10T16:59:43Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetMetricsRowGroupFilter.java", "diffHunk": "@@ -76,6 +81,9 @@ public ParquetMetricsRowGroupFilter(Schema schema, Expression unbound, boolean c\n    * @return false if the file cannot contain rows that match the expression, true otherwise.\n    */\n   public boolean shouldRead(MessageType fileSchema, BlockMetaData rowGroup) {\n+    StructType struct = schema.asStruct();\n+    this.expr = Binder.bind(struct, Expressions.rewriteNot(expr), caseSensitive);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk5NTIzMQ==", "bodyText": "Sure.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406995231", "createdAt": "2020-04-11T00:48:15Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetMetricsRowGroupFilter.java", "diffHunk": "@@ -76,6 +81,9 @@ public ParquetMetricsRowGroupFilter(Schema schema, Expression unbound, boolean c\n    * @return false if the file cannot contain rows that match the expression, true otherwise.\n    */\n   public boolean shouldRead(MessageType fileSchema, BlockMetaData rowGroup) {\n+    StructType struct = schema.asStruct();\n+    this.expr = Binder.bind(struct, Expressions.rewriteNot(expr), caseSensitive);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg0Nzg2Nw=="}, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNTAxNzg5OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNzowNzozNFrOGEAN7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQwMDo1MDoyMlrOGEJCTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1MTA1Mg==", "bodyText": "Let's pass nameMapping to addFallbackIds and have that choose whether to assign using the mapping or by position.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406851052", "createdAt": "2020-04-10T17:07:34Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -66,19 +63,27 @@\n   @SuppressWarnings(\"unchecked\")\n   ReadConf(InputFile file, ParquetReadOptions options, Schema expectedSchema, Expression filter,\n            Function<MessageType, ParquetValueReader<?>> readerFunc, Function<MessageType,\n-           VectorizedReader<?>> batchedReaderFunc, boolean reuseContainers,\n+           VectorizedReader<?>> batchedReaderFunc, NameMapping nameMapping, boolean reuseContainers,\n            boolean caseSensitive, Integer bSize) {\n     this.file = file;\n     this.options = options;\n     this.reader = newReader(file, options);\n     MessageType fileSchema = reader.getFileMetaData().getSchema();\n-\n     boolean hasIds = ParquetSchemaUtil.hasIds(fileSchema);\n-    MessageType typeWithIds = hasIds ? fileSchema : ParquetSchemaUtil.addFallbackIds(fileSchema);\n+    MessageType typeWithIds;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk5NTUzMg==", "bodyText": "OK.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406995532", "createdAt": "2020-04-11T00:50:22Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -66,19 +63,27 @@\n   @SuppressWarnings(\"unchecked\")\n   ReadConf(InputFile file, ParquetReadOptions options, Schema expectedSchema, Expression filter,\n            Function<MessageType, ParquetValueReader<?>> readerFunc, Function<MessageType,\n-           VectorizedReader<?>> batchedReaderFunc, boolean reuseContainers,\n+           VectorizedReader<?>> batchedReaderFunc, NameMapping nameMapping, boolean reuseContainers,\n            boolean caseSensitive, Integer bSize) {\n     this.file = file;\n     this.options = options;\n     this.reader = newReader(file, options);\n     MessageType fileSchema = reader.getFileMetaData().getSchema();\n-\n     boolean hasIds = ParquetSchemaUtil.hasIds(fileSchema);\n-    MessageType typeWithIds = hasIds ? fileSchema : ParquetSchemaUtil.addFallbackIds(fileSchema);\n+    MessageType typeWithIds;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1MTA1Mg=="}, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNTAyMDUzOnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNzowODozN1rOGEAPew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQwMDo0NjoyMlrOGEJAIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1MTQ1MQ==", "bodyText": "This is should recursively assign IDs using a visitor, not just assign to the top-level.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406851451", "createdAt": "2020-04-10T17:08:37Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -112,4 +127,56 @@ public static MessageType addFallbackIds(MessageType fileSchema) {\n \n     return builder.named(fileSchema.getName());\n   }\n+\n+  public static MessageType addFallbackIds(MessageType fileSchema, NameMapping nameMapping) {\n+    MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1MTU2MQ==", "bodyText": "I think we need a test suite for this assignment, too.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406851561", "createdAt": "2020-04-10T17:08:52Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -112,4 +127,56 @@ public static MessageType addFallbackIds(MessageType fileSchema) {\n \n     return builder.named(fileSchema.getName());\n   }\n+\n+  public static MessageType addFallbackIds(MessageType fileSchema, NameMapping nameMapping) {\n+    MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1MTQ1MQ=="}, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk5NDk3OQ==", "bodyText": "Hmm. My bad. Will update in the next commit.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406994979", "createdAt": "2020-04-11T00:46:22Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -112,4 +127,56 @@ public static MessageType addFallbackIds(MessageType fileSchema) {\n \n     return builder.named(fileSchema.getName());\n   }\n+\n+  public static MessageType addFallbackIds(MessageType fileSchema, NameMapping nameMapping) {\n+    MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1MTQ1MQ=="}, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNTAyMjU0OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNzowOToxN1rOGEAQrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNzowOToxN1rOGEAQrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1MTc1OA==", "bodyText": "This looks good.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406851758", "createdAt": "2020-04-10T17:09:17Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -112,4 +127,56 @@ public static MessageType addFallbackIds(MessageType fileSchema) {\n \n     return builder.named(fileSchema.getName());\n   }\n+\n+  public static MessageType addFallbackIds(MessageType fileSchema, NameMapping nameMapping) {\n+    MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+\n+    for (Type type : fileSchema.getFields()) {\n+      if (nameMapping.find(type.getName()) != null) {\n+        builder.addField(type.withId(nameMapping.find(type.getName()).id()));\n+      }\n+    }\n+\n+    return builder.named(fileSchema.getName());\n+  }\n+\n+  public static class HasIds extends ParquetTypeVisitor<Boolean> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNTAyOTY4OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetReadSupport.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNzoxMjoxMVrOGEAVIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQwMDo0Nzo1MFrOGEJA6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1Mjg5Ng==", "bodyText": "When a name mapping is used, column pruning should happen using the normal pruneColumns. The reason why the other fallback path uses pruneColumnsFallback is because ordinal IDs can only be assigned to the top-level columns and so projection must only use top-level column IDs. That's not a limitation of name mapping, so we can use normal pruning once the IDs are filled in.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406852896", "createdAt": "2020-04-10T17:12:11Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetReadSupport.java", "diffHunk": "@@ -55,9 +58,16 @@ public ReadContext init(Configuration configuration, Map<String, String> keyValu\n     // matching to the file's columns by full path, so this must select columns by using the path\n     // in the file's schema.\n \n-    MessageType projection = ParquetSchemaUtil.hasIds(fileSchema) ?\n-        ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n-        ParquetSchemaUtil.pruneColumnsFallback(fileSchema, expectedSchema);\n+    MessageType projection;\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      projection = ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema);\n+    } else {\n+      if (nameMapping != null) {\n+        projection = ParquetSchemaUtil.pruneColumnsByName(fileSchema, expectedSchema, nameMapping);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk5NTE3OQ==", "bodyText": "I got your point now. Will remove pruneColumnsByName.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406995179", "createdAt": "2020-04-11T00:47:50Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetReadSupport.java", "diffHunk": "@@ -55,9 +58,16 @@ public ReadContext init(Configuration configuration, Map<String, String> keyValu\n     // matching to the file's columns by full path, so this must select columns by using the path\n     // in the file's schema.\n \n-    MessageType projection = ParquetSchemaUtil.hasIds(fileSchema) ?\n-        ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n-        ParquetSchemaUtil.pruneColumnsFallback(fileSchema, expectedSchema);\n+    MessageType projection;\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      projection = ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema);\n+    } else {\n+      if (nameMapping != null) {\n+        projection = ParquetSchemaUtil.pruneColumnsByName(fileSchema, expectedSchema, nameMapping);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1Mjg5Ng=="}, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNTAzMjI0OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNzoxMjo1OVrOGEAWsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQwMDo0ODowMlrOGEJBCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1MzI5OA==", "bodyText": "We don't need this method. Instead, let's add the IDs and then use normal schema pruning.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406853298", "createdAt": "2020-04-10T17:12:59Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -82,23 +86,34 @@ public static MessageType pruneColumnsFallback(MessageType fileSchema, Schema ex\n     return builder.named(fileSchema.getName());\n   }\n \n-  public static boolean hasIds(MessageType fileSchema) {\n-    try {\n-      // Try to convert the type to Iceberg. If an ID assignment is needed, return false.\n-      ParquetTypeVisitor.visit(fileSchema, new MessageTypeToType(fileSchema) {\n-        @Override\n-        protected int nextId() {\n-          throw new IllegalStateException(\"Needed to assign ID\");\n-        }\n-      });\n+  /**\n+   * Prunes columns from a Parquet file schema that was written without field ids.\n+   * The order of columns in the resulting Parquet schema matches the Parquet file.\n+   *\n+   * @param fileSchema schema from a Parquet file that does not have field ids.\n+   * @param expectedSchema expected schema\n+   * @return a parquet schema pruned using the expected schema\n+   */\n+  public static MessageType pruneColumnsByName(MessageType fileSchema, Schema expectedSchema, NameMapping nameMapping) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk5NTIwOA==", "bodyText": "Agreed.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406995208", "createdAt": "2020-04-11T00:48:02Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -82,23 +86,34 @@ public static MessageType pruneColumnsFallback(MessageType fileSchema, Schema ex\n     return builder.named(fileSchema.getName());\n   }\n \n-  public static boolean hasIds(MessageType fileSchema) {\n-    try {\n-      // Try to convert the type to Iceberg. If an ID assignment is needed, return false.\n-      ParquetTypeVisitor.visit(fileSchema, new MessageTypeToType(fileSchema) {\n-        @Override\n-        protected int nextId() {\n-          throw new IllegalStateException(\"Needed to assign ID\");\n-        }\n-      });\n+  /**\n+   * Prunes columns from a Parquet file schema that was written without field ids.\n+   * The order of columns in the resulting Parquet schema matches the Parquet file.\n+   *\n+   * @param fileSchema schema from a Parquet file that does not have field ids.\n+   * @param expectedSchema expected schema\n+   * @return a parquet schema pruned using the expected schema\n+   */\n+  public static MessageType pruneColumnsByName(MessageType fileSchema, Schema expectedSchema, NameMapping nameMapping) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1MzI5OA=="}, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNTA0NTY0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNzoxODowMFrOGEAe4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQwMDo1MToyM1rOGEJCxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1NTM5Mw==", "bodyText": "I don't think a name-based reader is necessary. Instead, the file schema passed to the builder should have IDs assigned.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406855393", "createdAt": "2020-04-10T17:18:00Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java", "diffHunk": "@@ -111,6 +124,49 @@ private SparkParquetReaders() {\n     }\n   }\n \n+  private static class NameBasedReadBuilder extends ReadBuilder {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk5NTY1Mg==", "bodyText": "I 'm OK to remove this to make the parquet reader consistent.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406995652", "createdAt": "2020-04-11T00:51:23Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java", "diffHunk": "@@ -111,6 +124,49 @@ private SparkParquetReaders() {\n     }\n   }\n \n+  private static class NameBasedReadBuilder extends ReadBuilder {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1NTM5Mw=="}, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNTA0NzIzOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMFQxNzoxODozMlrOGEAf2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQwMDo1Mjo0NVrOGEJDgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1NTY0MQ==", "bodyText": "The fileSchema passed here should by the type with IDs filled in.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406855641", "createdAt": "2020-04-10T17:18:32Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java", "diffHunk": "@@ -171,10 +173,12 @@\n       InputFile location,\n       FileScanTask task,\n       Schema readSchema) {\n+    NameMapping nameMapping = MappingUtil.create(readSchema);\n     return Parquet.read(location)\n         .project(readSchema)\n+        .withNameMapping(nameMapping)\n         .split(task.start(), task.length())\n-        .createReaderFunc(fileSchema -> SparkParquetReaders.buildReader(readSchema, fileSchema))\n+        .createReaderFunc(fileSchema -> SparkParquetReaders.buildReader(readSchema, fileSchema, nameMapping))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk5NTg0Mg==", "bodyText": "Agreed, I think we don't need the third parameter namemapping as well.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r406995842", "createdAt": "2020-04-11T00:52:45Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java", "diffHunk": "@@ -171,10 +173,12 @@\n       InputFile location,\n       FileScanTask task,\n       Schema readSchema) {\n+    NameMapping nameMapping = MappingUtil.create(readSchema);\n     return Parquet.read(location)\n         .project(readSchema)\n+        .withNameMapping(nameMapping)\n         .split(task.start(), task.length())\n-        .createReaderFunc(fileSchema -> SparkParquetReaders.buildReader(readSchema, fileSchema))\n+        .createReaderFunc(fileSchema -> SparkParquetReaders.buildReader(readSchema, fileSchema, nameMapping))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjg1NTY0MQ=="}, "originalCommit": {"oid": "07ef73d3390826231d4f8d9f1908af23c9cb872e"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNjU0NDMzOnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQxMzo1MzoxNlrOGENYBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDo0NjoyN1rOGQs9AA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA2NjYyOQ==", "bodyText": "I add an extra parameter here because the field names built from the original ParquetTypeVisitor are different in name mapping. For example, if we have a field consists of a list of the map type like below:\noptional(1, \"list_of_maps\",Types.ListType.ofOptional(2, Types.MapType.ofOptional(3, 4,\n    Types.StringType.get(),Types.StringType.get());\n\nthe field names of the map key and value are list_of_map.list.element.map.key and list_of_map.list.element.map.value. While the names in name mapping are list_of_map.element.key and list_of_map.element.value.\nSince the NameMapping is used by Avro already, so I update ParquetTypeVisitor a bit to handle this.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r407066629", "createdAt": "2020-04-11T13:53:16Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java", "diffHunk": "@@ -34,9 +34,13 @@\n   protected Deque<String> fieldNames = Lists.newLinkedList();\n \n   public static <T> T visit(Type type, ParquetTypeVisitor<T> visitor) {\n+    return visit(type, visitor, false);\n+  }\n+\n+  public static <T> T visit(Type type, ParquetTypeVisitor<T> visitor, Boolean ignoreRepeated) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3a211f324bf2595071044dfff9522938762e8fa"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3ODY1MQ==", "bodyText": "Thanks for the explanation, this will be useful when I take a closer look.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r407078651", "createdAt": "2020-04-11T15:46:54Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java", "diffHunk": "@@ -34,9 +34,13 @@\n   protected Deque<String> fieldNames = Lists.newLinkedList();\n \n   public static <T> T visit(Type type, ParquetTypeVisitor<T> visitor) {\n+    return visit(type, visitor, false);\n+  }\n+\n+  public static <T> T visit(Type type, ParquetTypeVisitor<T> visitor, Boolean ignoreRepeated) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA2NjYyOQ=="}, "originalCommit": {"oid": "e3a211f324bf2595071044dfff9522938762e8fa"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTgwMDk2MA==", "bodyText": "I think we should use an alternative approach. I opened #1001 for this.\nThe problem with this is that it adds a parameter to customize behavior, when that should be the responsibility of the visitor. This is also a bit confusing because it isn't clear why some callers would want to skip repeated fields. It is better to delegate this to the visitor.\nThe PR I opened allows you to do that. It adds before and after callbacks for the repeated fields (element and key_value), as well as for the element, key, and value types. By default, those call beforeField and afterField, which are used to maintain the field names list. With that change, you can simply override the repeated element and repeated key value calls to do nothing:\n  @Override\n  public void beforeRepeatedElement(Type element) {\n    // do not add the repeated element's name\n  }\n\n  @Override\n  public void afterRepeatedElement(Type element) {\n    // do not remove the repeated element's name\n  }\n\n  @Override\n  public void beforeRepeatedKeyValue(Type keyValue) {\n    // do not add the repeated element's name\n  }\n\n  @Override\n  public void afterRepeatedKeyValue(Type keyValue) {\n    // do not remove the repeated element's name\n  }\nThat passes all of the tests.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r419800960", "createdAt": "2020-05-05T00:15:28Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java", "diffHunk": "@@ -34,9 +34,13 @@\n   protected Deque<String> fieldNames = Lists.newLinkedList();\n \n   public static <T> T visit(Type type, ParquetTypeVisitor<T> visitor) {\n+    return visit(type, visitor, false);\n+  }\n+\n+  public static <T> T visit(Type type, ParquetTypeVisitor<T> visitor, Boolean ignoreRepeated) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA2NjYyOQ=="}, "originalCommit": {"oid": "e3a211f324bf2595071044dfff9522938762e8fa"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE2NjkxMg==", "bodyText": "Great! I will update after the PR merged.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r420166912", "createdAt": "2020-05-05T14:46:27Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java", "diffHunk": "@@ -34,9 +34,13 @@\n   protected Deque<String> fieldNames = Lists.newLinkedList();\n \n   public static <T> T visit(Type type, ParquetTypeVisitor<T> visitor) {\n+    return visit(type, visitor, false);\n+  }\n+\n+  public static <T> T visit(Type type, ParquetTypeVisitor<T> visitor, Boolean ignoreRepeated) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA2NjYyOQ=="}, "originalCommit": {"oid": "e3a211f324bf2595071044dfff9522938762e8fa"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMzE0NzU4OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQwMDowOToxNFrOGQWf7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDo0ODoyOFrOGQtCbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5OTAyMw==", "bodyText": "Minor: this could be simplified to return hasId || array.getId() != null.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r419799023", "createdAt": "2020-05-05T00:09:14Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -83,33 +92,144 @@ public static MessageType pruneColumnsFallback(MessageType fileSchema, Schema ex\n   }\n \n   public static boolean hasIds(MessageType fileSchema) {\n-    try {\n-      // Try to convert the type to Iceberg. If an ID assignment is needed, return false.\n-      ParquetTypeVisitor.visit(fileSchema, new MessageTypeToType(fileSchema) {\n-        @Override\n-        protected int nextId() {\n-          throw new IllegalStateException(\"Needed to assign ID\");\n+    return ParquetTypeVisitor.visit(fileSchema, new HasIds(), true);\n+  }\n+\n+  public static MessageType addFallbackIds(MessageType fileSchema, NameMapping nameMapping) {\n+    if (nameMapping == null) {\n+      MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+\n+      int ordinal = 1; // ids are assigned starting at 1\n+      for (Type type : fileSchema.getFields()) {\n+        builder.addField(type.withId(ordinal));\n+        ordinal += 1;\n+      }\n+\n+      return builder.named(fileSchema.getName());\n+    } else {\n+      return (MessageType) ParquetTypeVisitor.visit(fileSchema, new AssignIdsByNameMapping(nameMapping), true);\n+    }\n+  }\n+\n+  public static class HasIds extends ParquetTypeVisitor<Boolean> {\n+    @Override\n+    public Boolean message(MessageType message, List<Boolean> fields) {\n+      return struct(message, fields);\n+    }\n+\n+    @Override\n+    public Boolean struct(GroupType struct, List<Boolean> hasIds) {\n+      for (Boolean hasId : hasIds) {\n+        if (hasId) {\n+          return true;\n         }\n-      });\n+      }\n+      return struct.getId() != null;\n+    }\n+\n+    @Override\n+    public Boolean list(GroupType array, Boolean hasId) {\n+      if (hasId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1115034c0db5ded6a7feab0a3cf7e8bbd1e24d9"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE2ODMwMA==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r420168300", "createdAt": "2020-05-05T14:48:28Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -83,33 +92,144 @@ public static MessageType pruneColumnsFallback(MessageType fileSchema, Schema ex\n   }\n \n   public static boolean hasIds(MessageType fileSchema) {\n-    try {\n-      // Try to convert the type to Iceberg. If an ID assignment is needed, return false.\n-      ParquetTypeVisitor.visit(fileSchema, new MessageTypeToType(fileSchema) {\n-        @Override\n-        protected int nextId() {\n-          throw new IllegalStateException(\"Needed to assign ID\");\n+    return ParquetTypeVisitor.visit(fileSchema, new HasIds(), true);\n+  }\n+\n+  public static MessageType addFallbackIds(MessageType fileSchema, NameMapping nameMapping) {\n+    if (nameMapping == null) {\n+      MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+\n+      int ordinal = 1; // ids are assigned starting at 1\n+      for (Type type : fileSchema.getFields()) {\n+        builder.addField(type.withId(ordinal));\n+        ordinal += 1;\n+      }\n+\n+      return builder.named(fileSchema.getName());\n+    } else {\n+      return (MessageType) ParquetTypeVisitor.visit(fileSchema, new AssignIdsByNameMapping(nameMapping), true);\n+    }\n+  }\n+\n+  public static class HasIds extends ParquetTypeVisitor<Boolean> {\n+    @Override\n+    public Boolean message(MessageType message, List<Boolean> fields) {\n+      return struct(message, fields);\n+    }\n+\n+    @Override\n+    public Boolean struct(GroupType struct, List<Boolean> hasIds) {\n+      for (Boolean hasId : hasIds) {\n+        if (hasId) {\n+          return true;\n         }\n-      });\n+      }\n+      return struct.getId() != null;\n+    }\n+\n+    @Override\n+    public Boolean list(GroupType array, Boolean hasId) {\n+      if (hasId) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5OTAyMw=="}, "originalCommit": {"oid": "f1115034c0db5ded6a7feab0a3cf7e8bbd1e24d9"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMzE0ODMxOnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQwMDowOTozN1rOGQWgWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDo0ODoyMlrOGQtCKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5OTEyOA==", "bodyText": "This could also be simplified to return keyHasId || valueHasId || map.getId() != null", "url": "https://github.com/apache/iceberg/pull/830#discussion_r419799128", "createdAt": "2020-05-05T00:09:37Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -83,33 +92,144 @@ public static MessageType pruneColumnsFallback(MessageType fileSchema, Schema ex\n   }\n \n   public static boolean hasIds(MessageType fileSchema) {\n-    try {\n-      // Try to convert the type to Iceberg. If an ID assignment is needed, return false.\n-      ParquetTypeVisitor.visit(fileSchema, new MessageTypeToType(fileSchema) {\n-        @Override\n-        protected int nextId() {\n-          throw new IllegalStateException(\"Needed to assign ID\");\n+    return ParquetTypeVisitor.visit(fileSchema, new HasIds(), true);\n+  }\n+\n+  public static MessageType addFallbackIds(MessageType fileSchema, NameMapping nameMapping) {\n+    if (nameMapping == null) {\n+      MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+\n+      int ordinal = 1; // ids are assigned starting at 1\n+      for (Type type : fileSchema.getFields()) {\n+        builder.addField(type.withId(ordinal));\n+        ordinal += 1;\n+      }\n+\n+      return builder.named(fileSchema.getName());\n+    } else {\n+      return (MessageType) ParquetTypeVisitor.visit(fileSchema, new AssignIdsByNameMapping(nameMapping), true);\n+    }\n+  }\n+\n+  public static class HasIds extends ParquetTypeVisitor<Boolean> {\n+    @Override\n+    public Boolean message(MessageType message, List<Boolean> fields) {\n+      return struct(message, fields);\n+    }\n+\n+    @Override\n+    public Boolean struct(GroupType struct, List<Boolean> hasIds) {\n+      for (Boolean hasId : hasIds) {\n+        if (hasId) {\n+          return true;\n         }\n-      });\n+      }\n+      return struct.getId() != null;\n+    }\n+\n+    @Override\n+    public Boolean list(GroupType array, Boolean hasId) {\n+      if (hasId) {\n+        return true;\n+      } else {\n+        return array.getId() != null;\n+      }\n+    }\n \n-      // no assignment was needed\n-      return true;\n+    @Override\n+    public Boolean map(GroupType map, Boolean keyHasId, Boolean valueHasId) {\n+      if (keyHasId || valueHasId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1115034c0db5ded6a7feab0a3cf7e8bbd1e24d9"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE2ODIzNQ==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r420168235", "createdAt": "2020-05-05T14:48:22Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -83,33 +92,144 @@ public static MessageType pruneColumnsFallback(MessageType fileSchema, Schema ex\n   }\n \n   public static boolean hasIds(MessageType fileSchema) {\n-    try {\n-      // Try to convert the type to Iceberg. If an ID assignment is needed, return false.\n-      ParquetTypeVisitor.visit(fileSchema, new MessageTypeToType(fileSchema) {\n-        @Override\n-        protected int nextId() {\n-          throw new IllegalStateException(\"Needed to assign ID\");\n+    return ParquetTypeVisitor.visit(fileSchema, new HasIds(), true);\n+  }\n+\n+  public static MessageType addFallbackIds(MessageType fileSchema, NameMapping nameMapping) {\n+    if (nameMapping == null) {\n+      MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+\n+      int ordinal = 1; // ids are assigned starting at 1\n+      for (Type type : fileSchema.getFields()) {\n+        builder.addField(type.withId(ordinal));\n+        ordinal += 1;\n+      }\n+\n+      return builder.named(fileSchema.getName());\n+    } else {\n+      return (MessageType) ParquetTypeVisitor.visit(fileSchema, new AssignIdsByNameMapping(nameMapping), true);\n+    }\n+  }\n+\n+  public static class HasIds extends ParquetTypeVisitor<Boolean> {\n+    @Override\n+    public Boolean message(MessageType message, List<Boolean> fields) {\n+      return struct(message, fields);\n+    }\n+\n+    @Override\n+    public Boolean struct(GroupType struct, List<Boolean> hasIds) {\n+      for (Boolean hasId : hasIds) {\n+        if (hasId) {\n+          return true;\n         }\n-      });\n+      }\n+      return struct.getId() != null;\n+    }\n+\n+    @Override\n+    public Boolean list(GroupType array, Boolean hasId) {\n+      if (hasId) {\n+        return true;\n+      } else {\n+        return array.getId() != null;\n+      }\n+    }\n \n-      // no assignment was needed\n-      return true;\n+    @Override\n+    public Boolean map(GroupType map, Boolean keyHasId, Boolean valueHasId) {\n+      if (keyHasId || valueHasId) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5OTEyOA=="}, "originalCommit": {"oid": "f1115034c0db5ded6a7feab0a3cf7e8bbd1e24d9"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMzE1MDA5OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQwMDoxMDoyOFrOGQWhWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDo0ODoxMVrOGQtBnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5OTM4NA==", "bodyText": "This overwrites any existing IDs, so let's call it applyNameMapping instead.\nAlso, this is quite a large class. Can you move it into its own file? It should be package-private.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r419799384", "createdAt": "2020-05-05T00:10:28Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -83,33 +92,144 @@ public static MessageType pruneColumnsFallback(MessageType fileSchema, Schema ex\n   }\n \n   public static boolean hasIds(MessageType fileSchema) {\n-    try {\n-      // Try to convert the type to Iceberg. If an ID assignment is needed, return false.\n-      ParquetTypeVisitor.visit(fileSchema, new MessageTypeToType(fileSchema) {\n-        @Override\n-        protected int nextId() {\n-          throw new IllegalStateException(\"Needed to assign ID\");\n+    return ParquetTypeVisitor.visit(fileSchema, new HasIds(), true);\n+  }\n+\n+  public static MessageType addFallbackIds(MessageType fileSchema, NameMapping nameMapping) {\n+    if (nameMapping == null) {\n+      MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+\n+      int ordinal = 1; // ids are assigned starting at 1\n+      for (Type type : fileSchema.getFields()) {\n+        builder.addField(type.withId(ordinal));\n+        ordinal += 1;\n+      }\n+\n+      return builder.named(fileSchema.getName());\n+    } else {\n+      return (MessageType) ParquetTypeVisitor.visit(fileSchema, new AssignIdsByNameMapping(nameMapping), true);\n+    }\n+  }\n+\n+  public static class HasIds extends ParquetTypeVisitor<Boolean> {\n+    @Override\n+    public Boolean message(MessageType message, List<Boolean> fields) {\n+      return struct(message, fields);\n+    }\n+\n+    @Override\n+    public Boolean struct(GroupType struct, List<Boolean> hasIds) {\n+      for (Boolean hasId : hasIds) {\n+        if (hasId) {\n+          return true;\n         }\n-      });\n+      }\n+      return struct.getId() != null;\n+    }\n+\n+    @Override\n+    public Boolean list(GroupType array, Boolean hasId) {\n+      if (hasId) {\n+        return true;\n+      } else {\n+        return array.getId() != null;\n+      }\n+    }\n \n-      // no assignment was needed\n-      return true;\n+    @Override\n+    public Boolean map(GroupType map, Boolean keyHasId, Boolean valueHasId) {\n+      if (keyHasId || valueHasId) {\n+        return true;\n+      } else {\n+        return map.getId() != null;\n+      }\n+    }\n \n-    } catch (IllegalStateException e) {\n-      // at least one field was missing an id.\n-      return false;\n+    @Override\n+    public Boolean primitive(PrimitiveType primitive) {\n+      return primitive.getId() != null;\n     }\n   }\n \n-  public static MessageType addFallbackIds(MessageType fileSchema) {\n-    MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+  public static class AssignIdsByNameMapping extends ParquetTypeVisitor<Type> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1115034c0db5ded6a7feab0a3cf7e8bbd1e24d9"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE2ODA5Mw==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r420168093", "createdAt": "2020-05-05T14:48:11Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -83,33 +92,144 @@ public static MessageType pruneColumnsFallback(MessageType fileSchema, Schema ex\n   }\n \n   public static boolean hasIds(MessageType fileSchema) {\n-    try {\n-      // Try to convert the type to Iceberg. If an ID assignment is needed, return false.\n-      ParquetTypeVisitor.visit(fileSchema, new MessageTypeToType(fileSchema) {\n-        @Override\n-        protected int nextId() {\n-          throw new IllegalStateException(\"Needed to assign ID\");\n+    return ParquetTypeVisitor.visit(fileSchema, new HasIds(), true);\n+  }\n+\n+  public static MessageType addFallbackIds(MessageType fileSchema, NameMapping nameMapping) {\n+    if (nameMapping == null) {\n+      MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+\n+      int ordinal = 1; // ids are assigned starting at 1\n+      for (Type type : fileSchema.getFields()) {\n+        builder.addField(type.withId(ordinal));\n+        ordinal += 1;\n+      }\n+\n+      return builder.named(fileSchema.getName());\n+    } else {\n+      return (MessageType) ParquetTypeVisitor.visit(fileSchema, new AssignIdsByNameMapping(nameMapping), true);\n+    }\n+  }\n+\n+  public static class HasIds extends ParquetTypeVisitor<Boolean> {\n+    @Override\n+    public Boolean message(MessageType message, List<Boolean> fields) {\n+      return struct(message, fields);\n+    }\n+\n+    @Override\n+    public Boolean struct(GroupType struct, List<Boolean> hasIds) {\n+      for (Boolean hasId : hasIds) {\n+        if (hasId) {\n+          return true;\n         }\n-      });\n+      }\n+      return struct.getId() != null;\n+    }\n+\n+    @Override\n+    public Boolean list(GroupType array, Boolean hasId) {\n+      if (hasId) {\n+        return true;\n+      } else {\n+        return array.getId() != null;\n+      }\n+    }\n \n-      // no assignment was needed\n-      return true;\n+    @Override\n+    public Boolean map(GroupType map, Boolean keyHasId, Boolean valueHasId) {\n+      if (keyHasId || valueHasId) {\n+        return true;\n+      } else {\n+        return map.getId() != null;\n+      }\n+    }\n \n-    } catch (IllegalStateException e) {\n-      // at least one field was missing an id.\n-      return false;\n+    @Override\n+    public Boolean primitive(PrimitiveType primitive) {\n+      return primitive.getId() != null;\n     }\n   }\n \n-  public static MessageType addFallbackIds(MessageType fileSchema) {\n-    MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+  public static class AssignIdsByNameMapping extends ParquetTypeVisitor<Type> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5OTM4NA=="}, "originalCommit": {"oid": "f1115034c0db5ded6a7feab0a3cf7e8bbd1e24d9"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMzE1MTcwOnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQwMDoxMToxOVrOGQWiQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDo0Nzo1OFrOGQtBHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5OTYxOA==", "bodyText": "Please rebase on master. #950 moved these methods into the ParquetTypeVisitor so you don't have to copy them.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r419799618", "createdAt": "2020-05-05T00:11:19Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -83,33 +92,144 @@ public static MessageType pruneColumnsFallback(MessageType fileSchema, Schema ex\n   }\n \n   public static boolean hasIds(MessageType fileSchema) {\n-    try {\n-      // Try to convert the type to Iceberg. If an ID assignment is needed, return false.\n-      ParquetTypeVisitor.visit(fileSchema, new MessageTypeToType(fileSchema) {\n-        @Override\n-        protected int nextId() {\n-          throw new IllegalStateException(\"Needed to assign ID\");\n+    return ParquetTypeVisitor.visit(fileSchema, new HasIds(), true);\n+  }\n+\n+  public static MessageType addFallbackIds(MessageType fileSchema, NameMapping nameMapping) {\n+    if (nameMapping == null) {\n+      MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+\n+      int ordinal = 1; // ids are assigned starting at 1\n+      for (Type type : fileSchema.getFields()) {\n+        builder.addField(type.withId(ordinal));\n+        ordinal += 1;\n+      }\n+\n+      return builder.named(fileSchema.getName());\n+    } else {\n+      return (MessageType) ParquetTypeVisitor.visit(fileSchema, new AssignIdsByNameMapping(nameMapping), true);\n+    }\n+  }\n+\n+  public static class HasIds extends ParquetTypeVisitor<Boolean> {\n+    @Override\n+    public Boolean message(MessageType message, List<Boolean> fields) {\n+      return struct(message, fields);\n+    }\n+\n+    @Override\n+    public Boolean struct(GroupType struct, List<Boolean> hasIds) {\n+      for (Boolean hasId : hasIds) {\n+        if (hasId) {\n+          return true;\n         }\n-      });\n+      }\n+      return struct.getId() != null;\n+    }\n+\n+    @Override\n+    public Boolean list(GroupType array, Boolean hasId) {\n+      if (hasId) {\n+        return true;\n+      } else {\n+        return array.getId() != null;\n+      }\n+    }\n \n-      // no assignment was needed\n-      return true;\n+    @Override\n+    public Boolean map(GroupType map, Boolean keyHasId, Boolean valueHasId) {\n+      if (keyHasId || valueHasId) {\n+        return true;\n+      } else {\n+        return map.getId() != null;\n+      }\n+    }\n \n-    } catch (IllegalStateException e) {\n-      // at least one field was missing an id.\n-      return false;\n+    @Override\n+    public Boolean primitive(PrimitiveType primitive) {\n+      return primitive.getId() != null;\n     }\n   }\n \n-  public static MessageType addFallbackIds(MessageType fileSchema) {\n-    MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+  public static class AssignIdsByNameMapping extends ParquetTypeVisitor<Type> {\n+    private final NameMapping nameMapping;\n \n-    int ordinal = 1; // ids are assigned starting at 1\n-    for (Type type : fileSchema.getFields()) {\n-      builder.addField(type.withId(ordinal));\n-      ordinal += 1;\n+    public AssignIdsByNameMapping(NameMapping nameMapping) {\n+      this.nameMapping = nameMapping;\n     }\n \n-    return builder.named(fileSchema.getName());\n+    private String[] currentPath() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1115034c0db5ded6a7feab0a3cf7e8bbd1e24d9"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE2Nzk2NA==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r420167964", "createdAt": "2020-05-05T14:47:58Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -83,33 +92,144 @@ public static MessageType pruneColumnsFallback(MessageType fileSchema, Schema ex\n   }\n \n   public static boolean hasIds(MessageType fileSchema) {\n-    try {\n-      // Try to convert the type to Iceberg. If an ID assignment is needed, return false.\n-      ParquetTypeVisitor.visit(fileSchema, new MessageTypeToType(fileSchema) {\n-        @Override\n-        protected int nextId() {\n-          throw new IllegalStateException(\"Needed to assign ID\");\n+    return ParquetTypeVisitor.visit(fileSchema, new HasIds(), true);\n+  }\n+\n+  public static MessageType addFallbackIds(MessageType fileSchema, NameMapping nameMapping) {\n+    if (nameMapping == null) {\n+      MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+\n+      int ordinal = 1; // ids are assigned starting at 1\n+      for (Type type : fileSchema.getFields()) {\n+        builder.addField(type.withId(ordinal));\n+        ordinal += 1;\n+      }\n+\n+      return builder.named(fileSchema.getName());\n+    } else {\n+      return (MessageType) ParquetTypeVisitor.visit(fileSchema, new AssignIdsByNameMapping(nameMapping), true);\n+    }\n+  }\n+\n+  public static class HasIds extends ParquetTypeVisitor<Boolean> {\n+    @Override\n+    public Boolean message(MessageType message, List<Boolean> fields) {\n+      return struct(message, fields);\n+    }\n+\n+    @Override\n+    public Boolean struct(GroupType struct, List<Boolean> hasIds) {\n+      for (Boolean hasId : hasIds) {\n+        if (hasId) {\n+          return true;\n         }\n-      });\n+      }\n+      return struct.getId() != null;\n+    }\n+\n+    @Override\n+    public Boolean list(GroupType array, Boolean hasId) {\n+      if (hasId) {\n+        return true;\n+      } else {\n+        return array.getId() != null;\n+      }\n+    }\n \n-      // no assignment was needed\n-      return true;\n+    @Override\n+    public Boolean map(GroupType map, Boolean keyHasId, Boolean valueHasId) {\n+      if (keyHasId || valueHasId) {\n+        return true;\n+      } else {\n+        return map.getId() != null;\n+      }\n+    }\n \n-    } catch (IllegalStateException e) {\n-      // at least one field was missing an id.\n-      return false;\n+    @Override\n+    public Boolean primitive(PrimitiveType primitive) {\n+      return primitive.getId() != null;\n     }\n   }\n \n-  public static MessageType addFallbackIds(MessageType fileSchema) {\n-    MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+  public static class AssignIdsByNameMapping extends ParquetTypeVisitor<Type> {\n+    private final NameMapping nameMapping;\n \n-    int ordinal = 1; // ids are assigned starting at 1\n-    for (Type type : fileSchema.getFields()) {\n-      builder.addField(type.withId(ordinal));\n-      ordinal += 1;\n+    public AssignIdsByNameMapping(NameMapping nameMapping) {\n+      this.nameMapping = nameMapping;\n     }\n \n-    return builder.named(fileSchema.getName());\n+    private String[] currentPath() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5OTYxOA=="}, "originalCommit": {"oid": "f1115034c0db5ded6a7feab0a3cf7e8bbd1e24d9"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMzE1MjEzOnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQwMDoxMTozN1rOGQWihA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDo0Nzo1MFrOGQtAxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5OTY4NA==", "bodyText": "Overall, the implementations in this class look correct to me.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r419799684", "createdAt": "2020-05-05T00:11:37Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -83,33 +92,144 @@ public static MessageType pruneColumnsFallback(MessageType fileSchema, Schema ex\n   }\n \n   public static boolean hasIds(MessageType fileSchema) {\n-    try {\n-      // Try to convert the type to Iceberg. If an ID assignment is needed, return false.\n-      ParquetTypeVisitor.visit(fileSchema, new MessageTypeToType(fileSchema) {\n-        @Override\n-        protected int nextId() {\n-          throw new IllegalStateException(\"Needed to assign ID\");\n+    return ParquetTypeVisitor.visit(fileSchema, new HasIds(), true);\n+  }\n+\n+  public static MessageType addFallbackIds(MessageType fileSchema, NameMapping nameMapping) {\n+    if (nameMapping == null) {\n+      MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+\n+      int ordinal = 1; // ids are assigned starting at 1\n+      for (Type type : fileSchema.getFields()) {\n+        builder.addField(type.withId(ordinal));\n+        ordinal += 1;\n+      }\n+\n+      return builder.named(fileSchema.getName());\n+    } else {\n+      return (MessageType) ParquetTypeVisitor.visit(fileSchema, new AssignIdsByNameMapping(nameMapping), true);\n+    }\n+  }\n+\n+  public static class HasIds extends ParquetTypeVisitor<Boolean> {\n+    @Override\n+    public Boolean message(MessageType message, List<Boolean> fields) {\n+      return struct(message, fields);\n+    }\n+\n+    @Override\n+    public Boolean struct(GroupType struct, List<Boolean> hasIds) {\n+      for (Boolean hasId : hasIds) {\n+        if (hasId) {\n+          return true;\n         }\n-      });\n+      }\n+      return struct.getId() != null;\n+    }\n+\n+    @Override\n+    public Boolean list(GroupType array, Boolean hasId) {\n+      if (hasId) {\n+        return true;\n+      } else {\n+        return array.getId() != null;\n+      }\n+    }\n \n-      // no assignment was needed\n-      return true;\n+    @Override\n+    public Boolean map(GroupType map, Boolean keyHasId, Boolean valueHasId) {\n+      if (keyHasId || valueHasId) {\n+        return true;\n+      } else {\n+        return map.getId() != null;\n+      }\n+    }\n \n-    } catch (IllegalStateException e) {\n-      // at least one field was missing an id.\n-      return false;\n+    @Override\n+    public Boolean primitive(PrimitiveType primitive) {\n+      return primitive.getId() != null;\n     }\n   }\n \n-  public static MessageType addFallbackIds(MessageType fileSchema) {\n-    MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+  public static class AssignIdsByNameMapping extends ParquetTypeVisitor<Type> {\n+    private final NameMapping nameMapping;\n \n-    int ordinal = 1; // ids are assigned starting at 1\n-    for (Type type : fileSchema.getFields()) {\n-      builder.addField(type.withId(ordinal));\n-      ordinal += 1;\n+    public AssignIdsByNameMapping(NameMapping nameMapping) {\n+      this.nameMapping = nameMapping;\n     }\n \n-    return builder.named(fileSchema.getName());\n+    private String[] currentPath() {\n+      String[] path = new String[fieldNames.size()];\n+      if (!fieldNames.isEmpty()) {\n+        Iterator<String> iter = fieldNames.descendingIterator();\n+        for (int i = 0; iter.hasNext(); i += 1) {\n+          path[i] = iter.next();\n+        }\n+      }\n+\n+      return path;\n+    }\n+\n+    @Override\n+    public Type message(MessageType message, List<Type> fields) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1115034c0db5ded6a7feab0a3cf7e8bbd1e24d9"}, "originalPosition": 123}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE2Nzg3Ng==", "bodyText": "Thanks.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r420167876", "createdAt": "2020-05-05T14:47:50Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java", "diffHunk": "@@ -83,33 +92,144 @@ public static MessageType pruneColumnsFallback(MessageType fileSchema, Schema ex\n   }\n \n   public static boolean hasIds(MessageType fileSchema) {\n-    try {\n-      // Try to convert the type to Iceberg. If an ID assignment is needed, return false.\n-      ParquetTypeVisitor.visit(fileSchema, new MessageTypeToType(fileSchema) {\n-        @Override\n-        protected int nextId() {\n-          throw new IllegalStateException(\"Needed to assign ID\");\n+    return ParquetTypeVisitor.visit(fileSchema, new HasIds(), true);\n+  }\n+\n+  public static MessageType addFallbackIds(MessageType fileSchema, NameMapping nameMapping) {\n+    if (nameMapping == null) {\n+      MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+\n+      int ordinal = 1; // ids are assigned starting at 1\n+      for (Type type : fileSchema.getFields()) {\n+        builder.addField(type.withId(ordinal));\n+        ordinal += 1;\n+      }\n+\n+      return builder.named(fileSchema.getName());\n+    } else {\n+      return (MessageType) ParquetTypeVisitor.visit(fileSchema, new AssignIdsByNameMapping(nameMapping), true);\n+    }\n+  }\n+\n+  public static class HasIds extends ParquetTypeVisitor<Boolean> {\n+    @Override\n+    public Boolean message(MessageType message, List<Boolean> fields) {\n+      return struct(message, fields);\n+    }\n+\n+    @Override\n+    public Boolean struct(GroupType struct, List<Boolean> hasIds) {\n+      for (Boolean hasId : hasIds) {\n+        if (hasId) {\n+          return true;\n         }\n-      });\n+      }\n+      return struct.getId() != null;\n+    }\n+\n+    @Override\n+    public Boolean list(GroupType array, Boolean hasId) {\n+      if (hasId) {\n+        return true;\n+      } else {\n+        return array.getId() != null;\n+      }\n+    }\n \n-      // no assignment was needed\n-      return true;\n+    @Override\n+    public Boolean map(GroupType map, Boolean keyHasId, Boolean valueHasId) {\n+      if (keyHasId || valueHasId) {\n+        return true;\n+      } else {\n+        return map.getId() != null;\n+      }\n+    }\n \n-    } catch (IllegalStateException e) {\n-      // at least one field was missing an id.\n-      return false;\n+    @Override\n+    public Boolean primitive(PrimitiveType primitive) {\n+      return primitive.getId() != null;\n     }\n   }\n \n-  public static MessageType addFallbackIds(MessageType fileSchema) {\n-    MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+  public static class AssignIdsByNameMapping extends ParquetTypeVisitor<Type> {\n+    private final NameMapping nameMapping;\n \n-    int ordinal = 1; // ids are assigned starting at 1\n-    for (Type type : fileSchema.getFields()) {\n-      builder.addField(type.withId(ordinal));\n-      ordinal += 1;\n+    public AssignIdsByNameMapping(NameMapping nameMapping) {\n+      this.nameMapping = nameMapping;\n     }\n \n-    return builder.named(fileSchema.getName());\n+    private String[] currentPath() {\n+      String[] path = new String[fieldNames.size()];\n+      if (!fieldNames.isEmpty()) {\n+        Iterator<String> iter = fieldNames.descendingIterator();\n+        for (int i = 0; iter.hasNext(); i += 1) {\n+          path[i] = iter.next();\n+        }\n+      }\n+\n+      return path;\n+    }\n+\n+    @Override\n+    public Type message(MessageType message, List<Type> fields) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc5OTY4NA=="}, "originalCommit": {"oid": "f1115034c0db5ded6a7feab0a3cf7e8bbd1e24d9"}, "originalPosition": 123}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMzE2MjQ3OnYy", "diffSide": "LEFT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQwMDoxNjoxOVrOGQWoYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDo0Njo1MFrOGQs-DA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTgwMTE4NA==", "bodyText": "Let's leave this line to avoid git conflicts.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r419801184", "createdAt": "2020-05-05T00:16:19Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -66,19 +63,21 @@\n   @SuppressWarnings(\"unchecked\")\n   ReadConf(InputFile file, ParquetReadOptions options, Schema expectedSchema, Expression filter,\n            Function<MessageType, ParquetValueReader<?>> readerFunc, Function<MessageType,\n-           VectorizedReader<?>> batchedReaderFunc, boolean reuseContainers,\n+           VectorizedReader<?>> batchedReaderFunc, NameMapping nameMapping, boolean reuseContainers,\n            boolean caseSensitive, Integer bSize) {\n     this.file = file;\n     this.options = options;\n     this.reader = newReader(file, options);\n     MessageType fileSchema = reader.getFileMetaData().getSchema();\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1115034c0db5ded6a7feab0a3cf7e8bbd1e24d9"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE2NzE4MA==", "bodyText": "ok.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r420167180", "createdAt": "2020-05-05T14:46:50Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -66,19 +63,21 @@\n   @SuppressWarnings(\"unchecked\")\n   ReadConf(InputFile file, ParquetReadOptions options, Schema expectedSchema, Expression filter,\n            Function<MessageType, ParquetValueReader<?>> readerFunc, Function<MessageType,\n-           VectorizedReader<?>> batchedReaderFunc, boolean reuseContainers,\n+           VectorizedReader<?>> batchedReaderFunc, NameMapping nameMapping, boolean reuseContainers,\n            boolean caseSensitive, Integer bSize) {\n     this.file = file;\n     this.options = options;\n     this.reader = newReader(file, options);\n     MessageType fileSchema = reader.getFileMetaData().getSchema();\n-", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTgwMTE4NA=="}, "originalCommit": {"oid": "f1115034c0db5ded6a7feab0a3cf7e8bbd1e24d9"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMzE3NDUzOnYy", "diffSide": "RIGHT", "path": "parquet/src/test/java/org/apache/iceberg/parquet/TestParquetSchemaUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQwMDoyMjo1N1rOGQWviw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDo0NzozMVrOGQs_9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTgwMzAxOQ==", "bodyText": "This should use asStruct() instead of toString.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r419803019", "createdAt": "2020-05-05T00:22:57Z", "author": {"login": "rdblue"}, "path": "parquet/src/test/java/org/apache/iceberg/parquet/TestParquetSchemaUtil.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.parquet;\n+\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+public class TestParquetSchemaUtil {\n+  private static final Types.StructType SUPPORTED_PRIMITIVES = Types.StructType.of(\n+      required(100, \"id\", Types.LongType.get()),\n+      optional(101, \"data\", Types.StringType.get()),\n+      required(102, \"b\", Types.BooleanType.get()),\n+      optional(103, \"i\", Types.IntegerType.get()),\n+      required(104, \"l\", Types.LongType.get()),\n+      optional(105, \"f\", Types.FloatType.get()),\n+      required(106, \"d\", Types.DoubleType.get()),\n+      optional(107, \"date\", Types.DateType.get()),\n+      required(108, \"ts\", Types.TimestampType.withZone()),\n+      required(110, \"s\", Types.StringType.get()),\n+      required(112, \"fixed\", Types.FixedType.ofLength(7)),\n+      optional(113, \"bytes\", Types.BinaryType.get()),\n+      required(114, \"dec_9_0\", Types.DecimalType.of(9, 0)),\n+      required(115, \"dec_11_2\", Types.DecimalType.of(11, 2)),\n+      required(116, \"dec_38_10\", Types.DecimalType.of(38, 10)) // spark's maximum precision\n+  );\n+\n+  @Test\n+  public void testAssignIdsByNameMapping() {\n+    Types.StructType structType = Types.StructType.of(\n+        required(0, \"id\", Types.LongType.get()),\n+        optional(1, \"list_of_maps\",\n+            Types.ListType.ofOptional(2, Types.MapType.ofOptional(3, 4,\n+                Types.StringType.get(),\n+                SUPPORTED_PRIMITIVES))),\n+        optional(5, \"map_of_lists\",\n+            Types.MapType.ofOptional(6, 7,\n+                Types.StringType.get(),\n+                Types.ListType.ofOptional(8, SUPPORTED_PRIMITIVES))),\n+        required(9, \"list_of_lists\",\n+            Types.ListType.ofOptional(10, Types.ListType.ofOptional(11, SUPPORTED_PRIMITIVES))),\n+        required(12, \"map_of_maps\",\n+            Types.MapType.ofOptional(13, 14,\n+                Types.StringType.get(),\n+                Types.MapType.ofOptional(15, 16,\n+                    Types.StringType.get(),\n+                    SUPPORTED_PRIMITIVES))),\n+        required(17, \"list_of_struct_of_nested_types\", Types.ListType.ofOptional(19, Types.StructType.of(\n+            Types.NestedField.required(20, \"m1\", Types.MapType.ofOptional(21, 22,\n+                Types.StringType.get(),\n+                SUPPORTED_PRIMITIVES)),\n+            Types.NestedField.optional(23, \"l1\", Types.ListType.ofRequired(24, SUPPORTED_PRIMITIVES)),\n+            Types.NestedField.required(25, \"l2\", Types.ListType.ofRequired(26, SUPPORTED_PRIMITIVES)),\n+            Types.NestedField.optional(27, \"m2\", Types.MapType.ofOptional(28, 29,\n+                Types.StringType.get(),\n+                SUPPORTED_PRIMITIVES))\n+        )))\n+    );\n+\n+    Schema schema = new Schema(TypeUtil.assignFreshIds(structType, new AtomicInteger(0)::incrementAndGet)\n+        .asStructType().fields());\n+    NameMapping nameMapping = MappingUtil.create(schema);\n+    MessageType messageType = ParquetSchemaUtil.convert(schema, \"complex_schema\");\n+    MessageType typeWithIdsFromNameMapping = ParquetSchemaUtil.addFallbackIds(messageType, nameMapping);\n+    Schema newSchema = ParquetSchemaUtil.convert(typeWithIdsFromNameMapping);\n+\n+    Assert.assertEquals(schema.toString(), newSchema.toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1115034c0db5ded6a7feab0a3cf7e8bbd1e24d9"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE2NzY3MA==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r420167670", "createdAt": "2020-05-05T14:47:31Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/test/java/org/apache/iceberg/parquet/TestParquetSchemaUtil.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.parquet;\n+\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+public class TestParquetSchemaUtil {\n+  private static final Types.StructType SUPPORTED_PRIMITIVES = Types.StructType.of(\n+      required(100, \"id\", Types.LongType.get()),\n+      optional(101, \"data\", Types.StringType.get()),\n+      required(102, \"b\", Types.BooleanType.get()),\n+      optional(103, \"i\", Types.IntegerType.get()),\n+      required(104, \"l\", Types.LongType.get()),\n+      optional(105, \"f\", Types.FloatType.get()),\n+      required(106, \"d\", Types.DoubleType.get()),\n+      optional(107, \"date\", Types.DateType.get()),\n+      required(108, \"ts\", Types.TimestampType.withZone()),\n+      required(110, \"s\", Types.StringType.get()),\n+      required(112, \"fixed\", Types.FixedType.ofLength(7)),\n+      optional(113, \"bytes\", Types.BinaryType.get()),\n+      required(114, \"dec_9_0\", Types.DecimalType.of(9, 0)),\n+      required(115, \"dec_11_2\", Types.DecimalType.of(11, 2)),\n+      required(116, \"dec_38_10\", Types.DecimalType.of(38, 10)) // spark's maximum precision\n+  );\n+\n+  @Test\n+  public void testAssignIdsByNameMapping() {\n+    Types.StructType structType = Types.StructType.of(\n+        required(0, \"id\", Types.LongType.get()),\n+        optional(1, \"list_of_maps\",\n+            Types.ListType.ofOptional(2, Types.MapType.ofOptional(3, 4,\n+                Types.StringType.get(),\n+                SUPPORTED_PRIMITIVES))),\n+        optional(5, \"map_of_lists\",\n+            Types.MapType.ofOptional(6, 7,\n+                Types.StringType.get(),\n+                Types.ListType.ofOptional(8, SUPPORTED_PRIMITIVES))),\n+        required(9, \"list_of_lists\",\n+            Types.ListType.ofOptional(10, Types.ListType.ofOptional(11, SUPPORTED_PRIMITIVES))),\n+        required(12, \"map_of_maps\",\n+            Types.MapType.ofOptional(13, 14,\n+                Types.StringType.get(),\n+                Types.MapType.ofOptional(15, 16,\n+                    Types.StringType.get(),\n+                    SUPPORTED_PRIMITIVES))),\n+        required(17, \"list_of_struct_of_nested_types\", Types.ListType.ofOptional(19, Types.StructType.of(\n+            Types.NestedField.required(20, \"m1\", Types.MapType.ofOptional(21, 22,\n+                Types.StringType.get(),\n+                SUPPORTED_PRIMITIVES)),\n+            Types.NestedField.optional(23, \"l1\", Types.ListType.ofRequired(24, SUPPORTED_PRIMITIVES)),\n+            Types.NestedField.required(25, \"l2\", Types.ListType.ofRequired(26, SUPPORTED_PRIMITIVES)),\n+            Types.NestedField.optional(27, \"m2\", Types.MapType.ofOptional(28, 29,\n+                Types.StringType.get(),\n+                SUPPORTED_PRIMITIVES))\n+        )))\n+    );\n+\n+    Schema schema = new Schema(TypeUtil.assignFreshIds(structType, new AtomicInteger(0)::incrementAndGet)\n+        .asStructType().fields());\n+    NameMapping nameMapping = MappingUtil.create(schema);\n+    MessageType messageType = ParquetSchemaUtil.convert(schema, \"complex_schema\");\n+    MessageType typeWithIdsFromNameMapping = ParquetSchemaUtil.addFallbackIds(messageType, nameMapping);\n+    Schema newSchema = ParquetSchemaUtil.convert(typeWithIdsFromNameMapping);\n+\n+    Assert.assertEquals(schema.toString(), newSchema.toString());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTgwMzAxOQ=="}, "originalCommit": {"oid": "f1115034c0db5ded6a7feab0a3cf7e8bbd1e24d9"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYxMzIxMjA3OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQwMDo0Mzo0OVrOGQXFpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNDo0ODo0NFrOGQtDUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTgwODY3Nw==", "bodyText": "This logic assumes what addFallbackIds is going to do. I think I'd rather structure this a bit differently:\nMessageType typeWithIds;\nif (ParquetSchemaUtil.hasIds(fileSchema)) {\n  typeWithIds = fileSchema;\n  this.projection = ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema);\n} else if (nameMapping != null) {\n  typeWithIds = ParquetSchemaUtil.applyNameMapping(fileSchema, nameMapping);\n  this.projection = ParquetSchemaUtil.pruneColumns(typeWithIds, expectedSchema);\n} else {\n  typeWithIds = ParquetSchemaUtil.addFallbackIds(fileSchema);\n  this.projection = ParquetSchemaUtil.pruneColumnsFallback(fileSchema, expectedSchema);\n}", "url": "https://github.com/apache/iceberg/pull/830#discussion_r419808677", "createdAt": "2020-05-05T00:43:49Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -66,19 +63,21 @@\n   @SuppressWarnings(\"unchecked\")\n   ReadConf(InputFile file, ParquetReadOptions options, Schema expectedSchema, Expression filter,\n            Function<MessageType, ParquetValueReader<?>> readerFunc, Function<MessageType,\n-           VectorizedReader<?>> batchedReaderFunc, boolean reuseContainers,\n+           VectorizedReader<?>> batchedReaderFunc, NameMapping nameMapping, boolean reuseContainers,\n            boolean caseSensitive, Integer bSize) {\n     this.file = file;\n     this.options = options;\n     this.reader = newReader(file, options);\n     MessageType fileSchema = reader.getFileMetaData().getSchema();\n-\n     boolean hasIds = ParquetSchemaUtil.hasIds(fileSchema);\n-    MessageType typeWithIds = hasIds ? fileSchema : ParquetSchemaUtil.addFallbackIds(fileSchema);\n+    MessageType typeWithIds = hasIds ? fileSchema : ParquetSchemaUtil.addFallbackIds(fileSchema, nameMapping);\n+\n+    if (nameMapping == null && !hasIds) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f1115034c0db5ded6a7feab0a3cf7e8bbd1e24d9"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDE2ODUyOA==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r420168528", "createdAt": "2020-05-05T14:48:44Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ReadConf.java", "diffHunk": "@@ -66,19 +63,21 @@\n   @SuppressWarnings(\"unchecked\")\n   ReadConf(InputFile file, ParquetReadOptions options, Schema expectedSchema, Expression filter,\n            Function<MessageType, ParquetValueReader<?>> readerFunc, Function<MessageType,\n-           VectorizedReader<?>> batchedReaderFunc, boolean reuseContainers,\n+           VectorizedReader<?>> batchedReaderFunc, NameMapping nameMapping, boolean reuseContainers,\n            boolean caseSensitive, Integer bSize) {\n     this.file = file;\n     this.options = options;\n     this.reader = newReader(file, options);\n     MessageType fileSchema = reader.getFileMetaData().getSchema();\n-\n     boolean hasIds = ParquetSchemaUtil.hasIds(fileSchema);\n-    MessageType typeWithIds = hasIds ? fileSchema : ParquetSchemaUtil.addFallbackIds(fileSchema);\n+    MessageType typeWithIds = hasIds ? fileSchema : ParquetSchemaUtil.addFallbackIds(fileSchema, nameMapping);\n+\n+    if (nameMapping == null && !hasIds) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTgwODY3Nw=="}, "originalCommit": {"oid": "f1115034c0db5ded6a7feab0a3cf7e8bbd1e24d9"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNTM0ODU0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxODo0MDowM1rOGSLaJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxODo0MDowM1rOGSLaJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTcxNDQ2OA==", "bodyText": "This shouldn't create a name mapping. It should use one from the table, if it exists.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r421714468", "createdAt": "2020-05-07T18:40:03Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java", "diffHunk": "@@ -192,8 +194,10 @@\n       FileScanTask task,\n       Schema readSchema,\n       Map<Integer, ?> idToConstant) {\n+    NameMapping nameMapping = MappingUtil.create(readSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNTM1NTg0OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetReadSupport.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxODo0MjowMlrOGSLejQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwOToxODoxNVrOGSfNQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTcxNTU5Nw==", "bodyText": "No need for a newline here.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r421715597", "createdAt": "2020-05-07T18:42:02Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetReadSupport.java", "diffHunk": "@@ -55,9 +58,16 @@ public ReadContext init(Configuration configuration, Map<String, String> keyValu\n     // matching to the file's columns by full path, so this must select columns by using the path\n     // in the file's schema.\n \n-    MessageType projection = ParquetSchemaUtil.hasIds(fileSchema) ?\n-        ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n-        ParquetSchemaUtil.pruneColumnsFallback(fileSchema, expectedSchema);\n+    MessageType projection;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjAzODg1MQ==", "bodyText": "Done", "url": "https://github.com/apache/iceberg/pull/830#discussion_r422038851", "createdAt": "2020-05-08T09:18:15Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetReadSupport.java", "diffHunk": "@@ -55,9 +58,16 @@ public ReadContext init(Configuration configuration, Map<String, String> keyValu\n     // matching to the file's columns by full path, so this must select columns by using the path\n     // in the file's schema.\n \n-    MessageType projection = ParquetSchemaUtil.hasIds(fileSchema) ?\n-        ParquetSchemaUtil.pruneColumns(fileSchema, expectedSchema) :\n-        ParquetSchemaUtil.pruneColumnsFallback(fileSchema, expectedSchema);\n+    MessageType projection;\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTcxNTU5Nw=="}, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNTM2MjQ1OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/AssignIdsByNameMapping.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxODo0Mzo1MlrOGSLitA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwNzo0ODoyMlrOGScv0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTcxNjY2MA==", "bodyText": "Let's rename this to mach the method, ApplyNameMapping.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r421716660", "createdAt": "2020-05-07T18:43:52Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/AssignIdsByNameMapping.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.parquet;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.mapping.MappedField;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.apache.parquet.schema.Types;\n+\n+class AssignIdsByNameMapping extends ParquetTypeVisitor<Type> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTk5ODU0Nw==", "bodyText": "OK.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r421998547", "createdAt": "2020-05-08T07:48:22Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/AssignIdsByNameMapping.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.parquet;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.mapping.MappedField;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.apache.parquet.schema.Types;\n+\n+class AssignIdsByNameMapping extends ParquetTypeVisitor<Type> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTcxNjY2MA=="}, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNTM2NzA4OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxODo0NDo1NlrOGSLlZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwOToxODoyNFrOGSfNeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTcxNzM0OA==", "bodyText": "Nit: Indentation is incorrect here.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r421717348", "createdAt": "2020-05-07T18:44:56Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -199,4 +204,41 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithIncompatibleSchema() throws Exception {\n+    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n+        .saveAsTable(\"original_table\");\n+\n+    // The field is different so that it will project with name mapping\n+    Schema filteredSchema = new Schema(\n+            optional(1, \"data\", Types.StringType.get())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjAzODkwNA==", "bodyText": "Done", "url": "https://github.com/apache/iceberg/pull/830#discussion_r422038904", "createdAt": "2020-05-08T09:18:24Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -199,4 +204,41 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithIncompatibleSchema() throws Exception {\n+    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n+        .saveAsTable(\"original_table\");\n+\n+    // The field is different so that it will project with name mapping\n+    Schema filteredSchema = new Schema(\n+            optional(1, \"data\", Types.StringType.get())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTcxNzM0OA=="}, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNTM2ODE4OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxODo0NToxMlrOGSLmDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwMzoyNTozNlrOGSXr6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTcxNzUxNg==", "bodyText": "Why isn't TableIdentifier imported?", "url": "https://github.com/apache/iceberg/pull/830#discussion_r421717516", "createdAt": "2020-05-07T18:45:12Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -199,4 +204,41 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithIncompatibleSchema() throws Exception {\n+    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n+        .saveAsTable(\"original_table\");\n+\n+    // The field is different so that it will project with name mapping\n+    Schema filteredSchema = new Schema(\n+            optional(1, \"data\", Types.StringType.get())\n+    );\n+\n+    TableIdentifier source = new TableIdentifier(\"original_table\");\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table\"),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTkxNTYyNw==", "bodyText": "TableIdentifier from spark is imported so that it uses the full qualified name to avoid conflict. We use it in other places in this file as well.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r421915627", "createdAt": "2020-05-08T03:25:36Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -199,4 +204,41 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithIncompatibleSchema() throws Exception {\n+    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n+        .saveAsTable(\"original_table\");\n+\n+    // The field is different so that it will project with name mapping\n+    Schema filteredSchema = new Schema(\n+            optional(1, \"data\", Types.StringType.get())\n+    );\n+\n+    TableIdentifier source = new TableIdentifier(\"original_table\");\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table\"),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTcxNzUxNg=="}, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNTM4NDIyOnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxODo0OTo1M1rOGSLwWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwNjozNjowMVrOGcQcjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTcyMDE1Mg==", "bodyText": "Name mapping should only be used if it is set on the table.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r421720152", "createdAt": "2020-05-07T18:49:53Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -199,4 +204,41 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithIncompatibleSchema() throws Exception {\n+    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n+        .saveAsTable(\"original_table\");\n+\n+    // The field is different so that it will project with name mapping", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjI4Mjc2Ng==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r432282766", "createdAt": "2020-05-29T06:36:01Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -199,4 +204,41 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithIncompatibleSchema() throws Exception {\n+    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n+        .saveAsTable(\"original_table\");\n+\n+    // The field is different so that it will project with name mapping", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTcyMDE1Mg=="}, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNTM5Mjk5OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxODo1MjoyNVrOGSL2FA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwOToxODozOFrOGSfN8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTcyMTYyMA==", "bodyText": "This should be named testImportWithNameMapping instead. The schema isn't incompatible, it just uses a different strategy (name mapping, instead of position mapping) to assign IDs for fields.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r421721620", "createdAt": "2020-05-07T18:52:25Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -199,4 +204,41 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithIncompatibleSchema() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjAzOTAyNw==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r422039027", "createdAt": "2020-05-08T09:18:38Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -199,4 +204,41 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithIncompatibleSchema() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTcyMTYyMA=="}, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNTQxMDg1OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/AssignIdsByNameMapping.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxODo1NzoyNlrOGSMBWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQwOToyNToxOFrOGSfZtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTcyNDUwNw==", "bodyText": "This should return the list with IDs assigned to sub-types.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r421724507", "createdAt": "2020-05-07T18:57:26Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/AssignIdsByNameMapping.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.parquet;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.mapping.MappedField;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.apache.parquet.schema.Types;\n+\n+class AssignIdsByNameMapping extends ParquetTypeVisitor<Type> {\n+  private final NameMapping nameMapping;\n+\n+  AssignIdsByNameMapping(NameMapping nameMapping) {\n+    this.nameMapping = nameMapping;\n+  }\n+\n+  @Override\n+  public Type message(MessageType message, List<Type> fields) {\n+    Types.MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+    fields.stream().filter(Objects::nonNull).forEach(builder::addField);\n+\n+    return builder.named(message.getName());\n+  }\n+\n+  @Override\n+  public Type struct(GroupType struct, List<Type> types) {\n+    MappedField field = nameMapping.find(currentPath());\n+    if (field == null) {\n+      return null;\n+    }\n+    List<Type> actualTypes = types.stream().filter(Objects::nonNull).collect(Collectors.toList());\n+\n+    return struct.withNewFields(actualTypes).withId(field.id());\n+  }\n+\n+  @Override\n+  public Type list(GroupType list, Type elementType) {\n+    MappedField field = nameMapping.find(currentPath());\n+    if (field == null) {\n+      return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjA0MjAzOA==", "bodyText": "It was intended to return null since the name mapping is created from the table's schema. I changed to return list with sub-types now since the assumption is removed.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r422042038", "createdAt": "2020-05-08T09:25:18Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/AssignIdsByNameMapping.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.parquet;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.mapping.MappedField;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.apache.parquet.schema.Types;\n+\n+class AssignIdsByNameMapping extends ParquetTypeVisitor<Type> {\n+  private final NameMapping nameMapping;\n+\n+  AssignIdsByNameMapping(NameMapping nameMapping) {\n+    this.nameMapping = nameMapping;\n+  }\n+\n+  @Override\n+  public Type message(MessageType message, List<Type> fields) {\n+    Types.MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+    fields.stream().filter(Objects::nonNull).forEach(builder::addField);\n+\n+    return builder.named(message.getName());\n+  }\n+\n+  @Override\n+  public Type struct(GroupType struct, List<Type> types) {\n+    MappedField field = nameMapping.find(currentPath());\n+    if (field == null) {\n+      return null;\n+    }\n+    List<Type> actualTypes = types.stream().filter(Objects::nonNull).collect(Collectors.toList());\n+\n+    return struct.withNewFields(actualTypes).withId(field.id());\n+  }\n+\n+  @Override\n+  public Type list(GroupType list, Type elementType) {\n+    MappedField field = nameMapping.find(currentPath());\n+    if (field == null) {\n+      return null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTcyNDUwNw=="}, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjYyNTQ4MDg5OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wN1QxOToxNzoyOVrOGSMsvg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwNTo1NDo0OFrOGZnrkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTczNTYxNA==", "bodyText": "When a Parquet file is imported, its schema is converted to Iceberg using MessageTypeToType, which assigns IDs to fields using the position-based strategy. Stats from those files are generated and stored in Iceberg metadata using the position-based IDs, not name-based. That means for this imported table, the stats in Iceberg metadata are incorrect.\nTo fix this, you also need to support name mapping in the conversion from Parquet to an Iceberg schema, and make sure that is called here. Alternatively, you could detect that the table has a name mapping and skip metrics when importing data files.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r421735614", "createdAt": "2020-05-07T19:17:29Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -199,4 +204,41 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithIncompatibleSchema() throws Exception {\n+    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n+        .saveAsTable(\"original_table\");\n+\n+    // The field is different so that it will project with name mapping\n+    Schema filteredSchema = new Schema(\n+            optional(1, \"data\", Types.StringType.get())\n+    );\n+\n+    TableIdentifier source = new TableIdentifier(\"original_table\");\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table\"),\n+        filteredSchema,\n+        SparkSchemaUtil.specForTable(spark, \"original_table\"));\n+    File stagingDir = temp.newFolder(\"staging-dir\");\n+    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTk5NzY2OA==", "bodyText": "Where should the name mapping come from? I thought the name mapping should be created via the table's schema because the typical case is we create a table, import the parquet files from an existing spark table. In case of that, we only care about the newly created fields and their stats, that's why the visitor returns null when we cannot find a field in name mapping regardless of its sub-types  (it is impossible to define a schema with list/map/struct without a name and has the subfields).\nIf the name mapping comes from external, for example, from the converted schema of parquet file schema, then the logic would be different as you pointed out. We might need 1) store name mapping for Table, 2) a Table API to get name mapping, 3) change MessageTypeToType to visit schema with name mapping. That seems to bring in too many complexities.\nBack to the issue itself, what we need is creating a name mapping when hasIds is false. So we can add another property to Parquet ReadBuilder, say enableNameMapping, then we could build a default name mapping from expectedSchema even though we don't pass a name mapping. The logic in the ReadConf.java would be:\nNameMapping actualNameMapping;\nif (enableNameMapping) {\n  if(nameMapping != null) {\n    actualNameMapping = nameMapping;\n  } else {\n    actualNameMapping = MappingUtil.create(expectedSchema);\n  }\n}\n...\nand in RowDataReader.java\nreturn Parquet.read(location)\n          .project(schema)\n          .enableNameMapping()\n           ......\nWhat do you think?", "url": "https://github.com/apache/iceberg/pull/830#discussion_r421997668", "createdAt": "2020-05-08T07:46:25Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -199,4 +204,41 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithIncompatibleSchema() throws Exception {\n+    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n+        .saveAsTable(\"original_table\");\n+\n+    // The field is different so that it will project with name mapping\n+    Schema filteredSchema = new Schema(\n+            optional(1, \"data\", Types.StringType.get())\n+    );\n+\n+    TableIdentifier source = new TableIdentifier(\"original_table\");\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table\"),\n+        filteredSchema,\n+        SparkSchemaUtil.specForTable(spark, \"original_table\"));\n+    File stagingDir = temp.newFolder(\"staging-dir\");\n+    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTczNTYxNA=="}, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODkwMDYxNg==", "bodyText": "Name mapping should be part of table configuration. If one is present, it should be used. Otherwise no name mapping should be used.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r428900616", "createdAt": "2020-05-21T20:37:56Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -199,4 +204,41 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithIncompatibleSchema() throws Exception {\n+    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n+        .saveAsTable(\"original_table\");\n+\n+    // The field is different so that it will project with name mapping\n+    Schema filteredSchema = new Schema(\n+            optional(1, \"data\", Types.StringType.get())\n+    );\n+\n+    TableIdentifier source = new TableIdentifier(\"original_table\");\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table\"),\n+        filteredSchema,\n+        SparkSchemaUtil.specForTable(spark, \"original_table\"));\n+    File stagingDir = temp.newFolder(\"staging-dir\");\n+    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTczNTYxNA=="}, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTAzMTM5MA==", "bodyText": "Parsing name mapping make sense to me, that could be used for avro and orc as well.  While this would need to parse the string from table property to NameMapping, how about we make this as a follow-up and use a default name mapping created from table schema as I describe above?", "url": "https://github.com/apache/iceberg/pull/830#discussion_r429031390", "createdAt": "2020-05-22T04:03:56Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -199,4 +204,41 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithIncompatibleSchema() throws Exception {\n+    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n+        .saveAsTable(\"original_table\");\n+\n+    // The field is different so that it will project with name mapping\n+    Schema filteredSchema = new Schema(\n+            optional(1, \"data\", Types.StringType.get())\n+    );\n+\n+    TableIdentifier source = new TableIdentifier(\"original_table\");\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table\"),\n+        filteredSchema,\n+        SparkSchemaUtil.specForTable(spark, \"original_table\"));\n+    File stagingDir = temp.newFolder(\"staging-dir\");\n+    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTczNTYxNA=="}, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQyNTU4Mg==", "bodyText": "No, we should not commit anything that creates a mapping and uses it automatically.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r429425582", "createdAt": "2020-05-22T19:42:08Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -199,4 +204,41 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithIncompatibleSchema() throws Exception {\n+    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n+        .saveAsTable(\"original_table\");\n+\n+    // The field is different so that it will project with name mapping\n+    Schema filteredSchema = new Schema(\n+            optional(1, \"data\", Types.StringType.get())\n+    );\n+\n+    TableIdentifier source = new TableIdentifier(\"original_table\");\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table\"),\n+        filteredSchema,\n+        SparkSchemaUtil.specForTable(spark, \"original_table\"));\n+    File stagingDir = temp.newFolder(\"staging-dir\");\n+    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTczNTYxNA=="}, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxNzcxMg==", "bodyText": "OK, I updated this.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r429517712", "createdAt": "2020-05-23T05:54:48Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -199,4 +204,41 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithIncompatibleSchema() throws Exception {\n+    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n+        .saveAsTable(\"original_table\");\n+\n+    // The field is different so that it will project with name mapping\n+    Schema filteredSchema = new Schema(\n+            optional(1, \"data\", Types.StringType.get())\n+    );\n+\n+    TableIdentifier source = new TableIdentifier(\"original_table\");\n+    Table table = catalog.createTable(\n+        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, \"target_table\"),\n+        filteredSchema,\n+        SparkSchemaUtil.specForTable(spark, \"original_table\"));\n+    File stagingDir = temp.newFolder(\"staging-dir\");\n+    SparkTableUtil.importSparkTable(spark, source, table, stagingDir.toString());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTczNTYxNA=="}, "originalCommit": {"oid": "8554c33ff2cc4090d864ef38139d36d9734e64fe"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3MTE3MTAyOnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMVQyMDozOTo1MFrOGZCEfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQwMzo1OTowMFrOGZJ8Gw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODkwMTUwMQ==", "bodyText": "Parquet should never automatically add a mapping. When reading, a name mapping should be parsed and added if one is present in table properties.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r428901501", "createdAt": "2020-05-21T20:39:50Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -419,17 +433,18 @@ public ReadBuilder recordsPerBatch(int numRowsPerBatch) {\n         ParquetReadOptions options = optionsBuilder.build();\n \n         if (batchedReaderFunc != null) {\n-          return new VectorizedParquetReader(file, schema, options, batchedReaderFunc, filter, reuseContainers,\n-              caseSensitive, maxRecordsPerBatch);\n+          return new VectorizedParquetReader(file, schema, options, batchedReaderFunc, nameMapping,\n+              applyNameMapping, filter, reuseContainers, caseSensitive, maxRecordsPerBatch);\n         } else {\n           return new org.apache.iceberg.parquet.ParquetReader<>(\n-              file, schema, options, readerFunc, filter, reuseContainers, caseSensitive);\n+              file, schema, options, readerFunc, nameMapping, applyNameMapping, filter, reuseContainers,\n+              caseSensitive);\n         }\n       }\n \n       ParquetReadBuilder<D> builder = new ParquetReadBuilder<>(ParquetIO.file(file));\n \n-      builder.project(schema);\n+      builder.project(schema).withNameMapping(MappingUtil.create(schema));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a12f430b02bda993c19c3b9f20cdd4d8a6ff0d37"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTAzMDQyNw==", "bodyText": "Agreed.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r429030427", "createdAt": "2020-05-22T03:59:00Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -419,17 +433,18 @@ public ReadBuilder recordsPerBatch(int numRowsPerBatch) {\n         ParquetReadOptions options = optionsBuilder.build();\n \n         if (batchedReaderFunc != null) {\n-          return new VectorizedParquetReader(file, schema, options, batchedReaderFunc, filter, reuseContainers,\n-              caseSensitive, maxRecordsPerBatch);\n+          return new VectorizedParquetReader(file, schema, options, batchedReaderFunc, nameMapping,\n+              applyNameMapping, filter, reuseContainers, caseSensitive, maxRecordsPerBatch);\n         } else {\n           return new org.apache.iceberg.parquet.ParquetReader<>(\n-              file, schema, options, readerFunc, filter, reuseContainers, caseSensitive);\n+              file, schema, options, readerFunc, nameMapping, applyNameMapping, filter, reuseContainers,\n+              caseSensitive);\n         }\n       }\n \n       ParquetReadBuilder<D> builder = new ParquetReadBuilder<>(ParquetIO.file(file));\n \n-      builder.project(schema);\n+      builder.project(schema).withNameMapping(MappingUtil.create(schema));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODkwMTUwMQ=="}, "originalCommit": {"oid": "a12f430b02bda993c19c3b9f20cdd4d8a6ff0d37"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NDQ0NjQ0OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMlQxOTo0NTowN1rOGZiHtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwNTo1NDoxOFrOGZnrcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQyNjYxNQ==", "bodyText": "What is the purpose of this? Why not always apply a mapping if it was supplied?\nI think it is confusing to only apply a mapping if this is called, even when the mapping is set.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r429426615", "createdAt": "2020-05-22T19:45:07Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -393,6 +396,16 @@ public ReadBuilder recordsPerBatch(int numRowsPerBatch) {\n       return this;\n     }\n \n+    public ReadBuilder withNameMapping(NameMapping newNameMapping) {\n+      this.nameMapping = newNameMapping;\n+      return this;\n+    }\n+\n+    public ReadBuilder applyNameMapping() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c8b3da646578e9ccd5a2da33fa0d88f8b141d14"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxMDUyMg==", "bodyText": "The idea is if there is no name mapping supplied, we use a default name mapping build from table schema.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r429510522", "createdAt": "2020-05-23T03:39:26Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -393,6 +396,16 @@ public ReadBuilder recordsPerBatch(int numRowsPerBatch) {\n       return this;\n     }\n \n+    public ReadBuilder withNameMapping(NameMapping newNameMapping) {\n+      this.nameMapping = newNameMapping;\n+      return this;\n+    }\n+\n+    public ReadBuilder applyNameMapping() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQyNjYxNQ=="}, "originalCommit": {"oid": "0c8b3da646578e9ccd5a2da33fa0d88f8b141d14"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxNzY4Mw==", "bodyText": "I removed this and use name mapping from table property now.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r429517683", "createdAt": "2020-05-23T05:54:18Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -393,6 +396,16 @@ public ReadBuilder recordsPerBatch(int numRowsPerBatch) {\n       return this;\n     }\n \n+    public ReadBuilder withNameMapping(NameMapping newNameMapping) {\n+      this.nameMapping = newNameMapping;\n+      return this;\n+    }\n+\n+    public ReadBuilder applyNameMapping() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQyNjYxNQ=="}, "originalCommit": {"oid": "0c8b3da646578e9ccd5a2da33fa0d88f8b141d14"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzYwMjM3OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetMetricsRowGroupFilter.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzowOTozNlrOGklITw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwMToyMDoxMVrOGkyXag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAxMDI1NQ==", "bodyText": "Looks like this is a non-functional change to an unrelated file. Could you revert it? Same with ParquetDictionaryRowGroupFilter.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441010255", "createdAt": "2020-06-16T17:09:36Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetMetricsRowGroupFilter.java", "diffHunk": "@@ -47,6 +47,7 @@\n import org.apache.parquet.schema.PrimitiveType;\n \n public class ParquetMetricsRowGroupFilter {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIyNzExNA==", "bodyText": "Strange, I remember I fixed things like this, maybe the fix was reverted by merging.  Anyway, let me fix this.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441227114", "createdAt": "2020-06-17T01:20:11Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ParquetMetricsRowGroupFilter.java", "diffHunk": "@@ -47,6 +47,7 @@\n import org.apache.parquet.schema.PrimitiveType;\n \n public class ParquetMetricsRowGroupFilter {\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAxMDI1NQ=="}, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzYxNTI0OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/VectorizedParquetReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzoxMzoyMlrOGklQrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzoxMzoyMlrOGklQrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAxMjM5OA==", "bodyText": "Please don't split types across lines. I think moving Function<MessageType to the previous line should be reverted.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441012398", "createdAt": "2020-06-16T17:13:22Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/VectorizedParquetReader.java", "diffHunk": "@@ -48,11 +49,12 @@\n   private boolean reuseContainers;\n   private final boolean caseSensitive;\n   private final int batchSize;\n+  private final NameMapping nameMapping;\n \n   public VectorizedParquetReader(\n-      InputFile input, Schema expectedSchema, ParquetReadOptions options,\n-      Function<MessageType, VectorizedReader<?>> readerFunc,\n-      Expression filter, boolean reuseContainers, boolean caseSensitive, int maxRecordsPerBatch) {\n+      InputFile input, Schema expectedSchema, ParquetReadOptions options, Function<MessageType,\n+      VectorizedReader<?>> readerFunc, NameMapping nameMapping, Expression filter, boolean reuseContainers,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzYyOTY0OnYy", "diffSide": "RIGHT", "path": "parquet/src/test/java/org/apache/iceberg/parquet/TestParquetSchemaUtil.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzoxNzozN1rOGklaVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwMTozNDoyMlrOGkyl8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAxNDg2OQ==", "bodyText": "There's a problem here: applyNameMapping could be a noop and this would still work because the conversion to Parquet and back preserves IDs. For Avro, there is a test utility to remove IDs from the schema so that we can test adding them back with the name mapping. I think it would make sense to take the Avro name mapping tests and adapt them for Parquet as well. That can be done in a follow-up, but this test does need to be fixed before committing.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441014869", "createdAt": "2020-06-16T17:17:37Z", "author": {"login": "rdblue"}, "path": "parquet/src/test/java/org/apache/iceberg/parquet/TestParquetSchemaUtil.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.parquet;\n+\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+public class TestParquetSchemaUtil {\n+  private static final Types.StructType SUPPORTED_PRIMITIVES = Types.StructType.of(\n+      required(100, \"id\", Types.LongType.get()),\n+      optional(101, \"data\", Types.StringType.get()),\n+      required(102, \"b\", Types.BooleanType.get()),\n+      optional(103, \"i\", Types.IntegerType.get()),\n+      required(104, \"l\", Types.LongType.get()),\n+      optional(105, \"f\", Types.FloatType.get()),\n+      required(106, \"d\", Types.DoubleType.get()),\n+      optional(107, \"date\", Types.DateType.get()),\n+      required(108, \"ts\", Types.TimestampType.withZone()),\n+      required(110, \"s\", Types.StringType.get()),\n+      required(112, \"fixed\", Types.FixedType.ofLength(7)),\n+      optional(113, \"bytes\", Types.BinaryType.get()),\n+      required(114, \"dec_9_0\", Types.DecimalType.of(9, 0)),\n+      required(115, \"dec_11_2\", Types.DecimalType.of(11, 2)),\n+      required(116, \"dec_38_10\", Types.DecimalType.of(38, 10)) // spark's maximum precision\n+  );\n+\n+  @Test\n+  public void testAssignIdsByNameMapping() {\n+    Types.StructType structType = Types.StructType.of(\n+        required(0, \"id\", Types.LongType.get()),\n+        optional(1, \"list_of_maps\",\n+            Types.ListType.ofOptional(2, Types.MapType.ofOptional(3, 4,\n+                Types.StringType.get(),\n+                SUPPORTED_PRIMITIVES))),\n+        optional(5, \"map_of_lists\",\n+            Types.MapType.ofOptional(6, 7,\n+                Types.StringType.get(),\n+                Types.ListType.ofOptional(8, SUPPORTED_PRIMITIVES))),\n+        required(9, \"list_of_lists\",\n+            Types.ListType.ofOptional(10, Types.ListType.ofOptional(11, SUPPORTED_PRIMITIVES))),\n+        required(12, \"map_of_maps\",\n+            Types.MapType.ofOptional(13, 14,\n+                Types.StringType.get(),\n+                Types.MapType.ofOptional(15, 16,\n+                    Types.StringType.get(),\n+                    SUPPORTED_PRIMITIVES))),\n+        required(17, \"list_of_struct_of_nested_types\", Types.ListType.ofOptional(19, Types.StructType.of(\n+            Types.NestedField.required(20, \"m1\", Types.MapType.ofOptional(21, 22,\n+                Types.StringType.get(),\n+                SUPPORTED_PRIMITIVES)),\n+            Types.NestedField.optional(23, \"l1\", Types.ListType.ofRequired(24, SUPPORTED_PRIMITIVES)),\n+            Types.NestedField.required(25, \"l2\", Types.ListType.ofRequired(26, SUPPORTED_PRIMITIVES)),\n+            Types.NestedField.optional(27, \"m2\", Types.MapType.ofOptional(28, 29,\n+                Types.StringType.get(),\n+                SUPPORTED_PRIMITIVES))\n+        )))\n+    );\n+\n+    Schema schema = new Schema(TypeUtil.assignFreshIds(structType, new AtomicInteger(0)::incrementAndGet)\n+        .asStructType().fields());\n+    NameMapping nameMapping = MappingUtil.create(schema);\n+    MessageType messageType = ParquetSchemaUtil.convert(schema, \"complex_schema\");\n+    MessageType typeWithIdsFromNameMapping = ParquetSchemaUtil.applyNameMapping(messageType, nameMapping);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 90}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTIzMDgzNQ==", "bodyText": "Make sense to me, let me file an issue to record this.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441230835", "createdAt": "2020-06-17T01:34:22Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/test/java/org/apache/iceberg/parquet/TestParquetSchemaUtil.java", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.parquet;\n+\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.schema.MessageType;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+public class TestParquetSchemaUtil {\n+  private static final Types.StructType SUPPORTED_PRIMITIVES = Types.StructType.of(\n+      required(100, \"id\", Types.LongType.get()),\n+      optional(101, \"data\", Types.StringType.get()),\n+      required(102, \"b\", Types.BooleanType.get()),\n+      optional(103, \"i\", Types.IntegerType.get()),\n+      required(104, \"l\", Types.LongType.get()),\n+      optional(105, \"f\", Types.FloatType.get()),\n+      required(106, \"d\", Types.DoubleType.get()),\n+      optional(107, \"date\", Types.DateType.get()),\n+      required(108, \"ts\", Types.TimestampType.withZone()),\n+      required(110, \"s\", Types.StringType.get()),\n+      required(112, \"fixed\", Types.FixedType.ofLength(7)),\n+      optional(113, \"bytes\", Types.BinaryType.get()),\n+      required(114, \"dec_9_0\", Types.DecimalType.of(9, 0)),\n+      required(115, \"dec_11_2\", Types.DecimalType.of(11, 2)),\n+      required(116, \"dec_38_10\", Types.DecimalType.of(38, 10)) // spark's maximum precision\n+  );\n+\n+  @Test\n+  public void testAssignIdsByNameMapping() {\n+    Types.StructType structType = Types.StructType.of(\n+        required(0, \"id\", Types.LongType.get()),\n+        optional(1, \"list_of_maps\",\n+            Types.ListType.ofOptional(2, Types.MapType.ofOptional(3, 4,\n+                Types.StringType.get(),\n+                SUPPORTED_PRIMITIVES))),\n+        optional(5, \"map_of_lists\",\n+            Types.MapType.ofOptional(6, 7,\n+                Types.StringType.get(),\n+                Types.ListType.ofOptional(8, SUPPORTED_PRIMITIVES))),\n+        required(9, \"list_of_lists\",\n+            Types.ListType.ofOptional(10, Types.ListType.ofOptional(11, SUPPORTED_PRIMITIVES))),\n+        required(12, \"map_of_maps\",\n+            Types.MapType.ofOptional(13, 14,\n+                Types.StringType.get(),\n+                Types.MapType.ofOptional(15, 16,\n+                    Types.StringType.get(),\n+                    SUPPORTED_PRIMITIVES))),\n+        required(17, \"list_of_struct_of_nested_types\", Types.ListType.ofOptional(19, Types.StructType.of(\n+            Types.NestedField.required(20, \"m1\", Types.MapType.ofOptional(21, 22,\n+                Types.StringType.get(),\n+                SUPPORTED_PRIMITIVES)),\n+            Types.NestedField.optional(23, \"l1\", Types.ListType.ofRequired(24, SUPPORTED_PRIMITIVES)),\n+            Types.NestedField.required(25, \"l2\", Types.ListType.ofRequired(26, SUPPORTED_PRIMITIVES)),\n+            Types.NestedField.optional(27, \"m2\", Types.MapType.ofOptional(28, 29,\n+                Types.StringType.get(),\n+                SUPPORTED_PRIMITIVES))\n+        )))\n+    );\n+\n+    Schema schema = new Schema(TypeUtil.assignFreshIds(structType, new AtomicInteger(0)::incrementAndGet)\n+        .asStructType().fields());\n+    NameMapping nameMapping = MappingUtil.create(schema);\n+    MessageType messageType = ParquetSchemaUtil.convert(schema, \"complex_schema\");\n+    MessageType typeWithIdsFromNameMapping = ParquetSchemaUtil.applyNameMapping(messageType, nameMapping);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAxNDg2OQ=="}, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzY3ODQ1OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/PruneColumns.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzozMTowMlrOGkl59w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNDoxODoyMFrOGlH-mA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyMjk2Nw==", "bodyText": "Why is this not if (fieldId != null && selectedIds.contains(fieldId))?\nThe else case is used when a sub-field is projected by ID. So the question is whether a sub-field can be projected if its parents aren't mapped. I think we should allow it because it would be confusing to have a value mapped, but still get nulls because a parent is not.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441022967", "createdAt": "2020-06-16T17:31:02Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/PruneColumns.java", "diffHunk": "@@ -45,13 +44,16 @@ public Type message(MessageType message, List<Type> fields) {\n     for (int i = 0; i < fields.size(); i += 1) {\n       Type originalField = message.getType(i);\n       Type field = fields.get(i);\n-      if (selectedIds.contains(getId(originalField))) {\n-        builder.addField(originalField);\n-        fieldCount += 1;\n-      } else if (field != null) {\n-        builder.addField(field);\n-        fieldCount += 1;\n-        hasChange = true;\n+      Integer fieldId = getId(originalField);\n+      if (fieldId != null) {\n+        if (selectedIds.contains(fieldId)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU4MTIwOA==", "bodyText": "Make sense to me. Updated.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441581208", "createdAt": "2020-06-17T14:18:20Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/PruneColumns.java", "diffHunk": "@@ -45,13 +44,16 @@ public Type message(MessageType message, List<Type> fields) {\n     for (int i = 0; i < fields.size(); i += 1) {\n       Type originalField = message.getType(i);\n       Type field = fields.get(i);\n-      if (selectedIds.contains(getId(originalField))) {\n-        builder.addField(originalField);\n-        fieldCount += 1;\n-      } else if (field != null) {\n-        builder.addField(field);\n-        fieldCount += 1;\n-        hasChange = true;\n+      Integer fieldId = getId(originalField);\n+      if (fieldId != null) {\n+        if (selectedIds.contains(fieldId)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyMjk2Nw=="}, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzY3OTM3OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/PruneColumns.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzozMToxN1rOGkl6iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNDoxODoyNVrOGlH-1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyMzExMg==", "bodyText": "Same logic as above applies here.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441023112", "createdAt": "2020-06-16T17:31:17Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/PruneColumns.java", "diffHunk": "@@ -71,11 +73,14 @@ public Type struct(GroupType struct, List<Type> fields) {\n     for (int i = 0; i < fields.size(); i += 1) {\n       Type originalField = struct.getType(i);\n       Type field = fields.get(i);\n-      if (selectedIds.contains(getId(originalField))) {\n-        filteredFields.add(originalField);\n-      } else if (field != null) {\n-        filteredFields.add(originalField);\n-        hasChange = true;\n+      Integer fieldId = getId(originalField);\n+      if (fieldId != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyMzU0Nw==", "bodyText": "And in the rest of the updates for this file.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441023547", "createdAt": "2020-06-16T17:31:58Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/PruneColumns.java", "diffHunk": "@@ -71,11 +73,14 @@ public Type struct(GroupType struct, List<Type> fields) {\n     for (int i = 0; i < fields.size(); i += 1) {\n       Type originalField = struct.getType(i);\n       Type field = fields.get(i);\n-      if (selectedIds.contains(getId(originalField))) {\n-        filteredFields.add(originalField);\n-      } else if (field != null) {\n-        filteredFields.add(originalField);\n-        hasChange = true;\n+      Integer fieldId = getId(originalField);\n+      if (fieldId != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyMzExMg=="}, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU4MTI2OQ==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441581269", "createdAt": "2020-06-17T14:18:25Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/PruneColumns.java", "diffHunk": "@@ -71,11 +73,14 @@ public Type struct(GroupType struct, List<Type> fields) {\n     for (int i = 0; i < fields.size(); i += 1) {\n       Type originalField = struct.getType(i);\n       Type field = fields.get(i);\n-      if (selectedIds.contains(getId(originalField))) {\n-        filteredFields.add(originalField);\n-      } else if (field != null) {\n-        filteredFields.add(originalField);\n-        hasChange = true;\n+      Integer fieldId = getId(originalField);\n+      if (fieldId != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyMzExMg=="}, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzY4MzUzOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzozMjoyMVrOGkl9Fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzozMjoyMVrOGkl9Fg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyMzc2Ng==", "bodyText": "Looks correct to me.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441023766", "createdAt": "2020-06-16T17:32:21Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java", "diffHunk": "@@ -142,9 +142,11 @@ private SparkParquetReaders() {\n       for (int i = 0; i < fields.size(); i += 1) {\n         Type fieldType = fields.get(i);\n         int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n-        int id = fieldType.getId().intValue();\n-        readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n-        typesById.put(id, fieldType);\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzY4Nzk0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzozMzo0NFrOGkmAJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzozMzo0NFrOGkmAJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyNDU0OQ==", "bodyText": "I'd prefer to keep the name mapping and building separate. There's no need to mix these together.\nif (nameMapping != null) {\n  builder.withNameMapping(NameMappingParser.fromJson(nameMapping));\n}\n\niter = builder.build();", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441024549", "createdAt": "2020-06-16T17:33:44Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "diffHunk": "@@ -65,8 +68,10 @@\n           // Spark eagerly consumes the batches. So the underlying memory allocated could be reused\n           // without worrying about subsequent reads clobbering over each other. This improves\n           // read performance as every batch read doesn't have to pay the cost of allocating memory.\n-          .reuseContainers()\n-          .build();\n+          .reuseContainers();\n+\n+      iter = nameMapping != null ?\n+          builder.withNameMapping(NameMappingParser.fromJson(nameMapping)).build() : builder.build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzY5NDgyOnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzozNTo0NlrOGkmEow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNDowOTo0NlrOGlHl5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyNTY5OQ==", "bodyText": "Can you add a test for name mapping with the vectorized read path? Just set the property on the table when you add the mapping and it should take that path.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441025699", "createdAt": "2020-06-16T17:35:46Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -200,4 +209,48 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithNameMapping() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU3NDg4NQ==", "bodyText": "Done, also updated the vectorized reader builder. Thanks for catching this", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441574885", "createdAt": "2020-06-17T14:09:46Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -200,4 +209,48 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithNameMapping() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyNTY5OQ=="}, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzY5Njg3OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzozNjoxNVrOGkmF6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzozNjoxNVrOGkmF6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyNjAyNQ==", "bodyText": "Nit: extra newline.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441026025", "createdAt": "2020-06-16T17:36:15Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTableUtil.java", "diffHunk": "@@ -200,4 +209,48 @@ public void testImportAsHiveTable() throws Exception {\n     long count2 = spark.read().format(\"iceberg\").load(DB_NAME + \".test_partitioned_table\").count();\n     Assert.assertEquals(\"three values \", 3, count2);\n   }\n+\n+  @Test\n+  public void testImportWithNameMapping() throws Exception {\n+    spark.table(qualifiedTableName).write().mode(\"overwrite\").format(\"parquet\")\n+        .saveAsTable(\"original_table\");\n+\n+    // The field is different so that it will project with name mapping\n+    Schema filteredSchema = new Schema(\n+        optional(1, \"data\", Types.StringType.get())\n+    );\n+\n+    NameMapping nameMapping = MappingUtil.create(filteredSchema);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzY5OTI0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzozNjo1OFrOGkmHeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNDoxMTowOFrOGlHp6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyNjQyNA==", "bodyText": "Same here, let's separate the name mapping from build.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441026424", "createdAt": "2020-06-16T17:36:58Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java", "diffHunk": "@@ -151,13 +154,15 @@\n       FileScanTask task,\n       Schema readSchema,\n       Map<Integer, ?> idToConstant) {\n-    return Parquet.read(location)\n-        .project(readSchema)\n+    Parquet.ReadBuilder builder = Parquet.read(location)\n         .split(task.start(), task.length())\n+        .project(readSchema)\n         .createReaderFunc(fileSchema -> SparkParquetReaders.buildReader(readSchema, fileSchema, idToConstant))\n         .filter(task.residual())\n-        .caseSensitive(caseSensitive)\n-        .build();\n+        .caseSensitive(caseSensitive);\n+\n+    return nameMapping != null ?\n+        builder.withNameMapping(NameMappingParser.fromJson(nameMapping)).build() : builder.build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTU3NTkxNA==", "bodyText": "Fixed.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441575914", "createdAt": "2020-06-17T14:11:08Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataReader.java", "diffHunk": "@@ -151,13 +154,15 @@\n       FileScanTask task,\n       Schema readSchema,\n       Map<Integer, ?> idToConstant) {\n-    return Parquet.read(location)\n-        .project(readSchema)\n+    Parquet.ReadBuilder builder = Parquet.read(location)\n         .split(task.start(), task.length())\n+        .project(readSchema)\n         .createReaderFunc(fileSchema -> SparkParquetReaders.buildReader(readSchema, fileSchema, idToConstant))\n         .filter(task.residual())\n-        .caseSensitive(caseSensitive)\n-        .build();\n+        .caseSensitive(caseSensitive);\n+\n+    return nameMapping != null ?\n+        builder.withNameMapping(NameMappingParser.fromJson(nameMapping)).build() : builder.build();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyNjQyNA=="}, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzcwMzQ2OnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/MessageTypeToType.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzozODoxNlrOGkmKNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwMzowMjozMFrOGkz-nw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyNzEyNg==", "bodyText": "I'd also prefer to revert this change. There may not be a use of it as a protected method any more, but we don't need to change this file and risk git conflicts.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441027126", "createdAt": "2020-06-16T17:38:16Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/MessageTypeToType.java", "diffHunk": "@@ -229,7 +229,7 @@ private void addAlias(String name, int fieldId) {\n     aliasToId.put(DOT.join(path(name)), fieldId);\n   }\n \n-  protected int nextId() {\n+  private int nextId() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI1MzUzNQ==", "bodyText": "OK", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441253535", "createdAt": "2020-06-17T03:02:30Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/MessageTypeToType.java", "diffHunk": "@@ -229,7 +229,7 @@ private void addAlias(String name, int fieldId) {\n     aliasToId.put(DOT.join(path(name)), fieldId);\n   }\n \n-  protected int nextId() {\n+  private int nextId() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyNzEyNg=="}, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0NzcwNDEzOnYy", "diffSide": "RIGHT", "path": "parquet/src/main/java/org/apache/iceberg/parquet/ApplyNameMapping.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNlQxNzozODozMVrOGkmKrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QwMzowMjoxNVrOGkz-YQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyNzI0Nw==", "bodyText": "Can you add comments to these methods to explain why they are here?", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441027247", "createdAt": "2020-06-16T17:38:31Z", "author": {"login": "rdblue"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ApplyNameMapping.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.parquet;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.mapping.MappedField;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.apache.parquet.schema.Types;\n+\n+class ApplyNameMapping extends ParquetTypeVisitor<Type> {\n+  private final NameMapping nameMapping;\n+\n+  ApplyNameMapping(NameMapping nameMapping) {\n+    this.nameMapping = nameMapping;\n+  }\n+\n+  @Override\n+  public Type message(MessageType message, List<Type> fields) {\n+    Types.MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+    fields.stream().filter(Objects::nonNull).forEach(builder::addField);\n+\n+    return builder.named(message.getName());\n+  }\n+\n+  @Override\n+  public Type struct(GroupType struct, List<Type> types) {\n+    MappedField field = nameMapping.find(currentPath());\n+    List<Type> actualTypes = types.stream().filter(Objects::nonNull).collect(Collectors.toList());\n+    Type structType = struct.withNewFields(actualTypes);\n+\n+    return field == null ? structType : structType.withId(field.id());\n+  }\n+\n+  @Override\n+  public Type list(GroupType list, Type elementType) {\n+    Preconditions.checkArgument(elementType != null,\n+        \"List type must have element field\");\n+\n+    MappedField field = nameMapping.find(currentPath());\n+    Type listType = org.apache.parquet.schema.Types.list(list.getRepetition())\n+        .element(elementType)\n+        .named(list.getName());\n+\n+    return field == null ? listType : listType.withId(field.id());\n+  }\n+\n+  @Override\n+  public Type map(GroupType map, Type keyType, Type valueType) {\n+    Preconditions.checkArgument(keyType != null && valueType != null,\n+        \"Map type must have both key field and value field\");\n+\n+    MappedField field = nameMapping.find(currentPath());\n+    Type mapType = org.apache.parquet.schema.Types.map(map.getRepetition())\n+        .key(keyType)\n+        .value(valueType)\n+        .named(map.getName());\n+\n+    return field == null ? mapType : mapType.withId(field.id());\n+  }\n+\n+  @Override\n+  public Type primitive(PrimitiveType primitive) {\n+    MappedField field = nameMapping.find(currentPath());\n+    return field == null ? primitive : primitive.withId(field.id());\n+  }\n+\n+  @Override\n+  public void beforeRepeatedElement(Type element) {\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTI1MzQ3Mw==", "bodyText": "OK, let me add the comments back.", "url": "https://github.com/apache/iceberg/pull/830#discussion_r441253473", "createdAt": "2020-06-17T03:02:15Z", "author": {"login": "chenjunjiedada"}, "path": "parquet/src/main/java/org/apache/iceberg/parquet/ApplyNameMapping.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.parquet;\n+\n+import java.util.List;\n+import java.util.Objects;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.mapping.MappedField;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+import org.apache.parquet.schema.Types;\n+\n+class ApplyNameMapping extends ParquetTypeVisitor<Type> {\n+  private final NameMapping nameMapping;\n+\n+  ApplyNameMapping(NameMapping nameMapping) {\n+    this.nameMapping = nameMapping;\n+  }\n+\n+  @Override\n+  public Type message(MessageType message, List<Type> fields) {\n+    Types.MessageTypeBuilder builder = org.apache.parquet.schema.Types.buildMessage();\n+    fields.stream().filter(Objects::nonNull).forEach(builder::addField);\n+\n+    return builder.named(message.getName());\n+  }\n+\n+  @Override\n+  public Type struct(GroupType struct, List<Type> types) {\n+    MappedField field = nameMapping.find(currentPath());\n+    List<Type> actualTypes = types.stream().filter(Objects::nonNull).collect(Collectors.toList());\n+    Type structType = struct.withNewFields(actualTypes);\n+\n+    return field == null ? structType : structType.withId(field.id());\n+  }\n+\n+  @Override\n+  public Type list(GroupType list, Type elementType) {\n+    Preconditions.checkArgument(elementType != null,\n+        \"List type must have element field\");\n+\n+    MappedField field = nameMapping.find(currentPath());\n+    Type listType = org.apache.parquet.schema.Types.list(list.getRepetition())\n+        .element(elementType)\n+        .named(list.getName());\n+\n+    return field == null ? listType : listType.withId(field.id());\n+  }\n+\n+  @Override\n+  public Type map(GroupType map, Type keyType, Type valueType) {\n+    Preconditions.checkArgument(keyType != null && valueType != null,\n+        \"Map type must have both key field and value field\");\n+\n+    MappedField field = nameMapping.find(currentPath());\n+    Type mapType = org.apache.parquet.schema.Types.map(map.getRepetition())\n+        .key(keyType)\n+        .value(valueType)\n+        .named(map.getName());\n+\n+    return field == null ? mapType : mapType.withId(field.id());\n+  }\n+\n+  @Override\n+  public Type primitive(PrimitiveType primitive) {\n+    MappedField field = nameMapping.find(currentPath());\n+    return field == null ? primitive : primitive.withId(field.id());\n+  }\n+\n+  @Override\n+  public void beforeRepeatedElement(Type element) {\n+  }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAyNzI0Nw=="}, "originalCommit": {"oid": "18a803f9bd033e4c5fada44394ee7ebfb4468216"}, "originalPosition": 93}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2835, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}