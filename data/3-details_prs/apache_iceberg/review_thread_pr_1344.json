{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY4MTIzNjI3", "number": 1344, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxOToyNTozMFrOEaHlTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQwMzozMzoyOFrOEaSiqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1ODIyNjcxOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxOToyNTozMFrOHDV8BA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxOToyNTozMFrOHDV8BA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzI2NzIwNA==", "bodyText": "We add these 2 arg versions so that we can specify metadata Json files directly, the single arg versions just use the current table state as before.", "url": "https://github.com/apache/iceberg/pull/1344#discussion_r473267204", "createdAt": "2020-08-19T19:25:30Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -86,17 +90,22 @@ protected String metadataTableName(MetadataTableType type) {\n   }\n \n   protected Dataset<Row> buildValidDataFileDF(SparkSession spark) {\n-    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return buildValidDataFileDF(spark, table().toString());\n+  }\n+\n+  protected Dataset<Row> buildValidDataFileDF(SparkSession spark, String tableName) {\n+    String allDataFilesMetadataTable = metadataTableName(tableName, MetadataTableType.ALL_DATA_FILES);\n     return spark.read().format(\"iceberg\").load(allDataFilesMetadataTable).select(\"file_path\");\n   }\n \n-  protected Dataset<Row> buildManifestFileDF(SparkSession spark) {\n-    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+  protected Dataset<Row> buildManifestFileDF(SparkSession spark, String tableName) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0596f8a7a7ab1faf20b73c89fab0c0247e8a4e60"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1ODc1MDAxOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMTo0MzoxM1rOHDbGOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMTo1Mzo1OFrOHDbqAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM1MTczNw==", "bodyText": "I think it is misleading to use snapshot here, since that term usually refers to a version of a table, not a table itself.", "url": "https://github.com/apache/iceberg/pull/1344#discussion_r473351737", "createdAt": "2020-08-19T21:43:13Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -86,17 +90,22 @@ protected String metadataTableName(MetadataTableType type) {\n   }\n \n   protected Dataset<Row> buildValidDataFileDF(SparkSession spark) {\n-    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return buildValidDataFileDF(spark, table().toString());\n+  }\n+\n+  protected Dataset<Row> buildValidDataFileDF(SparkSession spark, String tableName) {\n+    String allDataFilesMetadataTable = metadataTableName(tableName, MetadataTableType.ALL_DATA_FILES);\n     return spark.read().format(\"iceberg\").load(allDataFilesMetadataTable).select(\"file_path\");\n   }\n \n-  protected Dataset<Row> buildManifestFileDF(SparkSession spark) {\n-    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+  protected Dataset<Row> buildManifestFileDF(SparkSession spark, String tableName) {\n+    String allManifestsMetadataTable = metadataTableName(tableName, MetadataTableType.ALL_MANIFESTS);\n     return spark.read().format(\"iceberg\").load(allManifestsMetadataTable).selectExpr(\"path as file_path\");\n   }\n \n-  protected Dataset<Row> buildManifestListDF(SparkSession spark, Table table) {\n-    List<String> manifestLists = getManifestListPaths(table);\n+  protected Dataset<Row> buildManifestListDF(SparkSession spark, String tableName, TableOperations ops) {\n+    Table snapshot = new BaseTable(ops, tableName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0596f8a7a7ab1faf20b73c89fab0c0247e8a4e60"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM2MDg5OQ==", "bodyText": "True, I'll just switch it to table\nTable table = Table", "url": "https://github.com/apache/iceberg/pull/1344#discussion_r473360899", "createdAt": "2020-08-19T21:53:58Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -86,17 +90,22 @@ protected String metadataTableName(MetadataTableType type) {\n   }\n \n   protected Dataset<Row> buildValidDataFileDF(SparkSession spark) {\n-    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return buildValidDataFileDF(spark, table().toString());\n+  }\n+\n+  protected Dataset<Row> buildValidDataFileDF(SparkSession spark, String tableName) {\n+    String allDataFilesMetadataTable = metadataTableName(tableName, MetadataTableType.ALL_DATA_FILES);\n     return spark.read().format(\"iceberg\").load(allDataFilesMetadataTable).select(\"file_path\");\n   }\n \n-  protected Dataset<Row> buildManifestFileDF(SparkSession spark) {\n-    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+  protected Dataset<Row> buildManifestFileDF(SparkSession spark, String tableName) {\n+    String allManifestsMetadataTable = metadataTableName(tableName, MetadataTableType.ALL_MANIFESTS);\n     return spark.read().format(\"iceberg\").load(allManifestsMetadataTable).selectExpr(\"path as file_path\");\n   }\n \n-  protected Dataset<Row> buildManifestListDF(SparkSession spark, Table table) {\n-    List<String> manifestLists = getManifestListPaths(table);\n+  protected Dataset<Row> buildManifestListDF(SparkSession spark, String tableName, TableOperations ops) {\n+    Table snapshot = new BaseTable(ops, tableName);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM1MTczNw=="}, "originalCommit": {"oid": "0596f8a7a7ab1faf20b73c89fab0c0247e8a4e60"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1ODc3MDg2OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMTo0NzoxM1rOHDbTyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQwMzozMjoyOFrOHDoPMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM1NTIxMQ==", "bodyText": "Minor: the metadata file location is passed to buildManifestFileDF and buildValidDataFileDF, but StaticTableOperations is passed into buildManifestListDF. I think it would make a more consistent API if the location were also passed to buildManifestListDF.\nI know that the difference is that the method accepts a Table and doesn't use a metadata table. But it would be a bit cleaner to pass the base Table and metadata location, then create the StaticTableOperations in that method rather than here.", "url": "https://github.com/apache/iceberg/pull/1344#discussion_r473355211", "createdAt": "2020-08-19T21:47:13Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -147,49 +149,41 @@ public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n \n   @Override\n   public ExpireSnapshotsActionResult execute() {\n-    Dataset<Row> originalFiles = null;\n-    try {\n-      // Metadata before Expiration\n-      originalFiles = buildValidFileDF().persist();\n-      // Action to trigger persist\n-      originalFiles.count();\n-\n-      // Perform Expiration\n-      ExpireSnapshots expireSnaps = table.expireSnapshots().cleanExpiredFiles(false);\n-      for (final Long id : expireSnapshotIdValues) {\n-        expireSnaps = expireSnaps.expireSnapshotId(id);\n-      }\n-\n-      if (expireOlderThanValue != null) {\n-        expireSnaps = expireSnaps.expireOlderThan(expireOlderThanValue);\n-      }\n-\n-      if (retainLastValue != null) {\n-        expireSnaps = expireSnaps.retainLast(retainLastValue);\n-      }\n-\n-      expireSnaps.commit();\n-\n-      // Metadata after Expiration\n-      Dataset<Row> validFiles = buildValidFileDF();\n-      Dataset<Row> filesToDelete = originalFiles.except(validFiles);\n-\n-      return deleteFiles(filesToDelete.toLocalIterator());\n-    } finally {\n-      if (originalFiles != null) {\n-        originalFiles.unpersist();\n-      }\n+    // Metadata before Expiration\n+    Dataset<Row>  originalFiles = buildValidFileDF(ops.current());\n+\n+    // Perform Expiration\n+    ExpireSnapshots expireSnaps = table.expireSnapshots().cleanExpiredFiles(false);\n+    for (final Long id : expireSnapshotIdValues) {\n+      expireSnaps = expireSnaps.expireSnapshotId(id);\n+    }\n+\n+    if (expireOlderThanValue != null) {\n+      expireSnaps = expireSnaps.expireOlderThan(expireOlderThanValue);\n+    }\n+\n+    if (retainLastValue != null) {\n+      expireSnaps = expireSnaps.retainLast(retainLastValue);\n     }\n+\n+    expireSnaps.commit();\n+\n+    // Metadata after Expiration\n+    Dataset<Row> validFiles = buildValidFileDF(ops.refresh());\n+    Dataset<Row> filesToDelete = originalFiles.except(validFiles);\n+\n+    return deleteFiles(filesToDelete.toLocalIterator());\n   }\n \n   private Dataset<Row> appendTypeString(Dataset<Row> ds, String type) {\n     return ds.select(new Column(\"file_path\"), functions.lit(type).as(\"file_type\"));\n   }\n \n-  private Dataset<Row> buildValidFileDF() {\n-    return appendTypeString(buildValidDataFileDF(spark), DATA_FILE)\n-        .union(appendTypeString(buildManifestFileDF(spark), MANIFEST))\n-        .union(appendTypeString(buildManifestListDF(spark, table), MANIFEST_LIST));\n+  private Dataset<Row> buildValidFileDF(TableMetadata metadata) {\n+    StaticTableOperations staticOps = new StaticTableOperations(metadata.metadataFileLocation(), table.io());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0596f8a7a7ab1faf20b73c89fab0c0247e8a4e60"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzU2NzAyNQ==", "bodyText": "I think I understand what you are asking for here, but I'm not sure I like how it looks since I end up with two methods, one of which takes metadataFileLocation and one which takes \"Table\"\nThe metadataFileLocation version makes the StaticOps and BaseTable from them and passes to the table method,\nwhile the \"table\" method is used for the version used by Orphan files.\nTake a look  at the new version and see if we are on the same page", "url": "https://github.com/apache/iceberg/pull/1344#discussion_r473567025", "createdAt": "2020-08-20T03:32:28Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -147,49 +149,41 @@ public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n \n   @Override\n   public ExpireSnapshotsActionResult execute() {\n-    Dataset<Row> originalFiles = null;\n-    try {\n-      // Metadata before Expiration\n-      originalFiles = buildValidFileDF().persist();\n-      // Action to trigger persist\n-      originalFiles.count();\n-\n-      // Perform Expiration\n-      ExpireSnapshots expireSnaps = table.expireSnapshots().cleanExpiredFiles(false);\n-      for (final Long id : expireSnapshotIdValues) {\n-        expireSnaps = expireSnaps.expireSnapshotId(id);\n-      }\n-\n-      if (expireOlderThanValue != null) {\n-        expireSnaps = expireSnaps.expireOlderThan(expireOlderThanValue);\n-      }\n-\n-      if (retainLastValue != null) {\n-        expireSnaps = expireSnaps.retainLast(retainLastValue);\n-      }\n-\n-      expireSnaps.commit();\n-\n-      // Metadata after Expiration\n-      Dataset<Row> validFiles = buildValidFileDF();\n-      Dataset<Row> filesToDelete = originalFiles.except(validFiles);\n-\n-      return deleteFiles(filesToDelete.toLocalIterator());\n-    } finally {\n-      if (originalFiles != null) {\n-        originalFiles.unpersist();\n-      }\n+    // Metadata before Expiration\n+    Dataset<Row>  originalFiles = buildValidFileDF(ops.current());\n+\n+    // Perform Expiration\n+    ExpireSnapshots expireSnaps = table.expireSnapshots().cleanExpiredFiles(false);\n+    for (final Long id : expireSnapshotIdValues) {\n+      expireSnaps = expireSnaps.expireSnapshotId(id);\n+    }\n+\n+    if (expireOlderThanValue != null) {\n+      expireSnaps = expireSnaps.expireOlderThan(expireOlderThanValue);\n+    }\n+\n+    if (retainLastValue != null) {\n+      expireSnaps = expireSnaps.retainLast(retainLastValue);\n     }\n+\n+    expireSnaps.commit();\n+\n+    // Metadata after Expiration\n+    Dataset<Row> validFiles = buildValidFileDF(ops.refresh());\n+    Dataset<Row> filesToDelete = originalFiles.except(validFiles);\n+\n+    return deleteFiles(filesToDelete.toLocalIterator());\n   }\n \n   private Dataset<Row> appendTypeString(Dataset<Row> ds, String type) {\n     return ds.select(new Column(\"file_path\"), functions.lit(type).as(\"file_type\"));\n   }\n \n-  private Dataset<Row> buildValidFileDF() {\n-    return appendTypeString(buildValidDataFileDF(spark), DATA_FILE)\n-        .union(appendTypeString(buildManifestFileDF(spark), MANIFEST))\n-        .union(appendTypeString(buildManifestListDF(spark, table), MANIFEST_LIST));\n+  private Dataset<Row> buildValidFileDF(TableMetadata metadata) {\n+    StaticTableOperations staticOps = new StaticTableOperations(metadata.metadataFileLocation(), table.io());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM1NTIxMQ=="}, "originalCommit": {"oid": "0596f8a7a7ab1faf20b73c89fab0c0247e8a4e60"}, "originalPosition": 81}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk1ODc3OTU4OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMTo0OTowM1rOHDbZ1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQyMTo1MzozNlrOHDbo7g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM1Njc1Nw==", "bodyText": "What about changing getManifestListPaths to accept Iterable<Snapshot>? Then you wouldn't need to create a BaseTable out of a StaticTableOperations. Instead you could just pass staticOps.current().snapshots() here and table.snapshots() elsewhere.", "url": "https://github.com/apache/iceberg/pull/1344#discussion_r473356757", "createdAt": "2020-08-19T21:49:03Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -86,17 +90,22 @@ protected String metadataTableName(MetadataTableType type) {\n   }\n \n   protected Dataset<Row> buildValidDataFileDF(SparkSession spark) {\n-    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return buildValidDataFileDF(spark, table().toString());\n+  }\n+\n+  protected Dataset<Row> buildValidDataFileDF(SparkSession spark, String tableName) {\n+    String allDataFilesMetadataTable = metadataTableName(tableName, MetadataTableType.ALL_DATA_FILES);\n     return spark.read().format(\"iceberg\").load(allDataFilesMetadataTable).select(\"file_path\");\n   }\n \n-  protected Dataset<Row> buildManifestFileDF(SparkSession spark) {\n-    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+  protected Dataset<Row> buildManifestFileDF(SparkSession spark, String tableName) {\n+    String allManifestsMetadataTable = metadataTableName(tableName, MetadataTableType.ALL_MANIFESTS);\n     return spark.read().format(\"iceberg\").load(allManifestsMetadataTable).selectExpr(\"path as file_path\");\n   }\n \n-  protected Dataset<Row> buildManifestListDF(SparkSession spark, Table table) {\n-    List<String> manifestLists = getManifestListPaths(table);\n+  protected Dataset<Row> buildManifestListDF(SparkSession spark, String tableName, TableOperations ops) {\n+    Table snapshot = new BaseTable(ops, tableName);\n+    List<String> manifestLists = getManifestListPaths(snapshot);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0596f8a7a7ab1faf20b73c89fab0c0247e8a4e60"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM2MDYyMg==", "bodyText": "Sounds good to me", "url": "https://github.com/apache/iceberg/pull/1344#discussion_r473360622", "createdAt": "2020-08-19T21:53:36Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -86,17 +90,22 @@ protected String metadataTableName(MetadataTableType type) {\n   }\n \n   protected Dataset<Row> buildValidDataFileDF(SparkSession spark) {\n-    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return buildValidDataFileDF(spark, table().toString());\n+  }\n+\n+  protected Dataset<Row> buildValidDataFileDF(SparkSession spark, String tableName) {\n+    String allDataFilesMetadataTable = metadataTableName(tableName, MetadataTableType.ALL_DATA_FILES);\n     return spark.read().format(\"iceberg\").load(allDataFilesMetadataTable).select(\"file_path\");\n   }\n \n-  protected Dataset<Row> buildManifestFileDF(SparkSession spark) {\n-    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+  protected Dataset<Row> buildManifestFileDF(SparkSession spark, String tableName) {\n+    String allManifestsMetadataTable = metadataTableName(tableName, MetadataTableType.ALL_MANIFESTS);\n     return spark.read().format(\"iceberg\").load(allManifestsMetadataTable).selectExpr(\"path as file_path\");\n   }\n \n-  protected Dataset<Row> buildManifestListDF(SparkSession spark, Table table) {\n-    List<String> manifestLists = getManifestListPaths(table);\n+  protected Dataset<Row> buildManifestListDF(SparkSession spark, String tableName, TableOperations ops) {\n+    Table snapshot = new BaseTable(ops, tableName);\n+    List<String> manifestLists = getManifestListPaths(snapshot);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzM1Njc1Nw=="}, "originalCommit": {"oid": "0596f8a7a7ab1faf20b73c89fab0c0247e8a4e60"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk2MDAyMjE5OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQwMzozMzoyOFrOHDoQIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQwMzozMzoyOFrOHDoQIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzU2NzI2NA==", "bodyText": "You cannot pass a pure table name here since we aren't looking up the table using Spark, this path is for metadataFileLocation based tables only.", "url": "https://github.com/apache/iceberg/pull/1344#discussion_r473567264", "createdAt": "2020-08-20T03:33:28Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -86,27 +91,36 @@ protected String metadataTableName(MetadataTableType type) {\n   }\n \n   protected Dataset<Row> buildValidDataFileDF(SparkSession spark) {\n-    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return buildValidDataFileDF(spark, table().toString());\n+  }\n+\n+  protected Dataset<Row> buildValidDataFileDF(SparkSession spark, String tableName) {\n+    String allDataFilesMetadataTable = metadataTableName(tableName, MetadataTableType.ALL_DATA_FILES);\n     return spark.read().format(\"iceberg\").load(allDataFilesMetadataTable).select(\"file_path\");\n   }\n \n-  protected Dataset<Row> buildManifestFileDF(SparkSession spark) {\n-    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+  protected Dataset<Row> buildManifestFileDF(SparkSession spark, String tableName) {\n+    String allManifestsMetadataTable = metadataTableName(tableName, MetadataTableType.ALL_MANIFESTS);\n     return spark.read().format(\"iceberg\").load(allManifestsMetadataTable).selectExpr(\"path as file_path\");\n   }\n \n   protected Dataset<Row> buildManifestListDF(SparkSession spark, Table table) {\n-    List<String> manifestLists = getManifestListPaths(table);\n+    List<String> manifestLists = getManifestListPaths(table.snapshots());\n     return spark.createDataset(manifestLists, Encoders.STRING()).toDF(\"file_path\");\n   }\n \n+  protected Dataset<Row> buildManifestListDF(SparkSession spark, String metadataFileLocation) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2f76828614fab5a8d8230655aa97eb7bd99f0e2e"}, "originalPosition": 70}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3653, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}