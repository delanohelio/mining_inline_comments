{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3MTc0OTAz", "number": 1192, "title": "Implement Hive input format", "bodyText": "This PR implements:\n\na MR v1 input format that wraps the MR v2 input format\na Hive input format built on top of the MR v1 input format\n\nA few things:\n\nThe Hive input format successfully passes the Expedia test suite (see this branch). Once this work is merged, @massdosage will open a PR with a test suite based on HiveRunner.\nI've chosen to separate the MR and Hive implementations that way we can reuse the test suite for the MR input format and keep the Hive specific stuff on its own. In the future, I'd also like to experiment wit other in-memory  representations for Hive so that also allows that.\nI've taken the IcebergSplit class out off IcebergInputFormat and the class now extends mapreduce.InputSplit and implements mapred.InputSplit so it can be returned by both MR v1 and v2 input formats.\nI'm not a fan of the IcebergSplitContainer interface trick that I used to avoid overriding getRecordReader in subclasses of MapredIcebergInputFormat. Recommendations for a more elegant way to do so are welcome.\nI've refactored the TestIcebergInputFormat class quite a bit so it be can run against the two MR file formats implementations.\n\n@rdblue @rdsr @massdosage @cmathiesen\ncc @edgarRd @gustavoatt", "createdAt": "2020-07-10T01:39:46Z", "url": "https://github.com/apache/iceberg/pull/1192", "merged": true, "mergeCommit": {"oid": "129c9693683fe0f3937f467d1fd817cb602f8224"}, "closed": true, "closedAt": "2020-07-22T17:06:21Z", "author": {"login": "guilload"}, "timelineItems": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcztTbTAFqTQ0Njc2MTQ0Nw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc3OkY2AFqTQ1Mjg5MDY2Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2NzYxNDQ3", "url": "https://github.com/apache/iceberg/pull/1192#pullrequestreview-446761447", "createdAt": "2020-07-11T00:43:10Z", "commit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo0MzoxMFrOGwJCmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo0MzoxMFrOGwJCmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMjk1Mg==", "bodyText": "Looks like what's happening is the table location is used as the split's path so that Hive associates all splits with the same PartitionDesc that contains a TableDesc. Is that correct? If so, I think it would be better to add that as the comment. It's difficult to read the Hive code and figure out what's going on using just the pointers here.", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r453132952", "createdAt": "2020-07-11T00:43:10Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSplit.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+// Hive requires file formats to return splits that are instances of `FileSplit`.\n+public class HiveIcebergSplit extends FileSplit implements IcebergSplitContainer {\n+\n+  private IcebergSplit innerSplit;\n+\n+  // The table location of the split allows Hive to map a split to a table and/or partition.\n+  // See calls to `getPartitionDescFromPathRecursively` in `CombineHiveInputFormat` or `HiveInputFormat`.\n+  private String tableLocation;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "originalPosition": 38}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2NzYxNTM2", "url": "https://github.com/apache/iceberg/pull/1192#pullrequestreview-446761536", "createdAt": "2020-07-11T00:44:03Z", "commit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo0NDowM1rOGwJC9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo0NDowM1rOGwJC9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMzA0NQ==", "bodyText": "I don't think these last two functions need to change?", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r453133045", "createdAt": "2020-07-11T00:44:03Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/Container.java", "diffHunk": "@@ -21,48 +21,33 @@\n \n import java.io.DataInput;\n import java.io.DataOutput;\n+import java.io.IOException;\n import org.apache.hadoop.io.Writable;\n-import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.Record;\n \n /**\n- * Wraps an Iceberg Record in a Writable which Hive can use in the SerDe.\n+ * A simple container of objects that you can get and set.\n+ *\n+ * @param <T> the Java type of the object held by this container\n  */\n-public class IcebergWritable implements Writable {\n-\n-  private Record record;\n-  private Schema schema;\n-\n-  public IcebergWritable(Record record, Schema schema) {\n-    this.record = record;\n-    this.schema = schema;\n-  }\n+public class Container<T> implements Writable {\n \n-  @SuppressWarnings(\"checkstyle:HiddenField\")\n-  public void wrapRecord(Record record) {\n-    this.record = record;\n-  }\n-\n-  public Record record() {\n-    return record;\n-  }\n+  private T value;\n \n-  public Schema schema() {\n-    return schema;\n+  public T get() {\n+    return value;\n   }\n \n-  @SuppressWarnings(\"checkstyle:HiddenField\")\n-  public void wrapSchema(Schema schema) {\n-    this.schema = schema;\n+  public void set(T newValue) {\n+    this.value = newValue;\n   }\n \n   @Override\n-  public void write(DataOutput dataOutput) {\n-    throw new UnsupportedOperationException(\"write is not supported.\");\n+  public void readFields(DataInput in) throws IOException {\n+    throw new UnsupportedOperationException(\"readFields is not supported\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2NzYxODg4", "url": "https://github.com/apache/iceberg/pull/1192#pullrequestreview-446761888", "createdAt": "2020-07-11T00:46:49Z", "commit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo0Njo1MFrOGwJEkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo0Njo1MFrOGwJEkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMzQ1OA==", "bodyText": "Could you update this to Java class of records constructed by Iceberg; default is {@link Record}?\nIt is odd that this currently states that T could be either A or B, but defaults to C.", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r453133458", "createdAt": "2020-07-11T00:46:50Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptContextImpl;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+/**\n+ * Generic Mrv1 InputFormat API for Iceberg.\n+ *\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "originalPosition": 39}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2NzYyMjg2", "url": "https://github.com/apache/iceberg/pull/1192#pullrequestreview-446762286", "createdAt": "2020-07-11T00:49:48Z", "commit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo0OTo0OFrOGwJGGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo0OTo0OFrOGwJGGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMzg0OA==", "bodyText": "When would mapred.task.id be null? Should we throw an exception in that case?", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r453133848", "createdAt": "2020-07-11T00:49:48Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptContextImpl;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+/**\n+ * Generic Mrv1 InputFormat API for Iceberg.\n+ *\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class MapredIcebergInputFormat<T> implements InputFormat<Void, Container<T>> {\n+\n+  private final org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> innerInputFormat;\n+\n+  public MapredIcebergInputFormat() {\n+    this.innerInputFormat = new org.apache.iceberg.mr.mapreduce.IcebergInputFormat<>();\n+  }\n+\n+  /**\n+   * Configures the {@code JobConf} to use the {@code MapredIcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code JobConf} to configure\n+   */\n+  public static InputFormatConfig.ConfigBuilder configure(JobConf job) {\n+    job.setInputFormat(MapredIcebergInputFormat.class);\n+    return new InputFormatConfig.ConfigBuilder(job);\n+  }\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    return innerInputFormat.getSplits(getTaskAttemptContext(job))\n+                           .stream()\n+                           .map(InputSplit.class::cast)\n+                           .toArray(InputSplit[]::new);\n+  }\n+\n+  @Override\n+  public RecordReader<Void, Container<T>> getRecordReader(InputSplit split, JobConf job,\n+                                                          Reporter reporter) throws IOException {\n+    IcebergSplit icebergSplit = ((IcebergSplitContainer) split).icebergSplit();\n+    return new MapredIcebergRecordReader<>(innerInputFormat, icebergSplit, job, reporter);\n+  }\n+\n+  static TaskAttemptContext getTaskAttemptContext(JobConf job) {\n+    TaskAttemptID taskAttemptID = Optional.ofNullable(TaskAttemptID.forName(job.get(\"mapred.task.id\")))\n+                                          .orElse(new TaskAttemptID());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "originalPosition": 77}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2NzYzMDE5", "url": "https://github.com/apache/iceberg/pull/1192#pullrequestreview-446763019", "createdAt": "2020-07-11T00:56:18Z", "commit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo1NjoxOFrOGwJJXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMVQwMDo1NjoxOFrOGwJJXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzNDY4NA==", "bodyText": "getSplits accepts a JobContext and I think it makes sense to pass objects that are as close as possible to what the mapreduce framework would use. We have some helper methods in our branch for reading Hive tables from Spark's DSv2 that you might want to check out: https://github.com/Netflix/iceberg/blob/netflix-spark-2.4/metacat/src/main/java/com/netflix/iceberg/batch/MapReduceUtil.java.\nThose can help you create mapreduce objects after inspecting the mapred objects.", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r453134684", "createdAt": "2020-07-11T00:56:18Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptContextImpl;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+/**\n+ * Generic Mrv1 InputFormat API for Iceberg.\n+ *\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class MapredIcebergInputFormat<T> implements InputFormat<Void, Container<T>> {\n+\n+  private final org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> innerInputFormat;\n+\n+  public MapredIcebergInputFormat() {\n+    this.innerInputFormat = new org.apache.iceberg.mr.mapreduce.IcebergInputFormat<>();\n+  }\n+\n+  /**\n+   * Configures the {@code JobConf} to use the {@code MapredIcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code JobConf} to configure\n+   */\n+  public static InputFormatConfig.ConfigBuilder configure(JobConf job) {\n+    job.setInputFormat(MapredIcebergInputFormat.class);\n+    return new InputFormatConfig.ConfigBuilder(job);\n+  }\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    return innerInputFormat.getSplits(getTaskAttemptContext(job))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f46b38b081a8d2a3b6b439ab6368c29447808e1c"}, "originalPosition": 62}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e55aa7490f5e93da502acf58a3bcca0b805ba8bf", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/e55aa7490f5e93da502acf58a3bcca0b805ba8bf", "committedDate": "2020-07-14T01:54:32Z", "message": "Forward configuration settings to MR input format"}, "afterCommit": {"oid": "f3928c77ea2498fa363257ddec86ff5090288712", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/f3928c77ea2498fa363257ddec86ff5090288712", "committedDate": "2020-07-14T01:57:38Z", "message": "Forward configuration settings to MR input format"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f3928c77ea2498fa363257ddec86ff5090288712", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/f3928c77ea2498fa363257ddec86ff5090288712", "committedDate": "2020-07-14T01:57:38Z", "message": "Forward configuration settings to MR input format"}, "afterCommit": {"oid": "5109c3934aee502ec541f9a1f5a2e8ed08339011", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/5109c3934aee502ec541f9a1f5a2e8ed08339011", "committedDate": "2020-07-14T22:54:12Z", "message": "Forward configuration settings to MR input format"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5109c3934aee502ec541f9a1f5a2e8ed08339011", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/5109c3934aee502ec541f9a1f5a2e8ed08339011", "committedDate": "2020-07-14T22:54:12Z", "message": "Forward configuration settings to MR input format"}, "afterCommit": {"oid": "9351f1857b224b2a4a90296934d2513587b1245e", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/9351f1857b224b2a4a90296934d2513587b1245e", "committedDate": "2020-07-14T23:00:39Z", "message": "Forward configuration settings to MR input format"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9351f1857b224b2a4a90296934d2513587b1245e", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/9351f1857b224b2a4a90296934d2513587b1245e", "committedDate": "2020-07-14T23:00:39Z", "message": "Forward configuration settings to MR input format"}, "afterCommit": {"oid": "23ad535e8206efb4bbb1bef1a9504051aa4ec80c", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/23ad535e8206efb4bbb1bef1a9504051aa4ec80c", "committedDate": "2020-07-14T23:04:21Z", "message": "Forward configuration settings to MR input format"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ4ODQ1OTQ4", "url": "https://github.com/apache/iceberg/pull/1192#pullrequestreview-448845948", "createdAt": "2020-07-15T10:58:42Z", "commit": {"oid": "23ad535e8206efb4bbb1bef1a9504051aa4ec80c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMDo1ODo0MlrOGx4_tQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxMDo1ODo0MlrOGx4_tQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDk2NzIyMQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                projection = projectedColumns.isEmpty() ? schema : projection.select(projectedColumns);\n          \n          \n            \n                projection = projectedColumns.isEmpty() ? schema : schema.select(projectedColumns);\n          \n      \n    \n    \n  \n\nShouldn't the code be like above? If not I get a NPE like this when running a HiveRunner test:\n       Caused by:\n                java.lang.NullPointerException\n                    at org.apache.iceberg.mr.hive.HiveIcebergInputFormat.getSplits(HiveIcebergInputFormat.java:58)\n                    at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextSplits(FetchOperator.java:372)\n                    at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:304)\n                    at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:459)\n                    ... 13 more", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r454967221", "createdAt": "2020-07-15T10:58:42Z", "author": {"login": "massdosage"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.mr.mapred.MapredIcebergInputFormat;\n+import org.apache.iceberg.mr.mapred.TableResolver;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class HiveIcebergInputFormat extends MapredIcebergInputFormat<Record>\n+                                    implements CombineHiveInputFormat.AvoidSplitCombination {\n+\n+  private transient Table table;\n+  private transient Schema schema;\n+  private transient Schema projection;\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    table = TableResolver.resolveTableFromConfiguration(job);\n+    schema = table.schema();\n+\n+    List<String> projectedColumns = parseProjectedColumns(job);\n+    projection = projectedColumns.isEmpty() ? schema : projection.select(projectedColumns);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23ad535e8206efb4bbb1bef1a9504051aa4ec80c"}, "originalPosition": 58}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7c93671751cf5ded84fba084e784da444e7af05d", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/7c93671751cf5ded84fba084e784da444e7af05d", "committedDate": "2020-07-15T17:27:46Z", "message": "Delete unused file"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "23ad535e8206efb4bbb1bef1a9504051aa4ec80c", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/23ad535e8206efb4bbb1bef1a9504051aa4ec80c", "committedDate": "2020-07-14T23:04:21Z", "message": "Forward configuration settings to MR input format"}, "afterCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/b373262cf50391eb1b4d44d49bc3dcbc14b1e80a", "committedDate": "2020-07-15T17:28:05Z", "message": "Forward configuration settings to MR input format"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ4MTI0MjY4", "url": "https://github.com/apache/iceberg/pull/1192#pullrequestreview-448124268", "createdAt": "2020-07-14T13:49:43Z", "commit": {"oid": "f3928c77ea2498fa363257ddec86ff5090288712"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQxMzo0OTo0M1rOGxUjvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxOTo1OTo1M1rOGyNv4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDM3MDIzOQ==", "bodyText": "Can these properties ever be null?", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r454370239", "createdAt": "2020-07-14T13:49:43Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.mr.mapred.MapredIcebergInputFormat;\n+import org.apache.iceberg.mr.mapred.TableResolver;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class HiveIcebergInputFormat extends MapredIcebergInputFormat<Record>\n+                                    implements CombineHiveInputFormat.AvoidSplitCombination {\n+\n+  private transient Table table;\n+  private transient Schema schema;\n+  private transient Schema projection;\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    table = TableResolver.resolveTableFromConfiguration(job);\n+    schema = table.schema();\n+\n+    List<String> projectedColumns = parseProjectedColumns(job);\n+    projection = projectedColumns.isEmpty() ? schema : projection.select(projectedColumns);\n+\n+    forwardConfigSettings(job);\n+\n+    return Arrays.stream(super.getSplits(job, numSplits))\n+                 .map(split -> new HiveIcebergSplit((IcebergSplit) split, table.location()))\n+                 .toArray(InputSplit[]::new);\n+  }\n+\n+  @Override\n+  public RecordReader<Void, Container<Record>> getRecordReader(InputSplit split, JobConf job,\n+                                                               Reporter reporter) throws IOException {\n+    // Since Hive passes a copy of `job` in `getSplits`, we need to forward the conf settings again.\n+    forwardConfigSettings(job);\n+    return super.getRecordReader(split, job, reporter);\n+  }\n+\n+  @Override\n+  public boolean shouldSkipCombine(Path path, Configuration conf) {\n+    return true;\n+  }\n+\n+  /**\n+   * Forward configuration settings to the underlying MR input format.\n+   */\n+  private void forwardConfigSettings(JobConf job) {\n+    Preconditions.checkNotNull(table, \"Table cannot be null\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f3928c77ea2498fa363257ddec86ff5090288712"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI4NTg0Mw==", "bodyText": "seems like conf  will not be null", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r455285843", "createdAt": "2020-07-15T19:20:14Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.mr.mapred.MapredIcebergInputFormat;\n+import org.apache.iceberg.mr.mapred.TableResolver;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class HiveIcebergInputFormat extends MapredIcebergInputFormat<Record>\n+                                    implements CombineHiveInputFormat.AvoidSplitCombination {\n+\n+  private transient Table table;\n+  private transient Schema schema;\n+  private transient Schema projection;\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    table = TableResolver.resolveTableFromConfiguration(job);\n+    schema = table.schema();\n+\n+    List<String> projectedColumns = parseProjectedColumns(job);\n+    projection = projectedColumns.isEmpty() ? schema : schema.select(projectedColumns);\n+\n+    forwardConfigSettings(job);\n+\n+    return Arrays.stream(super.getSplits(job, numSplits))\n+                 .map(split -> new HiveIcebergSplit((IcebergSplit) split, table.location()))\n+                 .toArray(InputSplit[]::new);\n+  }\n+\n+  @Override\n+  public RecordReader<Void, Container<Record>> getRecordReader(InputSplit split, JobConf job,\n+                                                               Reporter reporter) throws IOException {\n+    // Since Hive passes a copy of `job` in `getSplits`, we need to forward the conf settings again.\n+    forwardConfigSettings(job);\n+    return super.getRecordReader(split, job, reporter);\n+  }\n+\n+  @Override\n+  public boolean shouldSkipCombine(Path path, Configuration conf) {\n+    return true;\n+  }\n+\n+  /**\n+   * Forward configuration settings to the underlying MR input format.\n+   */\n+  private void forwardConfigSettings(JobConf job) {\n+    Preconditions.checkNotNull(table, \"Table cannot be null\");\n+    Preconditions.checkNotNull(schema, \"Schema cannot be null\");\n+    Preconditions.checkNotNull(projection, \"Projection cannot be null\");\n+\n+    // Once mapred.TableResolver and mapreduce.TableResolver use the same property for the location of the table\n+    // (TABLE_LOCATION vs. TABLE_PATH), this line can be removed: see https://github.com/apache/iceberg/issues/1155.\n+    job.set(InputFormatConfig.TABLE_PATH, table.location());\n+    job.set(InputFormatConfig.TABLE_SCHEMA, SchemaParser.toJson(schema));\n+    job.set(InputFormatConfig.READ_SCHEMA, SchemaParser.toJson(projection));\n+  }\n+\n+  private static List<String> parseProjectedColumns(Configuration conf) {\n+    if (conf == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI4NzUyNA==", "bodyText": "Who is setting these configurations. IcebergStorageHandler?", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r455287524", "createdAt": "2020-07-15T19:23:19Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.mr.mapred.MapredIcebergInputFormat;\n+import org.apache.iceberg.mr.mapred.TableResolver;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class HiveIcebergInputFormat extends MapredIcebergInputFormat<Record>\n+                                    implements CombineHiveInputFormat.AvoidSplitCombination {\n+\n+  private transient Table table;\n+  private transient Schema schema;\n+  private transient Schema projection;\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    table = TableResolver.resolveTableFromConfiguration(job);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI5MzI1OQ==", "bodyText": "is this null check required?", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r455293259", "createdAt": "2020-07-15T19:33:35Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptContextImpl;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.Counter;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.JobID;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.task.JobContextImpl;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+/**\n+ * Generic MR v1 InputFormat API for Iceberg.\n+ *\n+ * @param <T> Java class of records constructed by Iceberg; default is {@link Record}\n+ */\n+public class MapredIcebergInputFormat<T> implements InputFormat<Void, Container<T>> {\n+\n+  private final org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> innerInputFormat;\n+\n+  public MapredIcebergInputFormat() {\n+    this.innerInputFormat = new org.apache.iceberg.mr.mapreduce.IcebergInputFormat<>();\n+  }\n+\n+  /**\n+   * Configures the {@code JobConf} to use the {@code MapredIcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code JobConf} to configure\n+   */\n+  public static InputFormatConfig.ConfigBuilder configure(JobConf job) {\n+    job.setInputFormat(MapredIcebergInputFormat.class);\n+    return new InputFormatConfig.ConfigBuilder(job);\n+  }\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    return innerInputFormat.getSplits(newJobContext(job))\n+                           .stream()\n+                           .map(InputSplit.class::cast)\n+                           .toArray(InputSplit[]::new);\n+  }\n+\n+  @Override\n+  public RecordReader<Void, Container<T>> getRecordReader(InputSplit split, JobConf job,\n+                                                          Reporter reporter) throws IOException {\n+    IcebergSplit icebergSplit = ((IcebergSplitContainer) split).icebergSplit();\n+    return new MapredIcebergRecordReader<>(innerInputFormat, icebergSplit, job, reporter);\n+  }\n+\n+  private static final class MapredIcebergRecordReader<T> implements RecordReader<Void, Container<T>> {\n+\n+    private final org.apache.hadoop.mapreduce.RecordReader<Void, T> innerReader;\n+    private final long splitLength; // for getPos()\n+\n+    MapredIcebergRecordReader(org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> mapreduceInputFormat,\n+                              IcebergSplit split, JobConf job, Reporter reporter) throws IOException {\n+      TaskAttemptContext context = newTaskAttemptContext(job, reporter);\n+\n+      try {\n+        innerReader = mapreduceInputFormat.createRecordReader(split, context);\n+        innerReader.initialize(split, context);\n+      } catch (InterruptedException e) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(e);\n+      }\n+\n+      splitLength = split.getLength();\n+    }\n+\n+    @Override\n+    public boolean next(Void key, Container<T> value) throws IOException {\n+      try {\n+        if (innerReader.nextKeyValue()) {\n+          value.set(innerReader.getCurrentValue());\n+          return true;\n+        }\n+      } catch (InterruptedException ie) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(ie);\n+      }\n+\n+      return false;\n+    }\n+\n+    @Override\n+    public Void createKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public Container<T> createValue() {\n+      return new Container<>();\n+    }\n+\n+    @Override\n+    public long getPos() throws IOException {\n+      return (long) (splitLength * getProgress());\n+    }\n+\n+    @Override\n+    public float getProgress() throws IOException {\n+      if (innerReader == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI5NDM2Ng==", "bodyText": "is JobContext.ID which maps to mapreduce.job.id an MRv2 setting and will not be set for Mrv1 jobs?", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r455294366", "createdAt": "2020-07-15T19:35:39Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapred/MapredIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapred;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+import org.apache.hadoop.mapred.InputFormat;\n+import org.apache.hadoop.mapred.InputSplit;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.RecordReader;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptContextImpl;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.Counter;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.JobID;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.task.JobContextImpl;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+/**\n+ * Generic MR v1 InputFormat API for Iceberg.\n+ *\n+ * @param <T> Java class of records constructed by Iceberg; default is {@link Record}\n+ */\n+public class MapredIcebergInputFormat<T> implements InputFormat<Void, Container<T>> {\n+\n+  private final org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> innerInputFormat;\n+\n+  public MapredIcebergInputFormat() {\n+    this.innerInputFormat = new org.apache.iceberg.mr.mapreduce.IcebergInputFormat<>();\n+  }\n+\n+  /**\n+   * Configures the {@code JobConf} to use the {@code MapredIcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code JobConf} to configure\n+   */\n+  public static InputFormatConfig.ConfigBuilder configure(JobConf job) {\n+    job.setInputFormat(MapredIcebergInputFormat.class);\n+    return new InputFormatConfig.ConfigBuilder(job);\n+  }\n+\n+  @Override\n+  public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {\n+    return innerInputFormat.getSplits(newJobContext(job))\n+                           .stream()\n+                           .map(InputSplit.class::cast)\n+                           .toArray(InputSplit[]::new);\n+  }\n+\n+  @Override\n+  public RecordReader<Void, Container<T>> getRecordReader(InputSplit split, JobConf job,\n+                                                          Reporter reporter) throws IOException {\n+    IcebergSplit icebergSplit = ((IcebergSplitContainer) split).icebergSplit();\n+    return new MapredIcebergRecordReader<>(innerInputFormat, icebergSplit, job, reporter);\n+  }\n+\n+  private static final class MapredIcebergRecordReader<T> implements RecordReader<Void, Container<T>> {\n+\n+    private final org.apache.hadoop.mapreduce.RecordReader<Void, T> innerReader;\n+    private final long splitLength; // for getPos()\n+\n+    MapredIcebergRecordReader(org.apache.iceberg.mr.mapreduce.IcebergInputFormat<T> mapreduceInputFormat,\n+                              IcebergSplit split, JobConf job, Reporter reporter) throws IOException {\n+      TaskAttemptContext context = newTaskAttemptContext(job, reporter);\n+\n+      try {\n+        innerReader = mapreduceInputFormat.createRecordReader(split, context);\n+        innerReader.initialize(split, context);\n+      } catch (InterruptedException e) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(e);\n+      }\n+\n+      splitLength = split.getLength();\n+    }\n+\n+    @Override\n+    public boolean next(Void key, Container<T> value) throws IOException {\n+      try {\n+        if (innerReader.nextKeyValue()) {\n+          value.set(innerReader.getCurrentValue());\n+          return true;\n+        }\n+      } catch (InterruptedException ie) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(ie);\n+      }\n+\n+      return false;\n+    }\n+\n+    @Override\n+    public Void createKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public Container<T> createValue() {\n+      return new Container<>();\n+    }\n+\n+    @Override\n+    public long getPos() throws IOException {\n+      return (long) (splitLength * getProgress());\n+    }\n+\n+    @Override\n+    public float getProgress() throws IOException {\n+      if (innerReader == null) {\n+        return 0;\n+      }\n+\n+      try {\n+        return innerReader.getProgress();\n+      } catch (InterruptedException e) {\n+        Thread.currentThread().interrupt();\n+        throw new RuntimeException(e);\n+      }\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      if (innerReader != null) {\n+        innerReader.close();\n+      }\n+    }\n+  }\n+\n+  private static JobContext newJobContext(JobConf job) {\n+    JobID jobID = Optional.ofNullable(JobID.forName(job.get(JobContext.ID)))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a"}, "originalPosition": 153}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMwNzIzNA==", "bodyText": "Not sure if this is a big win, but can we have the Container<T> as an interface which can then be implemented by Mrv1Value and HiveIcebergSplit", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r455307234", "createdAt": "2020-07-15T19:59:53Z", "author": {"login": "rdsr"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSplit.java", "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.FileSplit;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplit;\n+import org.apache.iceberg.mr.mapreduce.IcebergSplitContainer;\n+\n+// Hive requires file formats to return splits that are instances of `FileSplit`.\n+public class HiveIcebergSplit extends FileSplit implements IcebergSplitContainer {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a"}, "originalPosition": 32}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b373262cf50391eb1b4d44d49bc3dcbc14b1e80a", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/b373262cf50391eb1b4d44d49bc3dcbc14b1e80a", "committedDate": "2020-07-15T17:28:05Z", "message": "Forward configuration settings to MR input format"}, "afterCommit": {"oid": "3cdd9f1a9a408d1cdbf574126b2f96db9bdf32be", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/3cdd9f1a9a408d1cdbf574126b2f96db9bdf32be", "committedDate": "2020-07-16T02:03:46Z", "message": "Add HiveRunner tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3cdd9f1a9a408d1cdbf574126b2f96db9bdf32be", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/3cdd9f1a9a408d1cdbf574126b2f96db9bdf32be", "committedDate": "2020-07-16T02:03:46Z", "message": "Add HiveRunner tests"}, "afterCommit": {"oid": "61859ccf41e42ce6e18a0d281115e04d2bb03273", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/61859ccf41e42ce6e18a0d281115e04d2bb03273", "committedDate": "2020-07-16T02:39:44Z", "message": "Add HiveRunner tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwMjM5MjM1", "url": "https://github.com/apache/iceberg/pull/1192#pullrequestreview-450239235", "createdAt": "2020-07-16T21:46:58Z", "commit": {"oid": "61859ccf41e42ce6e18a0d281115e04d2bb03273"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMTo0Njo1OFrOGy97ew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMTo0Njo1OFrOGy97ew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjA5NjYzNQ==", "bodyText": "Hive keeps a cache of input format instance arounds (see HiveInputFormat) that breaks this logic so I've chosen to remove it for now.\nWe can re-implement this later but the logic will have to be a bit more robust.", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r456096635", "createdAt": "2020-07-16T21:46:58Z", "author": {"login": "guilload"}, "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -100,11 +94,6 @@\n \n   @Override\n   public List<InputSplit> getSplits(JobContext context) {\n-    if (splits != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "61859ccf41e42ce6e18a0d281115e04d2bb03273"}, "originalPosition": 38}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "207a416f603d16fb5cebc9c70bf187e350712fb6", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/207a416f603d16fb5cebc9c70bf187e350712fb6", "committedDate": "2020-07-16T22:02:30Z", "message": "Implement MapredIcebergInputFormat"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e5193186e1a05526870f8e822d5e1985512837af", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/e5193186e1a05526870f8e822d5e1985512837af", "committedDate": "2020-07-16T22:02:30Z", "message": "Implement HiveIcebergInputFormat"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6f4fbe2eab62b7454f0ddbacf276a4532b9df684", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/6f4fbe2eab62b7454f0ddbacf276a4532b9df684", "committedDate": "2020-07-16T22:02:30Z", "message": "Forward configuration settings to MR input format"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3b76337a3a198b07fd4140f5de972c1c72f6ca96", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/3b76337a3a198b07fd4140f5de972c1c72f6ca96", "committedDate": "2020-07-16T22:02:30Z", "message": "Remove splits caching"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f535af0e18b322082e83e33dc31a44e13719ef83", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/f535af0e18b322082e83e33dc31a44e13719ef83", "committedDate": "2020-07-16T22:02:31Z", "message": "Add HiveRunner tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "61859ccf41e42ce6e18a0d281115e04d2bb03273", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/61859ccf41e42ce6e18a0d281115e04d2bb03273", "committedDate": "2020-07-16T02:39:44Z", "message": "Add HiveRunner tests"}, "afterCommit": {"oid": "f535af0e18b322082e83e33dc31a44e13719ef83", "author": {"user": {"login": "guilload", "name": "Adrien Guillo"}}, "url": "https://github.com/apache/iceberg/commit/f535af0e18b322082e83e33dc31a44e13719ef83", "committedDate": "2020-07-16T22:02:31Z", "message": "Add HiveRunner tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUyODkwNjY2", "url": "https://github.com/apache/iceberg/pull/1192#pullrequestreview-452890666", "createdAt": "2020-07-21T23:10:19Z", "commit": {"oid": "f535af0e18b322082e83e33dc31a44e13719ef83"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMzoxMDoxOVrOG1M_gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQyMzoxMDoxOVrOG1M_gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODQ0MDU3OA==", "bodyText": "This should be Assert.assertEquals(3, descRows.size());", "url": "https://github.com/apache/iceberg/pull/1192#discussion_r458440578", "createdAt": "2020-07-21T23:10:19Z", "author": {"login": "HotSushi"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import com.klarna.hiverunner.HiveShell;\n+import com.klarna.hiverunner.StandaloneHiveRunner;\n+import com.klarna.hiverunner.annotations.HiveSQL;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.mr.TestHelper;\n+import org.apache.iceberg.mr.mapred.IcebergSerDe;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+@RunWith(StandaloneHiveRunner.class)\n+public class TestHiveIcebergInputFormat {\n+\n+  @HiveSQL(files = {}, autoStart = true)\n+  private HiveShell shell;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private static final Schema CUSTOMER_SCHEMA = new Schema(\n+          required(1, \"customer_id\", Types.LongType.get()),\n+          required(2, \"first_name\", Types.StringType.get())\n+  );\n+\n+  private static final List<Record> CUSTOMER_RECORDS = TestHelper.RecordsBuilder.newInstance(CUSTOMER_SCHEMA)\n+          .add(0L, \"Alice\")\n+          .add(1L, \"Bob\")\n+          .add(2L, \"Trudy\")\n+          .build();\n+\n+  private static final Schema ORDER_SCHEMA = new Schema(\n+          required(1, \"order_id\", Types.LongType.get()),\n+          required(2, \"customer_id\", Types.LongType.get()),\n+          required(3, \"total\", Types.DoubleType.get()));\n+\n+  private static final List<Record> ORDER_RECORDS = TestHelper.RecordsBuilder.newInstance(ORDER_SCHEMA)\n+          .add(100L, 0L, 11.11d)\n+          .add(101L, 0L, 22.22d)\n+          .add(102L, 1L, 33.33d)\n+          .build();\n+\n+  // before variables\n+  private HadoopTables tables;\n+  private Table customerTable;\n+  private Table orderTable;\n+\n+  @Before\n+  public void before() throws IOException {\n+    Configuration conf = new Configuration();\n+    tables = new HadoopTables(conf);\n+\n+    File customerLocation = temp.newFolder(\"customers\");\n+    Assert.assertTrue(customerLocation.delete());\n+\n+    TestHelper customerHelper = new TestHelper(\n+            conf, tables, CUSTOMER_SCHEMA, PartitionSpec.unpartitioned(), FileFormat.PARQUET, temp, customerLocation);\n+\n+    customerTable = customerHelper.createUnpartitionedTable();\n+    customerHelper.appendToTable(customerHelper.writeFile(null, CUSTOMER_RECORDS));\n+\n+    File orderLocation = temp.newFolder(\"orders\");\n+    Assert.assertTrue(orderLocation.delete());\n+\n+    TestHelper orderHelper = new TestHelper(\n+            conf, tables, ORDER_SCHEMA, PartitionSpec.unpartitioned(), FileFormat.PARQUET, temp, orderLocation);\n+\n+    orderTable = orderHelper.createUnpartitionedTable();\n+    orderHelper.appendToTable(orderHelper.writeFile(null, ORDER_RECORDS));\n+  }\n+\n+  @Test\n+  public void testScanEmptyTable() throws IOException {\n+    File emptyLocation = temp.newFolder(\"empty\");\n+    Assert.assertTrue(emptyLocation.delete());\n+\n+    Schema emptySchema = new Schema(required(1, \"empty\", Types.StringType.get()));\n+    Table emptyTable = tables.create(\n+            emptySchema, PartitionSpec.unpartitioned(), Collections.emptyMap(), emptyLocation.toString());\n+    createHiveTable(\"empty\", emptyTable.location());\n+\n+    List<Object[]> rows = shell.executeStatement(\"SELECT * FROM default.empty\");\n+    Assert.assertEquals(0, rows.size());\n+  }\n+\n+  @Test\n+  public void testScanTable() {\n+    createHiveTable(\"customers\", customerTable.location());\n+\n+    // Single fetch task: no MR job.\n+    List<Object[]> rows = shell.executeStatement(\"SELECT * FROM default.customers\");\n+\n+    Assert.assertEquals(3, rows.size());\n+    Assert.assertArrayEquals(new Object[] {0L, \"Alice\"}, rows.get(0));\n+    Assert.assertArrayEquals(new Object[] {1L, \"Bob\"}, rows.get(1));\n+    Assert.assertArrayEquals(new Object[] {2L, \"Trudy\"}, rows.get(2));\n+\n+    // Adding the ORDER BY clause will cause Hive to spawn a local MR job this time.\n+    List<Object[]> descRows = shell.executeStatement(\"SELECT * FROM default.customers ORDER BY customer_id DESC\");\n+\n+    Assert.assertEquals(3, rows.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f535af0e18b322082e83e33dc31a44e13719ef83"}, "originalPosition": 137}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4252, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}