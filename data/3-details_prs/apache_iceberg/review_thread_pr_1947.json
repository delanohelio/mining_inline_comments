{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQxNDc5ODI1", "number": 1947, "reviewThreads": {"totalCount": 55, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOTowMTo0MFrOFGmRnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMTozOTowN1rOFQMuDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNDYyODc5OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOTowMTo0MFrOIHrSjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMVQxMTozNDo1M1rOIJVUNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkyMDIwNA==", "bodyText": "Do we need this node? It seems we rewrite the operation into ReplaceData, no?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r544920204", "createdAt": "2020-12-17T09:01:40Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoProcessor\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoProcessor,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAwNzU0MQ==", "bodyText": "Well, I overlooked that we use MergeInto node in RewriteMergeInto.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r545007541", "createdAt": "2020-12-17T11:12:34Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoProcessor\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoProcessor,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkyMDIwNA=="}, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTAwNzY2Nw==", "bodyText": "I wonder whether we can use MapPartitions directly.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r545007667", "createdAt": "2020-12-17T11:12:50Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoProcessor\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoProcessor,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkyMDIwNA=="}, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUyMDczOQ==", "bodyText": "I think that MergeIntoProcessor and this node should be merged. That's really a physical plan node and it is strange how it is created and passed through the logical plan.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r545520739", "createdAt": "2020-12-18T01:49:48Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoProcessor\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoProcessor,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkyMDIwNA=="}, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjY1NzMzNQ==", "bodyText": "I agree with that. I think we can address this in the end. This bit is working and I'd focus on other things for now.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r546657335", "createdAt": "2020-12-21T11:34:53Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoProcessor\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoProcessor,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDkyMDIwNA=="}, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyNDc3OTIyOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOTozNToyMFrOIHsqQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QwOTozNToyMFrOIHsqQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDk0MjY1Nw==", "bodyText": "nit: these vals can be private", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r544942657", "createdAt": "2020-12-17T09:35:20Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner, JoinType}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+object RewriteMergeInto extends Rule[LogicalPlan]\n+  with PredicateHelper\n+  with Logging  {\n+  val ROW_ID_COL = \"_row_id_\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyODYzNDQzOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMTozNjowNFrOIIPqWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMTozNjowNFrOIIPqWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxNjEyMA==", "bodyText": "It would be helpful to group some of these plan nodes into sections, like in RewriteDelete where methods like buildFileFilterPlan and buildScanPlan give good context for what plans are being constructed and how they will be used.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r545516120", "createdAt": "2020-12-18T01:36:04Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner, JoinType}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+object RewriteMergeInto extends Rule[LogicalPlan]\n+  with PredicateHelper\n+  with Logging  {\n+  val ROW_ID_COL = \"_row_id_\"\n+  val FILE_NAME_COL = \"_file_name_\"\n+  val SOURCE_ROW_PRESENT_COL = \"_source_row_present_\"\n+  val TARGET_ROW_PRESENT_COL = \"_target_row_present_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        // Find the files in target that matches the JOIN condition from source.\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyODY0MzM0OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMTo0MDowM1rOIIPvWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMTo0MDowM1rOIIPvWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxNzQwMA==", "bodyText": "As Anton noted, there needs to be validation that the assignments here (for both inserts and updates) match up with the targetOutputCols.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r545517400", "createdAt": "2020-12-18T01:40:03Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,164 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import java.util.UUID\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner, JoinType}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2Relation, DataSourceV2ScanRelation}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types._\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+object RewriteMergeInto extends Rule[LogicalPlan]\n+  with PredicateHelper\n+  with Logging  {\n+  val ROW_ID_COL = \"_row_id_\"\n+  val FILE_NAME_COL = \"_file_name_\"\n+  val SOURCE_ROW_PRESENT_COL = \"_source_row_present_\"\n+  val TARGET_ROW_PRESENT_COL = \"_target_row_present_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        // Find the files in target that matches the JOIN condition from source.\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"delete\", writeInfo)\n+        val targetTableScan =  buildScanPlan(target.table, target.output, mergeBuilder, prunedTargetPlan)\n+        val sourceTableProj = source.output ++ Seq(Alias(lit(true).expr, SOURCE_ROW_PRESENT_COL)())\n+        val targetTableProj = target.output ++ Seq(Alias(lit(true).expr, TARGET_ROW_PRESENT_COL)())\n+        val newTargetTableScan = Project(targetTableProj, targetTableScan)\n+        val newSourceTableScan = Project(sourceTableProj, source)\n+        val joinPlan = Join(newSourceTableScan, newTargetTableScan, FullOuter, Some(cond), JoinHint.NONE)\n+\n+        val mergeIntoProcessor = new MergeIntoProcessor(\n+          isSourceRowNotPresent = resolveExprs(Seq(col(SOURCE_ROW_PRESENT_COL).isNull.expr), joinPlan).head,\n+          isTargetRowNotPresent = resolveExprs(Seq(col(TARGET_ROW_PRESENT_COL).isNull.expr), joinPlan).head,\n+          matchedConditions = actions.map(resolveClauseCondition(_, joinPlan)),\n+          matchedOutputs = actions.map(actionOutput(_, targetOutputCols, joinPlan)),\n+          notMatchedConditions = notActions.map(resolveClauseCondition(_, joinPlan)),\n+          notMatchedOutputs = notActions.map(actionOutput(_, targetOutputCols, joinPlan)),\n+          targetOutput = resolveExprs(targetOutputCols :+ Literal(false), joinPlan),\n+          joinedAttributes = joinPlan.output\n+        )\n+\n+        val mergePlan = MergeInto(mergeIntoProcessor, target, joinPlan)\n+        val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+        ReplaceData(target, batchWrite, mergePlan)\n+    }\n+  }\n+\n+  private def buildScanPlan(\n+      table: Table,\n+      output: Seq[AttributeReference],\n+      mergeBuilder: MergeBuilder,\n+      prunedTargetPlan: LogicalPlan): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+\n+    scan match {\n+      case _: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(prunedTargetPlan)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+  }\n+\n+  private def newWriteInfo(schema: StructType): LogicalWriteInfo = {\n+    val uuid = UUID.randomUUID()\n+    LogicalWriteInfoImpl(queryId = uuid.toString, schema, CaseInsensitiveStringMap.empty)\n+  }\n+\n+  private def buildFileFilterPlan(prunedTargetPlan: LogicalPlan): LogicalPlan = {\n+    val fileAttr = findOutputAttr(prunedTargetPlan, FILE_NAME_COL)\n+    Aggregate(Seq(fileAttr), Seq(fileAttr), prunedTargetPlan)\n+  }\n+\n+  private def findOutputAttr(plan: LogicalPlan, attrName: String): Attribute = {\n+    val resolver = SQLConf.get.resolver\n+    plan.output.find(attr => resolver(attr.name, attrName)).getOrElse {\n+      throw new AnalysisException(s\"Cannot find $attrName in ${plan.output}\")\n+    }\n+  }\n+\n+  private def resolveExprs(exprs: Seq[Expression], plan: LogicalPlan): Seq[Expression] = {\n+    val spark = SparkSession.active\n+    exprs.map { expr => resolveExpressionInternal(spark, expr, plan) }\n+  }\n+\n+  def getTargetOutputCols(target: DataSourceV2Relation): Seq[NamedExpression] = {\n+    target.schema.map { col =>\n+      target.output.find(attr => SQLConf.get.resolver(attr.name, col.name)).getOrElse {\n+        Alias(Literal(null, col.dataType), col.name)()\n+      }\n+    }\n+  }\n+\n+  def actionOutput(clause: MergeAction,\n+                   targetOutputCols: Seq[Expression],\n+                   plan: LogicalPlan): Seq[Expression] = {\n+    val exprs = clause match {\n+      case u: UpdateAction =>\n+        u.assignments.map(_.value) :+ Literal(false)\n+      case _: DeleteAction =>\n+        targetOutputCols :+ Literal(true)\n+      case i: InsertAction =>\n+        i.assignments.map(_.value) :+ Literal(false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 146}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyODY2NDMzOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/MergeInto.scala", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQwMTo0ODo1NlrOIIP7PA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOVQyMjozNDowMFrOII-_UQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUyMDQ0NA==", "bodyText": "This is essentially a physical plan node that is linked into both the physical plan and logical plan. I think it should be a normal physical plan node that is created in a strategy, just like other plans.\nThe main issue with the way this PR currently works is that it doesn't delegate enough to the rest of the Spark planner. All of the analysis is done during rewrite in the optimizer, for example. I think that this should be broken up into analysis rules to validate and update the MergeInto plan, the rewrite rule to build the optimizations and join, and a strategy to convert the logical plan into a MergeIntoExec. I think this should also have a validation rule that checks each action to ensure that the expressions for that action are correctly resolved.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r545520444", "createdAt": "2020-12-18T01:48:56Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/MergeInto.scala", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions.col\n+\n+case class MergeInto(mergeIntoProcessor: MergeIntoProcessor,\n+                     targetRelation: DataSourceV2Relation,\n+                     child: LogicalPlan) extends UnaryNode {\n+  override def output: Seq[Attribute] = targetRelation.output\n+}\n+\n+class MergeIntoProcessor(isSourceRowNotPresent: Expression,\n+                         isTargetRowNotPresent: Expression,\n+                         matchedConditions: Seq[Expression],\n+                         matchedOutputs: Seq[Seq[Expression]],\n+                         notMatchedConditions: Seq[Expression],\n+                         notMatchedOutputs: Seq[Seq[Expression]],\n+                         targetOutput: Seq[Expression],\n+                         joinedAttributes: Seq[Attribute]) extends Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk4MDM4Mw==", "bodyText": "@rdblue Can you please explain the idea bit more, specifically the should be broken up into analysis rules to validate and update MergeInto plan . Currently, we produce the MergeInto logical plan in the optimizer phase ? So we have gone past analysis at this point right ? The input SQL has already been parsed and resolved using MergeIntoTable by spark at this point i.e all the mergeinto inputs have been resolved ?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r545980383", "createdAt": "2020-12-18T17:29:34Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/MergeInto.scala", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions.col\n+\n+case class MergeInto(mergeIntoProcessor: MergeIntoProcessor,\n+                     targetRelation: DataSourceV2Relation,\n+                     child: LogicalPlan) extends UnaryNode {\n+  override def output: Seq[Attribute] = targetRelation.output\n+}\n+\n+class MergeIntoProcessor(isSourceRowNotPresent: Expression,\n+                         isTargetRowNotPresent: Expression,\n+                         matchedConditions: Seq[Expression],\n+                         matchedOutputs: Seq[Seq[Expression]],\n+                         notMatchedConditions: Seq[Expression],\n+                         notMatchedOutputs: Seq[Seq[Expression]],\n+                         targetOutput: Seq[Expression],\n+                         joinedAttributes: Seq[Attribute]) extends Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUyMDQ0NA=="}, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAwODIyOQ==", "bodyText": "We need to make sure there are analysis rules that guarantee the assumptions in this class. One possible issue that jumped out to both @aokolnychyi and I was that this assumes the expressions for insert and update actions are correct for the output of this node. We need to make sure that is the case.\nOriginally, I asked on Slack how that validation was being done, but I saw Anton's comment about it and I thought that probably meant that it isn't being done. If there are already rules in Spark to resolve and validate the plan, then that's great but we need to identify them and make a note here that we're relying on those for correctness. I still suspect that there aren't rules in Spark doing this because this is running the analyzer on expressions.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r546008229", "createdAt": "2020-12-18T18:19:54Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/MergeInto.scala", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions.col\n+\n+case class MergeInto(mergeIntoProcessor: MergeIntoProcessor,\n+                     targetRelation: DataSourceV2Relation,\n+                     child: LogicalPlan) extends UnaryNode {\n+  override def output: Seq[Attribute] = targetRelation.output\n+}\n+\n+class MergeIntoProcessor(isSourceRowNotPresent: Expression,\n+                         isTargetRowNotPresent: Expression,\n+                         matchedConditions: Seq[Expression],\n+                         matchedOutputs: Seq[Seq[Expression]],\n+                         notMatchedConditions: Seq[Expression],\n+                         notMatchedOutputs: Seq[Seq[Expression]],\n+                         targetOutput: Seq[Expression],\n+                         joinedAttributes: Seq[Attribute]) extends Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUyMDQ0NA=="}, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAzNDg2NA==", "bodyText": "@rdblue Sorry Ryan. I didn't notice your comment on slack until now. So currently in my understanding Spark's Analyzer ensures that the ResolveIntoTable is fully resolved.\ncode\nHowever, you are right that we don't do any semantics analysis on the plan currently. We should add it.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r546034864", "createdAt": "2020-12-18T19:08:56Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/MergeInto.scala", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions.col\n+\n+case class MergeInto(mergeIntoProcessor: MergeIntoProcessor,\n+                     targetRelation: DataSourceV2Relation,\n+                     child: LogicalPlan) extends UnaryNode {\n+  override def output: Seq[Attribute] = targetRelation.output\n+}\n+\n+class MergeIntoProcessor(isSourceRowNotPresent: Expression,\n+                         isTargetRowNotPresent: Expression,\n+                         matchedConditions: Seq[Expression],\n+                         matchedOutputs: Seq[Seq[Expression]],\n+                         notMatchedConditions: Seq[Expression],\n+                         notMatchedOutputs: Seq[Seq[Expression]],\n+                         targetOutput: Seq[Expression],\n+                         joinedAttributes: Seq[Attribute]) extends Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUyMDQ0NA=="}, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjI5MTUzNw==", "bodyText": "Thanks for pointing me to the code! Looks like I was looking into it at the time you were writing this, which is why my comment below was just a bit later. I think we're all on the same page now.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r546291537", "createdAt": "2020-12-19T22:34:00Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/MergeInto.scala", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions.col\n+\n+case class MergeInto(mergeIntoProcessor: MergeIntoProcessor,\n+                     targetRelation: DataSourceV2Relation,\n+                     child: LogicalPlan) extends UnaryNode {\n+  override def output: Seq[Attribute] = targetRelation.output\n+}\n+\n+class MergeIntoProcessor(isSourceRowNotPresent: Expression,\n+                         isTargetRowNotPresent: Expression,\n+                         matchedConditions: Seq[Expression],\n+                         matchedOutputs: Seq[Seq[Expression]],\n+                         notMatchedConditions: Seq[Expression],\n+                         notMatchedOutputs: Seq[Seq[Expression]],\n+                         targetOutput: Seq[Expression],\n+                         joinedAttributes: Seq[Attribute]) extends Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUyMDQ0NA=="}, "originalCommit": {"oid": "9f021994411ac1701802ed4adcc8db4af1832d8b"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNjMwMjk4OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTowNjowNFrOIUpqYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTowNjowNFrOIUpqYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODUyNTAyNw==", "bodyText": "Shall we make these variables private?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558525027", "createdAt": "2021-01-15T19:06:04Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNjMwNzIxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTowNzowMlrOIUptPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xN1QwMTo1MToyNVrOIVKkFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODUyNTc1OQ==", "bodyText": "Does the comment apply? It looks like it is valid for DELETE but not really for MERGE.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558525759", "createdAt": "2021-01-15T19:07:02Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA2NDA4NQ==", "bodyText": "@aokolnychyi cut-paste.. sorry, will remove.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559064085", "createdAt": "2021-01-17T01:51:25Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODUyNTc1OQ=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNjMzMDMyOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOToxMToyM1rOIUp8FA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOToxMToyM1rOIUp8FA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODUyOTU1Ng==", "bodyText": "nit: I think matchedActions and notMatchedActions would be better names here.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558529556", "createdAt": "2021-01-15T19:11:23Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNjM5Nzg5OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/iceberg/spark/extensions/IcebergSparkSessionExtensions.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOToyNDoxNVrOIUqpOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xN1QwMTo1NzoyM1rOIVKlwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU0MTExNA==", "bodyText": "@dilipbiswal, do we have to add MERGE operations to PullupCorrelatedPredicatesInRowLevelOperations? Could you test the current implementation with subqueries inside the merge as well as matched/not matched conditions?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558541114", "createdAt": "2021-01-15T19:24:15Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/iceberg/spark/extensions/IcebergSparkSessionExtensions.scala", "diffHunk": "@@ -43,6 +43,7 @@ class IcebergSparkSessionExtensions extends (SparkSessionExtensions => Unit) {\n     // TODO: PullupCorrelatedPredicates should handle row-level operations\n     extensions.injectOptimizerRule { _ => PullupCorrelatedPredicatesInRowLevelOperations }\n     extensions.injectOptimizerRule { _ => RewriteDelete }\n+    extensions.injectOptimizerRule { _ => RewriteMergeInto }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA2NDUxNA==", "bodyText": "@aokolnychyi will address this with a follow-up to not allow subqs and add tests to verify the same. Hope its okay.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559064514", "createdAt": "2021-01-17T01:57:23Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/main/scala/org/apache/iceberg/spark/extensions/IcebergSparkSessionExtensions.scala", "diffHunk": "@@ -43,6 +43,7 @@ class IcebergSparkSessionExtensions extends (SparkSessionExtensions => Unit) {\n     // TODO: PullupCorrelatedPredicates should handle row-level operations\n     extensions.injectOptimizerRule { _ => PullupCorrelatedPredicatesInRowLevelOperations }\n     extensions.injectOptimizerRule { _ => RewriteDelete }\n+    extensions.injectOptimizerRule { _ => RewriteMergeInto }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU0MTExNA=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNjQzODE1OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozMToxOVrOIUrDMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozMToxOVrOIUrDMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU0Nzc2Mg==", "bodyText": "When I was working on UPDATE, I also created a parent trait for row-level ops.\nWhat about a more specific name, like RewriteRowLevelOperation or similar?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558547762", "createdAt": "2021-01-15T19:31:19Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNjQzODk5OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozMTozMFrOIUrDxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozMTozMFrOIUrDxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU0NzkxMQ==", "bodyText": "Shall we make these protected?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558547911", "createdAt": "2021-01-15T19:31:30Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {\n+  val FILE_NAME_COL = \"_file\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNjQ0MjM3OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozMjoxMFrOIUrF-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozMjoxMFrOIUrF-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU0ODQ3Mg==", "bodyText": "I think we better make methods inside this trait protected, not public.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558548472", "createdAt": "2021-01-15T19:32:10Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNjQ0Njg4OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozMzoxNlrOIUrJBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozMzoxNlrOIUrJBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU0OTI1NQ==", "bodyText": "This does not do predicate push down as we have for DELETE.\nCan we take the implementation from RewriteDelete?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558549255", "createdAt": "2021-01-15T19:33:16Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {\n+  val FILE_NAME_COL = \"_file\"\n+  val ROW_POS_COL = \"_pos\"\n+\n+  def buildScanPlan(table: Table,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNjQ2NTc2OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozOToxMlrOIUrULg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQxOTozOToxMlrOIUrULg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU1MjExMA==", "bodyText": "We should probably pass it from the session like in AlignMergeIntoTable.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558552110", "createdAt": "2021-01-15T19:39:12Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {\n+  val FILE_NAME_COL = \"_file\"\n+  val ROW_POS_COL = \"_pos\"\n+\n+  def buildScanPlan(table: Table,\n+                    output: Seq[AttributeReference],\n+                    mergeBuilder: MergeBuilder,\n+                    prunedTargetPlan: LogicalPlan): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, toOutputAttrs(scan.readSchema(), output))\n+\n+    scan match {\n+      case filterable: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(prunedTargetPlan)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan, filterable)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+  }\n+\n+  private def buildFileFilterPlan(prunedTargetPlan: LogicalPlan): LogicalPlan = {\n+    val fileAttr = findOutputAttr(prunedTargetPlan, FILE_NAME_COL)\n+    Aggregate(Seq(fileAttr), Seq(fileAttr), prunedTargetPlan)\n+  }\n+\n+  def findOutputAttr(plan: LogicalPlan, attrName: String): Attribute = {\n+    val resolver = SQLConf.get.resolver", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNjU1ODY1OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMDowNzowOVrOIUsLUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoyNzowNVrOIV5k6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU2NjIyNg==", "bodyText": "@dilipbiswal, could you move these tests to TestMerge that was introduced recently?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558566226", "createdAt": "2021-01-15T20:07:09Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA5NDk1OQ==", "bodyText": "@aokolnychyi If you are ok, i want to create a final pr to remove this test case and merge to TestMerge since the other two prs also add tests to this class. I want them to rebase okay without much trouble. Let me know please.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559094959", "createdAt": "2021-01-17T08:10:37Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU2NjIyNg=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzNDM0Ng==", "bodyText": "Sounds fine to me.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559834346", "createdAt": "2021-01-18T23:27:05Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODU2NjIyNg=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNjkxMzAwOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjoxNDo1NlrOIUvgbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjoxNDo1NlrOIUvgbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODYyMDc4Mg==", "bodyText": "This line should use merge, instead of delete now. It should be supported.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558620782", "createdAt": "2021-01-15T22:14:56Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"delete\", writeInfo)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNjk0NzMxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjoyOTo1NVrOIUv0LQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjoyOTo1NVrOIUv0LQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODYyNTgzNw==", "bodyText": "Shall we make the helper methods private?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558625837", "createdAt": "2021-01-15T22:29:55Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"delete\", writeInfo)\n+        val targetTableScan =  buildScanPlan(target.table, target.output, mergeBuilder, prunedTargetPlan)\n+\n+        // Construct an outer join to help track changes in source and target.\n+        // TODO : Optimize this to use LEFT ANTI or RIGHT OUTER when applicable.\n+        val sourceTableProj = source.output ++ Seq(Alias(lit(true).expr, ROW_FROM_SOURCE)())\n+        val targetTableProj = target.output ++ Seq(Alias(lit(true).expr, ROW_FROM_TARGET)())\n+        val newTargetTableScan = Project(targetTableProj, targetTableScan)\n+        val newSourceTableScan = Project(sourceTableProj, source)\n+        val joinPlan = Join(newSourceTableScan, newTargetTableScan, FullOuter, Some(cond), JoinHint.NONE)\n+\n+        // Construct the plan to replace the data based on the output of `MergeInto`\n+        val mergeParams = MergeIntoParams(\n+          isSourceRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_SOURCE)),\n+          isTargetRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_TARGET)),\n+          matchedConditions = actions.map(getClauseCondition),\n+          matchedOutputs = actions.map(actionOutput(_, targetOutputCols)),\n+          notMatchedConditions = notActions.map(getClauseCondition),\n+          notMatchedOutputs = notActions.map(actionOutput(_, targetOutputCols)),\n+          targetOutput = targetOutputCols :+ Literal(false),\n+          deleteOutput = targetOutputCols :+ Literal(true),\n+          joinedAttributes = joinPlan.output\n+        )\n+        val mergePlan = MergeInto(mergeParams, target, joinPlan)\n+        val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+        ReplaceData(target, batchWrite, mergePlan)\n+    }\n+  }\n+\n+  def getTargetOutputCols(target: DataSourceV2Relation): Seq[NamedExpression] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNjk1NDA2OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjozMjo1N1rOIUv4Eg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjozMjo1N1rOIUv4Eg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODYyNjgzNA==", "bodyText": "It is probably better to accept SQLConf in this rule like in AlignMergeIntoTable.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558626834", "createdAt": "2021-01-15T22:32:57Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"delete\", writeInfo)\n+        val targetTableScan =  buildScanPlan(target.table, target.output, mergeBuilder, prunedTargetPlan)\n+\n+        // Construct an outer join to help track changes in source and target.\n+        // TODO : Optimize this to use LEFT ANTI or RIGHT OUTER when applicable.\n+        val sourceTableProj = source.output ++ Seq(Alias(lit(true).expr, ROW_FROM_SOURCE)())\n+        val targetTableProj = target.output ++ Seq(Alias(lit(true).expr, ROW_FROM_TARGET)())\n+        val newTargetTableScan = Project(targetTableProj, targetTableScan)\n+        val newSourceTableScan = Project(sourceTableProj, source)\n+        val joinPlan = Join(newSourceTableScan, newTargetTableScan, FullOuter, Some(cond), JoinHint.NONE)\n+\n+        // Construct the plan to replace the data based on the output of `MergeInto`\n+        val mergeParams = MergeIntoParams(\n+          isSourceRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_SOURCE)),\n+          isTargetRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_TARGET)),\n+          matchedConditions = actions.map(getClauseCondition),\n+          matchedOutputs = actions.map(actionOutput(_, targetOutputCols)),\n+          notMatchedConditions = notActions.map(getClauseCondition),\n+          notMatchedOutputs = notActions.map(actionOutput(_, targetOutputCols)),\n+          targetOutput = targetOutputCols :+ Literal(false),\n+          deleteOutput = targetOutputCols :+ Literal(true),\n+          joinedAttributes = joinPlan.output\n+        )\n+        val mergePlan = MergeInto(mergeParams, target, joinPlan)\n+        val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+        ReplaceData(target, batchWrite, mergePlan)\n+    }\n+  }\n+\n+  def getTargetOutputCols(target: DataSourceV2Relation): Seq[NamedExpression] = {\n+    target.schema.map { col =>\n+      target.output.find(attr => SQLConf.get.resolver(attr.name, col.name)).getOrElse {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNjk1NDcxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjozMzoyMFrOIUv4gA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xN1QwMjowOTozNFrOIVKpkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODYyNjk0NA==", "bodyText": "Is this method actually used?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558626944", "createdAt": "2021-01-15T22:33:20Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"delete\", writeInfo)\n+        val targetTableScan =  buildScanPlan(target.table, target.output, mergeBuilder, prunedTargetPlan)\n+\n+        // Construct an outer join to help track changes in source and target.\n+        // TODO : Optimize this to use LEFT ANTI or RIGHT OUTER when applicable.\n+        val sourceTableProj = source.output ++ Seq(Alias(lit(true).expr, ROW_FROM_SOURCE)())\n+        val targetTableProj = target.output ++ Seq(Alias(lit(true).expr, ROW_FROM_TARGET)())\n+        val newTargetTableScan = Project(targetTableProj, targetTableScan)\n+        val newSourceTableScan = Project(sourceTableProj, source)\n+        val joinPlan = Join(newSourceTableScan, newTargetTableScan, FullOuter, Some(cond), JoinHint.NONE)\n+\n+        // Construct the plan to replace the data based on the output of `MergeInto`\n+        val mergeParams = MergeIntoParams(\n+          isSourceRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_SOURCE)),\n+          isTargetRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_TARGET)),\n+          matchedConditions = actions.map(getClauseCondition),\n+          matchedOutputs = actions.map(actionOutput(_, targetOutputCols)),\n+          notMatchedConditions = notActions.map(getClauseCondition),\n+          notMatchedOutputs = notActions.map(actionOutput(_, targetOutputCols)),\n+          targetOutput = targetOutputCols :+ Literal(false),\n+          deleteOutput = targetOutputCols :+ Literal(true),\n+          joinedAttributes = joinPlan.output\n+        )\n+        val mergePlan = MergeInto(mergeParams, target, joinPlan)\n+        val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+        ReplaceData(target, batchWrite, mergePlan)\n+    }\n+  }\n+\n+  def getTargetOutputCols(target: DataSourceV2Relation): Seq[NamedExpression] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA2NTQ4OA==", "bodyText": "@aokolnychyi will remove. Thanks !!", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559065488", "createdAt": "2021-01-17T02:09:34Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"delete\", writeInfo)\n+        val targetTableScan =  buildScanPlan(target.table, target.output, mergeBuilder, prunedTargetPlan)\n+\n+        // Construct an outer join to help track changes in source and target.\n+        // TODO : Optimize this to use LEFT ANTI or RIGHT OUTER when applicable.\n+        val sourceTableProj = source.output ++ Seq(Alias(lit(true).expr, ROW_FROM_SOURCE)())\n+        val targetTableProj = target.output ++ Seq(Alias(lit(true).expr, ROW_FROM_TARGET)())\n+        val newTargetTableScan = Project(targetTableProj, targetTableScan)\n+        val newSourceTableScan = Project(sourceTableProj, source)\n+        val joinPlan = Join(newSourceTableScan, newTargetTableScan, FullOuter, Some(cond), JoinHint.NONE)\n+\n+        // Construct the plan to replace the data based on the output of `MergeInto`\n+        val mergeParams = MergeIntoParams(\n+          isSourceRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_SOURCE)),\n+          isTargetRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_TARGET)),\n+          matchedConditions = actions.map(getClauseCondition),\n+          matchedOutputs = actions.map(actionOutput(_, targetOutputCols)),\n+          notMatchedConditions = notActions.map(getClauseCondition),\n+          notMatchedOutputs = notActions.map(actionOutput(_, targetOutputCols)),\n+          targetOutput = targetOutputCols :+ Literal(false),\n+          deleteOutput = targetOutputCols :+ Literal(true),\n+          joinedAttributes = joinPlan.output\n+        )\n+        val mergePlan = MergeInto(mergeParams, target, joinPlan)\n+        val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+        ReplaceData(target, batchWrite, mergePlan)\n+    }\n+  }\n+\n+  def getTargetOutputCols(target: DataSourceV2Relation): Seq[NamedExpression] = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODYyNjk0NA=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNjk2MjIxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/MergeInto.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjozNjozOVrOIUv8yA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMjozNjozOVrOIUv8yA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODYyODA0MA==", "bodyText": "nit: I like Spark's way of formatting like this a bit more:\ncase class MergeInto(\n    mergeIntoProcessor: ...\n    targetRelation: ...\n    child: ...)", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558628040", "createdAt": "2021-01-15T22:36:39Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/MergeInto.scala", "diffHunk": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+\n+case class MergeInto(mergeIntoProcessor: MergeIntoParams,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNzEwNDAzOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMzoxODo0M1rOIUxXWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMzoxODo0M1rOIUxXWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODY1MTIyNQ==", "bodyText": "I think comments here would help people who will maintain/contribute in the future.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558651225", "createdAt": "2021-01-15T23:18:43Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)\n+        } else {\n+          projectTargetCols.apply(inputRow)\n+        }\n+    }\n+  }\n+\n+  def processPartition(params: MergeIntoParams,\n+                       rowIterator: Iterator[InternalRow]): Iterator[InternalRow] = {\n+    val joinedAttrs = params.joinedAttributes\n+    val isSourceRowNotPresentPred = generatePredicate(params.isSourceRowNotPresent, joinedAttrs)\n+    val isTargetRowNotPresentPred = generatePredicate(params.isTargetRowNotPresent, joinedAttrs)\n+    val matchedPreds = params.matchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val matchedProjs = params.matchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val notMatchedPreds = params.notMatchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val notMatchedProjs = params.notMatchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val projectTargetCols = generateProjection(params.targetOutput, joinedAttrs)\n+    val projectDeletedRow = generateProjection(params.deleteOutput, joinedAttrs)\n+\n+    def shouldDeleteRow(row: InternalRow): Boolean =\n+      row.getBoolean(params.targetOutput.size - 1)\n+\n+\n+    def processRow(inputRow: InternalRow): InternalRow = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNzE0NDA4OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNVQyMzoyNTo1NlrOIUxyyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xN1QwNzo0OTozNVrOIVMUjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODY1ODI1MA==", "bodyText": "I think this will require further explanation. There will be up to 2 matched cases and we try to find the first one that matches? Does find guarantee the order of the traversal?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558658250", "createdAt": "2021-01-15T23:25:56Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA1MzYwNw==", "bodyText": "I think that find does guarantee the order and gives you the first one, which would be the first case that matches. I didn't see the syntax with multiple clauses and extra predicates in the SQL 2003 spec, so this might just be an extension that some implementations use. We should definitely check the semantics.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559053607", "createdAt": "2021-01-16T23:42:02Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODY1ODI1MA=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA5Mjg3OA==", "bodyText": "@aokolnychyi @rdblue\nIn my understanding, if we have overlapping conditions in two match branches i.e for example ..\nWHEN MATCHED AND id > 0 and id < 10  UPDATE *\nWHEN MATCHED AND id = 5  or id = 21 DELETE\nThen we will just honor the first match and so if a row comes with id = 5, then we will update. And if a row with id 21 is present, then we will delete.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559092878", "createdAt": "2021-01-17T07:49:35Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODY1ODI1MA=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNzQzNDgxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDoyODozNlrOIU06Sg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDoyODozNlrOIU06Sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcwOTMyMg==", "bodyText": "The operation passed to the merge builder should be merge. And we will want to add tests that the isolation level is carried through correctly.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558709322", "createdAt": "2021-01-16T00:28:36Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"delete\", writeInfo)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNzQzNzgwOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDoyOTozM1rOIU08XA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDoyOTozM1rOIU08XA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcwOTg1Mg==", "bodyText": "Is there a better name than prunedTargetPlan? What about matchedRowsPlan?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558709852", "createdAt": "2021-01-16T00:29:33Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNzQ2MzM4OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDozNzoxNFrOIU1NSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xN1QwMjoxMzo0MlrOIVKqig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcxNDE4Nw==", "bodyText": "I think that this will require a reference to the target table because _file needs to come from the target table and not the source table. Right now it works because the target table is the only one with _file defined, but I think we should plan on selecting the right column if there are duplicates. If I were to define a column _file in the source data, it may not work.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558714187", "createdAt": "2021-01-16T00:37:14Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {\n+  val FILE_NAME_COL = \"_file\"\n+  val ROW_POS_COL = \"_pos\"\n+\n+  def buildScanPlan(table: Table,\n+                    output: Seq[AttributeReference],\n+                    mergeBuilder: MergeBuilder,\n+                    prunedTargetPlan: LogicalPlan): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, toOutputAttrs(scan.readSchema(), output))\n+\n+    scan match {\n+      case filterable: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(prunedTargetPlan)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan, filterable)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+  }\n+\n+  private def buildFileFilterPlan(prunedTargetPlan: LogicalPlan): LogicalPlan = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA2NTczOA==", "bodyText": "@rdblue We will have issue if target has a column named _file as well, right ? I will mark it as a TODO and address in a follow-up, if its okay ? Need to think how to disambiguate this .. probably though a alias which checks for presence of the column and assigns a unique alias ?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559065738", "createdAt": "2021-01-17T02:13:42Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {\n+  val FILE_NAME_COL = \"_file\"\n+  val ROW_POS_COL = \"_pos\"\n+\n+  def buildScanPlan(table: Table,\n+                    output: Seq[AttributeReference],\n+                    mergeBuilder: MergeBuilder,\n+                    prunedTargetPlan: LogicalPlan): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, toOutputAttrs(scan.readSchema(), output))\n+\n+    scan match {\n+      case filterable: SupportsFileFilter =>\n+        val matchingFilePlan = buildFileFilterPlan(prunedTargetPlan)\n+        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan, filterable)\n+        dynamicFileFilter\n+      case _ =>\n+        scanRelation\n+    }\n+  }\n+\n+  private def buildFileFilterPlan(prunedTargetPlan: LogicalPlan): LogicalPlan = {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcxNDE4Nw=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNzQ4NDYxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDo0Mzo0M1rOIU1bOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMTowNDozNFrOIU2Ihw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcxNzc1NQ==", "bodyText": "We want this scan to be shared by both the file filter plan and the scan relation. I think that means that this can't accept the matchedRowsPlan / prunedTargetPlan:\n\nWe don't want to plan the scan twice\nThe scan built by MergeBuilder is going to have _file and _pos automatically added. If we want to reference those, we should use this scan.\n\nThat means we'll have to add the inner join here, or pass the scanRelation to a function that produces the inner join. I think that's what @aokolnychyi was proposing in his version of this class.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558717755", "createdAt": "2021-01-16T00:43:43Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {\n+  val FILE_NAME_COL = \"_file\"\n+  val ROW_POS_COL = \"_pos\"\n+\n+  def buildScanPlan(table: Table,\n+                    output: Seq[AttributeReference],\n+                    mergeBuilder: MergeBuilder,\n+                    prunedTargetPlan: LogicalPlan): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, toOutputAttrs(scan.readSchema(), output))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcyMjM0NA==", "bodyText": "Also, we will need to extract the predicates from the match condition that only reference the target table. Because the scan relation is built here, we need to push all predicates that can be used to filter the target table in this method. It should be a matter of checking which expressions contain only references in the target table's attribute set.\nWe may want to do this in a follow-up. If so, it would be good to add a TODO comment for it.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558722344", "createdAt": "2021-01-16T00:51:45Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {\n+  val FILE_NAME_COL = \"_file\"\n+  val ROW_POS_COL = \"_pos\"\n+\n+  def buildScanPlan(table: Table,\n+                    output: Seq[AttributeReference],\n+                    mergeBuilder: MergeBuilder,\n+                    prunedTargetPlan: LogicalPlan): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, toOutputAttrs(scan.readSchema(), output))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcxNzc1NQ=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcyOTM1MQ==", "bodyText": "Nevermind on the second comment. Anton's update handles it.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558729351", "createdAt": "2021-01-16T01:04:34Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {\n+  val FILE_NAME_COL = \"_file\"\n+  val ROW_POS_COL = \"_pos\"\n+\n+  def buildScanPlan(table: Table,\n+                    output: Seq[AttributeReference],\n+                    mergeBuilder: MergeBuilder,\n+                    prunedTargetPlan: LogicalPlan): LogicalPlan = {\n+\n+    val scanBuilder = mergeBuilder.asScanBuilder\n+    val scan = scanBuilder.build()\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, toOutputAttrs(scan.readSchema(), output))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcxNzc1NQ=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNzUxOTI0OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDo1NDowOVrOIU1ybA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDo1NDowOVrOIU1ybA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcyMzY5Mg==", "bodyText": "Could you use one import per line? That will help avoid git conflicts in the future.\nAlso, we don't use wildcard imports in Iceberg because it can cause conflicts and it isn't clear where symbols are coming from when reading PRs.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558723692", "createdAt": "2021-01-16T00:54:09Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNzUyNDA3OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDo1NTo0OFrOIU115Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMDo1NTo0OFrOIU115Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODcyNDU4MQ==", "bodyText": "If you replace lit(true).expr with an expression, then this doesn't need to pull in sql.functions._. I added a constant:\n  private val TRUE = Literal(true, BooleanType)", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558724581", "createdAt": "2021-01-16T00:55:48Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,103 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.catalyst.expressions._\n+import org.apache.spark.sql.catalyst.plans.{FullOuter, Inner}\n+import org.apache.spark.sql.catalyst.plans.logical._\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.PlanHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.internal.SQLConf\n+\n+object RewriteMergeInto extends Rule[LogicalPlan] with PlanHelper with Logging  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      // rewrite all operations that require reading the table to delete records\n+      case MergeIntoTable(target: DataSourceV2Relation,\n+                          source: LogicalPlan, cond, actions, notActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val prunedTargetPlan = Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"delete\", writeInfo)\n+        val targetTableScan =  buildScanPlan(target.table, target.output, mergeBuilder, prunedTargetPlan)\n+\n+        // Construct an outer join to help track changes in source and target.\n+        // TODO : Optimize this to use LEFT ANTI or RIGHT OUTER when applicable.\n+        val sourceTableProj = source.output ++ Seq(Alias(lit(true).expr, ROW_FROM_SOURCE)())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNzU4MjI2OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedDataSourceV2Strategy.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToxNDo1NFrOIU2dTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToxNDo1NFrOIU2dTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODczNDY3MA==", "bodyText": "Nit: to make this class more readable, we've been separating cases with a blank line.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558734670", "createdAt": "2021-01-16T01:14:54Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedDataSourceV2Strategy.scala", "diffHunk": "@@ -75,6 +76,8 @@ case class ExtendedDataSourceV2Strategy(spark: SparkSession) extends Strategy {\n     case ReplaceData(_, batchWrite, query) =>\n       ReplaceDataExec(batchWrite, planLater(query)) :: Nil\n \n+    case MergeInto(mergeIntoProcessor, targetRelation, child) =>\n+      MergeIntoExec(mergeIntoProcessor, targetRelation, planLater(child)) :: Nil", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNzU4NTU1OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToxNTo0N1rOIU2fTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToxNTo0N1rOIU2fTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODczNTE4MA==", "bodyText": "Looks like this variable name wasn't updated when the processor was renamed to params.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558735180", "createdAt": "2021-01-16T01:15:47Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNzYwNTE3OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToyMTo1N1rOIU2sRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToyMTo1N1rOIU2sRQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODczODUwMQ==", "bodyText": "Why use a match here and not if (isSourceRowNotPresentPred.eval(inputRow))?\nIf you did that, it would be a bit cleaner:\n      if (isSourceRowNotPresentPred.eval(inputRow)) {\n        projectTargetCols.apply(inputRow)\n      } else if (isTargetRowNotPresentPred.eval(inputRow)) {\n        applyProjection(notMatchedPreds, notMatchedProjs, projectTargetCols,\n          projectDeletedRow, inputRow, true)\n      } else {\n        applyProjection(matchedPreds, matchedProjs, projectTargetCols,\n          projectDeletedRow, inputRow, false)\n      }", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558738501", "createdAt": "2021-01-16T01:21:57Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)\n+        } else {\n+          projectTargetCols.apply(inputRow)\n+        }\n+    }\n+  }\n+\n+  def processPartition(params: MergeIntoParams,\n+                       rowIterator: Iterator[InternalRow]): Iterator[InternalRow] = {\n+    val joinedAttrs = params.joinedAttributes\n+    val isSourceRowNotPresentPred = generatePredicate(params.isSourceRowNotPresent, joinedAttrs)\n+    val isTargetRowNotPresentPred = generatePredicate(params.isTargetRowNotPresent, joinedAttrs)\n+    val matchedPreds = params.matchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val matchedProjs = params.matchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val notMatchedPreds = params.notMatchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val notMatchedProjs = params.notMatchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val projectTargetCols = generateProjection(params.targetOutput, joinedAttrs)\n+    val projectDeletedRow = generateProjection(params.deleteOutput, joinedAttrs)\n+\n+    def shouldDeleteRow(row: InternalRow): Boolean =\n+      row.getBoolean(params.targetOutput.size - 1)\n+\n+\n+    def processRow(inputRow: InternalRow): InternalRow = {\n+      isSourceRowNotPresentPred.eval(inputRow) match {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNzYxNDk2OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToyNToxMVrOIU2y4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xN1QwMjoxODowMlrOIVKsAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0MDE5NQ==", "bodyText": "It seems a bit odd to apply this projection because the target row will be deleted. It seems like we could use the same lazily-initialized row for every delete.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558740195", "createdAt": "2021-01-16T01:25:11Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTAzNDQzNw==", "bodyText": "@rdblue I had thought about it. But couldn't think of a way to do it. How about, we create a materialized delete row once per partition like this :\nval deleteExpr = params.targetOutput.dropRight(1).map(e => Literal.default(e.dataType)) ++ Seq(Literal.create(true, BooleanType))\n    val deletedRow1 = UnsafeProjection.create(deleteExpr)\n    val deletedRow = deletedRow1.apply(null)\n\ndeteExpr will come from rewriteMergeInto just like its passed now. Here we will just create the InternalRow once and use it ? Will that work Ryan ?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559034437", "createdAt": "2021-01-16T20:19:19Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0MDE5NQ=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA1MzQxMA==", "bodyText": "What if we just returned null instead?\nI think the problem is that this is trying to create one output for each input row, then filtering happens afterward. An extra column is added to signal that the row should be kept or not. But we don't need to copy the row if it is going to be removed. We also don't need to copy incoming target rows just to add a true at the end if they are going to be kept.\nSo what if we changed all of the delete cases to produce null instead?\nLet's not worry about this for now, but I'll open a PR after this is merged to simplify and avoid some of the copies.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559053410", "createdAt": "2021-01-16T23:39:38Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0MDE5NQ=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA2NTk2OQ==", "bodyText": "@rdblue OK.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559065969", "createdAt": "2021-01-17T02:16:49Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0MDE5NQ=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA2NjExNA==", "bodyText": "@rdblue One thing to note is that, the output of the outer join is target cols + source cols. So we have to project out the necessary target columns, i think.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559066114", "createdAt": "2021-01-17T02:18:02Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0MDE5NQ=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNzYyNzU3OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMToyODo1NVrOIU27dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoyMDoyN1rOIV5fYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0MjM5MQ==", "bodyText": "These last two projections are only needed when notMatchedPreds or matchedPreds does not have a default case, i.e. lit(true).\nIn the rewrite, there is also a function, getClauseCondition, that fills in lit(true) if there is no clause condition. But I don't think that any predicates after the true condition are dropped.\nI think we could simplify the logic here and avoid extra clauses by ensuring that both matchedPreds and notMatchedPreds end with lit(true). Then this class would not need to account for the case where no predicate matches and we wouldn't have extra predicates passed through. Last, we wouldn't need the last two projections here or in MergeIntoParams because they would be added to notMatchedProjs or matchedProjs.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558742391", "createdAt": "2021-01-16T01:28:55Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)\n+        } else {\n+          projectTargetCols.apply(inputRow)\n+        }\n+    }\n+  }\n+\n+  def processPartition(params: MergeIntoParams,\n+                       rowIterator: Iterator[InternalRow]): Iterator[InternalRow] = {\n+    val joinedAttrs = params.joinedAttributes\n+    val isSourceRowNotPresentPred = generatePredicate(params.isSourceRowNotPresent, joinedAttrs)\n+    val isTargetRowNotPresentPred = generatePredicate(params.isTargetRowNotPresent, joinedAttrs)\n+    val matchedPreds = params.matchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val matchedProjs = params.matchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val notMatchedPreds = params.notMatchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val notMatchedProjs = params.notMatchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val projectTargetCols = generateProjection(params.targetOutput, joinedAttrs)\n+    val projectDeletedRow = generateProjection(params.deleteOutput, joinedAttrs)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0NzE5OQ==", "bodyText": "I guess projectTargetCols would still be needed for the case where the source row isn't present, but it would still make this a little simpler. Especially the applyProjection method.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558747199", "createdAt": "2021-01-16T01:37:14Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)\n+        } else {\n+          projectTargetCols.apply(inputRow)\n+        }\n+    }\n+  }\n+\n+  def processPartition(params: MergeIntoParams,\n+                       rowIterator: Iterator[InternalRow]): Iterator[InternalRow] = {\n+    val joinedAttrs = params.joinedAttributes\n+    val isSourceRowNotPresentPred = generatePredicate(params.isSourceRowNotPresent, joinedAttrs)\n+    val isTargetRowNotPresentPred = generatePredicate(params.isTargetRowNotPresent, joinedAttrs)\n+    val matchedPreds = params.matchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val matchedProjs = params.matchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val notMatchedPreds = params.notMatchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val notMatchedProjs = params.notMatchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val projectTargetCols = generateProjection(params.targetOutput, joinedAttrs)\n+    val projectDeletedRow = generateProjection(params.deleteOutput, joinedAttrs)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0MjM5MQ=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA5MjEwNg==", "bodyText": "@rdblue\n\nprojectTargetCols represents the expression that needs to be applied on the output of outer join which has columns from both the tables to only project the target output columns plus the deleted flag set to false.\nprojectDeletedRow  does the same but with the \"deleted flag\". I think in the earlier comment we discussed possible ideas to optimize this (will address in follow-up)\n``matchedPredsandnotMatchedPred```  go hand in hand with their corresponding projections that is specified by the user in the WHEN MATCHED  and `WHEN NOT MATCHED` clauses.\n\nGiven this background, can you please explain your idea a little bit ?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559092106", "createdAt": "2021-01-17T07:42:13Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)\n+        } else {\n+          projectTargetCols.apply(inputRow)\n+        }\n+    }\n+  }\n+\n+  def processPartition(params: MergeIntoParams,\n+                       rowIterator: Iterator[InternalRow]): Iterator[InternalRow] = {\n+    val joinedAttrs = params.joinedAttributes\n+    val isSourceRowNotPresentPred = generatePredicate(params.isSourceRowNotPresent, joinedAttrs)\n+    val isTargetRowNotPresentPred = generatePredicate(params.isTargetRowNotPresent, joinedAttrs)\n+    val matchedPreds = params.matchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val matchedProjs = params.matchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val notMatchedPreds = params.notMatchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val notMatchedProjs = params.notMatchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val projectTargetCols = generateProjection(params.targetOutput, joinedAttrs)\n+    val projectDeletedRow = generateProjection(params.deleteOutput, joinedAttrs)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0MjM5MQ=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMjkyOQ==", "bodyText": "Let's clear this up in a follow-up.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559832929", "createdAt": "2021-01-18T23:20:27Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)\n+        } else {\n+          projectTargetCols.apply(inputRow)\n+        }\n+    }\n+  }\n+\n+  def processPartition(params: MergeIntoParams,\n+                       rowIterator: Iterator[InternalRow]): Iterator[InternalRow] = {\n+    val joinedAttrs = params.joinedAttributes\n+    val isSourceRowNotPresentPred = generatePredicate(params.isSourceRowNotPresent, joinedAttrs)\n+    val isTargetRowNotPresentPred = generatePredicate(params.isTargetRowNotPresent, joinedAttrs)\n+    val matchedPreds = params.matchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val matchedProjs = params.matchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val notMatchedPreds = params.notMatchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val notMatchedProjs = params.notMatchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val projectTargetCols = generateProjection(params.targetOutput, joinedAttrs)\n+    val projectDeletedRow = generateProjection(params.deleteOutput, joinedAttrs)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0MjM5MQ=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNzY0MzU3OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMTozNDowNVrOIU3GhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQyMzozNToxMFrOIVJ4lg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0NTIyMA==", "bodyText": "I think it would be better to create these pairs just once instead of in every row. After this change and moving the projections into action cases (see comment below), this method signature would be much simpler:\n  def applyProjection(\n      actions: Seq[(BasePredicate, UnsafeProjection)],\n      inputRow: InternalRow): InternalRow = {\n    val pair = actions.find {\n      case (predicate, _) => predicate.eval(inputRow)\n    }\n    ...\n  }", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558745220", "createdAt": "2021-01-16T01:34:05Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTAzNTkwMw==", "bodyText": "@rdblue Sorry ryan, you refer to some comment below ? But i don't see any comment below that relates to this particular comment. I understood the part about creating the pairs once. But i don't know how we can avoid passing the projections ?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559035903", "createdAt": "2021-01-16T20:25:14Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0NTIyMA=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA1Mjk1MA==", "bodyText": "This one: #1947 (comment)", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559052950", "createdAt": "2021-01-16T23:35:10Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, BasePredicate, Expression, UnsafeProjection}\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.{SparkPlan, UnaryExecNode}\n+\n+case class MergeIntoExec(mergeIntoProcessor: MergeIntoParams,\n+                         @transient targetRelation: DataSourceV2Relation,\n+                         override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoProcessor, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(predicates: Seq[BasePredicate],\n+                      projections: Seq[UnsafeProjection],\n+                      projectTargetCols: UnsafeProjection,\n+                      projectDeleteRow: UnsafeProjection,\n+                      inputRow: InternalRow,\n+                      targetRowNotPresent: Boolean): InternalRow = {\n+    // Find the first combination where the predicate evaluates to true\n+    val pair = (predicates zip projections).find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0NTIyMA=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNzY1NzkwOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMTozODowOFrOIU3QXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQyMDoyNTo0OFrOIVI2Sw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0Nzc0Mg==", "bodyText": "Why is this passing an empty string?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558747742", "createdAt": "2021-01-16T01:38:08Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+                     \"USING \" + sourceName + \" AS source \\n\" +\n+                     \"ON target.id = source.id \\n\" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText, \"\");\n+    sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertOnlyMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+                     \"USING \" + sourceName + \" AS source \\n\" +\n+                     \"ON target.id = source.id \\n\" +\n+                     \"WHEN NOT MATCHED AND (source.id >= 2) THEN INSERT * \";\n+\n+    sql(sqlText, \"\");\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyUpdate() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+            \"USING \" + sourceName + \" AS source \\n\" +\n+            \"ON target.id = source.id \\n\" +\n+            \"WHEN MATCHED AND target.id = 1 THEN UPDATE SET * \";\n+\n+    sql(sqlText, \"\");\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(6, \"emp-id-6\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyDelete() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+            \"USING \" + sourceName + \" AS source \\n\" +\n+            \"ON target.id = source.id \\n\" +\n+            \"WHEN MATCHED AND target.id = 6 THEN DELETE\";\n+\n+    sql(sqlText, \"\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTAzNTk3OQ==", "bodyText": "@rdblue Sorry.. don't know why i was doing it. Will remove.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559035979", "createdAt": "2021-01-16T20:25:48Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+                     \"USING \" + sourceName + \" AS source \\n\" +\n+                     \"ON target.id = source.id \\n\" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText, \"\");\n+    sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertOnlyMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+                     \"USING \" + sourceName + \" AS source \\n\" +\n+                     \"ON target.id = source.id \\n\" +\n+                     \"WHEN NOT MATCHED AND (source.id >= 2) THEN INSERT * \";\n+\n+    sql(sqlText, \"\");\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyUpdate() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+            \"USING \" + sourceName + \" AS source \\n\" +\n+            \"ON target.id = source.id \\n\" +\n+            \"WHEN MATCHED AND target.id = 1 THEN UPDATE SET * \";\n+\n+    sql(sqlText, \"\");\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(6, \"emp-id-6\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyDelete() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+            \"USING \" + sourceName + \" AS source \\n\" +\n+            \"ON target.id = source.id \\n\" +\n+            \"WHEN MATCHED AND target.id = 6 THEN DELETE\";\n+\n+    sql(sqlText, \"\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0Nzc0Mg=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 129}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUxNzY1ODkyOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQwMTozODoyOVrOIU3RDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xNlQyMzozNDozMFrOIVJ4dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0NzkxNw==", "bodyText": "Why did you choose to include the newlines?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r558747917", "createdAt": "2021-01-16T01:38:29Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+                     \"USING \" + sourceName + \" AS source \\n\" +\n+                     \"ON target.id = source.id \\n\" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText, \"\");\n+    sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertOnlyMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+                     \"USING \" + sourceName + \" AS source \\n\" +\n+                     \"ON target.id = source.id \\n\" +\n+                     \"WHEN NOT MATCHED AND (source.id >= 2) THEN INSERT * \";\n+\n+    sql(sqlText, \"\");\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyUpdate() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+            \"USING \" + sourceName + \" AS source \\n\" +\n+            \"ON target.id = source.id \\n\" +\n+            \"WHEN MATCHED AND target.id = 1 THEN UPDATE SET * \";\n+\n+    sql(sqlText, \"\");\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(6, \"emp-id-6\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyDelete() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTAzNjE1Mg==", "bodyText": "@rdblue i guess, its not required. I thought if we print out the SQL as part of some error or debugging, it will just format better. I will remove the newlines.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559036152", "createdAt": "2021-01-16T20:27:28Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+                     \"USING \" + sourceName + \" AS source \\n\" +\n+                     \"ON target.id = source.id \\n\" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText, \"\");\n+    sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertOnlyMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+                     \"USING \" + sourceName + \" AS source \\n\" +\n+                     \"ON target.id = source.id \\n\" +\n+                     \"WHEN NOT MATCHED AND (source.id >= 2) THEN INSERT * \";\n+\n+    sql(sqlText, \"\");\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyUpdate() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+            \"USING \" + sourceName + \" AS source \\n\" +\n+            \"ON target.id = source.id \\n\" +\n+            \"WHEN MATCHED AND target.id = 1 THEN UPDATE SET * \";\n+\n+    sql(sqlText, \"\");\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(6, \"emp-id-6\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyDelete() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0NzkxNw=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTA1MjkxOQ==", "bodyText": "I'm fine either way, it was just a surprise.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559052919", "createdAt": "2021-01-16T23:34:30Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+                     \"USING \" + sourceName + \" AS source \\n\" +\n+                     \"ON target.id = source.id \\n\" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText, \"\");\n+    sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertOnlyMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+                     \"USING \" + sourceName + \" AS source \\n\" +\n+                     \"ON target.id = source.id \\n\" +\n+                     \"WHEN NOT MATCHED AND (source.id >= 2) THEN INSERT * \";\n+\n+    sql(sqlText, \"\");\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyUpdate() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +\n+            \"USING \" + sourceName + \" AS source \\n\" +\n+            \"ON target.id = source.id \\n\" +\n+            \"WHEN MATCHED AND target.id = 1 THEN UPDATE SET * \";\n+\n+    sql(sqlText, \"\");\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(6, \"emp-id-6\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyDelete() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \\n\" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1ODc0NzkxNw=="}, "originalCommit": {"oid": "946bbded4eb82639a9e3db7fb90172dab827508c"}, "originalPosition": 124}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTAxOTcxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMjo0Njo1MlrOIV4-sA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMDoyMjowOVrOIV6Rsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgyNDU2MA==", "bodyText": "@dilipbiswal, this extraction is already done in the pushFilters method that @aokolnychyi implemented for delete. That's one reason why this also passes down target.output. The filters that are pushed down are the ones that only reference those attributes:\n    val tableAttrSet = AttributeSet(tableAttrs)\n    val predicates = splitConjunctivePredicates(cond).filter(_.references.subsetOf(tableAttrSet))\n    if (predicates.nonEmpty) {\n      val normalizedPredicates = DataSourceStrategy.normalizeExprs(predicates, tableAttrs)\n      PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n    }", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559824560", "createdAt": "2021-01-18T22:46:52Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.catalyst.analysis.Resolver\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.InputFileName\n+import org.apache.spark.sql.catalyst.expressions.IsNull\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans.FullOuter\n+import org.apache.spark.sql.catalyst.plans.Inner\n+import org.apache.spark.sql.catalyst.plans.logical.DeleteAction\n+import org.apache.spark.sql.catalyst.plans.logical.InsertAction\n+import org.apache.spark.sql.catalyst.plans.logical.Join\n+import org.apache.spark.sql.catalyst.plans.logical.JoinHint\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.plans.logical.MergeAction\n+import org.apache.spark.sql.catalyst.plans.logical.MergeInto\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoTable\n+import org.apache.spark.sql.catalyst.plans.logical.Project\n+import org.apache.spark.sql.catalyst.plans.logical.ReplaceData\n+import org.apache.spark.sql.catalyst.plans.logical.UpdateAction\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.RewriteRowLevelOperationHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.BooleanType\n+\n+case class RewriteMergeInto(conf: SQLConf) extends Rule[LogicalPlan] with RewriteRowLevelOperationHelper  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+  private val TRUE_LITERAL = Literal(true, BooleanType)\n+  private val FALSE_LITERAL = Literal(false, BooleanType)\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def resolver: Resolver = conf.resolver\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      case MergeIntoTable(target: DataSourceV2Relation, source: LogicalPlan, cond, matchedActions, notMatchedActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"merge\", writeInfo)\n+        val matchingRowsPlanBuilder = (_: DataSourceV2ScanRelation) =>\n+          Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        // TODO - extract the local predicates that references the target from the join condition and\n+        // pass to buildScanPlan to ensure push-down.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cb2e86f1962fc02b65065253f5ab3c3c18ade09"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMDgyNA==", "bodyText": "@rdblue Yeah.. i saw it Ryan. I checked the spark code and there is an additional check for deterministic status of the expression. Not sure for delete statement, we need this check or not ? Wanted to think through and discuss with you and Anton and thats why put a to-do.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559830824", "createdAt": "2021-01-18T23:11:14Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.catalyst.analysis.Resolver\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.InputFileName\n+import org.apache.spark.sql.catalyst.expressions.IsNull\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans.FullOuter\n+import org.apache.spark.sql.catalyst.plans.Inner\n+import org.apache.spark.sql.catalyst.plans.logical.DeleteAction\n+import org.apache.spark.sql.catalyst.plans.logical.InsertAction\n+import org.apache.spark.sql.catalyst.plans.logical.Join\n+import org.apache.spark.sql.catalyst.plans.logical.JoinHint\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.plans.logical.MergeAction\n+import org.apache.spark.sql.catalyst.plans.logical.MergeInto\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoTable\n+import org.apache.spark.sql.catalyst.plans.logical.Project\n+import org.apache.spark.sql.catalyst.plans.logical.ReplaceData\n+import org.apache.spark.sql.catalyst.plans.logical.UpdateAction\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.RewriteRowLevelOperationHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.BooleanType\n+\n+case class RewriteMergeInto(conf: SQLConf) extends Rule[LogicalPlan] with RewriteRowLevelOperationHelper  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+  private val TRUE_LITERAL = Literal(true, BooleanType)\n+  private val FALSE_LITERAL = Literal(false, BooleanType)\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def resolver: Resolver = conf.resolver\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      case MergeIntoTable(target: DataSourceV2Relation, source: LogicalPlan, cond, matchedActions, notMatchedActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"merge\", writeInfo)\n+        val matchingRowsPlanBuilder = (_: DataSourceV2ScanRelation) =>\n+          Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        // TODO - extract the local predicates that references the target from the join condition and\n+        // pass to buildScanPlan to ensure push-down.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgyNDU2MA=="}, "originalCommit": {"oid": "9cb2e86f1962fc02b65065253f5ab3c3c18ade09"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTg0NTYxOA==", "bodyText": "The only predicates that will be pushed are those that can be converted to Filter. I don't think any non-deterministic expressions can be converted so it should be fine.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559845618", "createdAt": "2021-01-19T00:20:58Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.catalyst.analysis.Resolver\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.InputFileName\n+import org.apache.spark.sql.catalyst.expressions.IsNull\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans.FullOuter\n+import org.apache.spark.sql.catalyst.plans.Inner\n+import org.apache.spark.sql.catalyst.plans.logical.DeleteAction\n+import org.apache.spark.sql.catalyst.plans.logical.InsertAction\n+import org.apache.spark.sql.catalyst.plans.logical.Join\n+import org.apache.spark.sql.catalyst.plans.logical.JoinHint\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.plans.logical.MergeAction\n+import org.apache.spark.sql.catalyst.plans.logical.MergeInto\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoTable\n+import org.apache.spark.sql.catalyst.plans.logical.Project\n+import org.apache.spark.sql.catalyst.plans.logical.ReplaceData\n+import org.apache.spark.sql.catalyst.plans.logical.UpdateAction\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.RewriteRowLevelOperationHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.BooleanType\n+\n+case class RewriteMergeInto(conf: SQLConf) extends Rule[LogicalPlan] with RewriteRowLevelOperationHelper  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+  private val TRUE_LITERAL = Literal(true, BooleanType)\n+  private val FALSE_LITERAL = Literal(false, BooleanType)\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def resolver: Resolver = conf.resolver\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      case MergeIntoTable(target: DataSourceV2Relation, source: LogicalPlan, cond, matchedActions, notMatchedActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"merge\", writeInfo)\n+        val matchingRowsPlanBuilder = (_: DataSourceV2ScanRelation) =>\n+          Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        // TODO - extract the local predicates that references the target from the join condition and\n+        // pass to buildScanPlan to ensure push-down.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgyNDU2MA=="}, "originalCommit": {"oid": "9cb2e86f1962fc02b65065253f5ab3c3c18ade09"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTg0NTgxMA==", "bodyText": "okay.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559845810", "createdAt": "2021-01-19T00:22:09Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.catalyst.analysis.Resolver\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.InputFileName\n+import org.apache.spark.sql.catalyst.expressions.IsNull\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans.FullOuter\n+import org.apache.spark.sql.catalyst.plans.Inner\n+import org.apache.spark.sql.catalyst.plans.logical.DeleteAction\n+import org.apache.spark.sql.catalyst.plans.logical.InsertAction\n+import org.apache.spark.sql.catalyst.plans.logical.Join\n+import org.apache.spark.sql.catalyst.plans.logical.JoinHint\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.plans.logical.MergeAction\n+import org.apache.spark.sql.catalyst.plans.logical.MergeInto\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoTable\n+import org.apache.spark.sql.catalyst.plans.logical.Project\n+import org.apache.spark.sql.catalyst.plans.logical.ReplaceData\n+import org.apache.spark.sql.catalyst.plans.logical.UpdateAction\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.RewriteRowLevelOperationHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.BooleanType\n+\n+case class RewriteMergeInto(conf: SQLConf) extends Rule[LogicalPlan] with RewriteRowLevelOperationHelper  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+  private val TRUE_LITERAL = Literal(true, BooleanType)\n+  private val FALSE_LITERAL = Literal(false, BooleanType)\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def resolver: Resolver = conf.resolver\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      case MergeIntoTable(target: DataSourceV2Relation, source: LogicalPlan, cond, matchedActions, notMatchedActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"merge\", writeInfo)\n+        val matchingRowsPlanBuilder = (_: DataSourceV2ScanRelation) =>\n+          Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        // TODO - extract the local predicates that references the target from the join condition and\n+        // pass to buildScanPlan to ensure push-down.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgyNDU2MA=="}, "originalCommit": {"oid": "9cb2e86f1962fc02b65065253f5ab3c3c18ade09"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTAyNTM3OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMjo0OToyNlrOIV5Bng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMjo0OToyNlrOIV5Bng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgyNTMxMA==", "bodyText": "This file is no longer used, so it can be removed.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559825310", "createdAt": "2021-01-18T22:49:26Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/PlanHelper.scala", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.spark.sql.catalyst.utils\n+\n+import java.util.UUID\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, PredicateHelper}\n+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, DynamicFileFilter, LogicalPlan}\n+import org.apache.spark.sql.connector.catalog.Table\n+import org.apache.spark.sql.connector.iceberg.read.SupportsFileFilter\n+import org.apache.spark.sql.connector.iceberg.write.MergeBuilder\n+import org.apache.spark.sql.connector.write.{LogicalWriteInfo, LogicalWriteInfoImpl}\n+import org.apache.spark.sql.execution.datasources.v2.{DataSourceV2ScanRelation}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.sql.util.CaseInsensitiveStringMap\n+\n+trait PlanHelper extends PredicateHelper {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cb2e86f1962fc02b65065253f5ab3c3c18ade09"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTAyOTk1OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMjo1MjowNlrOIV5EPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMjo1MjowNlrOIV5EPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgyNTk4Mg==", "bodyText": "This can use TRUE_LITERAL.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559825982", "createdAt": "2021-01-18T22:52:06Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.catalyst.analysis.Resolver\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.InputFileName\n+import org.apache.spark.sql.catalyst.expressions.IsNull\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans.FullOuter\n+import org.apache.spark.sql.catalyst.plans.Inner\n+import org.apache.spark.sql.catalyst.plans.logical.DeleteAction\n+import org.apache.spark.sql.catalyst.plans.logical.InsertAction\n+import org.apache.spark.sql.catalyst.plans.logical.Join\n+import org.apache.spark.sql.catalyst.plans.logical.JoinHint\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.plans.logical.MergeAction\n+import org.apache.spark.sql.catalyst.plans.logical.MergeInto\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoTable\n+import org.apache.spark.sql.catalyst.plans.logical.Project\n+import org.apache.spark.sql.catalyst.plans.logical.ReplaceData\n+import org.apache.spark.sql.catalyst.plans.logical.UpdateAction\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.RewriteRowLevelOperationHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.BooleanType\n+\n+case class RewriteMergeInto(conf: SQLConf) extends Rule[LogicalPlan] with RewriteRowLevelOperationHelper  {\n+  val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  val ROW_FROM_TARGET = \"_row_from_target_\"\n+  private val TRUE_LITERAL = Literal(true, BooleanType)\n+  private val FALSE_LITERAL = Literal(false, BooleanType)\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def resolver: Resolver = conf.resolver\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      case MergeIntoTable(target: DataSourceV2Relation, source: LogicalPlan, cond, matchedActions, notMatchedActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"merge\", writeInfo)\n+        val matchingRowsPlanBuilder = (_: DataSourceV2ScanRelation) =>\n+          Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)\n+        // TODO - extract the local predicates that references the target from the join condition and\n+        // pass to buildScanPlan to ensure push-down.\n+        val targetTableScan = buildScanPlan(target.table, target.output, mergeBuilder, None, matchingRowsPlanBuilder)\n+\n+        // Construct an outer join to help track changes in source and target.\n+        // TODO : Optimize this to use LEFT ANTI or RIGHT OUTER when applicable.\n+        val sourceTableProj = source.output ++ Seq(Alias(TRUE_LITERAL, ROW_FROM_SOURCE)())\n+        val targetTableProj = target.output ++ Seq(Alias(TRUE_LITERAL, ROW_FROM_TARGET)())\n+        val newTargetTableScan = Project(targetTableProj, targetTableScan)\n+        val newSourceTableScan = Project(sourceTableProj, source)\n+        val joinPlan = Join(newSourceTableScan, newTargetTableScan, FullOuter, Some(cond), JoinHint.NONE)\n+\n+        // Construct the plan to replace the data based on the output of `MergeInto`\n+        val mergeParams = MergeIntoParams(\n+          isSourceRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_SOURCE)),\n+          isTargetRowNotPresent = IsNull(findOutputAttr(joinPlan, ROW_FROM_TARGET)),\n+          matchedConditions = matchedActions.map(getClauseCondition),\n+          matchedOutputs = matchedActions.map(actionOutput(_, targetOutputCols)),\n+          notMatchedConditions = notMatchedActions.map(getClauseCondition),\n+          notMatchedOutputs = notMatchedActions.map(actionOutput(_, targetOutputCols)),\n+          targetOutput = targetOutputCols :+ FALSE_LITERAL,\n+          deleteOutput = targetOutputCols :+ TRUE_LITERAL,\n+          joinedAttributes = joinPlan.output\n+        )\n+        val mergePlan = MergeInto(mergeParams, target, joinPlan)\n+        val batchWrite = mergeBuilder.asWriteBuilder.buildForBatch()\n+        ReplaceData(target, batchWrite, mergePlan)\n+    }\n+  }\n+\n+  private def actionOutput(clause: MergeAction, targetOutputCols: Seq[Expression]): Seq[Expression] = {\n+    clause match {\n+      case u: UpdateAction =>\n+        u.assignments.map(_.value) :+ FALSE_LITERAL\n+      case _: DeleteAction =>\n+        targetOutputCols :+ TRUE_LITERAL\n+      case i: InsertAction =>\n+        i.assignments.map(_.value) :+ FALSE_LITERAL\n+    }\n+  }\n+\n+  private def getClauseCondition(clause: MergeAction): Expression = {\n+    clause.condition.getOrElse(Literal(true))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9cb2e86f1962fc02b65065253f5ab3c3c18ade09"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTA2NDA1OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxMTowM1rOIV5W-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxMTowM1rOIV5W-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMDc3Ng==", "bodyText": "This newline isn't needed. Lines up to 120 characters are allowed.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559830776", "createdAt": "2021-01-18T23:11:03Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.catalyst.analysis.Resolver\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.InputFileName\n+import org.apache.spark.sql.catalyst.expressions.IsNull\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans.FullOuter\n+import org.apache.spark.sql.catalyst.plans.Inner\n+import org.apache.spark.sql.catalyst.plans.logical.DeleteAction\n+import org.apache.spark.sql.catalyst.plans.logical.InsertAction\n+import org.apache.spark.sql.catalyst.plans.logical.Join\n+import org.apache.spark.sql.catalyst.plans.logical.JoinHint\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.plans.logical.MergeAction\n+import org.apache.spark.sql.catalyst.plans.logical.MergeInto\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoTable\n+import org.apache.spark.sql.catalyst.plans.logical.Project\n+import org.apache.spark.sql.catalyst.plans.logical.ReplaceData\n+import org.apache.spark.sql.catalyst.plans.logical.UpdateAction\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.RewriteRowLevelOperationHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.BooleanType\n+\n+case class RewriteMergeInto(conf: SQLConf) extends Rule[LogicalPlan] with RewriteRowLevelOperationHelper  {\n+  private val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  private val ROW_FROM_TARGET = \"_row_from_target_\"\n+  private val TRUE_LITERAL = Literal(true, BooleanType)\n+  private val FALSE_LITERAL = Literal(false, BooleanType)\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def resolver: Resolver = conf.resolver\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      case MergeIntoTable(target: DataSourceV2Relation, source: LogicalPlan, cond, matchedActions, notMatchedActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTA2NDE0OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxMTowNlrOIV5XAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxMTowNlrOIV5XAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMDc4Nw==", "bodyText": "This plan must use the v2 scan relation. Otherwise, the _file column is not projected. This should be:\n    val matchingRowsPlanBuilder = (rel: DataSourceV2ScanRelation) =>\n        Join(source, rel, Inner, Some(cond), JoinHint.NONE)\nThen you can also remove newProjectCols and newTargetTable.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559830787", "createdAt": "2021-01-18T23:11:06Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteMergeInto.scala", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.optimizer\n+\n+import org.apache.spark.sql.catalyst.analysis.Resolver\n+import org.apache.spark.sql.catalyst.expressions.Alias\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.InputFileName\n+import org.apache.spark.sql.catalyst.expressions.IsNull\n+import org.apache.spark.sql.catalyst.expressions.Literal\n+import org.apache.spark.sql.catalyst.plans.FullOuter\n+import org.apache.spark.sql.catalyst.plans.Inner\n+import org.apache.spark.sql.catalyst.plans.logical.DeleteAction\n+import org.apache.spark.sql.catalyst.plans.logical.InsertAction\n+import org.apache.spark.sql.catalyst.plans.logical.Join\n+import org.apache.spark.sql.catalyst.plans.logical.JoinHint\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.plans.logical.MergeAction\n+import org.apache.spark.sql.catalyst.plans.logical.MergeInto\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoTable\n+import org.apache.spark.sql.catalyst.plans.logical.Project\n+import org.apache.spark.sql.catalyst.plans.logical.ReplaceData\n+import org.apache.spark.sql.catalyst.plans.logical.UpdateAction\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.catalyst.utils.RewriteRowLevelOperationHelper\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.BooleanType\n+\n+case class RewriteMergeInto(conf: SQLConf) extends Rule[LogicalPlan] with RewriteRowLevelOperationHelper  {\n+  private val ROW_FROM_SOURCE = \"_row_from_source_\"\n+  private val ROW_FROM_TARGET = \"_row_from_target_\"\n+  private val TRUE_LITERAL = Literal(true, BooleanType)\n+  private val FALSE_LITERAL = Literal(false, BooleanType)\n+\n+  import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Implicits._\n+\n+  override def resolver: Resolver = conf.resolver\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = {\n+    plan resolveOperators {\n+      case MergeIntoTable(target: DataSourceV2Relation, source: LogicalPlan, cond, matchedActions, notMatchedActions) =>\n+        val targetOutputCols = target.output\n+        val newProjectCols = target.output ++ Seq(Alias(InputFileName(), FILE_NAME_COL)())\n+        val newTargetTable = Project(newProjectCols, target)\n+\n+        // Construct the plan to prune target based on join condition between source and\n+        // target.\n+        val writeInfo = newWriteInfo(target.schema)\n+        val mergeBuilder = target.table.asMergeable.newMergeBuilder(\"merge\", writeInfo)\n+        val matchingRowsPlanBuilder = (_: DataSourceV2ScanRelation) =>\n+          Join(source, newTargetTable, Inner, Some(cond), JoinHint.NONE)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTA2ODk0OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/RewriteRowLevelOperationHelper.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxNDo0OVrOIV5Zxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMDoyNToxMFrOIV6T2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMTQ5NQ==", "bodyText": "You can solve this problem by passing the target table attrs from the DataSourceV2ScanRelation:\n    val matchingFilePlan = buildFileFilterPlan(scanRelation.output, matchingRowsPlanBuilder(scanRelation))\n  ...\n\n  private def buildFileFilterPlan(tableAttrs: Seq[AttributeReference], matchingRowsPlan: LogicalPlan): LogicalPlan = {\n    val fileAttr = findOutputAttr(tableAttrs, FILE_NAME_COL)\n    val agg = Aggregate(Seq(fileAttr), Seq(fileAttr), matchingRowsPlan)\n    Project(Seq(findOutputAttr(agg.output, FILE_NAME_COL)), agg)\n  }\n\n  protected def findOutputAttr(attrs: Seq[Attribute], attrName: String): Attribute = {\n    attrs.find(attr => resolver(attr.name, attrName)).getOrElse {\n      throw new AnalysisException(s\"Cannot find $attrName in $attrs\")\n    }\n  }", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559831495", "createdAt": "2021-01-18T23:14:49Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/RewriteRowLevelOperationHelper.scala", "diffHunk": "@@ -103,6 +103,7 @@ trait RewriteRowLevelOperationHelper extends PredicateHelper with Logging {\n   }\n \n   private def buildFileFilterPlan(matchingRowsPlan: LogicalPlan): LogicalPlan = {\n+    // TODO: For merge-into make sure _file is resolved only from target table.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMzU5NA==", "bodyText": "@rdblue Don't we have an issue of the target table has a column named \"_file\" ? I was thinking we may need a way to solve it by creating a distinct co-relation name if _file is existing in the target relation's output ?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559833594", "createdAt": "2021-01-18T23:23:49Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/RewriteRowLevelOperationHelper.scala", "diffHunk": "@@ -103,6 +103,7 @@ trait RewriteRowLevelOperationHelper extends PredicateHelper with Logging {\n   }\n \n   private def buildFileFilterPlan(matchingRowsPlan: LogicalPlan): LogicalPlan = {\n+    // TODO: For merge-into make sure _file is resolved only from target table.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMTQ5NQ=="}, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTg0NjM2Mg==", "bodyText": "I think it should be fine. We should throw an exception if the table has a _file column, but that's something we can do later.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559846362", "createdAt": "2021-01-19T00:25:10Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/RewriteRowLevelOperationHelper.scala", "diffHunk": "@@ -103,6 +103,7 @@ trait RewriteRowLevelOperationHelper extends PredicateHelper with Logging {\n   }\n \n   private def buildFileFilterPlan(matchingRowsPlan: LogicalPlan): LogicalPlan = {\n+    // TODO: For merge-into make sure _file is resolved only from target table.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMTQ5NQ=="}, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTA2OTc3OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/RewriteRowLevelOperationHelper.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxNToxOFrOIV5aPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzozOTozMVrOIV5v5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMTYxNQ==", "bodyText": "I don't think this change is needed because Anton's update already extracts the correct filters from cond.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559831615", "createdAt": "2021-01-18T23:15:18Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/RewriteRowLevelOperationHelper.scala", "diffHunk": "@@ -54,12 +54,12 @@ trait RewriteRowLevelOperationHelper extends PredicateHelper with Logging {\n       table: Table,\n       tableAttrs: Seq[AttributeReference],\n       mergeBuilder: MergeBuilder,\n-      cond: Expression,\n+      cond: Option[Expression] = None,\n       matchingRowsPlanBuilder: DataSourceV2ScanRelation => LogicalPlan): LogicalPlan = {\n \n     val scanBuilder = mergeBuilder.asScanBuilder\n \n-    pushFilters(scanBuilder, cond, tableAttrs)\n+    cond.map(pushFilters(scanBuilder, _, tableAttrs))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzNzE1Nw==", "bodyText": "@rdblue ok.. i will change it for now. We will discuss the deterministic thing later.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559837157", "createdAt": "2021-01-18T23:39:31Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/utils/RewriteRowLevelOperationHelper.scala", "diffHunk": "@@ -54,12 +54,12 @@ trait RewriteRowLevelOperationHelper extends PredicateHelper with Logging {\n       table: Table,\n       tableAttrs: Seq[AttributeReference],\n       mergeBuilder: MergeBuilder,\n-      cond: Expression,\n+      cond: Option[Expression] = None,\n       matchingRowsPlanBuilder: DataSourceV2ScanRelation => LogicalPlan): LogicalPlan = {\n \n     val scanBuilder = mergeBuilder.asScanBuilder\n \n-    pushFilters(scanBuilder, cond, tableAttrs)\n+    cond.map(pushFilters(scanBuilder, _, tableAttrs))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMTYxNQ=="}, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTA3MjI1OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxNjo0OFrOIV5bsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoxNjo0OFrOIV5bsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMTk4Nw==", "bodyText": "Style: this is a doc comment because it starts with /**. Usually, multi-line comments in code would use //   on each line.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559831987", "createdAt": "2021-01-18T23:16:48Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.BasePredicate\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.execution.UnaryExecNode\n+\n+case class MergeIntoExec(\n+    mergeIntoParams: MergeIntoParams,\n+    @transient targetRelation: DataSourceV2Relation,\n+    override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoParams, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(\n+     actions: Seq[(BasePredicate, UnsafeProjection)],\n+     projectTargetCols: UnsafeProjection,\n+     projectDeleteRow: UnsafeProjection,\n+     inputRow: InternalRow,\n+     targetRowNotPresent: Boolean): InternalRow = {\n+\n+    /**\n+     * Find the first combination where the predicate evaluates to true.\n+     * In case when there are overlapping condition in the MATCHED\n+     * clauses, for the first one that satisfies the predicate, the\n+     * corresponding action is applied. For example:\n+     *\n+     * WHEN MATCHED AND id > 1 AND id < 10 UPDATE *\n+     * WHEN MATCHED AND id = 5 OR id = 21 DELETE\n+     *\n+     * In above case, when id = 5, it applies both that matched predicates. In this\n+     * case the first one we see is applied.\n+     */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTA4NjkyOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoyNTozMlrOIV5jbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoyNTozMlrOIV5jbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzMzk2Nw==", "bodyText": "Nit: could use filterNot(shouldDeleteRow) instead.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559833967", "createdAt": "2021-01-18T23:25:32Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.BasePredicate\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.execution.UnaryExecNode\n+\n+case class MergeIntoExec(\n+    mergeIntoParams: MergeIntoParams,\n+    @transient targetRelation: DataSourceV2Relation,\n+    override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoParams, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(\n+     actions: Seq[(BasePredicate, UnsafeProjection)],\n+     projectTargetCols: UnsafeProjection,\n+     projectDeleteRow: UnsafeProjection,\n+     inputRow: InternalRow,\n+     targetRowNotPresent: Boolean): InternalRow = {\n+\n+    /**\n+     * Find the first combination where the predicate evaluates to true.\n+     * In case when there are overlapping condition in the MATCHED\n+     * clauses, for the first one that satisfies the predicate, the\n+     * corresponding action is applied. For example:\n+     *\n+     * WHEN MATCHED AND id > 1 AND id < 10 UPDATE *\n+     * WHEN MATCHED AND id = 5 OR id = 21 DELETE\n+     *\n+     * In above case, when id = 5, it applies both that matched predicates. In this\n+     * case the first one we see is applied.\n+     */\n+\n+    val pair = actions.find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)\n+        } else {\n+          projectTargetCols.apply(inputRow)\n+        }\n+    }\n+  }\n+\n+  def processPartition(\n+     params: MergeIntoParams,\n+     rowIterator: Iterator[InternalRow]): Iterator[InternalRow] = {\n+\n+    val joinedAttrs = params.joinedAttributes\n+    val isSourceRowNotPresentPred = generatePredicate(params.isSourceRowNotPresent, joinedAttrs)\n+    val isTargetRowNotPresentPred = generatePredicate(params.isTargetRowNotPresent, joinedAttrs)\n+    val matchedPreds = params.matchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val matchedProjs = params.matchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val notMatchedPreds = params.notMatchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val notMatchedProjs = params.notMatchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val projectTargetCols = generateProjection(params.targetOutput, joinedAttrs)\n+    val projectDeletedRow = generateProjection(params.deleteOutput, joinedAttrs)\n+    val nonMatchedPairs =   notMatchedPreds zip notMatchedProjs\n+    val matchedPairs = matchedPreds zip matchedProjs\n+\n+    def shouldDeleteRow(row: InternalRow): Boolean =\n+      row.getBoolean(params.targetOutput.size - 1)\n+\n+\n+    /**\n+     * This method is responsible for processing a input row to emit the resultant row with an\n+     * additional column that indicates whether the row is going to be included in the final\n+     * output of merge or not.\n+     * 1. If there is a target row for which there is no corresponding source row (join condition not met)\n+     *    - Only project the target columns with deleted flag set to false.\n+     * 2. If there is a source row for which there is no corresponding target row (join condition not met)\n+     *    - Apply the not matched actions (i.e INSERT actions) if non match conditions are met.\n+     * 3. If there is a source row for which there is a corresponding target row (join condition met)\n+     *    - Apply the matched actions (i.e DELETE or UPDATE actions) if match conditions are met.\n+     */\n+    def processRow(inputRow: InternalRow): InternalRow = {\n+      if (isSourceRowNotPresentPred.eval(inputRow)) {\n+        projectTargetCols.apply(inputRow)\n+      } else if (isTargetRowNotPresentPred.eval(inputRow)) {\n+        applyProjection(nonMatchedPairs, projectTargetCols, projectDeletedRow, inputRow, true)\n+      } else {\n+        applyProjection(matchedPairs, projectTargetCols, projectDeletedRow, inputRow, false)\n+      }\n+    }\n+\n+    rowIterator\n+      .map(processRow)\n+      .filter(!shouldDeleteRow(_))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 137}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTA5MDU4OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoyNzoyNlrOIV5lVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzoyNzoyNlrOIV5lVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzNDQ1Mw==", "bodyText": "This should be the merge equivalent.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559834453", "createdAt": "2021-01-18T23:27:26Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTExMjg1OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzo0MDoyN1rOIV5wzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMDowNzo0N1rOIV6GnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzNzM4OA==", "bodyText": "I don't think that there is a need to test this with both Hive and Hadoop catalogs or with all 3 formats, since the main thing that needs to be tested is conversion and Spark behavior. Also, this doesn't test partitioned tables at all. To fix those, I think this should customize parameters:\n  @Parameterized.Parameters(\n      name = \"catalogName = {0}, implementation = {1}, config = {2}, format = {3}, vectorized = {4}, partitioned = {5}\")\n  public static Object[][] parameters() {\n    return new Object[][] {\n        { \"testhive\", SparkCatalog.class.getName(),\n            ImmutableMap.of(\n                \"type\", \"hive\",\n                \"default-namespace\", \"default\"\n            ),\n            \"parquet\",\n            true,\n            false\n        },\n        { \"spark_catalog\", SparkSessionCatalog.class.getName(),\n            ImmutableMap.of(\n                \"type\", \"hive\",\n                \"default-namespace\", \"default\",\n                \"clients\", \"1\",\n                \"parquet-enabled\", \"false\",\n                \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n            ),\n            \"parquet\",\n            false,\n            true\n        }\n    };\n  }\n\n  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n                            String fileFormat, Boolean vectorized, Boolean partitioned) {\n    super(catalogName, implementation, config, fileFormat, vectorized);\n    this.partitioned = partitioned;\n    this.sourceName = tableName(\"source\");\n    this.targetName = tableName(\"target\");\n  }\nI also added a partitioned boolean and moved all of the createAndInit calls into a @Before:\n  @Before\n  public void createTables() {\n    if (partitioned) {\n      createAndInitPartitionedTargetTable(targetName);\n    } else {\n      createAndInitUnPartitionedTargetTable(targetName);\n    }\n    createAndInitSourceTable(sourceName);\n  }\nWith those changes, tests run faster and cover partitioned tables.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559837388", "createdAt": "2021-01-18T23:40:27Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzOTk3Nw==", "bodyText": "@rdblue Ryan, i had added the partitioning test cases in the grouby/sort pr ? I wasn't doing it via parameter as i was testing individual part and sort expressions. Can you please take a quick look ? I am thinking how those tests work with the proposed changes of creating tables in @before and also with the introduction of partition parameter.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559839977", "createdAt": "2021-01-18T23:52:56Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzNzM4OA=="}, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTg0MjY2Mg==", "bodyText": "Okay, sounds fine to me. Let's still make the parameter changes, but remove the partitioned parameter.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559842662", "createdAt": "2021-01-19T00:06:17Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzNzM4OA=="}, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTg0Mjk3Mg==", "bodyText": "ok.. will do. thank you Ryan for doing such a thorough review. Really appreciate !!", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559842972", "createdAt": "2021-01-19T00:07:47Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzNzM4OA=="}, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTEyMDgwOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzo0NTo1NVrOIV51JQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzo0NTo1NVrOIV51JQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzODUwMQ==", "bodyText": "Rather than embedding the names directly, you can pass them to the sql method, like removeTables does:\nsql(\"MERGE INTO %s AS target USING %s AS source ...\", targetName, sourceName);", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559838501", "createdAt": "2021-01-18T23:45:55Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \" +\n+                     \"USING \" + sourceName + \" AS source \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTEyMjMxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzo0Njo1OFrOIV51-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzo1MDoxMlrOIV546g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzODcxNQ==", "bodyText": "This select has no effect, can you remove it?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559838715", "createdAt": "2021-01-18T23:46:58Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \" +\n+                     \"USING \" + sourceName + \" AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText);\n+    sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzOTQ2Ng==", "bodyText": "Looks like there are other cases where SELECT statements run but are not used. Can you remove all of them?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559839466", "createdAt": "2021-01-18T23:50:12Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \" +\n+                     \"USING \" + sourceName + \" AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText);\n+    sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzODcxNQ=="}, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTEyNTEzOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzo0ODoyNlrOIV53Zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOFQyMzo0OTo1MVrOIV54oA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzOTA3OQ==", "bodyText": "res is not used in this test or others. Can you remove this line?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559839079", "createdAt": "2021-01-18T23:48:26Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \" +\n+                     \"USING \" + sourceName + \" AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText);\n+    sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertOnlyMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \" +\n+                     \"USING \" + sourceName + \" AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED AND (source.id >= 2) THEN INSERT * \";\n+\n+    sql(sqlText);\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzOTM5Mg==", "bodyText": "@rdblue sorry... had used this for debugging and forgot to remove it later.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559839392", "createdAt": "2021-01-18T23:49:51Z", "author": {"login": "dilipbiswal"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \" +\n+                     \"USING \" + sourceName + \" AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText);\n+    sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertOnlyMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \" +\n+                     \"USING \" + sourceName + \" AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED AND (source.id >= 2) THEN INSERT * \";\n+\n+    sql(sqlText);\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTgzOTA3OQ=="}, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTE0MzQwOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMDowMDo0NFrOIV6BPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMDowMDo0NFrOIV6BPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTg0MTU5Nw==", "bodyText": "The employee dep is identical for both records with id 6, so the assertion can't distinguish between the case where employee 6 is replaced or not. Could you update the original target data to emp-id-six and assert that it is unchanged because of the target.id = 1 requirement?", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559841597", "createdAt": "2021-01-19T00:00:44Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.DELETE_MODE, \"copy-on-write\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \" +\n+                     \"USING \" + sourceName + \" AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText);\n+    sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertOnlyMatchingRows() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO \" + targetName + \" AS target \" +\n+                     \"USING \" + sourceName + \" AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED AND (source.id >= 2) THEN INSERT * \";\n+\n+    sql(sqlText);\n+    List<Object[]> res = sql(\"SELECT * FROM %s ORDER BY id, dep\", targetName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyUpdate() throws NoSuchTableException {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "227a1081dfc460840c0c611a01d6eb5fed9de15f"}, "originalPosition": 104}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTE1MTI3OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMDowNjo0NVrOIV6FtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMDowNjo0NVrOIV6FtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTg0Mjc0MA==", "bodyText": "Nit: extra newline.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559842740", "createdAt": "2021-01-19T00:06:45Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.BasePredicate\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.execution.UnaryExecNode\n+\n+case class MergeIntoExec(\n+    mergeIntoParams: MergeIntoParams,\n+    @transient targetRelation: DataSourceV2Relation,\n+    override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoParams, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(\n+     actions: Seq[(BasePredicate, UnsafeProjection)],\n+     projectTargetCols: UnsafeProjection,\n+     projectDeleteRow: UnsafeProjection,\n+     inputRow: InternalRow,\n+     targetRowNotPresent: Boolean): InternalRow = {\n+\n+\n+    // Find the first combination where the predicate evaluates to true.\n+    // In case when there are overlapping condition in the MATCHED\n+    // clauses, for the first one that satisfies the predicate, the\n+    // corresponding action is applied. For example:\n+    //\n+    // WHEN MATCHED AND id > 1 AND id < 10 UPDATE *\n+    // WHEN MATCHED AND id = 5 OR id = 21 DELETE\n+    //\n+    // In above case, when id = 5, it applies both that matched predicates. In this\n+    // case the first one we see is applied.\n+    //\n+\n+    val pair = actions.find {\n+      case (predicate, _) => predicate.eval(inputRow)\n+    }\n+\n+    // Now apply the appropriate projection to either :\n+    // - Insert a row into target\n+    // - Update a row of target\n+    // - Delete a row in target. The projected row will have the deleted bit set.\n+    pair match {\n+      case Some((_, projection)) =>\n+        projection.apply(inputRow)\n+      case None =>\n+        if (targetRowNotPresent) {\n+          projectDeleteRow.apply(inputRow)\n+        } else {\n+          projectTargetCols.apply(inputRow)\n+        }\n+    }\n+  }\n+\n+  def processPartition(\n+     params: MergeIntoParams,\n+     rowIterator: Iterator[InternalRow]): Iterator[InternalRow] = {\n+\n+    val joinedAttrs = params.joinedAttributes\n+    val isSourceRowNotPresentPred = generatePredicate(params.isSourceRowNotPresent, joinedAttrs)\n+    val isTargetRowNotPresentPred = generatePredicate(params.isTargetRowNotPresent, joinedAttrs)\n+    val matchedPreds = params.matchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val matchedProjs = params.matchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val notMatchedPreds = params.notMatchedConditions.map(generatePredicate(_, joinedAttrs))\n+    val notMatchedProjs = params.notMatchedOutputs.map(generateProjection(_, joinedAttrs))\n+    val projectTargetCols = generateProjection(params.targetOutput, joinedAttrs)\n+    val projectDeletedRow = generateProjection(params.deleteOutput, joinedAttrs)\n+    val nonMatchedPairs =   notMatchedPreds zip notMatchedProjs\n+    val matchedPairs = matchedPreds zip matchedProjs\n+\n+    def shouldDeleteRow(row: InternalRow): Boolean =\n+      row.getBoolean(params.targetOutput.size - 1)\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f293356f830d1fe05056bfd820cc470b1de6110"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTI5MDE1OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMTozNDo0NlrOIV7SBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMTozNDo0NlrOIV7SBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTg2MjI3OQ==", "bodyText": "@dilipbiswal, this can be reverted as well.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559862279", "createdAt": "2021-01-19T01:34:46Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -56,7 +56,7 @@ case class RewriteDelete(conf: SQLConf) extends Rule[LogicalPlan] with RewriteRo\n       d\n \n     // rewrite all operations that require reading the table to delete records\n-    case DeleteFromTable(r: DataSourceV2Relation, Some(cond)) =>\n+    case DeleteFromTable(r: DataSourceV2Relation, optionalCond @ Some(cond)) =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7caece5de862437092fbbd58702f41fc66d8fdd"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTI5Mzk3OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMTozNjo1MlrOIV7UBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMTozNjo1MlrOIV7UBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTg2Mjc4OA==", "bodyText": "Nit: no need for an empty comment and an empty line.", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559862788", "createdAt": "2021-01-19T01:36:52Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/MergeIntoExec.scala", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.catalyst.expressions.BasePredicate\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection\n+import org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate\n+import org.apache.spark.sql.catalyst.plans.logical.MergeIntoParams\n+import org.apache.spark.sql.execution.SparkPlan\n+import org.apache.spark.sql.execution.UnaryExecNode\n+\n+case class MergeIntoExec(\n+    mergeIntoParams: MergeIntoParams,\n+    @transient targetRelation: DataSourceV2Relation,\n+    override val child: SparkPlan) extends UnaryExecNode {\n+\n+  override def output: Seq[Attribute] = targetRelation.output\n+\n+  protected override def doExecute(): RDD[InternalRow] = {\n+    child.execute().mapPartitions {\n+      processPartition(mergeIntoParams, _)\n+    }\n+  }\n+\n+  private def generateProjection(exprs: Seq[Expression], attrs: Seq[Attribute]): UnsafeProjection = {\n+    UnsafeProjection.create(exprs, attrs)\n+  }\n+\n+  private def generatePredicate(expr: Expression, attrs: Seq[Attribute]): BasePredicate = {\n+    GeneratePredicate.generate(expr, attrs)\n+  }\n+\n+  def applyProjection(\n+     actions: Seq[(BasePredicate, UnsafeProjection)],\n+     projectTargetCols: UnsafeProjection,\n+     projectDeleteRow: UnsafeProjection,\n+     inputRow: InternalRow,\n+     targetRowNotPresent: Boolean): InternalRow = {\n+\n+\n+    // Find the first combination where the predicate evaluates to true.\n+    // In case when there are overlapping condition in the MATCHED\n+    // clauses, for the first one that satisfies the predicate, the\n+    // corresponding action is applied. For example:\n+    //\n+    // WHEN MATCHED AND id > 1 AND id < 10 UPDATE *\n+    // WHEN MATCHED AND id = 5 OR id = 21 DELETE\n+    //\n+    // In above case, when id = 5, it applies both that matched predicates. In this\n+    // case the first one we see is applied.\n+    //\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7caece5de862437092fbbd58702f41fc66d8fdd"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzUyNTI5OTMzOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMTozOTowN1rOIV7Wrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMS0wMS0xOVQwMTozOTowN1rOIV7Wrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1OTg2MzQ3MA==", "bodyText": "Nit: it looks like there are unnecessary string literals. \" \" + \"MERGE ...\" can be updated to \" MERGE ...\".", "url": "https://github.com/apache/iceberg/pull/1947#discussion_r559863470", "createdAt": "2021-01-19T01:39:07Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeIntoTable.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n+\n+public class TestMergeIntoTable extends SparkRowLevelOperationsTestBase {\n+  private final String sourceName;\n+  private final String targetName;\n+\n+  @Parameterized.Parameters(\n+      name = \"catalogName = {0}, implementation = {1}, config = {2}, format = {3}, vectorized = {4}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        { \"testhive\", SparkCatalog.class.getName(),\n+            ImmutableMap.of(\n+                \"type\", \"hive\",\n+                \"default-namespace\", \"default\"\n+            ),\n+            \"parquet\",\n+            true\n+        },\n+        { \"spark_catalog\", SparkSessionCatalog.class.getName(),\n+            ImmutableMap.of(\n+                \"type\", \"hive\",\n+                \"default-namespace\", \"default\",\n+                \"clients\", \"1\",\n+                \"parquet-enabled\", \"false\",\n+                \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+            ),\n+            \"parquet\",\n+            false\n+        }\n+    };\n+  }\n+\n+  public TestMergeIntoTable(String catalogName, String implementation, Map<String, String> config,\n+                            String fileFormat, Boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+    this.sourceName = tableName(\"source\");\n+    this.targetName = tableName(\"target\");\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected Map<String, String> extraTableProperties() {\n+    return ImmutableMap.of(TableProperties.MERGE_MODE, TableProperties.MERGE_MODE_DEFAULT);\n+  }\n+\n+  @Before\n+  public void createTables() {\n+    createAndInitUnPartitionedTargetTable(targetName);\n+    createAndInitSourceTable(sourceName);\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", targetName);\n+    sql(\"DROP TABLE IF EXISTS %s\", sourceName);\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertAllNonMatchingRows() throws NoSuchTableException {\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO %s AS target \" +\n+                     \"USING %s AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED THEN INSERT * \";\n+\n+    sql(sqlText, targetName, sourceName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testEmptyTargetInsertOnlyMatchingRows() throws NoSuchTableException {\n+    append(sourceName, new Employee(1, \"emp-id-1\"), new Employee(2, \"emp-id-2\"), new Employee(3, \"emp-id-3\"));\n+    String sqlText = \"MERGE INTO %s AS target \" +\n+                     \"USING %s AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN NOT MATCHED AND (source.id >= 2) THEN INSERT * \";\n+\n+    sql(sqlText, targetName, sourceName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(2, \"emp-id-2\"), row(3, \"emp-id-3\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyUpdate() throws NoSuchTableException {\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-six\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO %s AS target \" +\n+            \"USING %s AS source \" +\n+            \"ON target.id = source.id \" +\n+            \"WHEN MATCHED AND target.id = 1 THEN UPDATE SET * \";\n+\n+    sql(sqlText, targetName, sourceName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(6, \"emp-id-six\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testOnlyDelete() throws NoSuchTableException {\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO %s AS target \" +\n+            \"USING %s AS source \" +\n+            \"ON target.id = source.id \" +\n+            \"WHEN MATCHED AND target.id = 6 THEN DELETE\";\n+\n+    sql(sqlText, targetName, sourceName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-one\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testAllCauses() throws NoSuchTableException {\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO %s AS target \" +\n+                     \"USING %s AS source \" +\n+                     \"ON target.id = source.id \" +\n+                     \"WHEN MATCHED AND target.id = 1 THEN UPDATE SET * \" +\n+                     \"WHEN MATCHED AND target.id = 6 THEN DELETE \" +\n+                     \"WHEN NOT MATCHED AND source.id = 2 THEN INSERT * \";\n+\n+    sql(sqlText, targetName, sourceName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testAllCausesWithExplicitColumnSpecification() throws NoSuchTableException {\n+    append(targetName, new Employee(1, \"emp-id-one\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-2\"), new Employee(1, \"emp-id-1\"), new Employee(6, \"emp-id-6\"));\n+    String sqlText = \"MERGE INTO %s AS target \" +\n+            \"USING %s AS source \" +\n+            \"ON target.id = source.id \" +\n+            \"WHEN MATCHED AND target.id = 1 THEN UPDATE SET target.id = source.id, target.dep = source.dep \" +\n+            \"WHEN MATCHED AND target.id = 6 THEN DELETE \" +\n+            \"WHEN NOT MATCHED AND source.id = 2 THEN INSERT (target.id, target.dep) VALUES (source.id, source.dep) \";\n+\n+    sql(sqlText, targetName, sourceName);\n+    assertEquals(\"Should have expected rows\",\n+            ImmutableList.of(row(1, \"emp-id-1\"), row(2, \"emp-id-2\")),\n+            sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", targetName));\n+  }\n+\n+  @Test\n+  public void testSourceCTE() throws NoSuchTableException {\n+    Assume.assumeFalse(catalogName.equalsIgnoreCase(\"testhadoop\"));\n+    Assume.assumeFalse(catalogName.equalsIgnoreCase(\"testhive\"));\n+\n+    append(targetName, new Employee(2, \"emp-id-two\"), new Employee(6, \"emp-id-6\"));\n+    append(sourceName, new Employee(2, \"emp-id-3\"), new Employee(1, \"emp-id-2\"), new Employee(5, \"emp-id-6\"));\n+    String sourceCTE = \"WITH cte1 AS (SELECT id + 1 AS id, dep FROM source)\";\n+    String sqlText = sourceCTE + \" \" + \"MERGE INTO %s AS target \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d7caece5de862437092fbbd58702f41fc66d8fdd"}, "originalPosition": 202}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3058, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}