{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTA1NDQyMTQx", "number": 1624, "reviewThreads": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwMzo1NToxMVrOEvHjlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxNDo0MjoxOFrOEyY0dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE3ODQyMzI1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwMzo1NToxMVrOHj6sQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwMzo1NToxMVrOHj6sQw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzQyMzgxMQ==", "bodyText": "Useless blank line.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r507423811", "createdAt": "2020-10-19T03:55:11Z", "author": {"login": "simonsssu"}, "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT, R> extends BaseSnapshotUpdateAction<ThisT, R> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesActionBase.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+  private PartitionSpec spec = null;\n+  private boolean caseSensitive = false;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec067455f6a2b1b89121fcf696944376919ae683"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE3ODQyODQ1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwMzo1NjoyMFrOHj6v6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwMToxNzo1OVrOHlWw9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzQyNDc0NQ==", "bodyText": "can we use try-with-resource to simply the logic of handle exception here ?", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r507424745", "createdAt": "2020-10-19T03:56:20Z", "author": {"login": "simonsssu"}, "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT, R> extends BaseSnapshotUpdateAction<ThisT, R> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesActionBase.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+  private PartitionSpec spec = null;\n+  private boolean caseSensitive = false;\n+\n+\n+  public RewriteDataFilesActionBase(Table table) {\n+    this.table = table;\n+    fileIO = table.io();\n+    this.filter = Expressions.alwaysTrue();\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+    this.encryptionManager = table.encryption();\n+    spec = table.spec();\n+  }\n+\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+\n+  public RewriteDataFilesActionBase<ThisT, R> filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT, R> outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task. Increasing this\n+   * usually makes tasks a bit more even by considering more ways to pack file regions into a single task with extra\n+   * planning cost.\n+   * <p>\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT, R> splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n+   * threshold, Iceberg will use this value to do count.\n+   * <p>\n+   * this configuration controls the number of files to compact for each task, small value would lead to a high\n+   * compaction, the default value is 4MB.\n+   *\n+   * @param openFileCost minimum file size to count to pack into one \"bin\".\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT, R> splitOpenFileCost(long openFileCost) {\n+    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n+    this.splitOpenFileCost = openFileCost;\n+    return this;\n+  }\n+\n+  protected void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<DataFile> addedDataFiles) {\n+    try {\n+      RewriteFiles rewriteFiles = table.newRewrite();\n+      rewriteFiles.rewriteFiles(Sets.newHashSet(deletedDataFiles), Sets.newHashSet(addedDataFiles));\n+      rewriteFiles.commit();\n+\n+    } catch (Exception e) {\n+      Tasks.foreach(Iterables.transform(addedDataFiles, f -> f.path().toString()))\n+          .noRetry()\n+          .suppressFailureWhenFinished()\n+          .onFailure((location, exc) -> LOG.warn(\"Failed to delete: {}\", location, exc))\n+          .run(fileIO::deleteFile);\n+\n+      throw e;\n+    }\n+  }\n+\n+  protected Map<StructLikeWrapper, Collection<FileScanTask>> groupTasksByPartition(\n+      CloseableIterator<FileScanTask> tasksIter) {\n+    ListMultimap<StructLikeWrapper, FileScanTask> tasksGroupedByPartition = Multimaps.newListMultimap(\n+        Maps.newHashMap(), Lists::newArrayList);\n+\n+    try {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec067455f6a2b1b89121fcf696944376919ae683"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODI0NzQwNA==", "bodyText": "I try it\uff0cbut get a compile error, Resource references are not supported at language level '8'", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r508247404", "createdAt": "2020-10-20T06:46:33Z", "author": {"login": "zhangjun0x01"}, "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT, R> extends BaseSnapshotUpdateAction<ThisT, R> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesActionBase.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+  private PartitionSpec spec = null;\n+  private boolean caseSensitive = false;\n+\n+\n+  public RewriteDataFilesActionBase(Table table) {\n+    this.table = table;\n+    fileIO = table.io();\n+    this.filter = Expressions.alwaysTrue();\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+    this.encryptionManager = table.encryption();\n+    spec = table.spec();\n+  }\n+\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+\n+  public RewriteDataFilesActionBase<ThisT, R> filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT, R> outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task. Increasing this\n+   * usually makes tasks a bit more even by considering more ways to pack file regions into a single task with extra\n+   * planning cost.\n+   * <p>\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT, R> splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n+   * threshold, Iceberg will use this value to do count.\n+   * <p>\n+   * this configuration controls the number of files to compact for each task, small value would lead to a high\n+   * compaction, the default value is 4MB.\n+   *\n+   * @param openFileCost minimum file size to count to pack into one \"bin\".\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT, R> splitOpenFileCost(long openFileCost) {\n+    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n+    this.splitOpenFileCost = openFileCost;\n+    return this;\n+  }\n+\n+  protected void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<DataFile> addedDataFiles) {\n+    try {\n+      RewriteFiles rewriteFiles = table.newRewrite();\n+      rewriteFiles.rewriteFiles(Sets.newHashSet(deletedDataFiles), Sets.newHashSet(addedDataFiles));\n+      rewriteFiles.commit();\n+\n+    } catch (Exception e) {\n+      Tasks.foreach(Iterables.transform(addedDataFiles, f -> f.path().toString()))\n+          .noRetry()\n+          .suppressFailureWhenFinished()\n+          .onFailure((location, exc) -> LOG.warn(\"Failed to delete: {}\", location, exc))\n+          .run(fileIO::deleteFile);\n+\n+      throw e;\n+    }\n+  }\n+\n+  protected Map<StructLikeWrapper, Collection<FileScanTask>> groupTasksByPartition(\n+      CloseableIterator<FileScanTask> tasksIter) {\n+    ListMultimap<StructLikeWrapper, FileScanTask> tasksGroupedByPartition = Multimaps.newListMultimap(\n+        Maps.newHashMap(), Lists::newArrayList);\n+\n+    try {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzQyNDc0NQ=="}, "originalCommit": {"oid": "ec067455f6a2b1b89121fcf696944376919ae683"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODI1MTQ0NQ==", "bodyText": "Can we try like this ?\ntry (CloseableIterator iter = tasksIter) {\niter.forEachRemaining(task -> {\nStructLikeWrapper structLike = StructLikeWrapper.forType(spec.partitionType()).set(task.file().partition());\ntasksGroupedByPartition.put(structLike, task);\n});\n} catch (IOException e) {\nLOG.warn(\"Failed to close task iterator\", e);\n}", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r508251445", "createdAt": "2020-10-20T06:55:02Z", "author": {"login": "simonsssu"}, "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT, R> extends BaseSnapshotUpdateAction<ThisT, R> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesActionBase.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+  private PartitionSpec spec = null;\n+  private boolean caseSensitive = false;\n+\n+\n+  public RewriteDataFilesActionBase(Table table) {\n+    this.table = table;\n+    fileIO = table.io();\n+    this.filter = Expressions.alwaysTrue();\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+    this.encryptionManager = table.encryption();\n+    spec = table.spec();\n+  }\n+\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+\n+  public RewriteDataFilesActionBase<ThisT, R> filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT, R> outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task. Increasing this\n+   * usually makes tasks a bit more even by considering more ways to pack file regions into a single task with extra\n+   * planning cost.\n+   * <p>\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT, R> splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n+   * threshold, Iceberg will use this value to do count.\n+   * <p>\n+   * this configuration controls the number of files to compact for each task, small value would lead to a high\n+   * compaction, the default value is 4MB.\n+   *\n+   * @param openFileCost minimum file size to count to pack into one \"bin\".\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT, R> splitOpenFileCost(long openFileCost) {\n+    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n+    this.splitOpenFileCost = openFileCost;\n+    return this;\n+  }\n+\n+  protected void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<DataFile> addedDataFiles) {\n+    try {\n+      RewriteFiles rewriteFiles = table.newRewrite();\n+      rewriteFiles.rewriteFiles(Sets.newHashSet(deletedDataFiles), Sets.newHashSet(addedDataFiles));\n+      rewriteFiles.commit();\n+\n+    } catch (Exception e) {\n+      Tasks.foreach(Iterables.transform(addedDataFiles, f -> f.path().toString()))\n+          .noRetry()\n+          .suppressFailureWhenFinished()\n+          .onFailure((location, exc) -> LOG.warn(\"Failed to delete: {}\", location, exc))\n+          .run(fileIO::deleteFile);\n+\n+      throw e;\n+    }\n+  }\n+\n+  protected Map<StructLikeWrapper, Collection<FileScanTask>> groupTasksByPartition(\n+      CloseableIterator<FileScanTask> tasksIter) {\n+    ListMultimap<StructLikeWrapper, FileScanTask> tasksGroupedByPartition = Multimaps.newListMultimap(\n+        Maps.newHashMap(), Lists::newArrayList);\n+\n+    try {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzQyNDc0NQ=="}, "originalCommit": {"oid": "ec067455f6a2b1b89121fcf696944376919ae683"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODkzMjM0MQ==", "bodyText": "This is ok, I updated the code", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r508932341", "createdAt": "2020-10-21T01:17:59Z", "author": {"login": "zhangjun0x01"}, "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT, R> extends BaseSnapshotUpdateAction<ThisT, R> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesActionBase.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+  private PartitionSpec spec = null;\n+  private boolean caseSensitive = false;\n+\n+\n+  public RewriteDataFilesActionBase(Table table) {\n+    this.table = table;\n+    fileIO = table.io();\n+    this.filter = Expressions.alwaysTrue();\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+    this.encryptionManager = table.encryption();\n+    spec = table.spec();\n+  }\n+\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+\n+  public RewriteDataFilesActionBase<ThisT, R> filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT, R> outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task. Increasing this\n+   * usually makes tasks a bit more even by considering more ways to pack file regions into a single task with extra\n+   * planning cost.\n+   * <p>\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT, R> splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n+   * threshold, Iceberg will use this value to do count.\n+   * <p>\n+   * this configuration controls the number of files to compact for each task, small value would lead to a high\n+   * compaction, the default value is 4MB.\n+   *\n+   * @param openFileCost minimum file size to count to pack into one \"bin\".\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT, R> splitOpenFileCost(long openFileCost) {\n+    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n+    this.splitOpenFileCost = openFileCost;\n+    return this;\n+  }\n+\n+  protected void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<DataFile> addedDataFiles) {\n+    try {\n+      RewriteFiles rewriteFiles = table.newRewrite();\n+      rewriteFiles.rewriteFiles(Sets.newHashSet(deletedDataFiles), Sets.newHashSet(addedDataFiles));\n+      rewriteFiles.commit();\n+\n+    } catch (Exception e) {\n+      Tasks.foreach(Iterables.transform(addedDataFiles, f -> f.path().toString()))\n+          .noRetry()\n+          .suppressFailureWhenFinished()\n+          .onFailure((location, exc) -> LOG.warn(\"Failed to delete: {}\", location, exc))\n+          .run(fileIO::deleteFile);\n+\n+      throw e;\n+    }\n+  }\n+\n+  protected Map<StructLikeWrapper, Collection<FileScanTask>> groupTasksByPartition(\n+      CloseableIterator<FileScanTask> tasksIter) {\n+    ListMultimap<StructLikeWrapper, FileScanTask> tasksGroupedByPartition = Multimaps.newListMultimap(\n+        Maps.newHashMap(), Lists::newArrayList);\n+\n+    try {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzQyNDc0NQ=="}, "originalCommit": {"oid": "ec067455f6a2b1b89121fcf696944376919ae683"}, "originalPosition": 178}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE3OTUxMDExOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQwOToxNzo1OFrOHkFN3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQxMToxMjozMlrOHkJVnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU5NjI1Mg==", "bodyText": "I don't think we did the thing as the commit log described exactly,  because what we really need it to abstract the following codes from spark module (that means the spark rewrite action should also use this common code),  rather than introducing a totally new RewriteDataFilesActionBase class and to be used for FLINK only.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r507596252", "createdAt": "2020-10-19T09:17:58Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT, R> extends BaseSnapshotUpdateAction<ThisT, R> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ec067455f6a2b1b89121fcf696944376919ae683"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzY2Mzc3NQ==", "bodyText": "I'm very sorry, this is my mistake. I wanted to do RewriteDataFilesAction of flink and extract the common logic to iceberg-core first, and then refactor spark, but finally found that there was a problem with checkstyle, so I put some common code in iceberg-core to solve the checkstyle, forgot to merge the spark code.\nI have update the code", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r507663775", "createdAt": "2020-10-19T11:12:32Z", "author": {"login": "zhangjun0x01"}, "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT, R> extends BaseSnapshotUpdateAction<ThisT, R> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU5NjI1Mg=="}, "originalCommit": {"oid": "ec067455f6a2b1b89121fcf696944376919ae683"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MzI1MTY0OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/actions/BaseAction.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMjo1OToxNlrOHkoyXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxNDoxNjowMlrOHo-ftw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE3OTAzOQ==", "bodyText": "Do we need this for flink ?", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r508179039", "createdAt": "2020-10-20T02:59:16Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+abstract class BaseAction<R> implements Action<R> {\n+\n+  protected abstract Table table();\n+\n+  protected String metadataTableName(MetadataTableType type) {\n+    return metadataTableName(table().name(), type);\n+  }\n+\n+  protected String metadataTableName(String tableName, MetadataTableType type) {\n+    if (tableName.contains(\"/\")) {\n+      return tableName + \"#\" + type;\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // for HadoopCatalog tables, use the table location to load the metadata table\n+      // because IcebergCatalog uses HiveCatalog when the table is identified by name\n+      return table().location() + \"#\" + type;\n+    } else if (tableName.startsWith(\"hive.\")) {\n+      // HiveCatalog prepend a logical name which we need to drop for Spark 2.4\n+      return tableName.replaceFirst(\"hive\\\\.\", \"\") + \".\" + type;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d3a6ba4c797e8fe4a4ccc8907d79904456bfa4a3"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODI2NzY5NA==", "bodyText": "Flink is not used for the time being. I think this method is used for metadata, so I also extracted it. Currently we have not done RewriteManifestsAction and ExpireSnapshotsAction for flink. I think if we do it in the future, it should be useful", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r508267694", "createdAt": "2020-10-20T07:25:24Z", "author": {"login": "zhangjun0x01"}, "path": "core/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+abstract class BaseAction<R> implements Action<R> {\n+\n+  protected abstract Table table();\n+\n+  protected String metadataTableName(MetadataTableType type) {\n+    return metadataTableName(table().name(), type);\n+  }\n+\n+  protected String metadataTableName(String tableName, MetadataTableType type) {\n+    if (tableName.contains(\"/\")) {\n+      return tableName + \"#\" + type;\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // for HadoopCatalog tables, use the table location to load the metadata table\n+      // because IcebergCatalog uses HiveCatalog when the table is identified by name\n+      return table().location() + \"#\" + type;\n+    } else if (tableName.startsWith(\"hive.\")) {\n+      // HiveCatalog prepend a logical name which we need to drop for Spark 2.4\n+      return tableName.replaceFirst(\"hive\\\\.\", \"\") + \".\" + type;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE3OTAzOQ=="}, "originalCommit": {"oid": "d3a6ba4c797e8fe4a4ccc8907d79904456bfa4a3"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk3MjgyMg==", "bodyText": "Yeah,  make sense to move them here for RewriteManifestsAction and ExpireSnapshotsAction.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r508972822", "createdAt": "2020-10-21T03:49:54Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+abstract class BaseAction<R> implements Action<R> {\n+\n+  protected abstract Table table();\n+\n+  protected String metadataTableName(MetadataTableType type) {\n+    return metadataTableName(table().name(), type);\n+  }\n+\n+  protected String metadataTableName(String tableName, MetadataTableType type) {\n+    if (tableName.contains(\"/\")) {\n+      return tableName + \"#\" + type;\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // for HadoopCatalog tables, use the table location to load the metadata table\n+      // because IcebergCatalog uses HiveCatalog when the table is identified by name\n+      return table().location() + \"#\" + type;\n+    } else if (tableName.startsWith(\"hive.\")) {\n+      // HiveCatalog prepend a logical name which we need to drop for Spark 2.4\n+      return tableName.replaceFirst(\"hive\\\\.\", \"\") + \".\" + type;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE3OTAzOQ=="}, "originalCommit": {"oid": "d3a6ba4c797e8fe4a4ccc8907d79904456bfa4a3"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjcyOTAxNQ==", "bodyText": "This I think may end up having to be split up for Spark and Flink once we start having to deal with multiple catalogs in Spark. I think this extraction is still ok because we can extend and override this method. But I think this method is going to end up being implementation specific based on what the parsers can handle.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r512729015", "createdAt": "2020-10-27T14:16:02Z", "author": {"login": "RussellSpitzer"}, "path": "core/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+abstract class BaseAction<R> implements Action<R> {\n+\n+  protected abstract Table table();\n+\n+  protected String metadataTableName(MetadataTableType type) {\n+    return metadataTableName(table().name(), type);\n+  }\n+\n+  protected String metadataTableName(String tableName, MetadataTableType type) {\n+    if (tableName.contains(\"/\")) {\n+      return tableName + \"#\" + type;\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // for HadoopCatalog tables, use the table location to load the metadata table\n+      // because IcebergCatalog uses HiveCatalog when the table is identified by name\n+      return table().location() + \"#\" + type;\n+    } else if (tableName.startsWith(\"hive.\")) {\n+      // HiveCatalog prepend a logical name which we need to drop for Spark 2.4\n+      return tableName.replaceFirst(\"hive\\\\.\", \"\") + \".\" + type;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE3OTAzOQ=="}, "originalCommit": {"oid": "d3a6ba4c797e8fe4a4ccc8907d79904456bfa4a3"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MzI4NDg1OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMzoxNjo1NVrOHkpFkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMzoxNjo1NVrOHkpFkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE4Mzk1Mw==", "bodyText": "We usually don't use getTable in iceberg code because the prefix get does not have much meaning. It's good to use protected Table table() directly here.\nBesides, I think we could just move the protected Table table() to the parent class, don't have to override it here then.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r508183953", "createdAt": "2020-10-20T03:16:55Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -112,170 +54,35 @@ protected RewriteDataFilesAction self() {\n \n   @Override\n   protected Table table() {\n-    return table;\n-  }\n-\n-  /**\n-   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n-   *\n-   * @param specId PartitionSpec id to rewrite\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction outputSpecId(int specId) {\n-    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n-    this.spec = table.specs().get(specId);\n-    return this;\n+    return getTable();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d3a6ba4c797e8fe4a4ccc8907d79904456bfa4a3"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MzMwMjQ1OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMzoyNDoyNFrOHkpO8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwNjoyNTowOFrOHksXnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE4NjM1Mw==", "bodyText": "I'd prefer to intergate all the line63~line71 to a protected method named planTasks :\n  /**\n   * Plan the {@link CombinedScanTask tasks} for this scan.\n   * <p>\n   * Tasks created by this method may read partial input files, multiple input files, or both.\n   *\n   * @return an Iterable of tasks for this scan\n   */\n  protected CloseableIterable<CombinedScanTask> planTasks(); \nThat's similar to the planTasks method in TableScan.java.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r508186353", "createdAt": "2020-10-20T03:24:24Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -112,170 +54,35 @@ protected RewriteDataFilesAction self() {\n \n   @Override\n   protected Table table() {\n-    return table;\n-  }\n-\n-  /**\n-   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n-   *\n-   * @param specId PartitionSpec id to rewrite\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction outputSpecId(int specId) {\n-    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n-    this.spec = table.specs().get(specId);\n-    return this;\n+    return getTable();\n   }\n \n-  /**\n-   * Specify the target rewrite data file size in bytes\n-   *\n-   * @param targetSize size in bytes of rewrite data file\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n-    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n-        targetSize);\n-    this.targetSizeInBytes = targetSize;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n-   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n-   * task with extra planning cost.\n-   * <p>\n-   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n-   * metadata, user can use a lookback of 1.\n-   *\n-   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitLookback(int lookback) {\n-    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n-    this.splitLookback = lookback;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n-   * threshold, Iceberg will use this value to do count.\n-   * <p>\n-   * this configuration controls the number of files to compact for each task, small value would lead to a\n-   * high compaction, the default value is 4MB.\n-   *\n-   * @param openFileCost minimum file size to count to pack into one \"bin\".\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitOpenFileCost(long openFileCost) {\n-    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n-    this.splitOpenFileCost = openFileCost;\n-    return this;\n-  }\n-\n-  /**\n-   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n-   * filter may be rewritten.\n-   *\n-   * @param expr Expression to filter out DataFiles\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction filter(Expression expr) {\n-    this.filter = Expressions.and(filter, expr);\n-    return this;\n-  }\n \n   @Override\n   public RewriteDataFilesActionResult execute() {\n-    CloseableIterable<FileScanTask> fileScanTasks = null;\n-    try {\n-      fileScanTasks = table.newScan()\n-          .caseSensitive(caseSensitive)\n-          .ignoreResiduals()\n-          .filter(filter)\n-          .planFiles();\n-    } finally {\n-      try {\n-        if (fileScanTasks != null) {\n-          fileScanTasks.close();\n-        }\n-      } catch (IOException ioe) {\n-        LOG.warn(\"Failed to close task iterable\", ioe);\n-      }\n-    }\n-\n-    Map<StructLikeWrapper, Collection<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n-    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n-        .filter(kv -> kv.getValue().size() > 1)\n-        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = getFilteredGroupedTasks();\n \n     // Nothing to rewrite if there's only one DataFile in each partition.\n     if (filteredGroupedTasks.isEmpty()) {\n       return RewriteDataFilesActionResult.empty();\n     }\n \n     // Split and combine tasks under each partition\n-    List<CombinedScanTask> combinedScanTasks = filteredGroupedTasks.values().stream()\n-        .map(scanTasks -> {\n-          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n-              CloseableIterable.withNoopClose(scanTasks), targetSizeInBytes);\n-          return TableScanUtil.planTasks(splitTasks, targetSizeInBytes, splitLookback, splitOpenFileCost);\n-        })\n-        .flatMap(Streams::stream)\n-        .collect(Collectors.toList());\n+    List<CombinedScanTask> combinedScanTasks = getCombinedScanTasks(filteredGroupedTasks);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d3a6ba4c797e8fe4a4ccc8907d79904456bfa4a3"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODIzNzcyNw==", "bodyText": "I did not extract into a method, because the following getCurrentDataFiles also used filteredGroupedTasks, if extracted into a method, then the groupTasksByPartition method will be executed twice", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r508237727", "createdAt": "2020-10-20T06:25:08Z", "author": {"login": "zhangjun0x01"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -112,170 +54,35 @@ protected RewriteDataFilesAction self() {\n \n   @Override\n   protected Table table() {\n-    return table;\n-  }\n-\n-  /**\n-   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n-   *\n-   * @param specId PartitionSpec id to rewrite\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction outputSpecId(int specId) {\n-    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n-    this.spec = table.specs().get(specId);\n-    return this;\n+    return getTable();\n   }\n \n-  /**\n-   * Specify the target rewrite data file size in bytes\n-   *\n-   * @param targetSize size in bytes of rewrite data file\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n-    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n-        targetSize);\n-    this.targetSizeInBytes = targetSize;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n-   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n-   * task with extra planning cost.\n-   * <p>\n-   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n-   * metadata, user can use a lookback of 1.\n-   *\n-   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitLookback(int lookback) {\n-    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n-    this.splitLookback = lookback;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n-   * threshold, Iceberg will use this value to do count.\n-   * <p>\n-   * this configuration controls the number of files to compact for each task, small value would lead to a\n-   * high compaction, the default value is 4MB.\n-   *\n-   * @param openFileCost minimum file size to count to pack into one \"bin\".\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitOpenFileCost(long openFileCost) {\n-    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n-    this.splitOpenFileCost = openFileCost;\n-    return this;\n-  }\n-\n-  /**\n-   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n-   * filter may be rewritten.\n-   *\n-   * @param expr Expression to filter out DataFiles\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction filter(Expression expr) {\n-    this.filter = Expressions.and(filter, expr);\n-    return this;\n-  }\n \n   @Override\n   public RewriteDataFilesActionResult execute() {\n-    CloseableIterable<FileScanTask> fileScanTasks = null;\n-    try {\n-      fileScanTasks = table.newScan()\n-          .caseSensitive(caseSensitive)\n-          .ignoreResiduals()\n-          .filter(filter)\n-          .planFiles();\n-    } finally {\n-      try {\n-        if (fileScanTasks != null) {\n-          fileScanTasks.close();\n-        }\n-      } catch (IOException ioe) {\n-        LOG.warn(\"Failed to close task iterable\", ioe);\n-      }\n-    }\n-\n-    Map<StructLikeWrapper, Collection<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n-    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n-        .filter(kv -> kv.getValue().size() > 1)\n-        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = getFilteredGroupedTasks();\n \n     // Nothing to rewrite if there's only one DataFile in each partition.\n     if (filteredGroupedTasks.isEmpty()) {\n       return RewriteDataFilesActionResult.empty();\n     }\n \n     // Split and combine tasks under each partition\n-    List<CombinedScanTask> combinedScanTasks = filteredGroupedTasks.values().stream()\n-        .map(scanTasks -> {\n-          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n-              CloseableIterable.withNoopClose(scanTasks), targetSizeInBytes);\n-          return TableScanUtil.planTasks(splitTasks, targetSizeInBytes, splitLookback, splitOpenFileCost);\n-        })\n-        .flatMap(Streams::stream)\n-        .collect(Collectors.toList());\n+    List<CombinedScanTask> combinedScanTasks = getCombinedScanTasks(filteredGroupedTasks);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE4NjM1Mw=="}, "originalCommit": {"oid": "d3a6ba4c797e8fe4a4ccc8907d79904456bfa4a3"}, "originalPosition": 211}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MzMzMTI4OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMzo0MTozNFrOHkpfjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMzo0MTozNFrOHkpfjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE5MDYwNg==", "bodyText": "In iceberg, we usually use this.fileIO=table.io to assign the argument to an internal member in constructor.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r508190606", "createdAt": "2020-10-20T03:41:34Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,277 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT, R> extends BaseSnapshotUpdateAction<ThisT, R> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesActionBase.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+  private PartitionSpec spec = null;\n+  private boolean caseSensitive;\n+\n+  public RewriteDataFilesActionBase(Table table) {\n+    this.table = table;\n+    fileIO = table.io();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d3a6ba4c797e8fe4a4ccc8907d79904456bfa4a3"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MzMzMjA5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMzo0MjowMFrOHkpf-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMFQwMzo0MjowMFrOHkpf-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODE5MDcxMw==", "bodyText": "ditto.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r508190713", "createdAt": "2020-10-20T03:42:00Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,277 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT, R> extends BaseSnapshotUpdateAction<ThisT, R> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesActionBase.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+  private PartitionSpec spec = null;\n+  private boolean caseSensitive;\n+\n+  public RewriteDataFilesActionBase(Table table) {\n+    this.table = table;\n+    fileIO = table.io();\n+    this.filter = Expressions.alwaysTrue();\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+    this.encryptionManager = table.encryption();\n+    spec = table.spec();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d3a6ba4c797e8fe4a4ccc8907d79904456bfa4a3"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4ODMwMDkxOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwMzo0NDo0NlrOHlZKUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQwOToyOToxN1rOHli1TA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk3MTYwMg==", "bodyText": "For my understanding,  this is only the different part between flink, spark, or other engines.  How about introducing  an abstracted method named List<DataFile> rewriteDataForTasks(List<CombinedScanTask> combinedScanTask, .. )  in this base rewrite action class.\nThen both spark and flink only need to implement this rewriteDataForTasks method,  that will make the code really simpler.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r508971602", "createdAt": "2020-10-21T03:44:46Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -19,263 +19,64 @@\n \n package org.apache.iceberg.actions;\n \n-import java.io.IOException;\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n-import java.util.stream.Collectors;\n import org.apache.iceberg.CombinedScanTask;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.FileScanTask;\n-import org.apache.iceberg.PartitionSpec;\n-import org.apache.iceberg.RewriteFiles;\n import org.apache.iceberg.Table;\n-import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.encryption.EncryptionManager;\n-import org.apache.iceberg.expressions.Expression;\n-import org.apache.iceberg.expressions.Expressions;\n-import org.apache.iceberg.io.CloseableIterable;\n-import org.apache.iceberg.io.CloseableIterator;\n import org.apache.iceberg.io.FileIO;\n-import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n-import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n-import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n-import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n-import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n import org.apache.iceberg.spark.SparkUtil;\n import org.apache.iceberg.spark.source.RowDataRewriter;\n-import org.apache.iceberg.util.PropertyUtil;\n import org.apache.iceberg.util.StructLikeWrapper;\n-import org.apache.iceberg.util.TableScanUtil;\n-import org.apache.iceberg.util.Tasks;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n import org.apache.spark.broadcast.Broadcast;\n import org.apache.spark.sql.SparkSession;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n public class RewriteDataFilesAction\n-    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+    extends RewriteDataFilesActionBase<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n \n   private final JavaSparkContext sparkContext;\n-  private final Table table;\n-  private final FileIO fileIO;\n-  private final EncryptionManager encryptionManager;\n-  private final boolean caseSensitive;\n-  private long targetSizeInBytes;\n-  private int splitLookback;\n-  private long splitOpenFileCost;\n-\n-  private PartitionSpec spec = null;\n-  private Expression filter;\n \n   RewriteDataFilesAction(SparkSession spark, Table table) {\n+    super(table);\n     this.sparkContext = new JavaSparkContext(spark.sparkContext());\n-    this.table = table;\n-    this.spec = table.spec();\n-    this.filter = Expressions.alwaysTrue();\n-    this.caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n-\n-    long splitSize = PropertyUtil.propertyAsLong(\n-        table.properties(),\n-        TableProperties.SPLIT_SIZE,\n-        TableProperties.SPLIT_SIZE_DEFAULT);\n-    long targetFileSize = PropertyUtil.propertyAsLong(\n-        table.properties(),\n-        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n-        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n-    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n-\n-    this.splitLookback = PropertyUtil.propertyAsInt(\n-        table.properties(),\n-        TableProperties.SPLIT_LOOKBACK,\n-        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n-    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n-        table.properties(),\n-        TableProperties.SPLIT_OPEN_FILE_COST,\n-        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n-\n-    this.fileIO = SparkUtil.serializableFileIO(table);\n-    this.encryptionManager = table.encryption();\n+    this.setCaseSensitive(Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\")));\n   }\n \n   @Override\n   protected RewriteDataFilesAction self() {\n     return this;\n   }\n \n-  @Override\n-  protected Table table() {\n-    return table;\n-  }\n-\n-  /**\n-   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n-   *\n-   * @param specId PartitionSpec id to rewrite\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction outputSpecId(int specId) {\n-    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n-    this.spec = table.specs().get(specId);\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the target rewrite data file size in bytes\n-   *\n-   * @param targetSize size in bytes of rewrite data file\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n-    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n-        targetSize);\n-    this.targetSizeInBytes = targetSize;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n-   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n-   * task with extra planning cost.\n-   * <p>\n-   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n-   * metadata, user can use a lookback of 1.\n-   *\n-   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitLookback(int lookback) {\n-    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n-    this.splitLookback = lookback;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n-   * threshold, Iceberg will use this value to do count.\n-   * <p>\n-   * this configuration controls the number of files to compact for each task, small value would lead to a\n-   * high compaction, the default value is 4MB.\n-   *\n-   * @param openFileCost minimum file size to count to pack into one \"bin\".\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitOpenFileCost(long openFileCost) {\n-    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n-    this.splitOpenFileCost = openFileCost;\n-    return this;\n-  }\n-\n-  /**\n-   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n-   * filter may be rewritten.\n-   *\n-   * @param expr Expression to filter out DataFiles\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction filter(Expression expr) {\n-    this.filter = Expressions.and(filter, expr);\n-    return this;\n-  }\n-\n   @Override\n   public RewriteDataFilesActionResult execute() {\n-    CloseableIterable<FileScanTask> fileScanTasks = null;\n-    try {\n-      fileScanTasks = table.newScan()\n-          .caseSensitive(caseSensitive)\n-          .ignoreResiduals()\n-          .filter(filter)\n-          .planFiles();\n-    } finally {\n-      try {\n-        if (fileScanTasks != null) {\n-          fileScanTasks.close();\n-        }\n-      } catch (IOException ioe) {\n-        LOG.warn(\"Failed to close task iterable\", ioe);\n-      }\n-    }\n-\n-    Map<StructLikeWrapper, Collection<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n-    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n-        .filter(kv -> kv.getValue().size() > 1)\n-        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = getFilteredGroupedTasks();\n \n     // Nothing to rewrite if there's only one DataFile in each partition.\n     if (filteredGroupedTasks.isEmpty()) {\n       return RewriteDataFilesActionResult.empty();\n     }\n \n     // Split and combine tasks under each partition\n-    List<CombinedScanTask> combinedScanTasks = filteredGroupedTasks.values().stream()\n-        .map(scanTasks -> {\n-          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n-              CloseableIterable.withNoopClose(scanTasks), targetSizeInBytes);\n-          return TableScanUtil.planTasks(splitTasks, targetSizeInBytes, splitLookback, splitOpenFileCost);\n-        })\n-        .flatMap(Streams::stream)\n-        .collect(Collectors.toList());\n+    List<CombinedScanTask> combinedScanTasks = getCombinedScanTasks(filteredGroupedTasks);\n \n     JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(combinedScanTasks, combinedScanTasks.size());\n \n-    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n-    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n+    Broadcast<FileIO> io = sparkContext.broadcast(SparkUtil.serializableFileIO(this.table()));\n+    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(this.table().encryption());\n \n-    RowDataRewriter rowDataRewriter = new RowDataRewriter(table, spec, caseSensitive, io, encryption);\n+    RowDataRewriter rowDataRewriter =\n+        new RowDataRewriter(this.table(), this.table().spec(), isCaseSensitive(), io, encryption);\n \n     List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(taskRDD);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df758e073d3cbd8ae11c077c1a0dbeaae3e715dd"}, "originalPosition": 225}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTEzMDA2MA==", "bodyText": "thanks for your suggestion,I extract execute method to RewriteDataFilesActionBase, and add a abstract method rewriteDataForTasks in RewriteDataFilesActionBase.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r509130060", "createdAt": "2020-10-21T09:29:17Z", "author": {"login": "zhangjun0x01"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -19,263 +19,64 @@\n \n package org.apache.iceberg.actions;\n \n-import java.io.IOException;\n import java.util.Collection;\n import java.util.List;\n import java.util.Map;\n-import java.util.stream.Collectors;\n import org.apache.iceberg.CombinedScanTask;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.FileScanTask;\n-import org.apache.iceberg.PartitionSpec;\n-import org.apache.iceberg.RewriteFiles;\n import org.apache.iceberg.Table;\n-import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.encryption.EncryptionManager;\n-import org.apache.iceberg.expressions.Expression;\n-import org.apache.iceberg.expressions.Expressions;\n-import org.apache.iceberg.io.CloseableIterable;\n-import org.apache.iceberg.io.CloseableIterator;\n import org.apache.iceberg.io.FileIO;\n-import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n-import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n-import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n-import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n-import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n import org.apache.iceberg.spark.SparkUtil;\n import org.apache.iceberg.spark.source.RowDataRewriter;\n-import org.apache.iceberg.util.PropertyUtil;\n import org.apache.iceberg.util.StructLikeWrapper;\n-import org.apache.iceberg.util.TableScanUtil;\n-import org.apache.iceberg.util.Tasks;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.api.java.JavaSparkContext;\n import org.apache.spark.broadcast.Broadcast;\n import org.apache.spark.sql.SparkSession;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n public class RewriteDataFilesAction\n-    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+    extends RewriteDataFilesActionBase<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n \n   private final JavaSparkContext sparkContext;\n-  private final Table table;\n-  private final FileIO fileIO;\n-  private final EncryptionManager encryptionManager;\n-  private final boolean caseSensitive;\n-  private long targetSizeInBytes;\n-  private int splitLookback;\n-  private long splitOpenFileCost;\n-\n-  private PartitionSpec spec = null;\n-  private Expression filter;\n \n   RewriteDataFilesAction(SparkSession spark, Table table) {\n+    super(table);\n     this.sparkContext = new JavaSparkContext(spark.sparkContext());\n-    this.table = table;\n-    this.spec = table.spec();\n-    this.filter = Expressions.alwaysTrue();\n-    this.caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n-\n-    long splitSize = PropertyUtil.propertyAsLong(\n-        table.properties(),\n-        TableProperties.SPLIT_SIZE,\n-        TableProperties.SPLIT_SIZE_DEFAULT);\n-    long targetFileSize = PropertyUtil.propertyAsLong(\n-        table.properties(),\n-        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n-        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n-    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n-\n-    this.splitLookback = PropertyUtil.propertyAsInt(\n-        table.properties(),\n-        TableProperties.SPLIT_LOOKBACK,\n-        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n-    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n-        table.properties(),\n-        TableProperties.SPLIT_OPEN_FILE_COST,\n-        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n-\n-    this.fileIO = SparkUtil.serializableFileIO(table);\n-    this.encryptionManager = table.encryption();\n+    this.setCaseSensitive(Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\")));\n   }\n \n   @Override\n   protected RewriteDataFilesAction self() {\n     return this;\n   }\n \n-  @Override\n-  protected Table table() {\n-    return table;\n-  }\n-\n-  /**\n-   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n-   *\n-   * @param specId PartitionSpec id to rewrite\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction outputSpecId(int specId) {\n-    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n-    this.spec = table.specs().get(specId);\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the target rewrite data file size in bytes\n-   *\n-   * @param targetSize size in bytes of rewrite data file\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n-    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n-        targetSize);\n-    this.targetSizeInBytes = targetSize;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n-   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n-   * task with extra planning cost.\n-   * <p>\n-   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n-   * metadata, user can use a lookback of 1.\n-   *\n-   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitLookback(int lookback) {\n-    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n-    this.splitLookback = lookback;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n-   * threshold, Iceberg will use this value to do count.\n-   * <p>\n-   * this configuration controls the number of files to compact for each task, small value would lead to a\n-   * high compaction, the default value is 4MB.\n-   *\n-   * @param openFileCost minimum file size to count to pack into one \"bin\".\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitOpenFileCost(long openFileCost) {\n-    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n-    this.splitOpenFileCost = openFileCost;\n-    return this;\n-  }\n-\n-  /**\n-   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n-   * filter may be rewritten.\n-   *\n-   * @param expr Expression to filter out DataFiles\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction filter(Expression expr) {\n-    this.filter = Expressions.and(filter, expr);\n-    return this;\n-  }\n-\n   @Override\n   public RewriteDataFilesActionResult execute() {\n-    CloseableIterable<FileScanTask> fileScanTasks = null;\n-    try {\n-      fileScanTasks = table.newScan()\n-          .caseSensitive(caseSensitive)\n-          .ignoreResiduals()\n-          .filter(filter)\n-          .planFiles();\n-    } finally {\n-      try {\n-        if (fileScanTasks != null) {\n-          fileScanTasks.close();\n-        }\n-      } catch (IOException ioe) {\n-        LOG.warn(\"Failed to close task iterable\", ioe);\n-      }\n-    }\n-\n-    Map<StructLikeWrapper, Collection<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n-    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n-        .filter(kv -> kv.getValue().size() > 1)\n-        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = getFilteredGroupedTasks();\n \n     // Nothing to rewrite if there's only one DataFile in each partition.\n     if (filteredGroupedTasks.isEmpty()) {\n       return RewriteDataFilesActionResult.empty();\n     }\n \n     // Split and combine tasks under each partition\n-    List<CombinedScanTask> combinedScanTasks = filteredGroupedTasks.values().stream()\n-        .map(scanTasks -> {\n-          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n-              CloseableIterable.withNoopClose(scanTasks), targetSizeInBytes);\n-          return TableScanUtil.planTasks(splitTasks, targetSizeInBytes, splitLookback, splitOpenFileCost);\n-        })\n-        .flatMap(Streams::stream)\n-        .collect(Collectors.toList());\n+    List<CombinedScanTask> combinedScanTasks = getCombinedScanTasks(filteredGroupedTasks);\n \n     JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(combinedScanTasks, combinedScanTasks.size());\n \n-    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n-    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n+    Broadcast<FileIO> io = sparkContext.broadcast(SparkUtil.serializableFileIO(this.table()));\n+    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(this.table().encryption());\n \n-    RowDataRewriter rowDataRewriter = new RowDataRewriter(table, spec, caseSensitive, io, encryption);\n+    RowDataRewriter rowDataRewriter =\n+        new RowDataRewriter(this.table(), this.table().spec(), isCaseSensitive(), io, encryption);\n \n     List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(taskRDD);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODk3MTYwMg=="}, "originalCommit": {"oid": "df758e073d3cbd8ae11c077c1a0dbeaae3e715dd"}, "originalPosition": 225}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NDEzOTI3OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwNjozODowNVrOHmSnNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwODo0MDoxN1rOHmW4WQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTkxMjg4NQ==", "bodyText": "nit:  we usually name it as BaseRewriteDataFilesAction  if it's an abstract class.  btw, seems we don't have to break extends .. into a new line because it does not exceed the max length .", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r509912885", "createdAt": "2020-10-22T06:38:05Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cb731b0866e7a5654c89edf91f824d65407951de"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTk4MjgwOQ==", "bodyText": "I use checkstyle to check, the number of characters in this line is 127, which is greater than the max of 120", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r509982809", "createdAt": "2020-10-22T08:40:17Z", "author": {"login": "zhangjun0x01"}, "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTkxMjg4NQ=="}, "originalCommit": {"oid": "cb731b0866e7a5654c89edf91f824d65407951de"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NDE1MDA4OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwNjo0MTo1NVrOHmStfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwNjo0MTo1NVrOHmStfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTkxNDQ5Mw==", "bodyText": "nit:   rename the isCaseSensitive to caseSensitive pls. for example, we ManifestGroup has the method :\n  ManifestGroup caseSensitive(boolean newCaseSensitive) {\n    this.caseSensitive = newCaseSensitive;\n    deleteIndexBuilder.caseSensitive(newCaseSensitive);\n    return this;\n  }", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r509914493", "createdAt": "2020-10-22T06:41:55Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT>\n+    extends BaseSnapshotUpdateAction<ThisT, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesActionBase.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private PartitionSpec spec;\n+  private boolean caseSensitive;\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+  private long rewriteScanLimit;\n+\n+  public RewriteDataFilesActionBase(Table table) {\n+    this.table = table;\n+    this.fileIO = table.io();\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+    this.rewriteScanLimit = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.REWRITE_SCAN_LIMIT,\n+        TableProperties.REWRITE_SCAN_LIMIT_DEFAULT);\n+  }\n+\n+  protected void setCaseSensitive(boolean caseSensitive) {\n+    this.caseSensitive = caseSensitive;\n+  }\n+\n+  protected boolean isCaseSensitive() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cb731b0866e7a5654c89edf91f824d65407951de"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NDE1NjY4OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwNjo0NDoxMlrOHmSxZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwODoyMzozOFrOHmWNbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTkxNTQ5Mw==", "bodyText": "This is a refactor PR while introducing the rewriteScanLimit seems to be a new feature,  we'd better not mix the refactor & new feature development in a single PR.  It's helpful for reviewing and providing full unit tests if we really need the rewriteScanLimit.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r509915493", "createdAt": "2020-10-22T06:44:12Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT>\n+    extends BaseSnapshotUpdateAction<ThisT, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesActionBase.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private PartitionSpec spec;\n+  private boolean caseSensitive;\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+  private long rewriteScanLimit;\n+\n+  public RewriteDataFilesActionBase(Table table) {\n+    this.table = table;\n+    this.fileIO = table.io();\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+    this.rewriteScanLimit = PropertyUtil.propertyAsLong(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cb731b0866e7a5654c89edf91f824d65407951de"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTk3MTgyMQ==", "bodyText": "I will rollback it, and I will open a new PR later if necessary", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r509971821", "createdAt": "2020-10-22T08:23:38Z", "author": {"login": "zhangjun0x01"}, "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT>\n+    extends BaseSnapshotUpdateAction<ThisT, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesActionBase.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private PartitionSpec spec;\n+  private boolean caseSensitive;\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+  private long rewriteScanLimit;\n+\n+  public RewriteDataFilesActionBase(Table table) {\n+    this.table = table;\n+    this.fileIO = table.io();\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+    this.rewriteScanLimit = PropertyUtil.propertyAsLong(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTkxNTQ5Mw=="}, "originalCommit": {"oid": "cb731b0866e7a5654c89edf91f824d65407951de"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NDE4NjI5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwNjo1MzoxOFrOHmTCdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwNjo1MzoxOFrOHmTCdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTkxOTg2MQ==", "bodyText": "Is it necessary to move the split & combine parts into a separate method ?  I think the original codes is more clear and we don't have to change it,  it break the steps into several code blocks and each block represent one step.  Pls keep the code as it is if we don't have a strong reason to change it.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r509919861", "createdAt": "2020-10-22T06:53:18Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionBase.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class RewriteDataFilesActionBase<ThisT>\n+    extends BaseSnapshotUpdateAction<ThisT, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesActionBase.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private PartitionSpec spec;\n+  private boolean caseSensitive;\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+  private long rewriteScanLimit;\n+\n+  public RewriteDataFilesActionBase(Table table) {\n+    this.table = table;\n+    this.fileIO = table.io();\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+    this.rewriteScanLimit = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.REWRITE_SCAN_LIMIT,\n+        TableProperties.REWRITE_SCAN_LIMIT_DEFAULT);\n+  }\n+\n+  protected void setCaseSensitive(boolean caseSensitive) {\n+    this.caseSensitive = caseSensitive;\n+  }\n+\n+  protected boolean isCaseSensitive() {\n+    return caseSensitive;\n+  }\n+\n+  /**\n+   * Set the size of the scanned file. If the file size is greater than this value, it will not be scanned and will not\n+   * be compressed.\n+   *\n+   * @param limitSize the limit size of the scanned file ,default is 100M\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT> rewriteScanLimit(long limitSize) {\n+    Preconditions.checkArgument(limitSize > 0L, \"Invalid rewriteScanLimit size .\");\n+    this.rewriteScanLimit = limitSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+\n+  public RewriteDataFilesActionBase<ThisT> filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT> outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task. Increasing this\n+   * usually makes tasks a bit more even by considering more ways to pack file regions into a single task with extra\n+   * planning cost.\n+   * <p>\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT> splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n+   * threshold, Iceberg will use this value to do count.\n+   * <p>\n+   * this configuration controls the number of files to compact for each task, small value would lead to a high\n+   * compaction, the default value is 4MB.\n+   *\n+   * @param openFileCost minimum file size to count to pack into one \"bin\".\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT> splitOpenFileCost(long openFileCost) {\n+    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n+    this.splitOpenFileCost = openFileCost;\n+    return this;\n+  }\n+\n+\n+  /**\n+   * Specify the target rewrite data file size in bytes\n+   *\n+   * @param targetSize size in bytes of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesActionBase<ThisT> targetSizeInBytes(long targetSize) {\n+    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n+        targetSize);\n+    this.targetSizeInBytes = targetSize;\n+    return this;\n+  }\n+\n+\n+  private void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<DataFile> addedDataFiles) {\n+    try {\n+      RewriteFiles rewriteFiles = table.newRewrite();\n+      rewriteFiles.rewriteFiles(Sets.newHashSet(deletedDataFiles), Sets.newHashSet(addedDataFiles));\n+      commit(rewriteFiles);\n+    } catch (Exception e) {\n+      Tasks.foreach(Iterables.transform(addedDataFiles, f -> f.path().toString()))\n+          .noRetry()\n+          .suppressFailureWhenFinished()\n+          .onFailure((location, exc) -> LOG.warn(\"Failed to delete: {}\", location, exc))\n+          .run(fileIO::deleteFile);\n+      throw e;\n+    }\n+  }\n+\n+  private Map<StructLikeWrapper, Collection<FileScanTask>> groupTasksByPartition(\n+      CloseableIterator<FileScanTask> tasksIter) {\n+    ListMultimap<StructLikeWrapper, FileScanTask> tasksGroupedByPartition = Multimaps.newListMultimap(\n+        Maps.newHashMap(), Lists::newArrayList);\n+    try (CloseableIterator<FileScanTask> iterator = tasksIter) {\n+      iterator.forEachRemaining(task -> {\n+        if (task.length() < rewriteScanLimit) {\n+          StructLikeWrapper structLike = StructLikeWrapper.forType(spec.partitionType()).set(task.file().partition());\n+          tasksGroupedByPartition.put(structLike, task);\n+        }\n+      });\n+    } catch (IOException e) {\n+      LOG.warn(\"Failed to close task iterator\", e);\n+    }\n+    return tasksGroupedByPartition.asMap();\n+  }\n+\n+  private List<CombinedScanTask> getCombinedScanTasks(\n+      Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks) {\n+    // Split and combine tasks under each partition\n+    List<CombinedScanTask> combinedScanTasks = filteredGroupedTasks.values().stream()\n+        .map(scanTasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n+              CloseableIterable.withNoopClose(scanTasks), targetSizeInBytes);\n+          return TableScanUtil.planTasks(splitTasks, targetSizeInBytes, splitLookback, splitOpenFileCost);\n+        })\n+        .flatMap(Streams::stream)\n+        .collect(Collectors.toList());\n+    return combinedScanTasks;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = null;\n+    try {\n+      fileScanTasks = table.newScan()\n+          .caseSensitive(caseSensitive)\n+          .ignoreResiduals()\n+          .filter(filter)\n+          .planFiles();\n+    } finally {\n+      try {\n+        if (fileScanTasks != null) {\n+          fileScanTasks.close();\n+        }\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Failed to close task iterable\", ioe);\n+      }\n+    }\n+\n+    Map<StructLikeWrapper, Collection<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n+        .filter(kv -> kv.getValue().size() > 1)\n+        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+    // Split and combine tasks under each partition\n+    List<CombinedScanTask> combinedScanTasks = getCombinedScanTasks(filteredGroupedTasks);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cb731b0866e7a5654c89edf91f824d65407951de"}, "originalPosition": 266}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5NDE5ODk4OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwNjo1NzozMFrOHmTKUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMlQwNjo1NzozMFrOHmTKUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTkyMTg3NA==", "bodyText": "nit:  we usually use this to assign value to a local field, so that we could distinguish it's a local field member assignment or normal field assignment.  If call the private or protected methods, we don't use this.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r509921874", "createdAt": "2020-10-22T06:57:30Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -111,171 +49,12 @@ protected RewriteDataFilesAction self() {\n   }\n \n   @Override\n-  protected Table table() {\n-    return table;\n-  }\n-\n-  /**\n-   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n-   *\n-   * @param specId PartitionSpec id to rewrite\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction outputSpecId(int specId) {\n-    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n-    this.spec = table.specs().get(specId);\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the target rewrite data file size in bytes\n-   *\n-   * @param targetSize size in bytes of rewrite data file\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n-    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n-        targetSize);\n-    this.targetSizeInBytes = targetSize;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n-   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n-   * task with extra planning cost.\n-   * <p>\n-   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n-   * metadata, user can use a lookback of 1.\n-   *\n-   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitLookback(int lookback) {\n-    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n-    this.splitLookback = lookback;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n-   * threshold, Iceberg will use this value to do count.\n-   * <p>\n-   * this configuration controls the number of files to compact for each task, small value would lead to a\n-   * high compaction, the default value is 4MB.\n-   *\n-   * @param openFileCost minimum file size to count to pack into one \"bin\".\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitOpenFileCost(long openFileCost) {\n-    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n-    this.splitOpenFileCost = openFileCost;\n-    return this;\n-  }\n-\n-  /**\n-   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n-   * filter may be rewritten.\n-   *\n-   * @param expr Expression to filter out DataFiles\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction filter(Expression expr) {\n-    this.filter = Expressions.and(filter, expr);\n-    return this;\n-  }\n-\n-  @Override\n-  public RewriteDataFilesActionResult execute() {\n-    CloseableIterable<FileScanTask> fileScanTasks = null;\n-    try {\n-      fileScanTasks = table.newScan()\n-          .caseSensitive(caseSensitive)\n-          .ignoreResiduals()\n-          .filter(filter)\n-          .planFiles();\n-    } finally {\n-      try {\n-        if (fileScanTasks != null) {\n-          fileScanTasks.close();\n-        }\n-      } catch (IOException ioe) {\n-        LOG.warn(\"Failed to close task iterable\", ioe);\n-      }\n-    }\n-\n-    Map<StructLikeWrapper, Collection<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n-    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n-        .filter(kv -> kv.getValue().size() > 1)\n-        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n-\n-    // Nothing to rewrite if there's only one DataFile in each partition.\n-    if (filteredGroupedTasks.isEmpty()) {\n-      return RewriteDataFilesActionResult.empty();\n-    }\n-\n-    // Split and combine tasks under each partition\n-    List<CombinedScanTask> combinedScanTasks = filteredGroupedTasks.values().stream()\n-        .map(scanTasks -> {\n-          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n-              CloseableIterable.withNoopClose(scanTasks), targetSizeInBytes);\n-          return TableScanUtil.planTasks(splitTasks, targetSizeInBytes, splitLookback, splitOpenFileCost);\n-        })\n-        .flatMap(Streams::stream)\n-        .collect(Collectors.toList());\n-\n+  protected List<DataFile> rewriteDataForTasks(List<CombinedScanTask> combinedScanTasks) {\n     JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(combinedScanTasks, combinedScanTasks.size());\n-\n-    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n-    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n-\n-    RowDataRewriter rowDataRewriter = new RowDataRewriter(table, spec, caseSensitive, io, encryption);\n-\n-    List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(taskRDD);\n-    List<DataFile> currentDataFiles = filteredGroupedTasks.values().stream()\n-        .flatMap(tasks -> tasks.stream().map(FileScanTask::file))\n-        .collect(Collectors.toList());\n-    replaceDataFiles(currentDataFiles, addedDataFiles);\n-\n-    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n-  }\n-\n-  private Map<StructLikeWrapper, Collection<FileScanTask>> groupTasksByPartition(\n-      CloseableIterator<FileScanTask> tasksIter) {\n-    ListMultimap<StructLikeWrapper, FileScanTask> tasksGroupedByPartition = Multimaps.newListMultimap(\n-        Maps.newHashMap(), Lists::newArrayList);\n-\n-    try {\n-      tasksIter.forEachRemaining(task -> {\n-        StructLikeWrapper structLike = StructLikeWrapper.forType(spec.partitionType()).set(task.file().partition());\n-        tasksGroupedByPartition.put(structLike, task);\n-      });\n-\n-    } finally {\n-      try {\n-        tasksIter.close();\n-      } catch (IOException ioe) {\n-        LOG.warn(\"Failed to close task iterator\", ioe);\n-      }\n-    }\n-\n-    return tasksGroupedByPartition.asMap();\n-  }\n-\n-  private void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<DataFile> addedDataFiles) {\n-    try {\n-      RewriteFiles rewriteFiles = table.newRewrite();\n-      rewriteFiles.rewriteFiles(Sets.newHashSet(deletedDataFiles), Sets.newHashSet(addedDataFiles));\n-      commit(rewriteFiles);\n-\n-    } catch (Exception e) {\n-      Tasks.foreach(Iterables.transform(addedDataFiles, f -> f.path().toString()))\n-          .noRetry()\n-          .suppressFailureWhenFinished()\n-          .onFailure((location, exc) -> LOG.warn(\"Failed to delete: {}\", location, exc))\n-          .run(fileIO::deleteFile);\n-\n-      throw e;\n-    }\n+    Broadcast<FileIO> io = sparkContext.broadcast(SparkUtil.serializableFileIO(this.table()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cb731b0866e7a5654c89edf91f824d65407951de"}, "originalPosition": 265}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5ODQxMDYxOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QwNDowODoxNFrOHm7djw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yM1QwNDowODoxNFrOHm7djw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDU4MjE1OQ==", "bodyText": "nit:  here we don't need an empty line.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r510582159", "createdAt": "2020-10-23T04:08:14Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class BaseRewriteDataFilesAction<ThisT>\n+    extends BaseSnapshotUpdateAction<ThisT, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(BaseRewriteDataFilesAction.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private PartitionSpec spec;\n+  private boolean caseSensitive;\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+\n+  public BaseRewriteDataFilesAction(Table table) {\n+    this.table = table;\n+    this.fileIO = table.io();\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+  }\n+\n+  protected void caseSensitive(boolean newCaseSensitive) {\n+    this.caseSensitive = newCaseSensitive;\n+  }\n+\n+  protected boolean isCaseSensitive() {\n+    return caseSensitive;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b44bac53fbd2882f746092405b179cfc0159e817"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNTg0NTIzOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQwMjo0Njo1M1rOHn_XVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QwMToxNjo1N1rOHon7rg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY5NDY3Nw==", "bodyText": "We've already had a table() method before, do we need the extra icebergTable argument ?", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r511694677", "createdAt": "2020-10-26T02:46:53Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class BaseRewriteDataFilesAction<ThisT>\n+    extends BaseSnapshotUpdateAction<ThisT, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(BaseRewriteDataFilesAction.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private boolean caseSensitive;\n+  private PartitionSpec spec;\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+\n+  public BaseRewriteDataFilesAction(Table table) {\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+\n+    this.fileIO = fileIO(table);\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  protected EncryptionManager encryptionManager() {\n+    return encryptionManager;\n+  }\n+\n+  protected FileIO fileIO() {\n+    return fileIO;\n+  }\n+\n+  protected void caseSensitive(boolean newCaseSensitive) {\n+    this.caseSensitive = newCaseSensitive;\n+  }\n+\n+  protected boolean isCaseSensitive() {\n+    return caseSensitive;\n+  }\n+\n+  /**\n+   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public BaseRewriteDataFilesAction<ThisT> outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the target rewrite data file size in bytes\n+   *\n+   * @param targetSize size in bytes of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public BaseRewriteDataFilesAction<ThisT> targetSizeInBytes(long targetSize) {\n+    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n+        targetSize);\n+    this.targetSizeInBytes = targetSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task. Increasing this\n+   * usually makes tasks a bit more even by considering more ways to pack file regions into a single task with extra\n+   * planning cost.\n+   * <p>\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public BaseRewriteDataFilesAction<ThisT> splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n+   * threshold, Iceberg will use this value to do count.\n+   * <p>\n+   * this configuration controls the number of files to compact for each task, small value would lead to a high\n+   * compaction, the default value is 4MB.\n+   *\n+   * @param openFileCost minimum file size to count to pack into one \"bin\".\n+   * @return this for method chaining\n+   */\n+  public BaseRewriteDataFilesAction<ThisT> splitOpenFileCost(long openFileCost) {\n+    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n+    this.splitOpenFileCost = openFileCost;\n+    return this;\n+  }\n+\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+\n+  public BaseRewriteDataFilesAction<ThisT> filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = null;\n+    try {\n+      fileScanTasks = table.newScan()\n+          .caseSensitive(caseSensitive)\n+          .ignoreResiduals()\n+          .filter(filter)\n+          .planFiles();\n+    } finally {\n+      try {\n+        if (fileScanTasks != null) {\n+          fileScanTasks.close();\n+        }\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Failed to close task iterable\", ioe);\n+      }\n+    }\n+\n+    Map<StructLikeWrapper, Collection<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n+        .filter(kv -> kv.getValue().size() > 1)\n+        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+    // Split and combine tasks under each partition\n+    List<CombinedScanTask> combinedScanTasks = filteredGroupedTasks.values().stream()\n+        .map(scanTasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n+              CloseableIterable.withNoopClose(scanTasks), targetSizeInBytes);\n+          return TableScanUtil.planTasks(splitTasks, targetSizeInBytes, splitLookback, splitOpenFileCost);\n+        })\n+        .flatMap(Streams::stream)\n+        .collect(Collectors.toList());\n+\n+    List<DataFile> addedDataFiles = rewriteDataForTasks(combinedScanTasks);\n+    List<DataFile> currentDataFiles = filteredGroupedTasks.values().stream()\n+        .flatMap(tasks -> tasks.stream().map(FileScanTask::file))\n+        .collect(Collectors.toList());\n+    replaceDataFiles(currentDataFiles, addedDataFiles);\n+\n+    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n+  }\n+\n+\n+  private Map<StructLikeWrapper, Collection<FileScanTask>> groupTasksByPartition(\n+      CloseableIterator<FileScanTask> tasksIter) {\n+    ListMultimap<StructLikeWrapper, FileScanTask> tasksGroupedByPartition = Multimaps.newListMultimap(\n+        Maps.newHashMap(), Lists::newArrayList);\n+    try (CloseableIterator<FileScanTask> iterator = tasksIter) {\n+      iterator.forEachRemaining(task -> {\n+        StructLikeWrapper structLike = StructLikeWrapper.forType(spec.partitionType()).set(task.file().partition());\n+        tasksGroupedByPartition.put(structLike, task);\n+      });\n+    } catch (IOException e) {\n+      LOG.warn(\"Failed to close task iterator\", e);\n+    }\n+    return tasksGroupedByPartition.asMap();\n+  }\n+\n+  private void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<DataFile> addedDataFiles) {\n+    try {\n+      RewriteFiles rewriteFiles = table.newRewrite();\n+      rewriteFiles.rewriteFiles(Sets.newHashSet(deletedDataFiles), Sets.newHashSet(addedDataFiles));\n+      commit(rewriteFiles);\n+    } catch (Exception e) {\n+      Tasks.foreach(Iterables.transform(addedDataFiles, f -> f.path().toString()))\n+          .noRetry()\n+          .suppressFailureWhenFinished()\n+          .onFailure((location, exc) -> LOG.warn(\"Failed to delete: {}\", location, exc))\n+          .run(fileIO::deleteFile);\n+      throw e;\n+    }\n+  }\n+\n+  protected abstract FileIO fileIO(Table icebergTable);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62474a447edb5f0b4e7734359c37ecb2fd1815fe"}, "originalPosition": 268}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjM1OTM0Mg==", "bodyText": "ok,I deleted the icebergTable argument", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r512359342", "createdAt": "2020-10-27T01:16:57Z", "author": {"login": "zhangjun0x01"}, "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class BaseRewriteDataFilesAction<ThisT>\n+    extends BaseSnapshotUpdateAction<ThisT, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(BaseRewriteDataFilesAction.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private boolean caseSensitive;\n+  private PartitionSpec spec;\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+\n+  public BaseRewriteDataFilesAction(Table table) {\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+\n+    this.fileIO = fileIO(table);\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  protected EncryptionManager encryptionManager() {\n+    return encryptionManager;\n+  }\n+\n+  protected FileIO fileIO() {\n+    return fileIO;\n+  }\n+\n+  protected void caseSensitive(boolean newCaseSensitive) {\n+    this.caseSensitive = newCaseSensitive;\n+  }\n+\n+  protected boolean isCaseSensitive() {\n+    return caseSensitive;\n+  }\n+\n+  /**\n+   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public BaseRewriteDataFilesAction<ThisT> outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the target rewrite data file size in bytes\n+   *\n+   * @param targetSize size in bytes of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public BaseRewriteDataFilesAction<ThisT> targetSizeInBytes(long targetSize) {\n+    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n+        targetSize);\n+    this.targetSizeInBytes = targetSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task. Increasing this\n+   * usually makes tasks a bit more even by considering more ways to pack file regions into a single task with extra\n+   * planning cost.\n+   * <p>\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public BaseRewriteDataFilesAction<ThisT> splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n+   * threshold, Iceberg will use this value to do count.\n+   * <p>\n+   * this configuration controls the number of files to compact for each task, small value would lead to a high\n+   * compaction, the default value is 4MB.\n+   *\n+   * @param openFileCost minimum file size to count to pack into one \"bin\".\n+   * @return this for method chaining\n+   */\n+  public BaseRewriteDataFilesAction<ThisT> splitOpenFileCost(long openFileCost) {\n+    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n+    this.splitOpenFileCost = openFileCost;\n+    return this;\n+  }\n+\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+\n+  public BaseRewriteDataFilesAction<ThisT> filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = null;\n+    try {\n+      fileScanTasks = table.newScan()\n+          .caseSensitive(caseSensitive)\n+          .ignoreResiduals()\n+          .filter(filter)\n+          .planFiles();\n+    } finally {\n+      try {\n+        if (fileScanTasks != null) {\n+          fileScanTasks.close();\n+        }\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Failed to close task iterable\", ioe);\n+      }\n+    }\n+\n+    Map<StructLikeWrapper, Collection<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n+        .filter(kv -> kv.getValue().size() > 1)\n+        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+    // Split and combine tasks under each partition\n+    List<CombinedScanTask> combinedScanTasks = filteredGroupedTasks.values().stream()\n+        .map(scanTasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n+              CloseableIterable.withNoopClose(scanTasks), targetSizeInBytes);\n+          return TableScanUtil.planTasks(splitTasks, targetSizeInBytes, splitLookback, splitOpenFileCost);\n+        })\n+        .flatMap(Streams::stream)\n+        .collect(Collectors.toList());\n+\n+    List<DataFile> addedDataFiles = rewriteDataForTasks(combinedScanTasks);\n+    List<DataFile> currentDataFiles = filteredGroupedTasks.values().stream()\n+        .flatMap(tasks -> tasks.stream().map(FileScanTask::file))\n+        .collect(Collectors.toList());\n+    replaceDataFiles(currentDataFiles, addedDataFiles);\n+\n+    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n+  }\n+\n+\n+  private Map<StructLikeWrapper, Collection<FileScanTask>> groupTasksByPartition(\n+      CloseableIterator<FileScanTask> tasksIter) {\n+    ListMultimap<StructLikeWrapper, FileScanTask> tasksGroupedByPartition = Multimaps.newListMultimap(\n+        Maps.newHashMap(), Lists::newArrayList);\n+    try (CloseableIterator<FileScanTask> iterator = tasksIter) {\n+      iterator.forEachRemaining(task -> {\n+        StructLikeWrapper structLike = StructLikeWrapper.forType(spec.partitionType()).set(task.file().partition());\n+        tasksGroupedByPartition.put(structLike, task);\n+      });\n+    } catch (IOException e) {\n+      LOG.warn(\"Failed to close task iterator\", e);\n+    }\n+    return tasksGroupedByPartition.asMap();\n+  }\n+\n+  private void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<DataFile> addedDataFiles) {\n+    try {\n+      RewriteFiles rewriteFiles = table.newRewrite();\n+      rewriteFiles.rewriteFiles(Sets.newHashSet(deletedDataFiles), Sets.newHashSet(addedDataFiles));\n+      commit(rewriteFiles);\n+    } catch (Exception e) {\n+      Tasks.foreach(Iterables.transform(addedDataFiles, f -> f.path().toString()))\n+          .noRetry()\n+          .suppressFailureWhenFinished()\n+          .onFailure((location, exc) -> LOG.warn(\"Failed to delete: {}\", location, exc))\n+          .run(fileIO::deleteFile);\n+      throw e;\n+    }\n+  }\n+\n+  protected abstract FileIO fileIO(Table icebergTable);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY5NDY3Nw=="}, "originalCommit": {"oid": "62474a447edb5f0b4e7734359c37ecb2fd1815fe"}, "originalPosition": 268}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIwNzAwNzIxOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMDo0NDoxMlrOHoJ5kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yNlQxMDo0NDoxMlrOHoJ5kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTg2NzI4Mw==", "bodyText": "nit:  useless empty line ?", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r511867283", "createdAt": "2020-10-26T10:44:12Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class BaseRewriteDataFilesAction<ThisT>\n+    extends BaseSnapshotUpdateAction<ThisT, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(BaseRewriteDataFilesAction.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private boolean caseSensitive;\n+  private PartitionSpec spec;\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+\n+  public BaseRewriteDataFilesAction(Table table) {\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+\n+    this.fileIO = fileIO(table);\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  protected EncryptionManager encryptionManager() {\n+    return encryptionManager;\n+  }\n+\n+  protected FileIO fileIO() {\n+    return fileIO;\n+  }\n+\n+  protected void caseSensitive(boolean newCaseSensitive) {\n+    this.caseSensitive = newCaseSensitive;\n+  }\n+\n+  protected boolean isCaseSensitive() {\n+    return caseSensitive;\n+  }\n+\n+  /**\n+   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public BaseRewriteDataFilesAction<ThisT> outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the target rewrite data file size in bytes\n+   *\n+   * @param targetSize size in bytes of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public BaseRewriteDataFilesAction<ThisT> targetSizeInBytes(long targetSize) {\n+    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n+        targetSize);\n+    this.targetSizeInBytes = targetSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task. Increasing this\n+   * usually makes tasks a bit more even by considering more ways to pack file regions into a single task with extra\n+   * planning cost.\n+   * <p>\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public BaseRewriteDataFilesAction<ThisT> splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n+   * threshold, Iceberg will use this value to do count.\n+   * <p>\n+   * this configuration controls the number of files to compact for each task, small value would lead to a high\n+   * compaction, the default value is 4MB.\n+   *\n+   * @param openFileCost minimum file size to count to pack into one \"bin\".\n+   * @return this for method chaining\n+   */\n+  public BaseRewriteDataFilesAction<ThisT> splitOpenFileCost(long openFileCost) {\n+    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n+    this.splitOpenFileCost = openFileCost;\n+    return this;\n+  }\n+\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62474a447edb5f0b4e7734359c37ecb2fd1815fe"}, "originalPosition": 184}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMjY4NTgyOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxNDozNzo1N1rOHo_p2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwMTo0MzoyOVrOHpW-aw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjc0Nzk5NA==", "bodyText": "I may be getting the pattern wrong here, but I know in most instances we use the method without a verb as the getter. So maybe it makes sense to have the setter be \"setCaseSensative\" and also have it return \"this\" for method chaining.\nThen have the getter be just \"caseSensitive()\"", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r512747994", "createdAt": "2020-10-27T14:37:57Z", "author": {"login": "RussellSpitzer"}, "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,265 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class BaseRewriteDataFilesAction<ThisT>\n+    extends BaseSnapshotUpdateAction<ThisT, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(BaseRewriteDataFilesAction.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private boolean caseSensitive;\n+  private PartitionSpec spec;\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+\n+  public BaseRewriteDataFilesAction(Table table) {\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+\n+    this.fileIO = fileIO();\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  protected EncryptionManager encryptionManager() {\n+    return encryptionManager;\n+  }\n+\n+  protected void caseSensitive(boolean newCaseSensitive) {\n+    this.caseSensitive = newCaseSensitive;\n+  }\n+\n+  protected boolean isCaseSensitive() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6718d20cde8ef0c54d8e84105ce16124572e49c8"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzEzMDA5MQ==", "bodyText": "I added a method: caseSensitive(boolean newCaseSensitive) for method chaining.and add the caseSensitive() for getter.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r513130091", "createdAt": "2020-10-28T01:43:29Z", "author": {"login": "zhangjun0x01"}, "path": "core/src/main/java/org/apache/iceberg/actions/BaseRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,265 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class BaseRewriteDataFilesAction<ThisT>\n+    extends BaseSnapshotUpdateAction<ThisT, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(BaseRewriteDataFilesAction.class);\n+\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private boolean caseSensitive;\n+  private PartitionSpec spec;\n+  private Expression filter;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+\n+  public BaseRewriteDataFilesAction(Table table) {\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    this.splitOpenFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+\n+    this.fileIO = fileIO();\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  protected EncryptionManager encryptionManager() {\n+    return encryptionManager;\n+  }\n+\n+  protected void caseSensitive(boolean newCaseSensitive) {\n+    this.caseSensitive = newCaseSensitive;\n+  }\n+\n+  protected boolean isCaseSensitive() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjc0Nzk5NA=="}, "originalCommit": {"oid": "6718d20cde8ef0c54d8e84105ce16124572e49c8"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIxMjcwOTAzOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QxNDo0MjoxOFrOHo_4gQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQwNDoxNDoyOFrOHpZa_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjc1MTc0NQ==", "bodyText": "should we make this lazy?\nprivate FileIO lazyFileIO;\nprotected FileIO fileIO { \n   if (lazyFileIO == null) { \n     lazyFileIO =...\n   }\n  return lazyFileIo\n}\n}", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r512751745", "createdAt": "2020-10-27T14:42:18Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -111,171 +49,17 @@ protected RewriteDataFilesAction self() {\n   }\n \n   @Override\n-  protected Table table() {\n-    return table;\n-  }\n-\n-  /**\n-   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n-   *\n-   * @param specId PartitionSpec id to rewrite\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction outputSpecId(int specId) {\n-    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n-    this.spec = table.specs().get(specId);\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the target rewrite data file size in bytes\n-   *\n-   * @param targetSize size in bytes of rewrite data file\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n-    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n-        targetSize);\n-    this.targetSizeInBytes = targetSize;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n-   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n-   * task with extra planning cost.\n-   * <p>\n-   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n-   * metadata, user can use a lookback of 1.\n-   *\n-   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitLookback(int lookback) {\n-    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n-    this.splitLookback = lookback;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n-   * threshold, Iceberg will use this value to do count.\n-   * <p>\n-   * this configuration controls the number of files to compact for each task, small value would lead to a\n-   * high compaction, the default value is 4MB.\n-   *\n-   * @param openFileCost minimum file size to count to pack into one \"bin\".\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitOpenFileCost(long openFileCost) {\n-    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n-    this.splitOpenFileCost = openFileCost;\n-    return this;\n-  }\n-\n-  /**\n-   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n-   * filter may be rewritten.\n-   *\n-   * @param expr Expression to filter out DataFiles\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction filter(Expression expr) {\n-    this.filter = Expressions.and(filter, expr);\n-    return this;\n+  protected FileIO fileIO() {\n+    return SparkUtil.serializableFileIO(table());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6718d20cde8ef0c54d8e84105ce16124572e49c8"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzEzMjQ2Ng==", "bodyText": "we could make it lazy, I looked up the code which uses SparkUtil#serializableFileIO method, they are all directly call, so I think make it lazy may not make much sense.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r513132466", "createdAt": "2020-10-28T01:52:07Z", "author": {"login": "zhangjun0x01"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -111,171 +49,17 @@ protected RewriteDataFilesAction self() {\n   }\n \n   @Override\n-  protected Table table() {\n-    return table;\n-  }\n-\n-  /**\n-   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n-   *\n-   * @param specId PartitionSpec id to rewrite\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction outputSpecId(int specId) {\n-    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n-    this.spec = table.specs().get(specId);\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the target rewrite data file size in bytes\n-   *\n-   * @param targetSize size in bytes of rewrite data file\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n-    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n-        targetSize);\n-    this.targetSizeInBytes = targetSize;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n-   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n-   * task with extra planning cost.\n-   * <p>\n-   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n-   * metadata, user can use a lookback of 1.\n-   *\n-   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitLookback(int lookback) {\n-    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n-    this.splitLookback = lookback;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n-   * threshold, Iceberg will use this value to do count.\n-   * <p>\n-   * this configuration controls the number of files to compact for each task, small value would lead to a\n-   * high compaction, the default value is 4MB.\n-   *\n-   * @param openFileCost minimum file size to count to pack into one \"bin\".\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitOpenFileCost(long openFileCost) {\n-    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n-    this.splitOpenFileCost = openFileCost;\n-    return this;\n-  }\n-\n-  /**\n-   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n-   * filter may be rewritten.\n-   *\n-   * @param expr Expression to filter out DataFiles\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction filter(Expression expr) {\n-    this.filter = Expressions.and(filter, expr);\n-    return this;\n+  protected FileIO fileIO() {\n+    return SparkUtil.serializableFileIO(table());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjc1MTc0NQ=="}, "originalCommit": {"oid": "6718d20cde8ef0c54d8e84105ce16124572e49c8"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE0Nzc2Nw==", "bodyText": "That would save lots of fileIO serialization if we make this lazy, I agree with @RussellSpitzer . The upper layer may call this fileIO method several times, while we only need to serialize once.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r513147767", "createdAt": "2020-10-28T02:49:18Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -111,171 +49,17 @@ protected RewriteDataFilesAction self() {\n   }\n \n   @Override\n-  protected Table table() {\n-    return table;\n-  }\n-\n-  /**\n-   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n-   *\n-   * @param specId PartitionSpec id to rewrite\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction outputSpecId(int specId) {\n-    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n-    this.spec = table.specs().get(specId);\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the target rewrite data file size in bytes\n-   *\n-   * @param targetSize size in bytes of rewrite data file\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n-    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n-        targetSize);\n-    this.targetSizeInBytes = targetSize;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n-   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n-   * task with extra planning cost.\n-   * <p>\n-   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n-   * metadata, user can use a lookback of 1.\n-   *\n-   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitLookback(int lookback) {\n-    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n-    this.splitLookback = lookback;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n-   * threshold, Iceberg will use this value to do count.\n-   * <p>\n-   * this configuration controls the number of files to compact for each task, small value would lead to a\n-   * high compaction, the default value is 4MB.\n-   *\n-   * @param openFileCost minimum file size to count to pack into one \"bin\".\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitOpenFileCost(long openFileCost) {\n-    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n-    this.splitOpenFileCost = openFileCost;\n-    return this;\n-  }\n-\n-  /**\n-   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n-   * filter may be rewritten.\n-   *\n-   * @param expr Expression to filter out DataFiles\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction filter(Expression expr) {\n-    this.filter = Expressions.and(filter, expr);\n-    return this;\n+  protected FileIO fileIO() {\n+    return SparkUtil.serializableFileIO(table());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjc1MTc0NQ=="}, "originalCommit": {"oid": "6718d20cde8ef0c54d8e84105ce16124572e49c8"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE2NDk1OQ==", "bodyText": "This fileIO() method should be a setter for field fileIO. This method only needs to be called once when BaseRewriteDataFilesAction is initialized. Other places need to refer to fileIO and only need to call its getter method,\nIf we rename fileIO() to setFileIO(), it might be better understood, but I don\u2019t know if this conforms to iceberg's specification.\n  // BaseRewriteDataFilesAction.java \n  protected FileIO fileIO() {\n    return fileIO;\n  }\n\n  protected abstract FileIO setFileIO();\n\n  // RewriteDataFilesAction.java \n  @Override\n  protected FileIO setFileIO() {\n    return SparkUtil.serializableFileIO(table());\n  }\n\nso I think the method will not be called multiple times. What do you think \uff1f@openinx", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r513164959", "createdAt": "2020-10-28T03:53:15Z", "author": {"login": "zhangjun0x01"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -111,171 +49,17 @@ protected RewriteDataFilesAction self() {\n   }\n \n   @Override\n-  protected Table table() {\n-    return table;\n-  }\n-\n-  /**\n-   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n-   *\n-   * @param specId PartitionSpec id to rewrite\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction outputSpecId(int specId) {\n-    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n-    this.spec = table.specs().get(specId);\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the target rewrite data file size in bytes\n-   *\n-   * @param targetSize size in bytes of rewrite data file\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n-    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n-        targetSize);\n-    this.targetSizeInBytes = targetSize;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n-   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n-   * task with extra planning cost.\n-   * <p>\n-   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n-   * metadata, user can use a lookback of 1.\n-   *\n-   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitLookback(int lookback) {\n-    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n-    this.splitLookback = lookback;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n-   * threshold, Iceberg will use this value to do count.\n-   * <p>\n-   * this configuration controls the number of files to compact for each task, small value would lead to a\n-   * high compaction, the default value is 4MB.\n-   *\n-   * @param openFileCost minimum file size to count to pack into one \"bin\".\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitOpenFileCost(long openFileCost) {\n-    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n-    this.splitOpenFileCost = openFileCost;\n-    return this;\n-  }\n-\n-  /**\n-   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n-   * filter may be rewritten.\n-   *\n-   * @param expr Expression to filter out DataFiles\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction filter(Expression expr) {\n-    this.filter = Expressions.and(filter, expr);\n-    return this;\n+  protected FileIO fileIO() {\n+    return SparkUtil.serializableFileIO(table());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjc1MTc0NQ=="}, "originalCommit": {"oid": "6718d20cde8ef0c54d8e84105ce16124572e49c8"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE3MDE3Mw==", "bodyText": "OK, sound like we've a local field to initialize the fileIO, this method will only call once now. I'm OK with it then.", "url": "https://github.com/apache/iceberg/pull/1624#discussion_r513170173", "createdAt": "2020-10-28T04:14:28Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -111,171 +49,17 @@ protected RewriteDataFilesAction self() {\n   }\n \n   @Override\n-  protected Table table() {\n-    return table;\n-  }\n-\n-  /**\n-   * Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite\n-   *\n-   * @param specId PartitionSpec id to rewrite\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction outputSpecId(int specId) {\n-    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n-    this.spec = table.specs().get(specId);\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the target rewrite data file size in bytes\n-   *\n-   * @param targetSize size in bytes of rewrite data file\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n-    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n-        targetSize);\n-    this.targetSizeInBytes = targetSize;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n-   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n-   * task with extra planning cost.\n-   * <p>\n-   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n-   * metadata, user can use a lookback of 1.\n-   *\n-   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitLookback(int lookback) {\n-    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n-    this.splitLookback = lookback;\n-    return this;\n-  }\n-\n-  /**\n-   * Specify the minimum file size to count to pack into one \"bin\". If the read file size is smaller than this specified\n-   * threshold, Iceberg will use this value to do count.\n-   * <p>\n-   * this configuration controls the number of files to compact for each task, small value would lead to a\n-   * high compaction, the default value is 4MB.\n-   *\n-   * @param openFileCost minimum file size to count to pack into one \"bin\".\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction splitOpenFileCost(long openFileCost) {\n-    Preconditions.checkArgument(openFileCost > 0L, \"Invalid split openFileCost %d\", openFileCost);\n-    this.splitOpenFileCost = openFileCost;\n-    return this;\n-  }\n-\n-  /**\n-   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n-   * filter may be rewritten.\n-   *\n-   * @param expr Expression to filter out DataFiles\n-   * @return this for method chaining\n-   */\n-  public RewriteDataFilesAction filter(Expression expr) {\n-    this.filter = Expressions.and(filter, expr);\n-    return this;\n+  protected FileIO fileIO() {\n+    return SparkUtil.serializableFileIO(table());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjc1MTc0NQ=="}, "originalCommit": {"oid": "6718d20cde8ef0c54d8e84105ce16124572e49c8"}, "originalPosition": 171}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3612, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}