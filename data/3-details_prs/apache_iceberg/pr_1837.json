{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI4MTg5ODMw", "number": 1837, "title": "Hive read path documentation for HadoopCatalog tables", "bodyText": "", "createdAt": "2020-11-26T16:55:17Z", "url": "https://github.com/apache/iceberg/pull/1837", "merged": true, "mergeCommit": {"oid": "1acbc2a3302c987981470b6faa031b2b31f5a1e7"}, "closed": true, "closedAt": "2020-11-30T21:49:58Z", "author": {"login": "massdosage"}, "timelineItems": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdgV0aaAH2gAyNTI4MTg5ODMwOjhiNGIyZmQ2ZTA0ZDdjN2M0NjVjMTZjNDgwYTM5ZDAwN2JjMjg2NTI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdhsg74AFqTU0MTMyMTk4Mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "8b4b2fd6e04d7c7c465c16c480a39d007bc28652", "author": {"user": {"login": "massdosage", "name": "Adrian Woodhead"}}, "url": "https://github.com/apache/iceberg/commit/8b4b2fd6e04d7c7c465c16c480a39d007bc28652", "committedDate": "2020-11-26T16:48:04Z", "message": "first cut at adding HadoopCatalog docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "21231dac75581eaccb4e338c9a3a814d19f2cce4", "author": {"user": {"login": "massdosage", "name": "Adrian Woodhead"}}, "url": "https://github.com/apache/iceberg/commit/21231dac75581eaccb4e338c9a3a814d19f2cce4", "committedDate": "2020-11-26T16:54:09Z", "message": "linebreak for TODO"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM5NDY4MDI1", "url": "https://github.com/apache/iceberg/pull/1837#pullrequestreview-539468025", "createdAt": "2020-11-26T17:01:59Z", "commit": {"oid": "21231dac75581eaccb4e338c9a3a814d19f2cce4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNlQxNzowMjowMFrOH6i3QQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNlQxNzowMjowMFrOH6i3QQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE1MDY1Nw==", "bodyText": "@pvary So for the above, if I create the Hive table but don't set iceberg.mr.catalog=hadoop or then I can actually query it successfully. However if I do set these (which the code at https://github.com/apache/iceberg/blob/master/mr/src/test/java/org/apache/iceberg/mr/hive/TestTables.java#L133 suggests one should) then I can't query the table. Instead I get this stack trace:\n2020-11-26T16:46:36,531 INFO  [f94f328b-fe24-4cbc-8723-423fb6ffea6e main([])]: mr.Catalogs (Catalogs.java:loadCatalog(212)) - Loaded Hadoop catalog HadoopCatalog{name=hadoop, location=/tmp/iceberg-test-hadoop-tables}\n2020-11-26T16:46:36,533 ERROR [f94f328b-fe24-4cbc-8723-423fb6ffea6e main([])]: hive.log (MetaStoreUtils.java:getDeserializer(456)) - error in initSerDe: org.apache.hadoop.hive.serde2.SerDeException Please provide an existing table or a valid schema\norg.apache.hadoop.hive.serde2.SerDeException: Please provide an existing table or a valid schema\n        at org.apache.iceberg.mr.hive.HiveIcebergSerDe.initialize(HiveIcebergSerDe.java:62) ~[?:?]\n        at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:54) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:533) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:450) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:437) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:263) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:838) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:872) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4356) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:354) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227) ~[hive-exec-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233) ~[hive-cli-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184) ~[hive-cli-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403) ~[hive-cli-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821) ~[hive-cli-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759) ~[hive-cli-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686) ~[hive-cli-2.3.6-amzn-2.jar:2.3.6-amzn-2]\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_252]\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_252]\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_252]\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_252]\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:239) ~[hadoop-common-2.8.5-amzn-6.jar:?]\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:153) ~[hadoop-common-2.8.5-amzn-6.jar:?]\nCaused by: org.apache.iceberg.exceptions.NoSuchTableException: Table does not exist: test.iceberg_table_from_hadoop_catalog\n        at org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:108) ~[?:?]\n        at org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:100) ~[?:?]\n        at org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:92) ~[?:?]\n        at org.apache.iceberg.mr.hive.HiveIcebergSerDe.initialize(HiveIcebergSerDe.java:60) ~[?:?]\n        ... 29 more\n2020-11-26T16:46:36,533 ERROR [f94f328b-fe24-4cbc-8723-423fb6ffea6e main([])]: exec.DDLTask (DDLTask.java:failed(639)) - org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: MetaException(message:org.apache.hadoop.hive.serde2.SerDeException Please provide an existing table or a valid schema)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:867)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:872)\n        at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4356)\n        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:354)\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:239)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:153)\nCaused by: java.lang.RuntimeException: MetaException(message:org.apache.hadoop.hive.serde2.SerDeException Please provide an existing table or a valid schema)\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:283)\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:263)\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:838)\n        ... 22 more\nCaused by: MetaException(message:org.apache.hadoop.hive.serde2.SerDeException Please provide an existing table or a valid schema)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:458)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:437)\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281)\n        ... 24 more\n\nAny ideas what's going on? I've got a feeling I'm missing something obvious, it's been a long day ;) I'll try debug it tomorrow, just thought you might have some ideas :)", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r531150657", "createdAt": "2020-11-26T17:02:00Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -84,7 +84,32 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the table is called `database_a.table_c` and that the table location is `hdfs://some_path/database_a/table_c`.\n+\n+##### Create a Hive table\n+Now overlay a Hive table on top of this Iceberg table by issuing Hive DDL like so:\n+```sql\n+CREATE EXTERNAL TABLE table_a \n+STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n+LOCATION 'hdfs://some_bucket/some_path/database_a/table_c';\n+```\n+\n+#### Query the Iceberg table via Hive\n+TODO: why does below work if no config settings are set in Hive but fails if we add `set iceberg.mr.catalog=hadoop` like the code suggests we need to do?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "21231dac75581eaccb4e338c9a3a814d19f2cce4"}, "originalPosition": 19}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM5NDY4NTQ3", "url": "https://github.com/apache/iceberg/pull/1837#pullrequestreview-539468547", "createdAt": "2020-11-26T17:02:55Z", "commit": {"oid": "21231dac75581eaccb4e338c9a3a814d19f2cce4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNlQxNzowMjo1NlrOH6i42w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNlQxNzowMjo1NlrOH6i42w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTE1MTA2Nw==", "bodyText": "This isn't related to the HadoopCatalog but since #1417 was just merged I thought I might as well just put it here.", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r531151067", "createdAt": "2020-11-26T17:02:56Z", "author": {"login": "massdosage"}, "path": "site/docs/hive.md", "diffHunk": "@@ -84,7 +84,32 @@ You should now be able to issue Hive SQL `SELECT` queries using the above table\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the table is called `database_a.table_c` and that the table location is `hdfs://some_path/database_a/table_c`.\n+\n+##### Create a Hive table\n+Now overlay a Hive table on top of this Iceberg table by issuing Hive DDL like so:\n+```sql\n+CREATE EXTERNAL TABLE table_a \n+STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n+LOCATION 'hdfs://some_bucket/some_path/database_a/table_c';\n+```\n+\n+#### Query the Iceberg table via Hive\n+TODO: why does below work if no config settings are set in Hive but fails if we add `set iceberg.mr.catalog=hadoop` like the code suggests we need to do?\n+\n+You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.\n+```sql\n+SELECT * from table_c;\n+```\n+\n ### Features\n \n #### Predicate pushdown\n Pushdown of the Hive SQL `WHERE` clause has been implemented so that these filters are used at the Iceberg TableScan level as well as by the Parquet and ORC Readers.\n+\n+#### Column Projection", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "21231dac75581eaccb4e338c9a3a814d19f2cce4"}, "originalPosition": 31}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "456159db6f0397f585edaeb695af60eb58d301a7", "author": {"user": {"login": "massdosage", "name": "Adrian Woodhead"}}, "url": "https://github.com/apache/iceberg/commit/456159db6f0397f585edaeb695af60eb58d301a7", "committedDate": "2020-11-27T15:38:28Z", "message": "working hadoop catalog table"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "276a88d9c9dab6bb3012cad544a83a1d00581499", "author": {"user": {"login": "massdosage", "name": "Adrian Woodhead"}}, "url": "https://github.com/apache/iceberg/commit/276a88d9c9dab6bb3012cad544a83a1d00581499", "committedDate": "2020-11-27T15:40:36Z", "message": "improve layout of table properties"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c", "author": {"user": {"login": "massdosage", "name": "Adrian Woodhead"}}, "url": "https://github.com/apache/iceberg/commit/1542d2662ea12c942d021294aaf2269d37612f0c", "committedDate": "2020-11-27T15:42:42Z", "message": "move query engines to a feature so not repeated"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwOTU3MzIy", "url": "https://github.com/apache/iceberg/pull/1837#pullrequestreview-540957322", "createdAt": "2020-11-30T14:26:58Z", "commit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoyNjo1OFrOH79fJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoyNjo1OFrOH79fJQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNTQyOQ==", "bodyText": "Why did we remove that both MR and Tez is supported?", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r532635429", "createdAt": "2020-11-30T14:26:58Z", "author": {"login": "pvary"}, "path": "site/docs/hive.md", "diffHunk": "@@ -79,12 +79,45 @@ In order to query a Hive table created by either of the HiveCatalog methods desc\n ```sql\n SET iceberg.mr.catalog=hive;\n ```\n-You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwOTU4NjI0", "url": "https://github.com/apache/iceberg/pull/1837#pullrequestreview-540958624", "createdAt": "2020-11-30T14:28:18Z", "commit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoyODoxOFrOH79izg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDoyODoxOFrOH79izg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNjM2Ng==", "bodyText": "It would be good to remove this restriction later", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r532636366", "createdAt": "2020-11-30T14:28:18Z", "author": {"login": "pvary"}, "path": "site/docs/hive.md", "diffHunk": "@@ -79,12 +79,45 @@ In order to query a Hive table created by either of the HiveCatalog methods desc\n ```sql\n SET iceberg.mr.catalog=hive;\n ```\n-You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.\n+You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table.\n ```sql\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the fully qualified table identifier is `database_a.table_c` and that the Hadoop Catalog warehouse location is `hdfs://some_bucket/path_to_hadoop_warehouse`. Iceberg will therefore create the table at the location `hdfs://some_bucket/path_to_hadoop_warehouse/database_a/table_c`.\n+\n+##### Create a Hive table\n+Now overlay a Hive table on top of this Iceberg table by issuing Hive DDL like so:\n+```sql\n+CREATE EXTERNAL TABLE database_a.table_c \n+STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n+LOCATION 'hdfs://some_bucket/path_to_hadoop_warehouse/database_a/table_c'\n+TBLPROPERTIES (\n+  'iceberg.mr.catalog'='hadoop', \n+  'iceberg.mr.catalog.hadoop.warehouse.location'='hdfs://some_bucket/path_to_hadoop_warehouse')\n+;\n+```\n+Note that the Hive database and table name *must* match the values used in the Iceberg `TableIdentifier` when the table was created. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "originalPosition": 36}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQwOTYwNTgx", "url": "https://github.com/apache/iceberg/pull/1837#pullrequestreview-540960581", "createdAt": "2020-11-30T14:30:22Z", "commit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDozMDoyM1rOH79omw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQxNDozMDoyM1rOH79omw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjYzNzg1MQ==", "bodyText": "After the latest changes now it is possible to use Hive CREATE TABLE command to create the Hive table and the Iceberg table at the same time. Do we want to document it here, or do we want to do it in a different PR?", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r532637851", "createdAt": "2020-11-30T14:30:23Z", "author": {"login": "pvary"}, "path": "site/docs/hive.md", "diffHunk": "@@ -79,12 +79,45 @@ In order to query a Hive table created by either of the HiveCatalog methods desc\n ```sql\n SET iceberg.mr.catalog=hive;\n ```\n-You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.\n+You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table.\n ```sql\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the fully qualified table identifier is `database_a.table_c` and that the Hadoop Catalog warehouse location is `hdfs://some_bucket/path_to_hadoop_warehouse`. Iceberg will therefore create the table at the location `hdfs://some_bucket/path_to_hadoop_warehouse/database_a/table_c`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "originalPosition": 23}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxMzIxMjY1", "url": "https://github.com/apache/iceberg/pull/1837#pullrequestreview-541321265", "createdAt": "2020-11-30T21:47:24Z", "commit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMTo0NzoyNFrOH8PI6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0zMFQyMTo0NzoyNFrOH8PI6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMjkyNDY1MQ==", "bodyText": "Does it work to set iceberg.mr.catalog in table properties for a Hive catalog table as well? If so, we should probably note that a Hive table can be created for an Iceberg table this way as well.", "url": "https://github.com/apache/iceberg/pull/1837#discussion_r532924651", "createdAt": "2020-11-30T21:47:24Z", "author": {"login": "rdblue"}, "path": "site/docs/hive.md", "diffHunk": "@@ -79,12 +79,45 @@ In order to query a Hive table created by either of the HiveCatalog methods desc\n ```sql\n SET iceberg.mr.catalog=hive;\n ```\n-You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table. Both the Map Reduce and Tez query execution engines are supported.\n+You should now be able to issue Hive SQL `SELECT` queries using the above table and see the results returned from the underlying Iceberg table.\n ```sql\n SELECT * from table_b;\n ```\n \n+#### Using Hadoop Catalog\n+Iceberg tables created using `HadoopCatalog` are stored entirely in a directory in a filesytem like HDFS. \n+\n+##### Create an Iceberg table\n+The first step is to create an Iceberg table using the Spark/Java/Python API and `HadoopCatalog`. For the purposes of this documentation we will assume that the fully qualified table identifier is `database_a.table_c` and that the Hadoop Catalog warehouse location is `hdfs://some_bucket/path_to_hadoop_warehouse`. Iceberg will therefore create the table at the location `hdfs://some_bucket/path_to_hadoop_warehouse/database_a/table_c`.\n+\n+##### Create a Hive table\n+Now overlay a Hive table on top of this Iceberg table by issuing Hive DDL like so:\n+```sql\n+CREATE EXTERNAL TABLE database_a.table_c \n+STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \n+LOCATION 'hdfs://some_bucket/path_to_hadoop_warehouse/database_a/table_c'\n+TBLPROPERTIES (\n+  'iceberg.mr.catalog'='hadoop', ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQxMzIxOTgy", "url": "https://github.com/apache/iceberg/pull/1837#pullrequestreview-541321982", "createdAt": "2020-11-30T21:48:32Z", "commit": {"oid": "1542d2662ea12c942d021294aaf2269d37612f0c"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3436, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}