{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQxNDg3MTAy", "number": 1948, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMzozMjo0MFrOFG86QQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODozNToyOFrOFHTOeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQyODMzNzI5OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xN1QyMzozMjo0MFrOIINGjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODoxNToyNlrOIItfyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ3NDE5MA==", "bodyText": "So, this seems really unlikely and I can't figure out a good way around it, but if someone had a \"add partition field\" as a column name doing a Spark alter table this would probably give a false positive, but that probably doesn't matter since even then most of the time the iceberg parser is a superset of the delegate parser. Does that sound right or am I off base with the assumption this block?", "url": "https://github.com/apache/iceberg/pull/1948#discussion_r545474190", "createdAt": "2020-12-17T23:32:40Z", "author": {"login": "holdenk"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -94,13 +94,20 @@ class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserI\n    */\n   override def parsePlan(sqlText: String): LogicalPlan = {\n     val sqlTextAfterSubstitution = substitutor.substitute(sqlText)\n-    if (sqlTextAfterSubstitution.toLowerCase(Locale.ROOT).trim().startsWith(\"call\")) {\n+    if (isIcebergCommand(sqlTextAfterSubstitution)) {\n       parse(sqlTextAfterSubstitution) { parser => astBuilder.visit(parser.singleStatement()) }.asInstanceOf[LogicalPlan]\n     } else {\n       delegate.parsePlan(sqlText)\n     }\n   }\n \n+  private def isIcebergCommand(sqlText: String): Boolean = {\n+    val normalized = sqlText.toLowerCase(Locale.ROOT).trim()\n+    normalized.startsWith(\"call\") ||\n+        (normalized.startsWith(\"alter table\") && (\n+            normalized.contains(\"add partition field\") || normalized.contains(\"drop partition field\")))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxNDMwNg==", "bodyText": "I think that the \"add partition\" syntax requires a partition \"spec\" that is something like (a=1, b=2) so the parentheses should prevent this from catching \"add partition\" commands.\nThat said, we used to fall back to the Spark parser whenever something couldn't be parsed by this parser. I'm not sure whether we want to move back to that or do something more complicated. One option is try the Iceberg parser, then the Spark parser, and then check the Spark parser's exception. If it complains about CALL or FIELD then the exception from the Iceberg parser should be used, otherwise it should re-throw Spark's exception.\n@RussellSpitzer, any ideas here?", "url": "https://github.com/apache/iceberg/pull/1948#discussion_r545514306", "createdAt": "2020-12-18T01:30:14Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -94,13 +94,20 @@ class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserI\n    */\n   override def parsePlan(sqlText: String): LogicalPlan = {\n     val sqlTextAfterSubstitution = substitutor.substitute(sqlText)\n-    if (sqlTextAfterSubstitution.toLowerCase(Locale.ROOT).trim().startsWith(\"call\")) {\n+    if (isIcebergCommand(sqlTextAfterSubstitution)) {\n       parse(sqlTextAfterSubstitution) { parser => astBuilder.visit(parser.singleStatement()) }.asInstanceOf[LogicalPlan]\n     } else {\n       delegate.parsePlan(sqlText)\n     }\n   }\n \n+  private def isIcebergCommand(sqlText: String): Boolean = {\n+    val normalized = sqlText.toLowerCase(Locale.ROOT).trim()\n+    normalized.startsWith(\"call\") ||\n+        (normalized.startsWith(\"alter table\") && (\n+            normalized.contains(\"add partition field\") || normalized.contains(\"drop partition field\")))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ3NDE5MA=="}, "originalCommit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTg4NjcyOQ==", "bodyText": "I was thinking about this before and I didn't want it to be that specific. I think maybe a cool solution could be trying both, and checking which parser parsed the furthest. Each of our analysis exceptions should give a position, so we can just pick whoever parsed the most. How does that sound?", "url": "https://github.com/apache/iceberg/pull/1948#discussion_r545886729", "createdAt": "2020-12-18T14:55:42Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -94,13 +94,20 @@ class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserI\n    */\n   override def parsePlan(sqlText: String): LogicalPlan = {\n     val sqlTextAfterSubstitution = substitutor.substitute(sqlText)\n-    if (sqlTextAfterSubstitution.toLowerCase(Locale.ROOT).trim().startsWith(\"call\")) {\n+    if (isIcebergCommand(sqlTextAfterSubstitution)) {\n       parse(sqlTextAfterSubstitution) { parser => astBuilder.visit(parser.singleStatement()) }.asInstanceOf[LogicalPlan]\n     } else {\n       delegate.parsePlan(sqlText)\n     }\n   }\n \n+  private def isIcebergCommand(sqlText: String): Boolean = {\n+    val normalized = sqlText.toLowerCase(Locale.ROOT).trim()\n+    normalized.startsWith(\"call\") ||\n+        (normalized.startsWith(\"alter table\") && (\n+            normalized.contains(\"add partition field\") || normalized.contains(\"drop partition field\")))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ3NDE5MA=="}, "originalCommit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk5MTIwMA==", "bodyText": "That sounds reasonable to me, but I think we would want to make sure we have tests for it so we know if the heuristic starts to fail. Also, that should be a follow-up and not in this PR, right?", "url": "https://github.com/apache/iceberg/pull/1948#discussion_r545991200", "createdAt": "2020-12-18T17:49:03Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -94,13 +94,20 @@ class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserI\n    */\n   override def parsePlan(sqlText: String): LogicalPlan = {\n     val sqlTextAfterSubstitution = substitutor.substitute(sqlText)\n-    if (sqlTextAfterSubstitution.toLowerCase(Locale.ROOT).trim().startsWith(\"call\")) {\n+    if (isIcebergCommand(sqlTextAfterSubstitution)) {\n       parse(sqlTextAfterSubstitution) { parser => astBuilder.visit(parser.singleStatement()) }.asInstanceOf[LogicalPlan]\n     } else {\n       delegate.parsePlan(sqlText)\n     }\n   }\n \n+  private def isIcebergCommand(sqlText: String): Boolean = {\n+    val normalized = sqlText.toLowerCase(Locale.ROOT).trim()\n+    normalized.startsWith(\"call\") ||\n+        (normalized.startsWith(\"alter table\") && (\n+            normalized.contains(\"add partition field\") || normalized.contains(\"drop partition field\")))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ3NDE5MA=="}, "originalCommit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk5OTcxNA==", "bodyText": "Not a deal breaker for me.", "url": "https://github.com/apache/iceberg/pull/1948#discussion_r545999714", "createdAt": "2020-12-18T18:05:19Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -94,13 +94,20 @@ class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserI\n    */\n   override def parsePlan(sqlText: String): LogicalPlan = {\n     val sqlTextAfterSubstitution = substitutor.substitute(sqlText)\n-    if (sqlTextAfterSubstitution.toLowerCase(Locale.ROOT).trim().startsWith(\"call\")) {\n+    if (isIcebergCommand(sqlTextAfterSubstitution)) {\n       parse(sqlTextAfterSubstitution) { parser => astBuilder.visit(parser.singleStatement()) }.asInstanceOf[LogicalPlan]\n     } else {\n       delegate.parsePlan(sqlText)\n     }\n   }\n \n+  private def isIcebergCommand(sqlText: String): Boolean = {\n+    val normalized = sqlText.toLowerCase(Locale.ROOT).trim()\n+    normalized.startsWith(\"call\") ||\n+        (normalized.startsWith(\"alter table\") && (\n+            normalized.contains(\"add partition field\") || normalized.contains(\"drop partition field\")))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ3NDE5MA=="}, "originalCommit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAwNDkzOA==", "bodyText": "Same, I don't think this needs to be addressed here, I was just bringing it up to make sure my understanding was correct.", "url": "https://github.com/apache/iceberg/pull/1948#discussion_r546004938", "createdAt": "2020-12-18T18:15:26Z", "author": {"login": "holdenk"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -94,13 +94,20 @@ class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserI\n    */\n   override def parsePlan(sqlText: String): LogicalPlan = {\n     val sqlTextAfterSubstitution = substitutor.substitute(sqlText)\n-    if (sqlTextAfterSubstitution.toLowerCase(Locale.ROOT).trim().startsWith(\"call\")) {\n+    if (isIcebergCommand(sqlTextAfterSubstitution)) {\n       parse(sqlTextAfterSubstitution) { parser => astBuilder.visit(parser.singleStatement()) }.asInstanceOf[LogicalPlan]\n     } else {\n       delegate.parsePlan(sqlText)\n     }\n   }\n \n+  private def isIcebergCommand(sqlText: String): Boolean = {\n+    val normalized = sqlText.toLowerCase(Locale.ROOT).trim()\n+    normalized.startsWith(\"call\") ||\n+        (normalized.startsWith(\"alter table\") && (\n+            normalized.contains(\"add partition field\") || normalized.contains(\"drop partition field\")))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ3NDE5MA=="}, "originalCommit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMTkwNjY4OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/iceberg/spark/extensions/IcebergSparkSessionExtensions.scala", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODoxMTowMVrOIItXGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODo0MTo0NlrOIIuYaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAwMjcxMg==", "bodyText": "Did the apply method not resolve here? Wondering why we needed to modify the anonymous arg", "url": "https://github.com/apache/iceberg/pull/1948#discussion_r546002712", "createdAt": "2020-12-18T18:11:01Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/iceberg/spark/extensions/IcebergSparkSessionExtensions.scala", "diffHunk": "@@ -37,6 +37,6 @@ class IcebergSparkSessionExtensions extends (SparkSessionExtensions => Unit) {\n     // TODO: PullupCorrelatedPredicates should handle row-level operations\n     extensions.injectOptimizerRule { _ => PullupCorrelatedPredicatesInRowLevelOperations }\n     extensions.injectOptimizerRule { _ => RewriteDelete }\n-    extensions.injectPlannerStrategy { _ => ExtendedDataSourceV2Strategy }\n+    extensions.injectPlannerStrategy { spark => ExtendedDataSourceV2Strategy(spark) }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAwNDU2Nw==", "bodyText": "This follows what we do above, and it seemed reasonably clean to me. I guess we could just pass ExtendedDataSourceV2Strategy and have apply automatically called.", "url": "https://github.com/apache/iceberg/pull/1948#discussion_r546004567", "createdAt": "2020-12-18T18:14:43Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/iceberg/spark/extensions/IcebergSparkSessionExtensions.scala", "diffHunk": "@@ -37,6 +37,6 @@ class IcebergSparkSessionExtensions extends (SparkSessionExtensions => Unit) {\n     // TODO: PullupCorrelatedPredicates should handle row-level operations\n     extensions.injectOptimizerRule { _ => PullupCorrelatedPredicatesInRowLevelOperations }\n     extensions.injectOptimizerRule { _ => RewriteDelete }\n-    extensions.injectPlannerStrategy { _ => ExtendedDataSourceV2Strategy }\n+    extensions.injectPlannerStrategy { spark => ExtendedDataSourceV2Strategy(spark) }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAwMjcxMg=="}, "originalCommit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAxOTQzNQ==", "bodyText": "Ah I was just following the pattern with injectOptimizer lines directly above", "url": "https://github.com/apache/iceberg/pull/1948#discussion_r546019435", "createdAt": "2020-12-18T18:41:46Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/iceberg/spark/extensions/IcebergSparkSessionExtensions.scala", "diffHunk": "@@ -37,6 +37,6 @@ class IcebergSparkSessionExtensions extends (SparkSessionExtensions => Unit) {\n     // TODO: PullupCorrelatedPredicates should handle row-level operations\n     extensions.injectOptimizerRule { _ => PullupCorrelatedPredicatesInRowLevelOperations }\n     extensions.injectOptimizerRule { _ => RewriteDelete }\n-    extensions.injectPlannerStrategy { _ => ExtendedDataSourceV2Strategy }\n+    extensions.injectPlannerStrategy { spark => ExtendedDataSourceV2Strategy(spark) }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAwMjcxMg=="}, "originalCommit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMTk1OTk5OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DropPartitionFieldExec.scala", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODoyNToyNVrOIIt3qg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODoyNToyNVrOIIt3qg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAxMTA1MA==", "bodyText": "extra newline?", "url": "https://github.com/apache/iceberg/pull/1948#discussion_r546011050", "createdAt": "2020-12-18T18:25:25Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DropPartitionFieldExec.scala", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.iceberg.spark.Spark3Util\n+import org.apache.iceberg.spark.source.SparkTable\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.connector.catalog.Identifier\n+import org.apache.spark.sql.connector.catalog.TableCatalog\n+import org.apache.spark.sql.connector.expressions.FieldReference\n+import org.apache.spark.sql.connector.expressions.IdentityTransform\n+import org.apache.spark.sql.connector.expressions.Transform\n+\n+case class DropPartitionFieldExec(\n+    catalog: TableCatalog,\n+    ident: Identifier,\n+    transform: Transform) extends V2CommandExec {\n+  import org.apache.spark.sql.connector.catalog.CatalogV2Implicits._\n+\n+  override lazy val output: Seq[Attribute] = Nil\n+\n+  override protected def run(): Seq[InternalRow] = {\n+    catalog.loadTable(ident) match {\n+      case iceberg: SparkTable =>\n+        val schema = iceberg.table.schema\n+        transform match {\n+          case IdentityTransform(FieldReference(parts)) if parts.size == 1 && schema.findField(parts.head) == null =>\n+            // the name is not present in the Iceberg schema, so it must be a partition field name, not a column name\n+            iceberg.table.updateSpec()\n+                .removeField(parts.head)\n+                .commit()\n+\n+          case _ =>\n+            iceberg.table.updateSpec()\n+                .removeField(Spark3Util.toIcebergTerm(transform))\n+                .commit()\n+        }\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQzMTk5MzUzOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedDataSourceV2Strategy.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODozNToyOFrOIIuLzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODo0MTozOFrOIIuYNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAxNjIwNA==", "bodyText": "I don't think you need the inner paren's here", "url": "https://github.com/apache/iceberg/pull/1948#discussion_r546016204", "createdAt": "2020-12-18T18:35:28Z", "author": {"login": "RussellSpitzer"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedDataSourceV2Strategy.scala", "diffHunk": "@@ -53,4 +76,18 @@ object ExtendedDataSourceV2Strategy extends Strategy {\n     }\n     new GenericInternalRow(values)\n   }\n+\n+  private object IcebergCatalogAndIdentifier {\n+    def unapply(identifier: Seq[String]): Option[(TableCatalog, Identifier)] = {\n+      val catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier.asJava)\n+      catalogAndIdentifier.catalog match {\n+        case icebergCatalog: SparkCatalog =>\n+          Some((icebergCatalog, catalogAndIdentifier.identifier))\n+        case icebergCatalog: SparkSessionCatalog[_] =>\n+          Some((icebergCatalog, catalogAndIdentifier.identifier))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAxOTM4Mg==", "bodyText": "I'm pretty sure they are needed. I've hit issues in the past with this, at least in Scala 2.11.", "url": "https://github.com/apache/iceberg/pull/1948#discussion_r546019382", "createdAt": "2020-12-18T18:41:38Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedDataSourceV2Strategy.scala", "diffHunk": "@@ -53,4 +76,18 @@ object ExtendedDataSourceV2Strategy extends Strategy {\n     }\n     new GenericInternalRow(values)\n   }\n+\n+  private object IcebergCatalogAndIdentifier {\n+    def unapply(identifier: Seq[String]): Option[(TableCatalog, Identifier)] = {\n+      val catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, identifier.asJava)\n+      catalogAndIdentifier.catalog match {\n+        case icebergCatalog: SparkCatalog =>\n+          Some((icebergCatalog, catalogAndIdentifier.identifier))\n+        case icebergCatalog: SparkSessionCatalog[_] =>\n+          Some((icebergCatalog, catalogAndIdentifier.identifier))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAxNjIwNA=="}, "originalCommit": {"oid": "251fdf7ab61ea9e5e5f8f3da72b7c165abf274df"}, "originalPosition": 67}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3061, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}