{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk5NTY0MjM3", "number": 1563, "title": "Close source data iterator for Spark data reader: fixing timeout due to S3A connection pool", "bodyText": "This PR closes data iterator when it is exhausted in Spark base data reader.\nObservation: Using S3 store, coalesced read sees timeout on http pool connections from S3A. Using our internal metrics, we do see constant number of pending http pool connections. By contrast, reading directly to the underlying S3 data path with parquet read (not iceberg) has zero pending connections.\nCause: Base reader for Spark does not close data iterator when it is exhausted\nQuestion: Why does this not happen more commonly iceberg read (non-coaleseced)?\n\nIn Spark, the data reader will be closed after Spark task is finished as it is registered in task completion callback: https://git.corp.stripe.com/stripe-private-oss-forks/spark/blob/stripe-2.4.4/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceRDD.scala#L43\nHowever, if the Spark read task has multiple partitions, we will not close the last data iterator for each partition. Only when the whole task is complete, the iterators will be released. We have leakage during lifetime of a task.\nIn a normal read without coalescing, we have each read task corresponding each parquet file part. So when we finish reading a parquet file, we finish the task and close connection.\nCoalesced read reveals this edge case as each task is given more partitions to process.\n\n\nTesting: I only managed to add unit tests for iteration logic here, but internally we have s3 mock which verified that this fix resolves the timeout issue.\nI have checked the other two S3A timeout issues #150 and #1474 and on mailing list, and believe that this is a distinct issue.\n\ncc @rdblue", "createdAt": "2020-10-07T23:08:23Z", "url": "https://github.com/apache/iceberg/pull/1563", "merged": true, "mergeCommit": {"oid": "25bc2b6ab749fdd5c753ff9e794a5262c130f361"}, "closed": true, "closedAt": "2020-10-10T21:13:28Z", "author": {"login": "mickjermsurawong-stripe"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdQU_IigH2gAyNDk5NTY0MjM3OjhhYzgwYTNlNmZjYmI2YzNhNmEzMDk5NTE5MmRlOTRlZWU3NGE1ZDE=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdQWvg7AFqTUwNDM2MDkwNg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "8ac80a3e6fcbb6c3a6a30995192de94eee74a5d1", "author": {"user": {"login": "mickjermsurawong-stripe", "name": null}}, "url": "https://github.com/apache/iceberg/commit/8ac80a3e6fcbb6c3a6a30995192de94eee74a5d1", "committedDate": "2020-10-07T22:47:05Z", "message": "close base data reader"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "83f5cc54ca026bdc6740780fa61713b4063353d6", "author": {"user": {"login": "mickjermsurawong-stripe", "name": null}}, "url": "https://github.com/apache/iceberg/commit/83f5cc54ca026bdc6740780fa61713b4063353d6", "committedDate": "2020-10-07T22:52:32Z", "message": "fix test style"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0MzI5NTQx", "url": "https://github.com/apache/iceberg/pull/1563#pullrequestreview-504329541", "createdAt": "2020-10-07T23:12:10Z", "commit": {"oid": "83f5cc54ca026bdc6740780fa61713b4063353d6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMzoxMjoxMFrOHeIqzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMzoxMjoxMFrOHeIqzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM2MTM1Ng==", "bodyText": "Looks correct to me.", "url": "https://github.com/apache/iceberg/pull/1563#discussion_r501361356", "createdAt": "2020-10-07T23:12:10Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseDataReader.java", "diffHunk": "@@ -84,6 +84,7 @@ public boolean next() throws IOException {\n         this.currentIterator.close();\n         this.currentIterator = open(tasks.next());\n       } else {\n+        this.currentIterator.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "83f5cc54ca026bdc6740780fa61713b4063353d6"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0MzMwMjEw", "url": "https://github.com/apache/iceberg/pull/1563#pullrequestreview-504330210", "createdAt": "2020-10-07T23:13:53Z", "commit": {"oid": "83f5cc54ca026bdc6740780fa61713b4063353d6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMzoxMzo1NFrOHeIsuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMzoxMzo1NFrOHeIsuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM2MTg0OA==", "bodyText": "Can you remove this? There is no need to modify this file.", "url": "https://github.com/apache/iceberg/pull/1563#discussion_r501361848", "createdAt": "2020-10-07T23:13:54Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java", "diffHunk": "@@ -101,6 +101,7 @@ protected Record writeAndRead(String desc, Schema writeSchema, Schema readSchema\n     File testFile = new File(dataFolder, format.addExtension(UUID.randomUUID().toString()));\n \n     Table table = TestTables.create(location, desc, writeSchema, PartitionSpec.unpartitioned());\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "83f5cc54ca026bdc6740780fa61713b4063353d6"}, "originalPosition": 4}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a79bc20c68fe9765e4498c62b6034a550d9d6ecd", "author": {"user": {"login": "mickjermsurawong-stripe", "name": null}}, "url": "https://github.com/apache/iceberg/commit/a79bc20c68fe9765e4498c62b6034a550d9d6ecd", "committedDate": "2020-10-07T23:17:08Z", "message": "remove line"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9cca21df5be3dc6f14454ea063397173ee6f40a7", "author": {"user": {"login": "mickjermsurawong-stripe", "name": null}}, "url": "https://github.com/apache/iceberg/commit/9cca21df5be3dc6f14454ea063397173ee6f40a7", "committedDate": "2020-10-07T23:22:13Z", "message": "fix test compile"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "57efc0132aad1d53fe29d316b6ef23e3b91121e7", "author": {"user": {"login": "mickjermsurawong-stripe", "name": null}}, "url": "https://github.com/apache/iceberg/commit/57efc0132aad1d53fe29d316b6ef23e3b91121e7", "committedDate": "2020-10-07T23:17:19Z", "message": "fix test compile"}, "afterCommit": {"oid": "9cca21df5be3dc6f14454ea063397173ee6f40a7", "author": {"user": {"login": "mickjermsurawong-stripe", "name": null}}, "url": "https://github.com/apache/iceberg/commit/9cca21df5be3dc6f14454ea063397173ee6f40a7", "committedDate": "2020-10-07T23:22:13Z", "message": "fix test compile"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0MzM0MTcx", "url": "https://github.com/apache/iceberg/pull/1563#pullrequestreview-504334171", "createdAt": "2020-10-07T23:24:55Z", "commit": {"oid": "83f5cc54ca026bdc6740780fa61713b4063353d6"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMzoyNDo1NVrOHeI66Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMzoyNDo1NVrOHeI66Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM2NTQ4MQ==", "bodyText": "I think this is a good test to have, but I'm a little concerned about the number of classes it takes.\nHave you considered an alternative structure? What about using a spy object that is injected the same way, in the ClosureTrackingReader.open method? Then you'd be able to use normal file splits and planning with TableTestBase, and you'd be able to keep track of all the spies in the reader you instantiate.\nI think that would get rid of a few of these static classes, which would make this easier to understand and maintain.", "url": "https://github.com/apache/iceberg/pull/1563#discussion_r501365481", "createdAt": "2020-10-07T23:24:55Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkBaseDataReader.java", "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.BaseCombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.PlaintextEncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public abstract class TestSparkBaseDataReader {\n+\n+  private static final Configuration CONF = new Configuration();\n+\n+  // Simulates the closeable iterator of data to be read\n+  private static class CloseableIntegerRange implements CloseableIterator<Integer> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "83f5cc54ca026bdc6740780fa61713b4063353d6"}, "originalPosition": 49}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8c036461bb28f5def37a2d0aec430d0f79df1377", "author": {"user": {"login": "mickjermsurawong-stripe", "name": null}}, "url": "https://github.com/apache/iceberg/commit/8c036461bb28f5def37a2d0aec430d0f79df1377", "committedDate": "2020-10-08T00:39:34Z", "message": "reduce helper class"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bb498f1f88d4ae9ba38281a123c666c838e3b66d", "author": {"user": {"login": "mickjermsurawong-stripe", "name": null}}, "url": "https://github.com/apache/iceberg/commit/bb498f1f88d4ae9ba38281a123c666c838e3b66d", "committedDate": "2020-10-08T00:38:05Z", "message": "reduce helper class"}, "afterCommit": {"oid": "8c036461bb28f5def37a2d0aec430d0f79df1377", "author": {"user": {"login": "mickjermsurawong-stripe", "name": null}}, "url": "https://github.com/apache/iceberg/commit/8c036461bb28f5def37a2d0aec430d0f79df1377", "committedDate": "2020-10-08T00:39:34Z", "message": "reduce helper class"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA0MzYwOTA2", "url": "https://github.com/apache/iceberg/pull/1563#pullrequestreview-504360906", "createdAt": "2020-10-08T00:49:49Z", "commit": {"oid": "8c036461bb28f5def37a2d0aec430d0f79df1377"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQwMDo0OTo1MFrOHeKXZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQwMDo0OTo1MFrOHeKXZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTM4OTE1OA==", "bodyText": "This method relies on planFiles contract that each file corresponds to one scan task, to get desired number of total task argument", "url": "https://github.com/apache/iceberg/pull/1563#discussion_r501389158", "createdAt": "2020-10-08T00:49:50Z", "author": {"login": "mickjermsurawong-stripe"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkBaseDataReader.java", "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import java.util.stream.StreamSupport;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.BaseCombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.PlaintextEncryptionManager;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.data.RandomData;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.FileFormat.PARQUET;\n+import static org.apache.iceberg.Files.localOutput;\n+\n+public abstract class TestSparkBaseDataReader {\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private static final Configuration CONFD = new Configuration();\n+\n+  // Simulates the closeable iterator of data to be read\n+  private static class CloseableIntegerRange implements CloseableIterator<Integer> {\n+    boolean closed;\n+    Iterator<Integer> iter;\n+\n+    CloseableIntegerRange(long range) {\n+      this.closed = false;\n+      this.iter = IntStream.range(0, (int) range).iterator();\n+    }\n+\n+    @Override\n+    public void close() {\n+      this.closed = true;\n+    }\n+\n+    @Override\n+    public boolean hasNext() {\n+      return iter.hasNext();\n+    }\n+\n+    @Override\n+    public Integer next() {\n+      return iter.next();\n+    }\n+  }\n+\n+  // Main reader class to test base class iteration logic.\n+  // Keeps track of iterator closure.\n+  private static class ClosureTrackingReader extends BaseDataReader<Integer> {\n+    private Map<String, CloseableIntegerRange> tracker = new HashMap<>();\n+\n+    ClosureTrackingReader(List<FileScanTask> tasks) {\n+      super(new BaseCombinedScanTask(tasks),\n+          new HadoopFileIO(CONFD),\n+          new PlaintextEncryptionManager());\n+    }\n+\n+    @Override\n+    CloseableIterator<Integer> open(FileScanTask task) {\n+      CloseableIntegerRange intRange = new CloseableIntegerRange(task.file().recordCount());\n+      tracker.put(getKey(task), intRange);\n+      return intRange;\n+    }\n+\n+    public Boolean isIteratorClosed(FileScanTask task) {\n+      return tracker.get(getKey(task)).closed;\n+    }\n+\n+    public Boolean hasIterator(FileScanTask task) {\n+      return tracker.containsKey(getKey(task));\n+    }\n+\n+    private String getKey(FileScanTask task) {\n+      return task.file().path().toString();\n+    }\n+  }\n+\n+  @Test\n+  public void testClosureOnDataExhaustion() throws IOException {\n+    Integer totalTasks = 10;\n+    Integer recordPerTask = 10;\n+    List<FileScanTask> tasks = createFileScanTasks(totalTasks, recordPerTask);\n+\n+    ClosureTrackingReader reader = new ClosureTrackingReader(tasks);\n+\n+    int countRecords = 0;\n+    while (reader.next()) {\n+      countRecords += 1;\n+      Assert.assertNotNull(\"Reader should return non-null value\", reader.get());\n+    }\n+\n+    Assert.assertEquals(\"Reader returned incorrect number of records\",\n+        totalTasks * recordPerTask,\n+        countRecords\n+    );\n+    tasks.forEach(t ->\n+        Assert.assertTrue(\"All iterators should be closed after read exhausion\",\n+            reader.isIteratorClosed(t))\n+    );\n+  }\n+\n+  @Test\n+  public void testClosureDuringIteration() throws IOException {\n+    Integer totalTasks = 2;\n+    Integer recordPerTask = 1;\n+    List<FileScanTask> tasks = createFileScanTasks(totalTasks, recordPerTask);\n+    Assert.assertEquals(2, tasks.size());\n+    FileScanTask firstTask = tasks.get(0);\n+    FileScanTask secondTask = tasks.get(1);\n+\n+    ClosureTrackingReader reader = new ClosureTrackingReader(tasks);\n+\n+    // Total of 2 elements\n+    Assert.assertTrue(reader.next());\n+    Assert.assertFalse(\"First iter should not be closed on its last element\",\n+        reader.isIteratorClosed(firstTask));\n+\n+    Assert.assertTrue(reader.next());\n+    Assert.assertTrue(\"First iter should be closed after moving to second iter\",\n+        reader.isIteratorClosed(firstTask));\n+    Assert.assertFalse(\"Second iter should not be closed on its last element\",\n+        reader.isIteratorClosed(secondTask));\n+\n+    Assert.assertFalse(reader.next());\n+    Assert.assertTrue(reader.isIteratorClosed(firstTask));\n+    Assert.assertTrue(reader.isIteratorClosed(secondTask));\n+  }\n+\n+  @Test\n+  public void testClosureWithoutAnyRead() throws IOException {\n+    Integer totalTasks = 10;\n+    Integer recordPerTask = 10;\n+    List<FileScanTask> tasks = createFileScanTasks(totalTasks, recordPerTask);\n+\n+    ClosureTrackingReader reader = new ClosureTrackingReader(tasks);\n+\n+    reader.close();\n+\n+    tasks.forEach(t ->\n+        Assert.assertFalse(\"Iterator should not be created eagerly for tasks\",\n+            reader.hasIterator(t))\n+    );\n+  }\n+\n+  @Test\n+  public void testExplicitClosure() throws IOException {\n+    Integer totalTasks = 10;\n+    Integer recordPerTask = 10;\n+    List<FileScanTask> tasks = createFileScanTasks(totalTasks, recordPerTask);\n+\n+    ClosureTrackingReader reader = new ClosureTrackingReader(tasks);\n+\n+    Integer halfDataSize = (totalTasks * recordPerTask) / 2;\n+    for (int i = 0; i < halfDataSize; i++) {\n+      Assert.assertTrue(\"Reader should have some element\", reader.next());\n+      Assert.assertNotNull(\"Reader should return non-null value\", reader.get());\n+    }\n+\n+    reader.close();\n+\n+    // Some tasks might have not been opened yet, so we don't have corresponding tracker for it.\n+    // But all that have been created must be closed.\n+    tasks.forEach(t -> {\n+      if (reader.hasIterator(t)) {\n+        Assert.assertTrue(\"Iterator should be closed after read exhausion\",\n+            reader.isIteratorClosed(t));\n+      }\n+    });\n+  }\n+\n+  @Test\n+  public void testIdempotentExplicitClosure() throws IOException {\n+    Integer totalTasks = 10;\n+    Integer recordPerTask = 10;\n+    List<FileScanTask> tasks = createFileScanTasks(totalTasks, recordPerTask);\n+\n+    ClosureTrackingReader reader = new ClosureTrackingReader(tasks);\n+\n+    // Total 100 elements, only 5 iterators have been created\n+    for (int i = 0; i < 45; i++) {\n+      Assert.assertTrue(\"eader should have some element\", reader.next());\n+      Assert.assertNotNull(\"Reader should return non-null value\", reader.get());\n+    }\n+\n+    for (int closeAttempt = 0; closeAttempt < 5; closeAttempt++) {\n+      reader.close();\n+      for (int i = 0; i < 5; i++) {\n+        Assert.assertTrue(\"Iterator should be closed after read exhausion\",\n+            reader.isIteratorClosed(tasks.get(i)));\n+      }\n+      for (int i = 5; i < 10; i++) {\n+        Assert.assertFalse(\"Iterator should not be created eagerly for tasks\",\n+            reader.hasIterator(tasks.get(i)));\n+      }\n+    }\n+  }\n+\n+  private List<FileScanTask> createFileScanTasks(Integer totalTasks, Integer recordPerTask) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8c036461bb28f5def37a2d0aec430d0f79df1377"}, "originalPosition": 241}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3910, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}