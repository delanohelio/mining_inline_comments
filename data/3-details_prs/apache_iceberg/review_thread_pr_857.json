{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzkxNjkxNjA1", "number": 857, "reviewThreads": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTozMzoyNFrODrCEVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOVQxNzoyNDo0NlrODscP3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NDQ5MjM3OnYy", "diffSide": "RIGHT", "path": "orc/src/main/java/org/apache/iceberg/orc/OrcFileAppender.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTozMzoyNFrOF7JI6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTozMzoyNFrOF7JI6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU2MDA0Mg==", "bodyText": "Why comment out this?", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397560042", "createdAt": "2020-03-25T01:33:24Z", "author": {"login": "chenjunjiedada"}, "path": "orc/src/main/java/org/apache/iceberg/orc/OrcFileAppender.java", "diffHunk": "@@ -96,8 +96,8 @@ public Metrics metrics() {\n \n   @Override\n   public long length() {\n-    Preconditions.checkState(isClosed,\n-        \"Cannot return length while appending to an open file.\");\n+//    Preconditions.checkState(isClosed,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1765d3c2249f9e44dec8f9cd518184701a842b39"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NDQ5NDgzOnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/NestedRecord.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTozNDoyNFrOF7JKOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTozNDoyNFrOF7JKOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU2MDM3OQ==", "bodyText": "Please add apache license", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397560379", "createdAt": "2020-03-25T01:34:24Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/NestedRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+package org.apache.iceberg.spark.source;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1765d3c2249f9e44dec8f9cd518184701a842b39"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NDQ5OTAzOnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/SimpleRecord.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTozNjozM1rOF7JMkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTozNjozM1rOF7JMkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU2MDk3Nw==", "bodyText": "Please remove these unrelated changes.", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397560977", "createdAt": "2020-03-25T01:36:33Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/SimpleRecord.java", "diffHunk": "@@ -20,6 +20,9 @@\n package org.apache.iceberg.spark.source;\n \n import com.google.common.base.Objects;\n+import com.oracle.tools.packager.mac.MacAppBundler;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1765d3c2249f9e44dec8f9cd518184701a842b39"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NDUwNDk3OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestORCWrite.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTozOTo0N1rOF7JP6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTozOTo0N1rOF7JP6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU2MTgzNA==", "bodyText": "The Iceberg code style doesn't allow using the wildcard import. You might want to change your IDE setting to disable the import optimization.", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397561834", "createdAt": "2020-03-25T01:39:47Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestORCWrite.java", "diffHunk": "@@ -0,0 +1,99 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.*;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1765d3c2249f9e44dec8f9cd518184701a842b39"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NDUwNTM2OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestORCWrite.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTozOTo1NlrOF7JQGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTozOTo1NlrOF7JQGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU2MTg4MQ==", "bodyText": "ditto.", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397561881", "createdAt": "2020-03-25T01:39:56Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestORCWrite.java", "diffHunk": "@@ -0,0 +1,99 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.*;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.*;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1765d3c2249f9e44dec8f9cd518184701a842b39"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NDUwNzA4OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestORCWrite.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTo0MToxMVrOF7JRJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTo0MToxMVrOF7JRJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU2MjE1MQ==", "bodyText": "Please sort the import alphabetically.", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397562151", "createdAt": "2020-03-25T01:41:11Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestORCWrite.java", "diffHunk": "@@ -0,0 +1,99 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.*;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.*;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.File;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1765d3c2249f9e44dec8f9cd518184701a842b39"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NDUwOTU2OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestORCWrite.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTo0MjozOFrOF7JSgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTo0MjozOFrOF7JSgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU2MjQ5Nw==", "bodyText": "unnecessary blank line.", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397562497", "createdAt": "2020-03-25T01:42:38Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestORCWrite.java", "diffHunk": "@@ -0,0 +1,99 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.*;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.*;\n+import org.junit.rules.TemporaryFolder;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestORCWrite {\n+  private static final Configuration CONF = new Configuration();\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"id\", Types.IntegerType.get()),\n+      optional(2, \"data\", Types.StringType.get()),\n+      optional(3, \"info\", Types.MapType.ofOptional(4,5,Types.StringType.get(),Types.StringType.get()))\n+  );\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private static SparkSession spark = null;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestORCWrite.spark = SparkSession.builder().master(\"local[1]\").getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestORCWrite.spark;\n+    TestORCWrite.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Test\n+  public void testBasicWrite() throws IOException {\n+    File parent = temp.newFolder(\"orc\");\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+    Map<String, String> info = ImmutableMap.of(\"a\", \"A\", \"b\", \"B\");\n+\n+    List<NestedRecord> expected = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"a\", info),\n+        new NestedRecord(3, \"b\", info),\n+        new NestedRecord(4, \"b\", info),\n+        new NestedRecord(5, \"c\", info),\n+        new NestedRecord(6, \"c\", info),\n+        new NestedRecord(7, \"d\", info),\n+        new NestedRecord(8, \"d\", info),\n+        new NestedRecord(9, \"e\", info),\n+        new NestedRecord(10, \"e\", info),\n+        new NestedRecord(11, \"f\", info),\n+        new NestedRecord(12, \"f\", info)\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, NestedRecord.class);\n+\n+    // TODO: incoming columns must be ordered according to the table's schema\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", \"orc\")\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1765d3c2249f9e44dec8f9cd518184701a842b39"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NDUxMDg0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Writer.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTo0MzoyNFrOF7JTPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMTo0MzoyNFrOF7JTPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU2MjY4NA==", "bodyText": "Please add a blank line before this.", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397562684", "createdAt": "2020-03-25T01:43:24Z", "author": {"login": "chenjunjiedada"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -308,6 +310,13 @@ public String toString() {\n                   .schema(dsSchema)\n                   .overwrite()\n                   .build();\n+            case ORC:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1765d3c2249f9e44dec8f9cd518184701a842b39"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NDczMzcyOnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/NestedRecord.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMzo1NTowNlrOF7LWBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMzo1NTowNlrOF7LWBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU5NjE2Nw==", "bodyText": "remove commented code", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397596167", "createdAt": "2020-03-25T03:55:06Z", "author": {"login": "rdsr"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/NestedRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Objects;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class NestedRecord {\n+  private Integer id;\n+  private String data;\n+  private Map info;// = new HashMap<String, String>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1765d3c2249f9e44dec8f9cd518184701a842b39"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NDczNzcwOnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/NestedRecord.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMzo1Nzo1MVrOF7LYeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMzo1Nzo1MVrOF7LYeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU5Njc5Mg==", "bodyText": "this shouldn't be here?", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397596792", "createdAt": "2020-03-25T03:57:51Z", "author": {"login": "rdsr"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/NestedRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Objects;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class NestedRecord {\n+  private Integer id;\n+  private String data;\n+  private Map info;// = new HashMap<String, String>();\n+\n+  public NestedRecord() {\n+  }\n+\n+  NestedRecord(Integer id, String data, Map<String, String> info) {\n+    this.id = id;\n+    this.data = data;\n+    this.info = info;\n+  }\n+\n+  public Integer getId() {\n+    return id;\n+  }\n+\n+  public void setId(Integer id) {\n+    this.id = id;\n+  }\n+\n+  public String getData() {\n+    return data;\n+  }\n+\n+  public void setData(String data) {\n+    this.data = data;\n+  }\n+\n+  public Map<String, String> getInfo() {\n+    return info;\n+  }\n+\n+  public void setInfo(Map<String, String> info) {\n+    this.info = info;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+\n+    NestedRecord record = (NestedRecord) o;\n+    return Objects.equal(id, record.id) && Objects.equal(data, record.data) && Objects.equal(info, record.info);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return Objects.hashCode(id, data, info);\n+  }\n+\n+  @Override\n+  public String toString() {\n+    StringBuilder buffer = new StringBuilder();\n+    buffer.append(\"{\\\"id\\\"=\");\n+    buffer.append(id);\n+    buffer.append(\",\\\"data\\\"=\\\"\");\n+    buffer.append(data);\n+    buffer.append(\"\\\"}\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1765d3c2249f9e44dec8f9cd518184701a842b39"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NDc0MTEyOnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/NestedRecord.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMzo1OTo1N1rOF7LaYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwNToyNzo1N1rOF7Mlww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU5NzI4MQ==", "bodyText": "Q. Why did we create a new class, why not SimpleRecord?", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397597281", "createdAt": "2020-03-25T03:59:57Z", "author": {"login": "rdsr"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/NestedRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Objects;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class NestedRecord {\n+  private Integer id;\n+  private String data;\n+  private Map info;// = new HashMap<String, String>();\n+\n+  public NestedRecord() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1765d3c2249f9e44dec8f9cd518184701a842b39"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYxNjU3OQ==", "bodyText": "I think, ORC file not support Nested type, so I want to confirmed that could work correctly", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397616579", "createdAt": "2020-03-25T05:27:57Z", "author": {"login": "XiaokunDing"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/NestedRecord.java", "diffHunk": "@@ -0,0 +1,77 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Objects;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+public class NestedRecord {\n+  private Integer id;\n+  private String data;\n+  private Map info;// = new HashMap<String, String>();\n+\n+  public NestedRecord() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU5NzI4MQ=="}, "originalCommit": {"oid": "1765d3c2249f9e44dec8f9cd518184701a842b39"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2NDc0ODE3OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestORCWrite.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwNDowNDo1M1rOF7LeSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwNTozMToxNlrOF7Mo4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU5ODI4Mw==", "bodyText": "@shawnding  what are your thoughts on making the class org.apache.iceberg.spark.source.TestParquetWrite parameterized and making it work for both Parquet and ORC. Seems like that's less work and it also tests with a lot more test cases. Thoughts?\nIf you want to see some examples of parameterized tests, please see org.apache.iceberg.data.TestLocalScan", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397598283", "createdAt": "2020-03-25T04:04:53Z", "author": {"login": "rdsr"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestORCWrite.java", "diffHunk": "@@ -0,0 +1,99 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1765d3c2249f9e44dec8f9cd518184701a842b39"}, "originalPosition": 3}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYxNzM3Nw==", "bodyText": "Ok thanks Rdsr, that I will rewrite it.", "url": "https://github.com/apache/iceberg/pull/857#discussion_r397617377", "createdAt": "2020-03-25T05:31:16Z", "author": {"login": "XiaokunDing"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestORCWrite.java", "diffHunk": "@@ -0,0 +1,99 @@\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzU5ODI4Mw=="}, "originalCommit": {"oid": "1765d3c2249f9e44dec8f9cd518184701a842b39"}, "originalPosition": 3}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3OTI2MDY4OnYy", "diffSide": "LEFT", "path": "orc/src/main/java/org/apache/iceberg/orc/ORCSchemaUtil.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOVQxNzoxNzo1MVrOF9TekA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMVQyMjowMDo0MlrOF-p0tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNjU3Ng==", "bodyText": "Why was this change required?", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399826576", "createdAt": "2020-03-29T17:17:51Z", "author": {"login": "rdsr"}, "path": "orc/src/main/java/org/apache/iceberg/orc/ORCSchemaUtil.java", "diffHunk": "@@ -269,7 +269,7 @@ private static TypeDescription buildOrcProjection(Integer fieldId, Type type, bo\n         break;\n       case MAP:\n         Types.MapType map = (Types.MapType) type;\n-        TypeDescription keyType = buildOrcProjection(map.keyId(), map.keyType(), true, mapping);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg5MTQ2Mg==", "bodyText": "While a MapType  column is optional if insert an  empty map like null,  the key and value  both are null", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399891462", "createdAt": "2020-03-30T01:58:02Z", "author": {"login": "XiaokunDing"}, "path": "orc/src/main/java/org/apache/iceberg/orc/ORCSchemaUtil.java", "diffHunk": "@@ -269,7 +269,7 @@ private static TypeDescription buildOrcProjection(Integer fieldId, Type type, bo\n         break;\n       case MAP:\n         Types.MapType map = (Types.MapType) type;\n-        TypeDescription keyType = buildOrcProjection(map.keyId(), map.keyType(), true, mapping);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNjU3Ng=="}, "originalCommit": {"oid": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTI0MTI3MA==", "bodyText": "Map keys are always required. Does this make the map key optional?", "url": "https://github.com/apache/iceberg/pull/857#discussion_r401241270", "createdAt": "2020-03-31T22:00:42Z", "author": {"login": "rdblue"}, "path": "orc/src/main/java/org/apache/iceberg/orc/ORCSchemaUtil.java", "diffHunk": "@@ -269,7 +269,7 @@ private static TypeDescription buildOrcProjection(Integer fieldId, Type type, bo\n         break;\n       case MAP:\n         Types.MapType map = (Types.MapType) type;\n-        TypeDescription keyType = buildOrcProjection(map.keyId(), map.keyType(), true, mapping);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNjU3Ng=="}, "originalCommit": {"oid": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3OTI2MzA5OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Writer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOVQxNzoyMDoxMlrOF9Tfsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMjoxNjowNlrOF9Xoww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNjg2Nw==", "bodyText": "What exception you see if when running with ORC? I would have thought that it should work fine.\nAlso I think it is  disabled for Avro also now.", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399826867", "createdAt": "2020-03-29T17:20:12Z", "author": {"login": "rdsr"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -389,7 +399,9 @@ public EncryptedOutputFile newOutputFile(PartitionKey key) {\n     public abstract void write(InternalRow row) throws IOException;\n \n     public void writeInternal(InternalRow row)  throws IOException {\n-      if (currentRows % ROWS_DIVISOR == 0 && currentAppender.length() >= targetFileSize) {\n+      //TODO: ORC file now not support target file size", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg5NDcyMw==", "bodyText": "Yes in Avro file is work,  cannot get the file length before closed the  ORC file, I have fix it.", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399894723", "createdAt": "2020-03-30T02:16:06Z", "author": {"login": "XiaokunDing"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Writer.java", "diffHunk": "@@ -389,7 +399,9 @@ public EncryptedOutputFile newOutputFile(PartitionKey key) {\n     public abstract void write(InternalRow row) throws IOException;\n \n     public void writeInternal(InternalRow row)  throws IOException {\n-      if (currentRows % ROWS_DIVISOR == 0 && currentAppender.length() >= targetFileSize) {\n+      //TODO: ORC file now not support target file size", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNjg2Nw=="}, "originalCommit": {"oid": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3OTI2Mzc0OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOVQxNzoyMTowMFrOF9TgDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMjoyMDoyN1rOF9XsBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNjk1OQ==", "bodyText": "nit: would it be easier to address this change as a separate PR. Seems un-related to current work", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399826959", "createdAt": "2020-03-29T17:21:00Z", "author": {"login": "rdsr"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReader.java", "diffHunk": "@@ -49,7 +49,7 @@ protected void writeAndValidate(Schema schema) throws IOException {\n     }\n \n     try (CloseableIterable<InternalRow> reader = ORC.read(Files.localInput(testFile))\n-        .schema(schema)\n+        .project(schema)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg5NTU1OQ==", "bodyText": "OK I split it to another PR", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399895559", "createdAt": "2020-03-30T02:20:27Z", "author": {"login": "XiaokunDing"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReader.java", "diffHunk": "@@ -49,7 +49,7 @@ protected void writeAndValidate(Schema schema) throws IOException {\n     }\n \n     try (CloseableIterable<InternalRow> reader = ORC.read(Files.localInput(testFile))\n-        .schema(schema)\n+        .project(schema)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNjk1OQ=="}, "originalCommit": {"oid": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3OTI2NTI0OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOVQxNzoyMjo0MFrOF9Tg2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMTo0MzoyN1rOF9XR0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNzE2MA==", "bodyText": "Does this not work for Avro as well?", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399827160", "createdAt": "2020-03-29T17:22:40Z", "author": {"login": "rdsr"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java", "diffHunk": "@@ -0,0 +1,457 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestReader;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+@RunWith(Parameterized.class)\n+public class TestSparkDataWrite {\n+  private static final Configuration CONF = new Configuration();\n+  private final FileFormat format;\n+  private static SparkSession spark = null;\n+  private static final Map<String, String> info = ImmutableMap.of(\"a\", \"A\", \"b\", \"B\");\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"id\", Types.IntegerType.get()),\n+      optional(2, \"data\", Types.StringType.get()),\n+      optional(3, \"info\", Types.MapType.ofOptional(\n+          4, 5, Types.StringType.get(), Types.StringType.get()))\n+  );\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { \"parquet\" },\n+        new Object[] { \"avro\" },\n+        new Object[] { \"orc\" }\n+    };\n+  }\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestSparkDataWrite.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestSparkDataWrite.spark;\n+    TestSparkDataWrite.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  public TestSparkDataWrite(String format) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Test\n+  public void testBasicWrite() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    List<NestedRecord> expected = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"b\", info),\n+        new NestedRecord(3, \"c\", info)\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, NestedRecord.class);\n+    // TODO: incoming columns must be ordered according to the table's schema\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+    for (ManifestFile manifest : table.currentSnapshot().manifests()) {\n+      for (DataFile file : ManifestReader.read(manifest, table.io())) {\n+        // TODO: avro not support split\n+        if (!format.equals(FileFormat.AVRO)) {\n+          Assert.assertNotNull(\"Split offsets not present\", file.splitOffsets());\n+        }\n+        Assert.assertEquals(\"Should have reported record count as 1\", 1, file.recordCount());\n+        //TODO: append more metric info\n+        if (format.equals(FileFormat.PARQUET)) {\n+          Assert.assertNotNull(\"Column sizes metric not present\", file.columnSizes());\n+          Assert.assertNotNull(\"Counts metric not present\", file.valueCounts());\n+          Assert.assertNotNull(\"Null value counts metric not present\", file.nullValueCounts());\n+          Assert.assertNotNull(\"Lower bounds metric not present\", file.lowerBounds());\n+          Assert.assertNotNull(\"Upper bounds metric not present\", file.upperBounds());\n+        }\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testAppend() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    List<NestedRecord> records = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"b\", info),\n+        new NestedRecord(3, \"c\", info)\n+    );\n+\n+    List<NestedRecord> expected = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"b\", info),\n+        new NestedRecord(3, \"c\", info),\n+        new NestedRecord(4, \"a\", info),\n+        new NestedRecord(5, \"b\", info),\n+        new NestedRecord(6, \"c\", info)\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(records, NestedRecord.class);\n+\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    df.withColumn(\"id\", df.col(\"id\").plus(3)).select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+  }\n+\n+  @Test\n+  public void testOverwrite() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"id\").build();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    List<NestedRecord> records = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"b\", info),\n+        new NestedRecord(3, \"c\", info)\n+    );\n+\n+    List<NestedRecord> expected = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"a\", info),\n+        new NestedRecord(3, \"c\", info),\n+        new NestedRecord(4, \"b\", info),\n+        new NestedRecord(6, \"c\", info)\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(records, NestedRecord.class);\n+\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    // overwrite with 2*id to replace record 2, append 4 and 6\n+    df.withColumn(\"id\", df.col(\"id\").multiply(2)).select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"overwrite\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+  }\n+\n+  @Test\n+  public void testUnpartitionedOverwrite() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    List<NestedRecord> expected = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"b\", info),\n+        new NestedRecord(3, \"c\", info)\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, NestedRecord.class);\n+\n+    df.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    // overwrite with the same data; should not produce two copies\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"overwrite\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+  }\n+\n+  @Test\n+  public void testUnpartitionedCreateWithTargetFileSizeViaTableProperties() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    table.updateProperties()\n+        .set(TableProperties.WRITE_TARGET_FILE_SIZE_BYTES, \"4\") // ~4 bytes; low enough to trigger\n+        .commit();\n+\n+    List<NestedRecord> expected = Lists.newArrayListWithCapacity(4000);\n+    for (int i = 0; i < 4000; i++) {\n+      expected.add(new NestedRecord(i, \"a\", info));\n+    }\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, NestedRecord.class);\n+\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+\n+    List<DataFile> files = Lists.newArrayList();\n+    for (ManifestFile manifest : table.currentSnapshot().manifests()) {\n+      for (DataFile file : ManifestReader.read(manifest, table.io())) {\n+        files.add(file);\n+      }\n+    }\n+    // TODO: ORC file now not support target file size\n+    if (format.equals(FileFormat.PARQUET)) {\n+      Assert.assertEquals(\"Should have 4 DataFiles\", 4, files.size());\n+      Assert.assertTrue(\"All DataFiles contain 1000 rows\", files.stream().allMatch(d -> d.recordCount() == 1000));\n+    }\n+  }\n+\n+  @Test\n+  public void testPartitionedCreateWithTargetFileSizeViaOption() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    List<NestedRecord> expected = Lists.newArrayListWithCapacity(8000);\n+    for (int i = 0; i < 2000; i++) {\n+      expected.add(new NestedRecord(i, \"a\", info));\n+      expected.add(new NestedRecord(i, \"b\", info));\n+      expected.add(new NestedRecord(i, \"c\", info));\n+      expected.add(new NestedRecord(i, \"d\", info));\n+    }\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, NestedRecord.class);\n+\n+    df.select(\"id\", \"data\", \"info\").sort(\"data\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .option(\"target-file-size-bytes\", 4) // ~4 bytes; low enough to trigger\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+\n+    List<DataFile> files = Lists.newArrayList();\n+    for (ManifestFile manifest : table.currentSnapshot().manifests()) {\n+      for (DataFile file : ManifestReader.read(manifest, table.io())) {\n+        files.add(file);\n+      }\n+    }\n+    // TODO: ORC file now not support target file size\n+    if (format.equals(FileFormat.PARQUET)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe"}, "originalPosition": 379}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4ODg1MQ==", "bodyText": "Yes now target-file-size-byte only  not work on Orc file. I fix it", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399888851", "createdAt": "2020-03-30T01:43:27Z", "author": {"login": "XiaokunDing"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java", "diffHunk": "@@ -0,0 +1,457 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestReader;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+@RunWith(Parameterized.class)\n+public class TestSparkDataWrite {\n+  private static final Configuration CONF = new Configuration();\n+  private final FileFormat format;\n+  private static SparkSession spark = null;\n+  private static final Map<String, String> info = ImmutableMap.of(\"a\", \"A\", \"b\", \"B\");\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"id\", Types.IntegerType.get()),\n+      optional(2, \"data\", Types.StringType.get()),\n+      optional(3, \"info\", Types.MapType.ofOptional(\n+          4, 5, Types.StringType.get(), Types.StringType.get()))\n+  );\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { \"parquet\" },\n+        new Object[] { \"avro\" },\n+        new Object[] { \"orc\" }\n+    };\n+  }\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestSparkDataWrite.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestSparkDataWrite.spark;\n+    TestSparkDataWrite.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  public TestSparkDataWrite(String format) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Test\n+  public void testBasicWrite() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    List<NestedRecord> expected = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"b\", info),\n+        new NestedRecord(3, \"c\", info)\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, NestedRecord.class);\n+    // TODO: incoming columns must be ordered according to the table's schema\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+    for (ManifestFile manifest : table.currentSnapshot().manifests()) {\n+      for (DataFile file : ManifestReader.read(manifest, table.io())) {\n+        // TODO: avro not support split\n+        if (!format.equals(FileFormat.AVRO)) {\n+          Assert.assertNotNull(\"Split offsets not present\", file.splitOffsets());\n+        }\n+        Assert.assertEquals(\"Should have reported record count as 1\", 1, file.recordCount());\n+        //TODO: append more metric info\n+        if (format.equals(FileFormat.PARQUET)) {\n+          Assert.assertNotNull(\"Column sizes metric not present\", file.columnSizes());\n+          Assert.assertNotNull(\"Counts metric not present\", file.valueCounts());\n+          Assert.assertNotNull(\"Null value counts metric not present\", file.nullValueCounts());\n+          Assert.assertNotNull(\"Lower bounds metric not present\", file.lowerBounds());\n+          Assert.assertNotNull(\"Upper bounds metric not present\", file.upperBounds());\n+        }\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testAppend() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    List<NestedRecord> records = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"b\", info),\n+        new NestedRecord(3, \"c\", info)\n+    );\n+\n+    List<NestedRecord> expected = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"b\", info),\n+        new NestedRecord(3, \"c\", info),\n+        new NestedRecord(4, \"a\", info),\n+        new NestedRecord(5, \"b\", info),\n+        new NestedRecord(6, \"c\", info)\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(records, NestedRecord.class);\n+\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    df.withColumn(\"id\", df.col(\"id\").plus(3)).select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+  }\n+\n+  @Test\n+  public void testOverwrite() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"id\").build();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    List<NestedRecord> records = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"b\", info),\n+        new NestedRecord(3, \"c\", info)\n+    );\n+\n+    List<NestedRecord> expected = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"a\", info),\n+        new NestedRecord(3, \"c\", info),\n+        new NestedRecord(4, \"b\", info),\n+        new NestedRecord(6, \"c\", info)\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(records, NestedRecord.class);\n+\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    // overwrite with 2*id to replace record 2, append 4 and 6\n+    df.withColumn(\"id\", df.col(\"id\").multiply(2)).select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"overwrite\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+  }\n+\n+  @Test\n+  public void testUnpartitionedOverwrite() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    List<NestedRecord> expected = Lists.newArrayList(\n+        new NestedRecord(1, \"a\", info),\n+        new NestedRecord(2, \"b\", info),\n+        new NestedRecord(3, \"c\", info)\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, NestedRecord.class);\n+\n+    df.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    // overwrite with the same data; should not produce two copies\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"overwrite\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+  }\n+\n+  @Test\n+  public void testUnpartitionedCreateWithTargetFileSizeViaTableProperties() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    table.updateProperties()\n+        .set(TableProperties.WRITE_TARGET_FILE_SIZE_BYTES, \"4\") // ~4 bytes; low enough to trigger\n+        .commit();\n+\n+    List<NestedRecord> expected = Lists.newArrayListWithCapacity(4000);\n+    for (int i = 0; i < 4000; i++) {\n+      expected.add(new NestedRecord(i, \"a\", info));\n+    }\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, NestedRecord.class);\n+\n+    df.select(\"id\", \"data\", \"info\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+\n+    List<DataFile> files = Lists.newArrayList();\n+    for (ManifestFile manifest : table.currentSnapshot().manifests()) {\n+      for (DataFile file : ManifestReader.read(manifest, table.io())) {\n+        files.add(file);\n+      }\n+    }\n+    // TODO: ORC file now not support target file size\n+    if (format.equals(FileFormat.PARQUET)) {\n+      Assert.assertEquals(\"Should have 4 DataFiles\", 4, files.size());\n+      Assert.assertTrue(\"All DataFiles contain 1000 rows\", files.stream().allMatch(d -> d.recordCount() == 1000));\n+    }\n+  }\n+\n+  @Test\n+  public void testPartitionedCreateWithTargetFileSizeViaOption() throws IOException {\n+    File parent = temp.newFolder(format.toString());\n+    File location = new File(parent, \"test\");\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"data\").build();\n+    Table table = tables.create(SCHEMA, spec, location.toString());\n+\n+    List<NestedRecord> expected = Lists.newArrayListWithCapacity(8000);\n+    for (int i = 0; i < 2000; i++) {\n+      expected.add(new NestedRecord(i, \"a\", info));\n+      expected.add(new NestedRecord(i, \"b\", info));\n+      expected.add(new NestedRecord(i, \"c\", info));\n+      expected.add(new NestedRecord(i, \"d\", info));\n+    }\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, NestedRecord.class);\n+\n+    df.select(\"id\", \"data\", \"info\").sort(\"data\").write()\n+        .format(\"iceberg\")\n+        .option(\"write-format\", format.toString())\n+        .mode(\"append\")\n+        .option(\"target-file-size-bytes\", 4) // ~4 bytes; low enough to trigger\n+        .save(location.toString());\n+\n+    table.refresh();\n+\n+    Dataset<Row> result = spark.read()\n+        .format(\"iceberg\")\n+        .load(location.toString());\n+\n+    List<NestedRecord> actual = result.orderBy(\"id\").as(Encoders.bean(NestedRecord.class)).collectAsList();\n+    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+\n+    List<DataFile> files = Lists.newArrayList();\n+    for (ManifestFile manifest : table.currentSnapshot().manifests()) {\n+      for (DataFile file : ManifestReader.read(manifest, table.io())) {\n+        files.add(file);\n+      }\n+    }\n+    // TODO: ORC file now not support target file size\n+    if (format.equals(FileFormat.PARQUET)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNzE2MA=="}, "originalCommit": {"oid": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe"}, "originalPosition": 379}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ3OTI2NzQ5OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yOVQxNzoyNDo0NlrOF9Th7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQwMTozNTo0OFrOF9XMuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNzQzNg==", "bodyText": "Does this file replace org.apache.iceberg.spark.source.TestParquetWrite or do u think both are required?", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399827436", "createdAt": "2020-03-29T17:24:46Z", "author": {"login": "rdsr"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java", "diffHunk": "@@ -0,0 +1,457 @@\n+/*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTg4NzU0NQ==", "bodyText": "Yes , I Think this file replace org.apache.iceberg.spark.source.TestParquetWrite I deleted the  org.apache.iceberg.spark.source.TestParquetWrite", "url": "https://github.com/apache/iceberg/pull/857#discussion_r399887545", "createdAt": "2020-03-30T01:35:48Z", "author": {"login": "XiaokunDing"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java", "diffHunk": "@@ -0,0 +1,457 @@\n+/*", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5OTgyNzQzNg=="}, "originalCommit": {"oid": "fc5e9a6640fff195f71dbb9b2fb4f1aa1ee86abe"}, "originalPosition": 1}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2865, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}