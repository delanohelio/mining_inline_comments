{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkxNTE2NTM0", "number": 1487, "title": "Spark: add E2E test on partition pruning for filter pushdown", "bodyText": "This PR proposes to add E2E tests on partition pruning for both Spark 2.4 and Spark 3.\nThis PR tests below queries:\n\nidentity with String type\nbucket with Integer type\ntruncate with String type\nhour with Timestamp type\n\nwhich the table has all of them in partition spec.", "createdAt": "2020-09-23T04:58:23Z", "url": "https://github.com/apache/iceberg/pull/1487", "merged": true, "mergeCommit": {"oid": "baa1bb8d4f35a484eb182b9a766425f283da40c8"}, "closed": true, "closedAt": "2020-10-06T16:55:49Z", "author": {"login": "HeartSaVioR"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdLlO9pAH2gAyNDkxNTE2NTM0OjMxYzYzOWNlMjQxNWVlZDMxZGFkNzdiMWZmMWNlM2I4YjNlODAwNTU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdMH7c-AH2gAyNDkxNTE2NTM0Ojc3NjhkZjQ4MDNiNjM3ZWI1ZDFlNmM2Y2ZlYjIxMDVkOTQ3ZTQ1ODI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "31c639ce2415eed31dad77b1ff1ce3b8b3e80055", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/31c639ce2415eed31dad77b1ff1ce3b8b3e80055", "committedDate": "2020-09-23T04:53:14Z", "message": "Spark: add E2E test on partition pruning for filter pushdown"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "67734e5bd8e2ff13284071af986e326e3b6272b5", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/67734e5bd8e2ff13284071af986e326e3b6272b5", "committedDate": "2020-09-23T07:03:01Z", "message": "Fix checkstyle"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1MTU2Njk4", "url": "https://github.com/apache/iceberg/pull/1487#pullrequestreview-495156698", "createdAt": "2020-09-24T01:16:14Z", "commit": {"oid": "67734e5bd8e2ff13284071af986e326e3b6272b5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMToxNjoxNFrOHXGYlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMToxNjoxNFrOHXGYlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4Mzg5Mg==", "bodyText": "I don't think it is a good idea for tests to create Timestamp or Date objects because those representations are tied to a time zone in the JVM implementation. If you have to use them, then pass an instant in milliseconds or microseconds that is produced by Iceberg's literals: Literal.of(\"2020-02-02 01:00:00\").to(TimestampType.withoutZone()).value()", "url": "https://github.com/apache/iceberg/pull/1487#discussion_r493983892", "createdAt": "2020-09-24T01:16:14Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionPruning.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.sql.Timestamp;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RawLocalFileSystem;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.transforms.Transform;\n+import org.apache.iceberg.transforms.Transforms;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestPartitionPruning {\n+\n+  private static final Configuration CONF = new Configuration();\n+  private static final HadoopTables TABLES = new HadoopTables(CONF);\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { \"parquet\", false },\n+        new Object[] { \"parquet\", true },\n+        new Object[] { \"avro\", false },\n+        new Object[] { \"orc\", false },\n+        new Object[] { \"orc\", true },\n+    };\n+  }\n+\n+  private final String format;\n+  private final boolean vectorized;\n+\n+  public TestPartitionPruning(String format, boolean vectorized) {\n+    this.format = format;\n+    this.vectorized = vectorized;\n+  }\n+\n+  private static SparkSession spark = null;\n+\n+  private static Transform<Object, Integer> bucketTransform = Transforms.bucket(Types.IntegerType.get(), 3);\n+  private static Transform<Object, Object> truncateTransform = Transforms.truncate(Types.StringType.get(), 5);\n+  private static Transform<Object, Integer> hourTransform = Transforms.hour(Types.TimestampType.withZone());\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestPartitionPruning.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n+    String optionKey = String.format(\"fs.%s.impl\", CountOpenLocalFileSystem.scheme);\n+\n+    CONF.set(optionKey, CountOpenLocalFileSystem.class.getName());\n+    spark.conf().set(optionKey, CountOpenLocalFileSystem.class.getName());\n+    spark.udf().register(\"bucket3\", (Integer num) -> bucketTransform.apply(num), DataTypes.IntegerType);\n+    spark.udf().register(\"truncate5\", (String str) -> truncateTransform.apply(str), DataTypes.StringType);\n+    // NOTE: date transforms take the type long, not Timestamp\n+    spark.udf().register(\"hour\", (Timestamp ts) -> hourTransform.apply(\n+        (Long) org.apache.spark.sql.catalyst.util.DateTimeUtils.fromJavaTimestamp(ts)),\n+        DataTypes.IntegerType);\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestPartitionPruning.spark;\n+    TestPartitionPruning.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  private static final Schema LOG_SCHEMA = new Schema(\n+      Types.NestedField.optional(1, \"id\", Types.IntegerType.get()),\n+      Types.NestedField.optional(2, \"date\", Types.StringType.get()),\n+      Types.NestedField.optional(3, \"level\", Types.StringType.get()),\n+      Types.NestedField.optional(4, \"message\", Types.StringType.get()),\n+      Types.NestedField.optional(5, \"timestamp\", Types.TimestampType.withZone())\n+  );\n+\n+  private static final List<LogMessage> LOGS = ImmutableList.of(\n+      LogMessage.debug(\"2020-02-02\", \"debug event 1\", Timestamp.valueOf(\"2020-02-02 00:00:00\")),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67734e5bd8e2ff13284071af986e326e3b6272b5"}, "originalPosition": 126}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "27617d8affb39284401a15486759495bc65096f6", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/27617d8affb39284401a15486759495bc65096f6", "committedDate": "2020-09-24T03:28:10Z", "message": "Change the test code to be resilient on concurrent test runs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "572ecf1964e871bbafb650ec0acde6a12d92b002", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/572ecf1964e871bbafb650ec0acde6a12d92b002", "committedDate": "2020-09-24T04:59:40Z", "message": "Add some more hints, as I see Travis is complaining on JDK 8"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "67adafef46a94de63757ebddf9d763dc7b76ce43", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/67adafef46a94de63757ebddf9d763dc7b76ce43", "committedDate": "2020-09-24T07:04:24Z", "message": "Fix test failures on sequential order of test executions across suites"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7429dc0e4b173686bec097ae4c3eecb9e9c55b66", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/7429dc0e4b173686bec097ae4c3eecb9e9c55b66", "committedDate": "2020-09-24T08:02:12Z", "message": "Use java.time.Instant instead of java.sql.Timestamp"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk1OTA3ODA3", "url": "https://github.com/apache/iceberg/pull/1487#pullrequestreview-495907807", "createdAt": "2020-09-24T19:59:20Z", "commit": {"oid": "7429dc0e4b173686bec097ae4c3eecb9e9c55b66"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxOTo1OToyMVrOHXqjyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQxOTo1OToyMVrOHXqjyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU3NjU4NQ==", "bodyText": "Why does this create its own random number instead of just getting a new temp folder? I don't see much benefit to doing it this way.", "url": "https://github.com/apache/iceberg/pull/1487#discussion_r494576585", "createdAt": "2020-09-24T19:59:21Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionPruning.java", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.sql.Timestamp;\n+import java.time.Instant;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RawLocalFileSystem;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.expressions.Literal;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.transforms.Transform;\n+import org.apache.iceberg.transforms.Transforms;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.unsafe.types.UTF8String;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestPartitionPruning {\n+\n+  private static final Configuration CONF = new Configuration();\n+  private static final HadoopTables TABLES = new HadoopTables(CONF);\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { \"parquet\", false },\n+        new Object[] { \"parquet\", true },\n+        new Object[] { \"avro\", false },\n+        new Object[] { \"orc\", false },\n+        new Object[] { \"orc\", true },\n+    };\n+  }\n+\n+  private final String format;\n+  private final boolean vectorized;\n+\n+  public TestPartitionPruning(String format, boolean vectorized) {\n+    this.format = format;\n+    this.vectorized = vectorized;\n+  }\n+\n+  private static SparkSession spark = null;\n+  private static JavaSparkContext sparkContext = null;\n+\n+  private static Transform<Object, Integer> bucketTransform = Transforms.bucket(Types.IntegerType.get(), 3);\n+  private static Transform<Object, Object> truncateTransform = Transforms.truncate(Types.StringType.get(), 5);\n+  private static Transform<Object, Integer> hourTransform = Transforms.hour(Types.TimestampType.withoutZone());\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestPartitionPruning.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n+    TestPartitionPruning.sparkContext = new JavaSparkContext(spark.sparkContext());\n+\n+    String optionKey = String.format(\"fs.%s.impl\", CountOpenLocalFileSystem.scheme);\n+    CONF.set(optionKey, CountOpenLocalFileSystem.class.getName());\n+    spark.conf().set(optionKey, CountOpenLocalFileSystem.class.getName());\n+    spark.conf().set(\"spark.sql.session.timeZone\", \"UTC\");\n+    spark.udf().register(\"bucket3\", (Integer num) -> bucketTransform.apply(num), DataTypes.IntegerType);\n+    spark.udf().register(\"truncate5\", (String str) -> truncateTransform.apply(str), DataTypes.StringType);\n+    // NOTE: date transforms take the type long, not Timestamp\n+    spark.udf().register(\"hour\", (Timestamp ts) -> hourTransform.apply(\n+        org.apache.spark.sql.catalyst.util.DateTimeUtils.fromJavaTimestamp(ts)),\n+        DataTypes.IntegerType);\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestPartitionPruning.spark;\n+    TestPartitionPruning.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  private static final Schema LOG_SCHEMA = new Schema(\n+      Types.NestedField.optional(1, \"id\", Types.IntegerType.get()),\n+      Types.NestedField.optional(2, \"date\", Types.StringType.get()),\n+      Types.NestedField.optional(3, \"level\", Types.StringType.get()),\n+      Types.NestedField.optional(4, \"message\", Types.StringType.get()),\n+      Types.NestedField.optional(5, \"timestamp\", Types.TimestampType.withZone())\n+  );\n+\n+  private static final List<LogMessage> LOGS = ImmutableList.of(\n+      LogMessage.debug(\"2020-02-02\", \"debug event 1\", getInstant(\"2020-02-02T00:00:00\")),\n+      LogMessage.info(\"2020-02-02\", \"info event 1\", getInstant(\"2020-02-02T01:00:00\")),\n+      LogMessage.debug(\"2020-02-02\", \"debug event 2\", getInstant(\"2020-02-02T02:00:00\")),\n+      LogMessage.info(\"2020-02-03\", \"info event 2\", getInstant(\"2020-02-03T00:00:00\")),\n+      LogMessage.debug(\"2020-02-03\", \"debug event 3\", getInstant(\"2020-02-03T01:00:00\")),\n+      LogMessage.info(\"2020-02-03\", \"info event 3\", getInstant(\"2020-02-03T02:00:00\")),\n+      LogMessage.error(\"2020-02-03\", \"error event 1\", getInstant(\"2020-02-03T03:00:00\")),\n+      LogMessage.debug(\"2020-02-04\", \"debug event 4\", getInstant(\"2020-02-04T01:00:00\")),\n+      LogMessage.warn(\"2020-02-04\", \"warn event 1\", getInstant(\"2020-02-04T02:00:00\")),\n+      LogMessage.debug(\"2020-02-04\", \"debug event 5\", getInstant(\"2020-02-04T03:00:00\"))\n+  );\n+\n+  private static Instant getInstant(String timestampWithoutZone) {\n+    Long epochMicros = (Long) Literal.of(timestampWithoutZone).to(Types.TimestampType.withoutZone()).value();\n+    return Instant.ofEpochMilli(TimeUnit.MICROSECONDS.toMillis(epochMicros));\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private PartitionSpec spec = PartitionSpec.builderFor(LOG_SCHEMA)\n+      .identity(\"date\")\n+      .identity(\"level\")\n+      .bucket(\"id\", 3)\n+      .truncate(\"message\", 5)\n+      .hour(\"timestamp\")\n+      .build();\n+\n+  private Random random = new Random();\n+\n+  @Test\n+  public void testPartitionPruningIdentityString() {\n+    String filterCond = \"date >= '2020-02-03' AND level = 'DEBUG'\";\n+    Predicate<Row> partCondition = (Row r) -> {\n+      String date = r.getString(0);\n+      String level = r.getString(1);\n+      return date.compareTo(\"2020-02-03\") >= 0 && level.equals(\"DEBUG\");\n+    };\n+\n+    runTest(filterCond, partCondition);\n+  }\n+\n+  @Test\n+  public void testPartitionPruningBucketingInteger() {\n+    final int[] ids = new int[]{\n+        LOGS.get(3).getId(),\n+        LOGS.get(7).getId()\n+    };\n+    String condForIds = Arrays.stream(ids).mapToObj(String::valueOf)\n+        .collect(Collectors.joining(\",\", \"(\", \")\"));\n+    String filterCond = \"id in \" + condForIds;\n+    Predicate<Row> partCondition = (Row r) -> {\n+      int bucketId = r.getInt(2);\n+      Set<Integer> buckets = Arrays.stream(ids).map(bucketTransform::apply)\n+          .boxed().collect(Collectors.toSet());\n+      return buckets.contains(bucketId);\n+    };\n+\n+    runTest(filterCond, partCondition);\n+  }\n+\n+  @Test\n+  public void testPartitionPruningTruncatedString() {\n+    String filterCond = \"message like 'info event%'\";\n+    Predicate<Row> partCondition = (Row r) -> {\n+      String truncatedMessage = r.getString(3);\n+      return truncatedMessage.equals(\"info \");\n+    };\n+\n+    runTest(filterCond, partCondition);\n+  }\n+\n+  @Test\n+  public void testPartitionPruningTruncatedStringComparingValueShorterThanPartitionValue() {\n+    String filterCond = \"message like 'inf%'\";\n+    Predicate<Row> partCondition = (Row r) -> {\n+      String truncatedMessage = r.getString(3);\n+      return truncatedMessage.startsWith(\"inf\");\n+    };\n+\n+    runTest(filterCond, partCondition);\n+  }\n+\n+  @Test\n+  public void testPartitionPruningHourlyPartition() {\n+    String filterCond;\n+    if (spark.version().startsWith(\"2\")) {\n+      // Looks like from Spark 2 we need to compare timestamp with timestamp to push down the filter.\n+      filterCond = \"timestamp >= to_timestamp('2020-02-03T01:00:00')\";\n+    } else {\n+      filterCond = \"timestamp >= '2020-02-03T01:00:00'\";\n+    }\n+    Predicate<Row> partCondition = (Row r) -> {\n+      int hourValue = r.getInt(4);\n+      Instant instant = getInstant(\"2020-02-03T01:00:00\");\n+      Integer hourValueToFilter = hourTransform.apply(TimeUnit.MILLISECONDS.toMicros(instant.toEpochMilli()));\n+      return hourValue >= hourValueToFilter;\n+    };\n+\n+    runTest(filterCond, partCondition);\n+  }\n+\n+  private void runTest(String filterCond, Predicate<Row> partCondition) {\n+    File originTableLocation = createTempDir();\n+    Assert.assertTrue(\"Temp folder should exist\", originTableLocation.exists());\n+\n+    Table table = createTable(originTableLocation);\n+    Dataset<Row> logs = createTestDataset();\n+    saveTestDatasetToTable(logs, table);\n+\n+    List<Row> expected = logs\n+        .select(\"id\", \"date\", \"level\", \"message\", \"timestamp\")\n+        .filter(filterCond)\n+        .orderBy(\"id\")\n+        .collectAsList();\n+    Assert.assertFalse(\"Expected rows should be not empty\", expected.isEmpty());\n+\n+    // remove records which may be recorded during storing to table\n+    CountOpenLocalFileSystem.resetRecordsInPathPrefix(originTableLocation.getAbsolutePath());\n+\n+    List<Row> actual = spark.read()\n+        .format(\"iceberg\")\n+        .option(\"vectorization-enabled\", String.valueOf(vectorized))\n+        .load(table.location())\n+        .select(\"id\", \"date\", \"level\", \"message\", \"timestamp\")\n+        .filter(filterCond)\n+        .orderBy(\"id\")\n+        .collectAsList();\n+    Assert.assertFalse(\"Actual rows should not be empty\", actual.isEmpty());\n+\n+    Assert.assertEquals(\"Rows should match\", expected, actual);\n+\n+    assertAccessOnDataFiles(originTableLocation, table, partCondition);\n+  }\n+\n+  private File createTempDir() {\n+    try {\n+      int rand = random.nextInt(1000000);\n+      return temp.newFolder(String.format(\"logs-%d\", rand));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7429dc0e4b173686bec097ae4c3eecb9e9c55b66"}, "originalPosition": 275}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7768df4803b637eb5d1e6c6cfeb2105d947e4582", "author": {"user": {"login": "HeartSaVioR", "name": "Jungtaek Lim"}}, "url": "https://github.com/apache/iceberg/commit/7768df4803b637eb5d1e6c6cfeb2105d947e4582", "committedDate": "2020-09-24T21:18:36Z", "message": "Fix"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3814, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}