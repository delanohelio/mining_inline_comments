{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQzODU5NTQ2", "number": 1974, "title": "Flink: Add ChangeLog DataStream end-to-end unit tests.", "bodyText": "Add unit tests to proof that flink DataStream job could write the CDC events correctly.  Will open a separate PR to address the flink SQL unit tests.", "createdAt": "2020-12-22T04:33:40Z", "url": "https://github.com/apache/iceberg/pull/1974", "merged": true, "mergeCommit": {"oid": "1b66bdfc084ac73fe999299d041aa2e5677f43c9"}, "closed": true, "closedAt": "2020-12-23T06:55:38Z", "author": {"login": "openinx"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdoi2augH2gAyNTQzODU5NTQ2OjU5ZDc3YWEwZTE4NjRlZDcxMTQyZWVlMmM3NzM2MzUzYTQ1MDNmODM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdo2xIwgH2gAyNTQzODU5NTQ2OjExM2JlMjUyMGMzMzBmZWNjMWIwMDI5ZDUwOWQ4YTM4NjYwNjlhMzI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "59d77aa0e1864ed71142eee2c7736353a4503f83", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/59d77aa0e1864ed71142eee2c7736353a4503f83", "committedDate": "2020-12-22T04:30:25Z", "message": "Flink: Add ChangeLog DataStream end-to-end unit tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8a64f26e7088759802b176f3a630e5dd7765349a", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/8a64f26e7088759802b176f3a630e5dd7765349a", "committedDate": "2020-12-22T04:57:35Z", "message": "Minor changes."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/c4c75e4f8c371587f53c6e568a3826c6c02f60bb", "committedDate": "2020-12-22T12:20:19Z", "message": "Add equalityFieldColumns in FlinkSink API."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3MjUyNTQ2", "url": "https://github.com/apache/iceberg/pull/1974#pullrequestreview-557252546", "createdAt": "2020-12-22T17:42:29Z", "commit": {"oid": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzo0MjozMFrOIKDXGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzo0MjozMFrOIKDXGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxMTczNg==", "bodyText": "Do you think that we should consider adding primary key columns to the spec?", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547411736", "createdAt": "2020-12-22T17:42:30Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -169,6 +172,17 @@ public Builder writeParallelism(int newWriteParallelism) {\n       return this;\n     }\n \n+    /**\n+     * Configuring the equality field columns for iceberg table that accept CDC or UPSERT events.\n+     *\n+     * @param columns defines the iceberg table's key.\n+     * @return {@link Builder} to connect the iceberg table.\n+     */\n+    public Builder equalityFieldColumns(List<String> columns) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3MjUzMzI2", "url": "https://github.com/apache/iceberg/pull/1974#pullrequestreview-557253326", "createdAt": "2020-12-22T17:43:53Z", "commit": {"oid": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzo0Mzo1M1rOIKDZqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzo0Mzo1M1rOIKDZqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxMjM5NA==", "bodyText": "Why not do this conversion in equalityFieldColumns and keep the column ids in the builder instead of the source column names?", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547412394", "createdAt": "2020-12-22T17:43:53Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -184,7 +198,18 @@ public Builder writeParallelism(int newWriteParallelism) {\n         }\n       }\n \n-      IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n+      // Find out the equality field id list based on the user-provided equality field column names.\n+      List<Integer> equalityFieldIds = Lists.newArrayList();\n+      if (equalityFieldColumns != null && equalityFieldColumns.size() > 0) {\n+        for (String column : equalityFieldColumns) {\n+          org.apache.iceberg.types.Types.NestedField field = table.schema().findField(column);\n+          Preconditions.checkNotNull(field, \"Missing required equality field column '%s' in table schema %s\",\n+              column, table.schema());\n+          equalityFieldIds.add(field.fieldId());\n+        }\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3MjU0Mzk5", "url": "https://github.com/apache/iceberg/pull/1974#pullrequestreview-557254399", "createdAt": "2020-12-22T17:45:25Z", "commit": {"oid": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzo0NToyNVrOIKDcbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzo0NToyNVrOIKDcbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxMzEwMw==", "bodyText": "Nit: I think you mean \"executing tasks in parallel\" rather than \"parallelism\".", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547413103", "createdAt": "2020-12-22T17:45:25Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java", "diffHunk": "@@ -0,0 +1,330 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.types.RowKind;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.data.IcebergGenerics;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.flink.SimpleDataUtil;\n+import org.apache.iceberg.flink.TestTableLoader;\n+import org.apache.iceberg.flink.source.BoundedTestSource;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkIcebergSinkV2 extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+  private static final TypeInformation<Row> ROW_TYPE_INFO =\n+      new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+  private final boolean partitioned;\n+\n+  private StreamExecutionEnvironment env;\n+  private TestTableLoader tableLoader;\n+\n+  @Parameterized.Parameters(name = \"FileFormat = {0}, Parallelism = {1}, Partitioned={2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", 1, true},\n+        new Object[] {\"avro\", 1, false},\n+        new Object[] {\"avro\", 2, true},\n+        new Object[] {\"avro\", 2, false},\n+        new Object[] {\"parquet\", 1, true},\n+        new Object[] {\"parquet\", 1, false},\n+        new Object[] {\"parquet\", 2, true},\n+        new Object[] {\"parquet\", 2, false}\n+    };\n+  }\n+\n+  public TestFlinkIcebergSinkV2(String format, int parallelism, boolean partitioned) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void setupTable() throws IOException {\n+    this.tableDir = temp.newFolder();\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+    Assert.assertTrue(tableDir.delete());\n+\n+    if (!partitioned) {\n+      table = create(SimpleDataUtil.SCHEMA, PartitionSpec.unpartitioned());\n+    } else {\n+      table = create(SimpleDataUtil.SCHEMA, PartitionSpec.builderFor(SimpleDataUtil.SCHEMA).identity(\"data\").build());\n+    }\n+\n+    table.updateProperties()\n+        .set(TableProperties.DEFAULT_FILE_FORMAT, format.name())\n+        .commit();\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment()\n+        .enableCheckpointing(100L)\n+        .setParallelism(parallelism)\n+        .setMaxParallelism(parallelism);\n+\n+    tableLoader = new TestTableLoader(tableDir.getAbsolutePath());\n+  }\n+\n+  private List<Snapshot> findValidSnapshots(Table table) {\n+    List<Snapshot> validSnapshots = Lists.newArrayList();\n+    for (Snapshot snapshot : table.snapshots()) {\n+      if (snapshot.allManifests().stream().anyMatch(m -> snapshot.snapshotId() == m.snapshotId())) {\n+        validSnapshots.add(snapshot);\n+      }\n+    }\n+    return validSnapshots;\n+  }\n+\n+  private void testChangeLogs(List<String> equalityFieldColumns,\n+                              KeySelector<Row, Row> keySelector,\n+                              List<List<Row>> elementsPerCheckpoint,\n+                              List<List<Record>> expectedRecordsPerCheckpoint) throws Exception {\n+    DataStream<Row> dataStream = env.addSource(new BoundedTestSource<>(elementsPerCheckpoint), ROW_TYPE_INFO);\n+\n+    // Shuffle by the equality key, so that different operations from the same key could be wrote in order when\n+    // executing tasks in parallelism.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb"}, "originalPosition": 131}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3MjU0ODE3", "url": "https://github.com/apache/iceberg/pull/1974#pullrequestreview-557254817", "createdAt": "2020-12-22T17:46:07Z", "commit": {"oid": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzo0NjowN1rOIKDeTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzo0NjowN1rOIKDeTA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxMzU4MA==", "bodyText": "Could this be a private static map instead of defining it each time a row is created?", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547413580", "createdAt": "2020-12-22T17:46:07Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java", "diffHunk": "@@ -0,0 +1,330 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.types.RowKind;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.data.IcebergGenerics;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.flink.SimpleDataUtil;\n+import org.apache.iceberg.flink.TestTableLoader;\n+import org.apache.iceberg.flink.source.BoundedTestSource;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkIcebergSinkV2 extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+  private static final TypeInformation<Row> ROW_TYPE_INFO =\n+      new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+  private final boolean partitioned;\n+\n+  private StreamExecutionEnvironment env;\n+  private TestTableLoader tableLoader;\n+\n+  @Parameterized.Parameters(name = \"FileFormat = {0}, Parallelism = {1}, Partitioned={2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", 1, true},\n+        new Object[] {\"avro\", 1, false},\n+        new Object[] {\"avro\", 2, true},\n+        new Object[] {\"avro\", 2, false},\n+        new Object[] {\"parquet\", 1, true},\n+        new Object[] {\"parquet\", 1, false},\n+        new Object[] {\"parquet\", 2, true},\n+        new Object[] {\"parquet\", 2, false}\n+    };\n+  }\n+\n+  public TestFlinkIcebergSinkV2(String format, int parallelism, boolean partitioned) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void setupTable() throws IOException {\n+    this.tableDir = temp.newFolder();\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+    Assert.assertTrue(tableDir.delete());\n+\n+    if (!partitioned) {\n+      table = create(SimpleDataUtil.SCHEMA, PartitionSpec.unpartitioned());\n+    } else {\n+      table = create(SimpleDataUtil.SCHEMA, PartitionSpec.builderFor(SimpleDataUtil.SCHEMA).identity(\"data\").build());\n+    }\n+\n+    table.updateProperties()\n+        .set(TableProperties.DEFAULT_FILE_FORMAT, format.name())\n+        .commit();\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment()\n+        .enableCheckpointing(100L)\n+        .setParallelism(parallelism)\n+        .setMaxParallelism(parallelism);\n+\n+    tableLoader = new TestTableLoader(tableDir.getAbsolutePath());\n+  }\n+\n+  private List<Snapshot> findValidSnapshots(Table table) {\n+    List<Snapshot> validSnapshots = Lists.newArrayList();\n+    for (Snapshot snapshot : table.snapshots()) {\n+      if (snapshot.allManifests().stream().anyMatch(m -> snapshot.snapshotId() == m.snapshotId())) {\n+        validSnapshots.add(snapshot);\n+      }\n+    }\n+    return validSnapshots;\n+  }\n+\n+  private void testChangeLogs(List<String> equalityFieldColumns,\n+                              KeySelector<Row, Row> keySelector,\n+                              List<List<Row>> elementsPerCheckpoint,\n+                              List<List<Record>> expectedRecordsPerCheckpoint) throws Exception {\n+    DataStream<Row> dataStream = env.addSource(new BoundedTestSource<>(elementsPerCheckpoint), ROW_TYPE_INFO);\n+\n+    // Shuffle by the equality key, so that different operations from the same key could be wrote in order when\n+    // executing tasks in parallelism.\n+    dataStream = dataStream.keyBy(keySelector);\n+\n+    FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+        .tableLoader(tableLoader)\n+        .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n+        .writeParallelism(parallelism)\n+        .equalityFieldColumns(equalityFieldColumns)\n+        .build();\n+\n+    // Execute the program.\n+    env.execute(\"Test Iceberg Change-Log DataStream.\");\n+\n+    table.refresh();\n+    List<Snapshot> snapshots = findValidSnapshots(table);\n+    int expectedSnapshotNum = expectedRecordsPerCheckpoint.size();\n+    Assert.assertEquals(\"Should have the expected snapshot number\", expectedSnapshotNum, snapshots.size());\n+\n+    for (int i = 0; i < expectedSnapshotNum; i++) {\n+      long snapshotId = snapshots.get(i).snapshotId();\n+      List<Record> expectedRecords = expectedRecordsPerCheckpoint.get(i);\n+      Assert.assertEquals(\"Should have the expected records for the checkpoint#\" + i,\n+          expectedRowSet(expectedRecords.toArray(new Record[0])), actualRowSet(snapshotId, \"*\"));\n+    }\n+  }\n+\n+  private Row row(String rowKind, int id, String data) {\n+    Map<String, RowKind> mapping = ImmutableMap.of(\n+        \"+I\", RowKind.INSERT,\n+        \"-D\", RowKind.DELETE,\n+        \"-U\", RowKind.UPDATE_BEFORE,\n+        \"+U\", RowKind.UPDATE_AFTER);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb"}, "originalPosition": 162}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3MjU3MDIx", "url": "https://github.com/apache/iceberg/pull/1974#pullrequestreview-557257021", "createdAt": "2020-12-22T17:49:53Z", "commit": {"oid": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzo0OTo1NFrOIKDlsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzo0OTo1NFrOIKDlsA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxNTQ3Mg==", "bodyText": "Should this be equalityFieldNames?", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547415472", "createdAt": "2020-12-22T17:49:54Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java", "diffHunk": "@@ -0,0 +1,330 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.types.RowKind;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.data.IcebergGenerics;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.flink.SimpleDataUtil;\n+import org.apache.iceberg.flink.TestTableLoader;\n+import org.apache.iceberg.flink.source.BoundedTestSource;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkIcebergSinkV2 extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+  private static final TypeInformation<Row> ROW_TYPE_INFO =\n+      new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+  private final boolean partitioned;\n+\n+  private StreamExecutionEnvironment env;\n+  private TestTableLoader tableLoader;\n+\n+  @Parameterized.Parameters(name = \"FileFormat = {0}, Parallelism = {1}, Partitioned={2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", 1, true},\n+        new Object[] {\"avro\", 1, false},\n+        new Object[] {\"avro\", 2, true},\n+        new Object[] {\"avro\", 2, false},\n+        new Object[] {\"parquet\", 1, true},\n+        new Object[] {\"parquet\", 1, false},\n+        new Object[] {\"parquet\", 2, true},\n+        new Object[] {\"parquet\", 2, false}\n+    };\n+  }\n+\n+  public TestFlinkIcebergSinkV2(String format, int parallelism, boolean partitioned) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void setupTable() throws IOException {\n+    this.tableDir = temp.newFolder();\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+    Assert.assertTrue(tableDir.delete());\n+\n+    if (!partitioned) {\n+      table = create(SimpleDataUtil.SCHEMA, PartitionSpec.unpartitioned());\n+    } else {\n+      table = create(SimpleDataUtil.SCHEMA, PartitionSpec.builderFor(SimpleDataUtil.SCHEMA).identity(\"data\").build());\n+    }\n+\n+    table.updateProperties()\n+        .set(TableProperties.DEFAULT_FILE_FORMAT, format.name())\n+        .commit();\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment()\n+        .enableCheckpointing(100L)\n+        .setParallelism(parallelism)\n+        .setMaxParallelism(parallelism);\n+\n+    tableLoader = new TestTableLoader(tableDir.getAbsolutePath());\n+  }\n+\n+  private List<Snapshot> findValidSnapshots(Table table) {\n+    List<Snapshot> validSnapshots = Lists.newArrayList();\n+    for (Snapshot snapshot : table.snapshots()) {\n+      if (snapshot.allManifests().stream().anyMatch(m -> snapshot.snapshotId() == m.snapshotId())) {\n+        validSnapshots.add(snapshot);\n+      }\n+    }\n+    return validSnapshots;\n+  }\n+\n+  private void testChangeLogs(List<String> equalityFieldColumns,\n+                              KeySelector<Row, Row> keySelector,\n+                              List<List<Row>> elementsPerCheckpoint,\n+                              List<List<Record>> expectedRecordsPerCheckpoint) throws Exception {\n+    DataStream<Row> dataStream = env.addSource(new BoundedTestSource<>(elementsPerCheckpoint), ROW_TYPE_INFO);\n+\n+    // Shuffle by the equality key, so that different operations from the same key could be wrote in order when\n+    // executing tasks in parallelism.\n+    dataStream = dataStream.keyBy(keySelector);\n+\n+    FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+        .tableLoader(tableLoader)\n+        .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n+        .writeParallelism(parallelism)\n+        .equalityFieldColumns(equalityFieldColumns)\n+        .build();\n+\n+    // Execute the program.\n+    env.execute(\"Test Iceberg Change-Log DataStream.\");\n+\n+    table.refresh();\n+    List<Snapshot> snapshots = findValidSnapshots(table);\n+    int expectedSnapshotNum = expectedRecordsPerCheckpoint.size();\n+    Assert.assertEquals(\"Should have the expected snapshot number\", expectedSnapshotNum, snapshots.size());\n+\n+    for (int i = 0; i < expectedSnapshotNum; i++) {\n+      long snapshotId = snapshots.get(i).snapshotId();\n+      List<Record> expectedRecords = expectedRecordsPerCheckpoint.get(i);\n+      Assert.assertEquals(\"Should have the expected records for the checkpoint#\" + i,\n+          expectedRowSet(expectedRecords.toArray(new Record[0])), actualRowSet(snapshotId, \"*\"));\n+    }\n+  }\n+\n+  private Row row(String rowKind, int id, String data) {\n+    Map<String, RowKind> mapping = ImmutableMap.of(\n+        \"+I\", RowKind.INSERT,\n+        \"-D\", RowKind.DELETE,\n+        \"-U\", RowKind.UPDATE_BEFORE,\n+        \"+U\", RowKind.UPDATE_AFTER);\n+\n+    RowKind kind = mapping.get(rowKind);\n+    if (kind == null) {\n+      throw new IllegalArgumentException(\"Unknown row kind: \" + rowKind);\n+    }\n+\n+    return Row.ofKind(kind, id, data);\n+  }\n+\n+  private Record record(int id, String data) {\n+    return SimpleDataUtil.createRecord(id, data);\n+  }\n+\n+  @Test\n+  public void testChangeLogOnIdKey() throws Exception {\n+    List<String> equalityFieldIds = ImmutableList.of(\"id\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb"}, "originalPosition": 178}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3MjU5NzM0", "url": "https://github.com/apache/iceberg/pull/1974#pullrequestreview-557259734", "createdAt": "2020-12-22T17:54:38Z", "commit": {"oid": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzo1NDozOFrOIKDt9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yMlQxNzo1NDozOFrOIKDt9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NzQxNzU5MQ==", "bodyText": "Minor: This makes it look like the row has an operation as its first column, but that doesn't align with the key selector below that uses row.getField(0) to get the ID. I think it would make tests easier to read if row passed the row kind at the end. That way the fields align.\nI'm not sure if it is worth changing all of the rows. Up to you.", "url": "https://github.com/apache/iceberg/pull/1974#discussion_r547417591", "createdAt": "2020-12-22T17:54:38Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java", "diffHunk": "@@ -0,0 +1,330 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.functions.KeySelector;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.types.RowKind;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.data.IcebergGenerics;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.flink.SimpleDataUtil;\n+import org.apache.iceberg.flink.TestTableLoader;\n+import org.apache.iceberg.flink.source.BoundedTestSource;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkIcebergSinkV2 extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+  private static final TypeInformation<Row> ROW_TYPE_INFO =\n+      new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+  private final boolean partitioned;\n+\n+  private StreamExecutionEnvironment env;\n+  private TestTableLoader tableLoader;\n+\n+  @Parameterized.Parameters(name = \"FileFormat = {0}, Parallelism = {1}, Partitioned={2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"avro\", 1, true},\n+        new Object[] {\"avro\", 1, false},\n+        new Object[] {\"avro\", 2, true},\n+        new Object[] {\"avro\", 2, false},\n+        new Object[] {\"parquet\", 1, true},\n+        new Object[] {\"parquet\", 1, false},\n+        new Object[] {\"parquet\", 2, true},\n+        new Object[] {\"parquet\", 2, false}\n+    };\n+  }\n+\n+  public TestFlinkIcebergSinkV2(String format, int parallelism, boolean partitioned) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+    this.partitioned = partitioned;\n+  }\n+\n+  @Before\n+  public void setupTable() throws IOException {\n+    this.tableDir = temp.newFolder();\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+    Assert.assertTrue(tableDir.delete());\n+\n+    if (!partitioned) {\n+      table = create(SimpleDataUtil.SCHEMA, PartitionSpec.unpartitioned());\n+    } else {\n+      table = create(SimpleDataUtil.SCHEMA, PartitionSpec.builderFor(SimpleDataUtil.SCHEMA).identity(\"data\").build());\n+    }\n+\n+    table.updateProperties()\n+        .set(TableProperties.DEFAULT_FILE_FORMAT, format.name())\n+        .commit();\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment()\n+        .enableCheckpointing(100L)\n+        .setParallelism(parallelism)\n+        .setMaxParallelism(parallelism);\n+\n+    tableLoader = new TestTableLoader(tableDir.getAbsolutePath());\n+  }\n+\n+  private List<Snapshot> findValidSnapshots(Table table) {\n+    List<Snapshot> validSnapshots = Lists.newArrayList();\n+    for (Snapshot snapshot : table.snapshots()) {\n+      if (snapshot.allManifests().stream().anyMatch(m -> snapshot.snapshotId() == m.snapshotId())) {\n+        validSnapshots.add(snapshot);\n+      }\n+    }\n+    return validSnapshots;\n+  }\n+\n+  private void testChangeLogs(List<String> equalityFieldColumns,\n+                              KeySelector<Row, Row> keySelector,\n+                              List<List<Row>> elementsPerCheckpoint,\n+                              List<List<Record>> expectedRecordsPerCheckpoint) throws Exception {\n+    DataStream<Row> dataStream = env.addSource(new BoundedTestSource<>(elementsPerCheckpoint), ROW_TYPE_INFO);\n+\n+    // Shuffle by the equality key, so that different operations from the same key could be wrote in order when\n+    // executing tasks in parallelism.\n+    dataStream = dataStream.keyBy(keySelector);\n+\n+    FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+        .tableLoader(tableLoader)\n+        .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n+        .writeParallelism(parallelism)\n+        .equalityFieldColumns(equalityFieldColumns)\n+        .build();\n+\n+    // Execute the program.\n+    env.execute(\"Test Iceberg Change-Log DataStream.\");\n+\n+    table.refresh();\n+    List<Snapshot> snapshots = findValidSnapshots(table);\n+    int expectedSnapshotNum = expectedRecordsPerCheckpoint.size();\n+    Assert.assertEquals(\"Should have the expected snapshot number\", expectedSnapshotNum, snapshots.size());\n+\n+    for (int i = 0; i < expectedSnapshotNum; i++) {\n+      long snapshotId = snapshots.get(i).snapshotId();\n+      List<Record> expectedRecords = expectedRecordsPerCheckpoint.get(i);\n+      Assert.assertEquals(\"Should have the expected records for the checkpoint#\" + i,\n+          expectedRowSet(expectedRecords.toArray(new Record[0])), actualRowSet(snapshotId, \"*\"));\n+    }\n+  }\n+\n+  private Row row(String rowKind, int id, String data) {\n+    Map<String, RowKind> mapping = ImmutableMap.of(\n+        \"+I\", RowKind.INSERT,\n+        \"-D\", RowKind.DELETE,\n+        \"-U\", RowKind.UPDATE_BEFORE,\n+        \"+U\", RowKind.UPDATE_AFTER);\n+\n+    RowKind kind = mapping.get(rowKind);\n+    if (kind == null) {\n+      throw new IllegalArgumentException(\"Unknown row kind: \" + rowKind);\n+    }\n+\n+    return Row.ofKind(kind, id, data);\n+  }\n+\n+  private Record record(int id, String data) {\n+    return SimpleDataUtil.createRecord(id, data);\n+  }\n+\n+  @Test\n+  public void testChangeLogOnIdKey() throws Exception {\n+    List<String> equalityFieldIds = ImmutableList.of(\"id\");\n+    List<List<Row>> elementsPerCheckpoint = ImmutableList.of(\n+        ImmutableList.of(\n+            row(\"+I\", 1, \"aaa\"),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb"}, "originalPosition": 181}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU3MjYxMjEx", "url": "https://github.com/apache/iceberg/pull/1974#pullrequestreview-557261211", "createdAt": "2020-12-22T17:56:54Z", "commit": {"oid": "c4c75e4f8c371587f53c6e568a3826c6c02f60bb"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "113be2520c330fecc1b0029d509d8a3866069a32", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/113be2520c330fecc1b0029d509d8a3866069a32", "committedDate": "2020-12-23T03:42:45Z", "message": "Addressing comments from Ryan."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3313, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}