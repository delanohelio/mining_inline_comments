{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkxNzIzOTg4", "number": 1495, "reviewThreads": {"totalCount": 25, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMDo1ODo1NlrOEm1eVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxOTozNzowMVrOEswTaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTU3NDYwOnYy", "diffSide": "RIGHT", "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "isResolved": true, "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMDo1ODo1NlrOHXGH5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDozNTo0OVrOHaAQRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3OTYyMA==", "bodyText": "Is this telling Iceberg to overwrite the Hive table? I think this property should probably not be persisted to Hive in table properties.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r493979620", "createdAt": "2020-09-24T00:58:56Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -139,6 +140,8 @@ protected void doRefresh() {\n \n   @Override\n   protected void doCommit(TableMetadata base, TableMetadata metadata) {\n+    boolean updateTable = base != null || metadata.propertyAsBoolean(TABLE_FROM_HIVE, false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDI4MzkzOQ==", "bodyText": "If we do not want to use properties for signaling this behavior (looking at it again with fresh eyes seems like a hack), then we have to modify:\n\nadd method TableBuilder.updateExisting\nadd method TableOperations.commit with new parameter signaling update\nadd method HiveTableOperations/HadoopTableOperations to signal the update\n\nOr we have to resort to parameters sending/hacking through some level and removing the parameter from the lower levels.\nI do not like any of these solutions, so happy to go with any other viable solution...", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r494283939", "createdAt": "2020-09-24T12:40:33Z", "author": {"login": "pvary"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -139,6 +140,8 @@ protected void doRefresh() {\n \n   @Override\n   protected void doCommit(TableMetadata base, TableMetadata metadata) {\n+    boolean updateTable = base != null || metadata.propertyAsBoolean(TABLE_FROM_HIVE, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3OTYyMA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU3MTQ4Mg==", "bodyText": "What is the purpose of this? Is it because the table already exists in the Hive MetaStore but there is no Iceberg metadata?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r494571482", "createdAt": "2020-09-24T19:49:45Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -139,6 +140,8 @@ protected void doRefresh() {\n \n   @Override\n   protected void doCommit(TableMetadata base, TableMetadata metadata) {\n+    boolean updateTable = base != null || metadata.propertyAsBoolean(TABLE_FROM_HIVE, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3OTYyMA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU4NjA4Ng==", "bodyText": "Yes, you are right.\nWith the HiveMetaHooks we have the possibility to do some preCreate and commitCreate stuff, but the table should be created by the HMS between these phases. I have even thought about dropping the HMS created table in the commitCreate method but that seems like a serious waste of effort, and also I do not have a HMS client at hand.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r494586086", "createdAt": "2020-09-24T20:18:16Z", "author": {"login": "pvary"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -139,6 +140,8 @@ protected void doRefresh() {\n \n   @Override\n   protected void doCommit(TableMetadata base, TableMetadata metadata) {\n+    boolean updateTable = base != null || metadata.propertyAsBoolean(TABLE_FROM_HIVE, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3OTYyMA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDYwMzgyNw==", "bodyText": "Okay, thanks for the context.\nI think we may be able to catch this case within HiveTableOperations. When creating a table, we will create a new TableOperations and that will attempt to load the current metadata. Normally, that will fail and because the currentMetadataLocation was null, the failure is caught and metadata is set to null because the table doesn't exist. If the table exists but doesn't have a metadata location, then we will get a table from Hive, but the location property won't exist. We can detect that the location property doesn't exist and set a flag, updateToCreate.\nThat avoids the need for a table property because we detect the state of the Hive MetaStore, and it doesn't incur extra thrift calls.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r494603827", "createdAt": "2020-09-24T20:52:56Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -139,6 +140,8 @@ protected void doRefresh() {\n \n   @Override\n   protected void doCommit(TableMetadata base, TableMetadata metadata) {\n+    boolean updateTable = base != null || metadata.propertyAsBoolean(TABLE_FROM_HIVE, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3OTYyMA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDkxNTQ3Ng==", "bodyText": "This would be a slight modification to the API.\n\nBefore the change we throw a NoSuchIcebergTableException if we tried to load a Hive table which did not have the correct table type and the metadata location set.\nAfter the change we will try to update the table with the correct parameters\n\nWould this be acceptable change in the API behavior?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r494915476", "createdAt": "2020-09-25T11:08:49Z", "author": {"login": "pvary"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -139,6 +140,8 @@ protected void doRefresh() {\n \n   @Override\n   protected void doCommit(TableMetadata base, TableMetadata metadata) {\n+    boolean updateTable = base != null || metadata.propertyAsBoolean(TABLE_FROM_HIVE, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3OTYyMA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIxNjA5NQ==", "bodyText": "Could we avoid the behavior change by adding a property in the pre-create hook? If we signal that Hive is creating the table, then this could set the flag to update internally, do the update, and drop the flag from Hive metadata. It still avoids additional calls to Hive.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r495216095", "createdAt": "2020-09-25T20:29:35Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -139,6 +140,8 @@ protected void doRefresh() {\n \n   @Override\n   protected void doCommit(TableMetadata base, TableMetadata metadata) {\n+    boolean updateTable = base != null || metadata.propertyAsBoolean(TABLE_FROM_HIVE, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3OTYyMA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjExOTk5OQ==", "bodyText": "This flag is the HiveTableOperations#TABLE_FROM_HIVE (\"iceberg.table.from.hive\").\nWe do not store this to the HMS, but we store this to the Iceberg table since in the HiveTableOperations.doCommit(base, metadata) the metadata is Immutable.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r496119999", "createdAt": "2020-09-28T17:33:15Z", "author": {"login": "pvary"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -139,6 +140,8 @@ protected void doRefresh() {\n \n   @Override\n   protected void doCommit(TableMetadata base, TableMetadata metadata) {\n+    boolean updateTable = base != null || metadata.propertyAsBoolean(TABLE_FROM_HIVE, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3OTYyMA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkxMTg3Mg==", "bodyText": "I agree that this should not change the TableMetadata that was passed in. If we pass the flag in TableMetadata, then it would be stored in the Iceberg metadata.\nBut why can't we add this flag to the HMS table that is passed to the pre-commit phase in the Hive hooks? If we add it to the Hive table that gets created and should be replaced, then there is no need to pass it in Iceberg metadata. And we can remove it when we create the Iceberg table underneath and set the metadata location.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r496911872", "createdAt": "2020-09-29T17:23:51Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -139,6 +140,8 @@ protected void doRefresh() {\n \n   @Override\n   protected void doCommit(TableMetadata base, TableMetadata metadata) {\n+    boolean updateTable = base != null || metadata.propertyAsBoolean(TABLE_FROM_HIVE, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3OTYyMA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkxNzExNQ==", "bodyText": "Oh... now I get it.\nSeems like a good idea, I will check it!", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r496917115", "createdAt": "2020-09-29T17:31:49Z", "author": {"login": "pvary"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -139,6 +140,8 @@ protected void doRefresh() {\n \n   @Override\n   protected void doCommit(TableMetadata base, TableMetadata metadata) {\n+    boolean updateTable = base != null || metadata.propertyAsBoolean(TABLE_FROM_HIVE, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3OTYyMA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkxODg0MA==", "bodyText": "Actually, I see that we already set the table type property in the pre-commit hook. We could use that to detect this case and avoid throwing NoSuchIcebergTableException, then use update.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r496918840", "createdAt": "2020-09-29T17:34:45Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -139,6 +140,8 @@ protected void doRefresh() {\n \n   @Override\n   protected void doCommit(TableMetadata base, TableMetadata metadata) {\n+    boolean updateTable = base != null || metadata.propertyAsBoolean(TABLE_FROM_HIVE, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3OTYyMA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAyOTE4OA==", "bodyText": "Uploaded the change.\nWe need to call HMSClient.getTable every time in HiveTableOperations.doCommit, so we can decide if this is a new table or already created by the Hook.\nThis is an extra call in case the new table creation is started from Spark. This might be acceptable.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497029188", "createdAt": "2020-09-29T20:35:49Z", "author": {"login": "pvary"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -139,6 +140,8 @@ protected void doRefresh() {\n \n   @Override\n   protected void doCommit(TableMetadata base, TableMetadata metadata) {\n+    boolean updateTable = base != null || metadata.propertyAsBoolean(TABLE_FROM_HIVE, false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk3OTYyMA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTU3OTQwOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMTowMTozOVrOHXGKow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNzoxNDoyOFrOHXMyog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MDMyMw==", "bodyText": "It would be helpful to have some context for this. Why did this change? Could you add some comments to make it more clear what's happening in the code?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r493980323", "createdAt": "2020-09-24T01:01:39Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "diffHunk": "@@ -27,21 +27,32 @@\n import org.apache.hadoop.hive.serde2.SerDeStats;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n import org.apache.hadoop.io.Writable;\n-import org.apache.iceberg.Table;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergObjectInspector;\n import org.apache.iceberg.mr.mapred.Container;\n \n public class HiveIcebergSerDe extends AbstractSerDe {\n-\n+  private Schema schema;\n   private ObjectInspector inspector;\n \n   @Override\n   public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n-    Table table = Catalogs.loadTable(configuration, serDeProperties);\n-\n     try {\n-      this.inspector = IcebergObjectInspector.create(table.schema());\n+      String schemaString = (String) serDeProperties.get(InputFormatConfig.TABLE_SCHEMA);\n+      if (schemaString != null) {\n+        schema = SchemaParser.fromJson(schemaString);\n+      } else {\n+        try {\n+          schema = Catalogs.loadTable(configuration, serDeProperties).schema();\n+        } catch (NoSuchTableException nte) {\n+          throw new SerDeException(\"Please provide an existing table or a valid schema\", nte);\n+        }\n+      }\n+      inspector = IcebergObjectInspector.create(schema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA4ODg2Ng==", "bodyText": "Added a comment:\nHiveIcebergSerDe.initialize is called multiple places in Hive code:\n\nWhen we are trying to create a table - HiveDDL data is stored at the serDeProperties, but no Iceberg table is created yet.\nWhen we are compiling the Hive query on HiveServer2 side - We only have table information (location/name), and we have to read the schema using the table data. This is called multiple times so there is room for optimizing here.\nWhen we are executing the Hive query in the execution engine - We do not want to load the table data on every executor, but serDeProperties are populated by HiveIcebergStorageHandler.configureInputJobProperties() and the resulting properties are serialized and distributed to the executors", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r494088866", "createdAt": "2020-09-24T07:14:28Z", "author": {"login": "pvary"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "diffHunk": "@@ -27,21 +27,32 @@\n import org.apache.hadoop.hive.serde2.SerDeStats;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n import org.apache.hadoop.io.Writable;\n-import org.apache.iceberg.Table;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergObjectInspector;\n import org.apache.iceberg.mr.mapred.Container;\n \n public class HiveIcebergSerDe extends AbstractSerDe {\n-\n+  private Schema schema;\n   private ObjectInspector inspector;\n \n   @Override\n   public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n-    Table table = Catalogs.loadTable(configuration, serDeProperties);\n-\n     try {\n-      this.inspector = IcebergObjectInspector.create(table.schema());\n+      String schemaString = (String) serDeProperties.get(InputFormatConfig.TABLE_SCHEMA);\n+      if (schemaString != null) {\n+        schema = SchemaParser.fromJson(schemaString);\n+      } else {\n+        try {\n+          schema = Catalogs.loadTable(configuration, serDeProperties).schema();\n+        } catch (NoSuchTableException nte) {\n+          throw new SerDeException(\"Please provide an existing table or a valid schema\", nte);\n+        }\n+      }\n+      inspector = IcebergObjectInspector.create(schema);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MDMyMw=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTU4OTk1OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMTowODoxNlrOHXGQrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNzoxNTo1N1rOHXM1kA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MTg2OQ==", "bodyText": "When setting instance fields, we prefix the field with this. to make it obvious it isn't a local variable.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r493981869", "createdAt": "2020-09-24T01:08:16Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA4OTYxNg==", "bodyText": "You have already mentioned once. Will not forget next time. Sorry.\nDone", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r494089616", "createdAt": "2020-09-24T07:15:57Z", "author": {"login": "pvary"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MTg2OQ=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTU5MDk2OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMTowODo1NFrOHXGRRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwNzoxNzo1NlrOHXM5bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjAyMg==", "bodyText": "Isn't this already logged in the catalog implementation?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r493982022", "createdAt": "2020-09-24T01:08:54Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");\n+      }\n+\n+      // Set the table type even for non HiveCatalog based tables\n+      hmsTable.getParameters().put(BaseMetastoreTableOperations.TABLE_TYPE_PROP,\n+          BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase());\n+\n+      // Remove creation related properties\n+      PARAMETERS_TO_REMOVE.forEach(hmsTable.getParameters()::remove);\n+    }\n+  }\n+\n+  @Override\n+  public void rollbackCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    // do nothing\n+  }\n+\n+  @Override\n+  public void commitCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    if (icebergTable == null) {\n+      catalogProperties.put(HiveTableOperations.TABLE_FROM_HIVE, true);\n+      LOG.info(\"Iceberg table creation with the following properties {}\", catalogProperties.keySet());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA5MDYwNw==", "bodyText": "Removed", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r494090607", "createdAt": "2020-09-24T07:17:56Z", "author": {"login": "pvary"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");\n+      }\n+\n+      // Set the table type even for non HiveCatalog based tables\n+      hmsTable.getParameters().put(BaseMetastoreTableOperations.TABLE_TYPE_PROP,\n+          BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase());\n+\n+      // Remove creation related properties\n+      PARAMETERS_TO_REMOVE.forEach(hmsTable.getParameters()::remove);\n+    }\n+  }\n+\n+  @Override\n+  public void rollbackCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    // do nothing\n+  }\n+\n+  @Override\n+  public void commitCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    if (icebergTable == null) {\n+      catalogProperties.put(HiveTableOperations.TABLE_FROM_HIVE, true);\n+      LOG.info(\"Iceberg table creation with the following properties {}\", catalogProperties.keySet());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjAyMg=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 108}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTU5MjE5OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "isResolved": true, "comments": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMTowOTozOFrOHXGR9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNDo0NDowNlrOHbN1sA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA==", "bodyText": "Can you explain this option?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r493982198", "createdAt": "2020-09-24T01:09:38Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDExNDU1OQ==", "bodyText": "I can see 2 different use-cases for Hive tables Iceberg tables:\n\nTable is created and used only from Hive. When we drop the table we want to drop the underlying Iceberg table as well\nTable is created outside of Hive but want to read it from Hive as well. We create the Hive table above the Iceberg table but when we drop the Hive table we do not want to drop the underlying Iceberg table.\n\nThe proposed solution is:\n\nDefault behavior: When the table is created from Hive and it needs to create the underlying Iceberg table then Hive will master the data. If the underlying Iceberg table is already there then Hive will not master the data. When the Hive table is dropped then the underlying Iceberg table is only dropped if Hive is mastering the data.\nHIVE_DELETE_BACKING_TABLE could override the default behavior when dropping the table\n\nThere is 1 exception: If the HiveCatalog is used it is obviously not possible to keep the table if we drop the HMS table. That is why this check is needed.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r494114559", "createdAt": "2020-09-24T07:59:10Z", "author": {"login": "pvary"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU2ODc3Ng==", "bodyText": "Would it work to drop the table if it is a HMS table and not if it uses a non-Hive catalog?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r494568776", "createdAt": "2020-09-24T19:44:36Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU3MDAwNA==", "bodyText": "Or alternatively, could we use the existing EXTERNAL option? I think that does basically the same thing for data in paths that Hive points to.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r494570004", "createdAt": "2020-09-24T19:46:50Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU4OTUzOQ==", "bodyText": "The EXTERNAL, MANAGED definition shifted a little bit in Hive lately.\nPreviously the only real difference was that when the table was dropped, the data of EXTERNAL tables was not removed, while the data of the MANAGED tables were cleaned up. Since Hive 3 MANAGED means \"do not touch my directories because there will be a data loss\", and even for EXTERNAL tables we have a new property external.table.purge, which means that the data should be cleaned up if the table is dropped.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r494589539", "createdAt": "2020-09-24T20:24:58Z", "author": {"login": "pvary"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDYwNDc3Ng==", "bodyText": "I don't quite understand the new definition of MANAGED that you're talking about, but I think we should try to make it work. We could still respect external.table.purge. I'll try to find some docs on the new definition.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r494604776", "createdAt": "2020-09-24T20:54:48Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDYwNjEyNQ==", "bodyText": "Also: if we know that Hive is creating the table, we should definitely set the correct storage handler and serde. Tables that are created by Spark might be used by Hive (and should depend on hive-site.xml) but we can assume that tables created by Hive have the Iceberg library present and can be set up automatically, I think.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r494606125", "createdAt": "2020-09-24T20:57:24Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDk4NDg1Mg==", "bodyText": "Maybe this doc can help understand the new MANAGED tables: https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/using-hiveql/content/hive_hive_3_tables.html", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r494984852", "createdAt": "2020-09-25T13:24:41Z", "author": {"login": "pvary"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIxNjY4OQ==", "bodyText": "It seems like the distinction between external and managed still fits. @omalley, what do you think about this?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r495216689", "createdAt": "2020-09-25T20:30:56Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA3NzMyNw==", "bodyText": "Since managed means that it is hive-owned and in hive acid layout, it doesn't seem like we should have iceberg tables that hive thinks are managed.\nIt doesn't seem like we should have a separate property for deleting the external table's data and should just use the external.table.purge table property that Hive already uses. It probably makes sense to also delete the data when:\n\nWe are using the HiveCatalog.\nThe iceberg location property is set.\n\nsince once the Hive metadata is deleted, it won't be recoverable. Rather than having separate logic, it might make sense to have the HiveCatalog set that property when it creates a table.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497077327", "createdAt": "2020-09-29T21:42:19Z", "author": {"login": "omalley"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA5ODc3OQ==", "bodyText": "managed means that it is hive-owned and in hive acid layout\n\nThanks! I didn't realize that managed now means that it uses the Hive ACID layout.\nI agree that if we can reuse a property, then we should. I like the idea to use external.table.purge.\nI would set that automatically when creating a table from Hive, but I don't think it makes sense to drop the table data automatically based on using HiveCatalog. Leaving metadata in place to be cleaned up by something like RemoveOrphanFilesAction is nice so that you can un-delete tables when a user dropped one accidentally. For example, we've seen lots of cases where tables in the wrong db are dropped because the user thought they had run USE.\nI'd say let's keep it simple and just use the existing Hive property.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497098779", "createdAt": "2020-09-29T22:32:40Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc3OTcxNw==", "bodyText": "Moved to using the external.table.purge, but realized that calling Catalogs.dropTable for HiveCatalog tables is not enough, since the table had been already dropped from the HMS, so the HiveCatalog will not find it.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497779717", "createdAt": "2020-09-30T20:26:33Z", "author": {"login": "pvary"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc4NTMyMQ==", "bodyText": "Could we detect this in the pre-delete method and set a flag so the post-delete method can purge?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497785321", "createdAt": "2020-09-30T20:37:17Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc5MTIwMQ==", "bodyText": "I was thinking around these lines.\nThe problem is that when we drop the table we drop the current snapshot as well.\nWe can access everything through Table object but for removing the data / metadata files, we need some more info. I know of CatalogUtil.dropTableData(FileIO io, TableMetadata metadata), but for that we would need TableMetadata. I might be able to store the snapshot location and later use that, but it is quite late here and this needs more digging \ud83d\ude04", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497791201", "createdAt": "2020-09-30T20:48:54Z", "author": {"login": "pvary"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgzODE0Mw==", "bodyText": "StaticTableOperations might help here. We use that to inspect metadata that has already expired in the ExpireSnapshotsAction. That allows you to create a static table based on an older metadata location.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497838143", "createdAt": "2020-09-30T22:33:42Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODMwMDMzNg==", "bodyText": "Good pointers! Big thanks!\nFound solution which looks nice \ud83d\ude04", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r498300336", "createdAt": "2020-10-01T14:44:06Z", "author": {"login": "pvary"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjE5OA=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTU5NDM0OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwMToxMDo1MVrOHXGTLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNFQwOTo0Mzo0NlrOHXSYOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjUwOQ==", "bodyText": "I think what is happening here should be documented clearly in the code.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r493982509", "createdAt": "2020-09-24T01:10:51Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");\n+      }\n+\n+      // Set the table type even for non HiveCatalog based tables\n+      hmsTable.getParameters().put(BaseMetastoreTableOperations.TABLE_TYPE_PROP,\n+          BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase());\n+\n+      // Remove creation related properties\n+      PARAMETERS_TO_REMOVE.forEach(hmsTable.getParameters()::remove);\n+    }\n+  }\n+\n+  @Override\n+  public void rollbackCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    // do nothing\n+  }\n+\n+  @Override\n+  public void commitCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    if (icebergTable == null) {\n+      catalogProperties.put(HiveTableOperations.TABLE_FROM_HIVE, true);\n+      LOG.info(\"Iceberg table creation with the following properties {}\", catalogProperties.keySet());\n+      Catalogs.createTable(conf, catalogProperties);\n+    }\n+  }\n+\n+  @Override\n+  public void preDropTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) throws MetaException {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    deleteIcebergTable = hmsTable.getParameters() != null &&\n+        \"TRUE\".equalsIgnoreCase(hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE));\n+\n+    if (!deleteIcebergTable) {\n+      if (!Catalogs.canWorkWithoutHive(conf)) {\n+        // This should happen only if someone were manually removing this property from the table, or\n+        // added the table from outside of Hive\n+        throw new MetaException(\"Can not drop Hive table and keep Iceberg table data when using HiveCatalog. \" +\n+            \"Please add \" + InputFormatConfig.HIVE_DELETE_BACKING_TABLE + \"='TRUE' to TBLPROPERTIES \" +\n+            \"of the Hive table to enable dropping\");\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void rollbackDropTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    // do nothing\n+  }\n+\n+  @Override\n+  public void commitDropTable(org.apache.hadoop.hive.metastore.api.Table hmsTable, boolean deleteData) {\n+    if (deleteData && deleteIcebergTable) {\n+      LOG.info(\"Dropping with purge all the data for table {}.{}\", hmsTable.getDbName(), hmsTable.getTableName());\n+      Catalogs.dropTable(conf, catalogProperties);\n+    }\n+  }\n+\n+  private Properties getCatalogProperties(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    Properties properties = new Properties();\n+    properties.putAll(hmsTable.getParameters());\n+\n+    if (properties.get(Catalogs.LOCATION) == null &&\n+        hmsTable.getSd() != null && hmsTable.getSd().getLocation() != null) {\n+      properties.put(Catalogs.LOCATION, hmsTable.getSd().getLocation());\n+    }\n+\n+    if (properties.get(Catalogs.NAME) == null) {\n+      properties.put(Catalogs.NAME, hmsTable.getDbName() + \".\" + hmsTable.getTableName());\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDE4MDQxMA==", "bodyText": "Added a comment, and modified a bit to use TableIdentifier instead of concatenating the strings", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r494180410", "createdAt": "2020-09-24T09:43:46Z", "author": {"login": "pvary"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,161 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.HashSet;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = Stream\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME)\n+      .collect(Collectors.toCollection(HashSet::new));\n+  private static final Set<String> PROPERTIES_TO_REMOVE = Stream\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"storage_handler\", \"EXTERNAL\")\n+      .collect(Collectors.toCollection(HashSet::new));\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);\n+      }\n+\n+      // Allow purging table data if the table is created now and not set otherwise\n+      if (hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE) == null) {\n+        hmsTable.getParameters().put(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, \"TRUE\");\n+      }\n+\n+      // Set the table type even for non HiveCatalog based tables\n+      hmsTable.getParameters().put(BaseMetastoreTableOperations.TABLE_TYPE_PROP,\n+          BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase());\n+\n+      // Remove creation related properties\n+      PARAMETERS_TO_REMOVE.forEach(hmsTable.getParameters()::remove);\n+    }\n+  }\n+\n+  @Override\n+  public void rollbackCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    // do nothing\n+  }\n+\n+  @Override\n+  public void commitCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    if (icebergTable == null) {\n+      catalogProperties.put(HiveTableOperations.TABLE_FROM_HIVE, true);\n+      LOG.info(\"Iceberg table creation with the following properties {}\", catalogProperties.keySet());\n+      Catalogs.createTable(conf, catalogProperties);\n+    }\n+  }\n+\n+  @Override\n+  public void preDropTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) throws MetaException {\n+    catalogProperties = getCatalogProperties(hmsTable);\n+    deleteIcebergTable = hmsTable.getParameters() != null &&\n+        \"TRUE\".equalsIgnoreCase(hmsTable.getParameters().get(InputFormatConfig.HIVE_DELETE_BACKING_TABLE));\n+\n+    if (!deleteIcebergTable) {\n+      if (!Catalogs.canWorkWithoutHive(conf)) {\n+        // This should happen only if someone were manually removing this property from the table, or\n+        // added the table from outside of Hive\n+        throw new MetaException(\"Can not drop Hive table and keep Iceberg table data when using HiveCatalog. \" +\n+            \"Please add \" + InputFormatConfig.HIVE_DELETE_BACKING_TABLE + \"='TRUE' to TBLPROPERTIES \" +\n+            \"of the Hive table to enable dropping\");\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void rollbackDropTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    // do nothing\n+  }\n+\n+  @Override\n+  public void commitDropTable(org.apache.hadoop.hive.metastore.api.Table hmsTable, boolean deleteData) {\n+    if (deleteData && deleteIcebergTable) {\n+      LOG.info(\"Dropping with purge all the data for table {}.{}\", hmsTable.getDbName(), hmsTable.getTableName());\n+      Catalogs.dropTable(conf, catalogProperties);\n+    }\n+  }\n+\n+  private Properties getCatalogProperties(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    Properties properties = new Properties();\n+    properties.putAll(hmsTable.getParameters());\n+\n+    if (properties.get(Catalogs.LOCATION) == null &&\n+        hmsTable.getSd() != null && hmsTable.getSd().getLocation() != null) {\n+      properties.put(Catalogs.LOCATION, hmsTable.getSd().getLocation());\n+    }\n+\n+    if (properties.get(Catalogs.NAME) == null) {\n+      properties.put(Catalogs.NAME, hmsTable.getDbName() + \".\" + hmsTable.getTableName());\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4MjUwOQ=="}, "originalCommit": {"oid": "dc7c1c61197c93bb26215fd945f8e7c83749bd29"}, "originalPosition": 154}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwMTg2NzA1OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yN1QwODoyNjowMVrOHYltdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxODozMjo1MFrOHZKx3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTU0NTcxOQ==", "bodyText": "PartitionSpecParser.fromJson(schema, schemaString)\n\nshould be replaced with\n\nPartitionSpecParser.fromJson(schema, specString)\n\nRight?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r495545719", "createdAt": "2020-09-27T08:26:01Z", "author": {"login": "qphien"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.Properties;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = ImmutableSet\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME);\n+  private static final Set<String> PROPERTIES_TO_REMOVE = ImmutableSet\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, hive_metastoreConstants.META_TABLE_STORAGE, \"EXTERNAL\");\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    this.catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      this.icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bc13a60e8987e53828b1f5c6cd3dd1f2654c1e57"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE1MzA1NA==", "bodyText": "Good catch!\nThanks.\nFixed, and added tests", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r496153054", "createdAt": "2020-09-28T18:32:50Z", "author": {"login": "pvary"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.Properties;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = ImmutableSet\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME);\n+  private static final Set<String> PROPERTIES_TO_REMOVE = ImmutableSet\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, hive_metastoreConstants.META_TABLE_STORAGE, \"EXTERNAL\");\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;\n+  private Properties catalogProperties;\n+  private boolean deleteIcebergTable;\n+\n+  public HiveIcebergMetaHook(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void preCreateTable(org.apache.hadoop.hive.metastore.api.Table hmsTable) {\n+    this.catalogProperties = getCatalogProperties(hmsTable);\n+    try {\n+      this.icebergTable = Catalogs.loadTable(conf, catalogProperties);\n+\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA) == null,\n+          \"Iceberg table already created - can not use provided schema\");\n+      Preconditions.checkArgument(catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC) == null,\n+          \"Iceberg table already created - can not use provided partition specification\");\n+\n+      LOG.info(\"Iceberg table already exists {}\", icebergTable);\n+    } catch (NoSuchTableException nte) {\n+      String schemaString = catalogProperties.getProperty(InputFormatConfig.TABLE_SCHEMA);\n+      Preconditions.checkNotNull(schemaString, \"Please provide a table schema\");\n+      // Just check if it is parsable, and later use for partition specification parsing\n+      Schema schema = SchemaParser.fromJson(schemaString);\n+\n+      String specString = catalogProperties.getProperty(InputFormatConfig.PARTITION_SPEC);\n+      if (specString != null) {\n+        // Just check if it is parsable\n+        PartitionSpecParser.fromJson(schema, schemaString);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTU0NTcxOQ=="}, "originalCommit": {"oid": "bc13a60e8987e53828b1f5c6cd3dd1f2654c1e57"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMDc3OTEzOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNzoyMTowMlrOHZ4_Pg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDozNjozN1rOHaATyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkxMDE0Mg==", "bodyText": "Is this correct?\nIf this happens, then the table exists and is not an Iceberg table. That is expected if we are creating the table from the Hive hook, but in that case I think that we will have injected metadata in the hook that signals to the TableOperations implementation that the table was pre-created by Hive and should be replaced (so update will be used for the initial commit). If we detect that in TableOperations then we should be able to avoid throwing this exception and return null like before.\nIf the operation is not happening inside of the Hive hooks, then we do want to let the exception propagate because there is an existing Hive table.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r496910142", "createdAt": "2020-09-29T17:21:02Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java", "diffHunk": "@@ -215,8 +216,15 @@ public TableBuilder withProperty(String key, String value) {\n     @Override\n     public Table create() {\n       TableOperations ops = newTableOps(identifier);\n-      if (ops.current() != null) {\n-        throw new AlreadyExistsException(\"Table already exists: %s\", identifier);\n+      try {\n+        if (ops.current() != null) {\n+          throw new AlreadyExistsException(\"Table already exists: %s\", identifier);\n+        }\n+      } catch (NoSuchIcebergTableException ne) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "794b8390c3d9adbe1d3a8358a57b84e22fd3c0c7"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzMDA5MA==", "bodyText": "Removed. With the new HMS table property this is not needed anymore.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497030090", "createdAt": "2020-09-29T20:36:37Z", "author": {"login": "pvary"}, "path": "core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java", "diffHunk": "@@ -215,8 +216,15 @@ public TableBuilder withProperty(String key, String value) {\n     @Override\n     public Table create() {\n       TableOperations ops = newTableOps(identifier);\n-      if (ops.current() != null) {\n-        throw new AlreadyExistsException(\"Table already exists: %s\", identifier);\n+      try {\n+        if (ops.current() != null) {\n+          throw new AlreadyExistsException(\"Table already exists: %s\", identifier);\n+        }\n+      } catch (NoSuchIcebergTableException ne) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkxMDE0Mg=="}, "originalCommit": {"oid": "794b8390c3d9adbe1d3a8358a57b84e22fd3c0c7"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMDg0MTIzOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNzozNjo0OVrOHZ5mKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDo1Mzo0MFrOHaBXnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkyMDEwNQ==", "bodyText": "Are we sure that it is okay for this class to have state? What is the lifecycle of hooks in Hive? I would assume that there is one instance of this class that is used for all operations. But the instance fields that hold a table that is being created seems to assume that there is one instance of the class per operation.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r496920105", "createdAt": "2020-09-29T17:36:49Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.Properties;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = ImmutableSet\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME);\n+  private static final Set<String> PROPERTIES_TO_REMOVE = ImmutableSet\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, hive_metastoreConstants.META_TABLE_STORAGE, \"EXTERNAL\");\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "794b8390c3d9adbe1d3a8358a57b84e22fd3c0c7"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA0MzA3MA==", "bodyText": "The hook is created by the HiveIcebergStorageHandler every time:\nHive code\nOur code:\n  @Override\n  public HiveMetaHook getMetaHook() {\n    return new HiveIcebergMetaHook(conf);\n  }\n\nUsed by hive like this:\n    HiveMetaHook hook = getHook(tbl);\n    if (hook != null) {\n      hook.preCreateTable(tbl);\n    }\n    boolean success = false;\n    try {\n      // Subclasses can override this step (for example, for temporary tables)\n      client.create_table_req(request);\n      if (hook != null) {\n        hook.commitCreateTable(tbl);\n      }\n      success = true;\n    } finally {\n      if (!success && (hook != null)) {\n        try {\n          hook.rollbackCreateTable(tbl);\n        } catch (Exception e) {\n          LOG.error(\"Create rollback failed with\", e);\n        }\n      }\n    }\nThis is fairly stable part of the code as integration with other systems depends on it, but I have not seen the behavior documented", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497043070", "createdAt": "2020-09-29T20:49:20Z", "author": {"login": "pvary"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.Properties;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = ImmutableSet\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME);\n+  private static final Set<String> PROPERTIES_TO_REMOVE = ImmutableSet\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, hive_metastoreConstants.META_TABLE_STORAGE, \"EXTERNAL\");\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkyMDEwNQ=="}, "originalCommit": {"oid": "794b8390c3d9adbe1d3a8358a57b84e22fd3c0c7"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA0NzQ1Mg==", "bodyText": "Sounds good to me. Thanks!", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497047452", "createdAt": "2020-09-29T20:53:40Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java", "diffHunk": "@@ -0,0 +1,169 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.Properties;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.HiveMetaHook;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.hadoop.hive.metastore.api.hive_metastoreConstants;\n+import org.apache.iceberg.BaseMetastoreTableOperations;\n+import org.apache.iceberg.PartitionSpecParser;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.hive.HiveTableOperations;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class HiveIcebergMetaHook implements HiveMetaHook {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergMetaHook.class);\n+  private static final Set<String> PARAMETERS_TO_REMOVE = ImmutableSet\n+      .of(InputFormatConfig.TABLE_SCHEMA, InputFormatConfig.PARTITION_SPEC, Catalogs.LOCATION, Catalogs.NAME);\n+  private static final Set<String> PROPERTIES_TO_REMOVE = ImmutableSet\n+      .of(InputFormatConfig.HIVE_DELETE_BACKING_TABLE, hive_metastoreConstants.META_TABLE_STORAGE, \"EXTERNAL\");\n+\n+  private final Configuration conf;\n+  private Table icebergTable = null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkyMDEwNQ=="}, "originalCommit": {"oid": "794b8390c3d9adbe1d3a8358a57b84e22fd3c0c7"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTU4OTczOnYy", "diffSide": "RIGHT", "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDo0MzoyNFrOHaAu2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMTowMDoxMFrOHaBxRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzNzAxOA==", "bodyText": "Can we remove this? If the debug log shows that this path was taken, then it is redundant.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497037018", "createdAt": "2020-09-29T20:43:24Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -142,16 +146,22 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {\n     String newMetadataLocation = writeNewMetadata(metadata, currentVersion() + 1);\n \n     boolean threw = true;\n+    boolean updateHiveTable = false;\n     Optional<Long> lockId = Optional.empty();\n     try {\n       lockId = Optional.of(acquireLock());\n       // TODO add lock heart beating for cases where default lock timeout is too low.\n       Table tbl;\n-      if (base != null) {\n-        LOG.debug(\"Committing existing table: {}\", fullName);\n+      try {\n         tbl = metaClients.run(client -> client.getTable(database, tableName));\n+        if (base == null && tbl.getParameters().get(TABLE_CREATION_FROM_HIVE) == null) {\n+          throw new AlreadyExistsException(\"Table already exists: %s.%s\", database, tableName);\n+        }\n+        updateHiveTable = true;\n+        LOG.debug(\"Committing existing table: {}\", fullName);\n         tbl.setSd(storageDescriptor(metadata)); // set to pickup any schema changes\n-      } else {\n+      } catch (NoSuchObjectException nte) {\n+        LOG.trace(\"Table not found {}\", fullName, nte);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c18a3d580ac5b2f9d6bc4a01ca18a64d9f9062b"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA0NjE2OA==", "bodyText": "Can we suppress the warning instead?\n> Task :iceberg-hive-metastore:compileJava\n/Users/petervary/dev/upstream/iceberg/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java:163: error: [CatchBlockLogException] Catch block contains log statements but thrown exception is never logged.\n      } catch (NoSuchObjectException nte) {\n        ^\n    (see https://github.com/palantir/gradle-baseline#baseline-error-prone-checks)\n  Did you mean 'LOG.debug(\"Committing new table: {}\", fullName, nte);'?\n\nOr we should just log the exception on debug level?\nI wanted to dig the exception as deep as possible, as it is normal to have it \ud83d\ude04", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497046168", "createdAt": "2020-09-29T20:52:22Z", "author": {"login": "pvary"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -142,16 +146,22 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {\n     String newMetadataLocation = writeNewMetadata(metadata, currentVersion() + 1);\n \n     boolean threw = true;\n+    boolean updateHiveTable = false;\n     Optional<Long> lockId = Optional.empty();\n     try {\n       lockId = Optional.of(acquireLock());\n       // TODO add lock heart beating for cases where default lock timeout is too low.\n       Table tbl;\n-      if (base != null) {\n-        LOG.debug(\"Committing existing table: {}\", fullName);\n+      try {\n         tbl = metaClients.run(client -> client.getTable(database, tableName));\n+        if (base == null && tbl.getParameters().get(TABLE_CREATION_FROM_HIVE) == null) {\n+          throw new AlreadyExistsException(\"Table already exists: %s.%s\", database, tableName);\n+        }\n+        updateHiveTable = true;\n+        LOG.debug(\"Committing existing table: {}\", fullName);\n         tbl.setSd(storageDescriptor(metadata)); // set to pickup any schema changes\n-      } else {\n+      } catch (NoSuchObjectException nte) {\n+        LOG.trace(\"Table not found {}\", fullName, nte);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzNzAxOA=="}, "originalCommit": {"oid": "2c18a3d580ac5b2f9d6bc4a01ca18a64d9f9062b"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA0OTMwMQ==", "bodyText": "It's fine to suppress the exception. I just would normally not bother logging it.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497049301", "createdAt": "2020-09-29T20:55:17Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -142,16 +146,22 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {\n     String newMetadataLocation = writeNewMetadata(metadata, currentVersion() + 1);\n \n     boolean threw = true;\n+    boolean updateHiveTable = false;\n     Optional<Long> lockId = Optional.empty();\n     try {\n       lockId = Optional.of(acquireLock());\n       // TODO add lock heart beating for cases where default lock timeout is too low.\n       Table tbl;\n-      if (base != null) {\n-        LOG.debug(\"Committing existing table: {}\", fullName);\n+      try {\n         tbl = metaClients.run(client -> client.getTable(database, tableName));\n+        if (base == null && tbl.getParameters().get(TABLE_CREATION_FROM_HIVE) == null) {\n+          throw new AlreadyExistsException(\"Table already exists: %s.%s\", database, tableName);\n+        }\n+        updateHiveTable = true;\n+        LOG.debug(\"Committing existing table: {}\", fullName);\n         tbl.setSd(storageDescriptor(metadata)); // set to pickup any schema changes\n-      } else {\n+      } catch (NoSuchObjectException nte) {\n+        LOG.trace(\"Table not found {}\", fullName, nte);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzNzAxOA=="}, "originalCommit": {"oid": "2c18a3d580ac5b2f9d6bc4a01ca18a64d9f9062b"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA1NDAyMw==", "bodyText": "Actually, I think you're right. Let's just log it at debug. I see what you're saying now.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497054023", "createdAt": "2020-09-29T21:00:10Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -142,16 +146,22 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {\n     String newMetadataLocation = writeNewMetadata(metadata, currentVersion() + 1);\n \n     boolean threw = true;\n+    boolean updateHiveTable = false;\n     Optional<Long> lockId = Optional.empty();\n     try {\n       lockId = Optional.of(acquireLock());\n       // TODO add lock heart beating for cases where default lock timeout is too low.\n       Table tbl;\n-      if (base != null) {\n-        LOG.debug(\"Committing existing table: {}\", fullName);\n+      try {\n         tbl = metaClients.run(client -> client.getTable(database, tableName));\n+        if (base == null && tbl.getParameters().get(TABLE_CREATION_FROM_HIVE) == null) {\n+          throw new AlreadyExistsException(\"Table already exists: %s.%s\", database, tableName);\n+        }\n+        updateHiveTable = true;\n+        LOG.debug(\"Committing existing table: {}\", fullName);\n         tbl.setSd(storageDescriptor(metadata)); // set to pickup any schema changes\n-      } else {\n+      } catch (NoSuchObjectException nte) {\n+        LOG.trace(\"Table not found {}\", fullName, nte);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzNzAxOA=="}, "originalCommit": {"oid": "2c18a3d580ac5b2f9d6bc4a01ca18a64d9f9062b"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTU5NzIxOnYy", "diffSide": "RIGHT", "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDo0NDo0MFrOHaAz6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDo1Mzo0MVrOHaBXtw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzODMxMg==", "bodyText": "Why rename the variable here?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497038312", "createdAt": "2020-09-29T20:44:40Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -179,17 +189,18 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {\n \n       setParameters(newMetadataLocation, tbl);\n \n-      if (base != null) {\n+      Table newTable = tbl;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c18a3d580ac5b2f9d6bc4a01ca18a64d9f9062b"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA0NzQ3OQ==", "bodyText": "Otherwise I can not use it to the lambda expressions. We need \"final\" variables in lambda calls.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497047479", "createdAt": "2020-09-29T20:53:41Z", "author": {"login": "pvary"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -179,17 +189,18 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {\n \n       setParameters(newMetadataLocation, tbl);\n \n-      if (base != null) {\n+      Table newTable = tbl;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzODMxMg=="}, "originalCommit": {"oid": "2c18a3d580ac5b2f9d6bc4a01ca18a64d9f9062b"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTY3NDI5OnYy", "diffSide": "RIGHT", "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDo1NzoxOVrOHaBmog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMDoxMTowN1rOHatktQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA1MTI5OA==", "bodyText": "Is this needed? TABLE_TYPE_PROP is set in the pre-commit hook. In fact, can we just use that instead of TABLE_CREATION_FROM_HIVE?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497051298", "createdAt": "2020-09-29T20:57:19Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -313,6 +327,11 @@ protected void doUnlock(long lockId) throws TException, InterruptedException {\n   }\n \n   static void validateTableIsIceberg(Table table, String fullName) {\n+    if (\"TRUE\".equalsIgnoreCase(table.getParameters().get(TABLE_CREATION_FROM_HIVE))) {\n+      // No check is needed. The table has been created from Hive (HiveIcebergMetaHook), and we are about to initialize\n+      // the Iceberg metadata\n+      return;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2c18a3d580ac5b2f9d6bc4a01ca18a64d9f9062b"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA1NTA2Ng==", "bodyText": "TABLE_TYPE_PROP is set, but METADATA_LOCATION_PROP is not set, and it would throw the exception. We want to bail out of the checks only if this is indeed in the table creation phase. In other places I have tried to keep to the original behavior.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497055066", "createdAt": "2020-09-29T21:01:21Z", "author": {"login": "pvary"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -313,6 +327,11 @@ protected void doUnlock(long lockId) throws TException, InterruptedException {\n   }\n \n   static void validateTableIsIceberg(Table table, String fullName) {\n+    if (\"TRUE\".equalsIgnoreCase(table.getParameters().get(TABLE_CREATION_FROM_HIVE))) {\n+      // No check is needed. The table has been created from Hive (HiveIcebergMetaHook), and we are about to initialize\n+      // the Iceberg metadata\n+      return;\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA1MTI5OA=="}, "originalCommit": {"oid": "2c18a3d580ac5b2f9d6bc4a01ca18a64d9f9062b"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA2NTUyMw==", "bodyText": "Okay, I see. I think I would prefer to simplify this and make it so that the table is Iceberg if and only if the table property is there. If we don't have the metadata location, then we can set metadata to null and everything else should work like normal.\nThat avoids the need for an extra property, which is a good thing because we want to keep the TableOperations as simple as possible.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497065523", "createdAt": "2020-09-29T21:18:15Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -313,6 +327,11 @@ protected void doUnlock(long lockId) throws TException, InterruptedException {\n   }\n \n   static void validateTableIsIceberg(Table table, String fullName) {\n+    if (\"TRUE\".equalsIgnoreCase(table.getParameters().get(TABLE_CREATION_FROM_HIVE))) {\n+      // No check is needed. The table has been created from Hive (HiveIcebergMetaHook), and we are about to initialize\n+      // the Iceberg metadata\n+      return;\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA1MTI5OA=="}, "originalCommit": {"oid": "2c18a3d580ac5b2f9d6bc4a01ca18a64d9f9062b"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc3MTcwMQ==", "bodyText": "Done. Had to change a single tests, but that's seems ok.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497771701", "createdAt": "2020-09-30T20:11:07Z", "author": {"login": "pvary"}, "path": "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java", "diffHunk": "@@ -313,6 +327,11 @@ protected void doUnlock(long lockId) throws TException, InterruptedException {\n   }\n \n   static void validateTableIsIceberg(Table table, String fullName) {\n+    if (\"TRUE\".equalsIgnoreCase(table.getParameters().get(TABLE_CREATION_FROM_HIVE))) {\n+      // No check is needed. The table has been created from Hive (HiveIcebergMetaHook), and we are about to initialize\n+      // the Iceberg metadata\n+      return;\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA1MTI5OA=="}, "originalCommit": {"oid": "2c18a3d580ac5b2f9d6bc4a01ca18a64d9f9062b"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjM0MTk3OnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMDo0NTo0NFrOHauqdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMDo0NTo0NFrOHauqdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc4OTU1Nw==", "bodyText": "SchemaParser has a cache, so that should help some.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497789557", "createdAt": "2020-09-30T20:45:44Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "diffHunk": "@@ -27,21 +27,41 @@\n import org.apache.hadoop.hive.serde2.SerDeStats;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n import org.apache.hadoop.io.Writable;\n-import org.apache.iceberg.Table;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergObjectInspector;\n import org.apache.iceberg.mr.mapred.Container;\n \n public class HiveIcebergSerDe extends AbstractSerDe {\n-\n+  private Schema schema;\n   private ObjectInspector inspector;\n \n   @Override\n   public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n-    Table table = Catalogs.loadTable(configuration, serDeProperties);\n-\n     try {\n-      this.inspector = IcebergObjectInspector.create(table.schema());\n+      // HiveIcebergSerDe.initialize is called multiple places in Hive code:\n+      // - When we are trying to create a table - HiveDDL data is stored at the serDeProperties, but no Iceberg table\n+      // is created yet.\n+      // - When we are compiling the Hive query on HiveServer2 side - We only have table information (location/name),\n+      // and we have to read the schema using the table data. This is called multiple times so there is room for\n+      // optimizing here.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjM2OTkwOnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMDo1MzozNVrOHau7Og==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxMToxNDo0NVrOHduV3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc5Mzg1MA==", "bodyText": "Just thinking about how this interacts with #1505: This PR relies on setting the storage handler here and it is stored in parameters. Right now, nothing will remove it, but I think the right thing to do in #1505 is to remove this if Hive is not enabled for the table.\nIf we remove the property when Hive isn't enabled, then this test would break. To fix it, I think we should always set the engine.hive.enabled property to true in the hook. That could be done either in this PR or the other one if this one is merged first.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497793850", "createdAt": "2020-09-30T20:53:35Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDkzMDAxMg==", "bodyText": "Done", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r500930012", "createdAt": "2020-10-07T11:14:45Z", "author": {"login": "pvary"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc5Mzg1MA=="}, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjM3MzgwOnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMDo1NDo0OFrOHau9rw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMzoxNjowM1rOHbJ28Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc5NDQ3OQ==", "bodyText": "You should be able to use PartitionSpec in the assertEquals because its equals method is implemented.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497794479", "createdAt": "2020-09-30T20:54:48Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODIzNTEyMQ==", "bodyText": "Done", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r498235121", "createdAt": "2020-10-01T13:16:03Z", "author": {"login": "pvary"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc5NDQ3OQ=="}, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjM3NjgyOnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMDo1NTozOVrOHau_fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxMzoxNjoxM1rOHbJ3Zg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc5NDk0Mw==", "bodyText": "Instead of comparing the strings, it would be better to compare icebergTable.schema().asStruct(). StructType implements equals, but Schema doesn't because it may have extra metadata (like aliases).", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497794943", "createdAt": "2020-09-30T20:55:39Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODIzNTIzOA==", "bodyText": "Done", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r498235238", "createdAt": "2020-10-01T13:16:13Z", "author": {"login": "pvary"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzc5NDk0Mw=="}, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjY2ODAxOnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjozNzo1NVrOHaxt_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwNzoxMDo0N1rOHgYV1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgzOTYxMg==", "bodyText": "We might want to use a shared HiveClientPool for this. We've had problems in the past where too many clients led to the HMS becoming unresponsive in tests. It's really annoying and makes tests flaky. Sharing a pool across all tests fixes the problem, and makes us more confident that if we hit a connection issue, it is probably in prod code and not test.\nI think it would also make the test cases smaller because you wouldn't need try/finally blocks.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497839612", "createdAt": "2020-09-30T22:37:55Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+    Assert.assertEquals(Collections.singletonMap(\"dummy\", \"test\"), icebergTable.properties());\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI2NTgyNQ==", "bodyText": "We are starting a new TestHiveMetastore() in the before method. So every test uses its own HMS instance.\nSo I think reusing HMS connection is not really an option until we use the same HMS instance throughout the tests", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r498265825", "createdAt": "2020-10-01T13:58:36Z", "author": {"login": "pvary"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+    Assert.assertEquals(Collections.singletonMap(\"dummy\", \"test\"), icebergTable.properties());\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgzOTYxMg=="}, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODM5NDIzOQ==", "bodyText": "Oh, we aren't using a shared metastore across test cases?\nWe should probably fix that eventually (maybe get this PR in first though). We do want to keep the metastore around long enough that we detect Iceberg connection leaks. Although it is annoying to have tests fail because the metastore hangs, it's better than releasing a version that leaves connections open and takes down a metastore.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r498394239", "createdAt": "2020-10-01T17:07:01Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+    Assert.assertEquals(Collections.singletonMap(\"dummy\", \"test\"), icebergTable.properties());\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgzOTYxMg=="}, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQwMjIwOQ==", "bodyText": "It would also save a serious amount of testing time.\nI think it might need changes in HiveRunner though, we have to check", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r498402209", "createdAt": "2020-10-01T17:21:46Z", "author": {"login": "pvary"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+    Assert.assertEquals(Collections.singletonMap(\"dummy\", \"test\"), icebergTable.properties());\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgzOTYxMg=="}, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE2NjQ3Ng==", "bodyText": "We just made the change to use a suite-level metastore in #1478 and it appears to work fine. We should explore doing the same here, but probably in a follow-up.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r501166476", "createdAt": "2020-10-07T16:55:15Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+    Assert.assertEquals(Collections.singletonMap(\"dummy\", \"test\"), icebergTable.properties());\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgzOTYxMg=="}, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzcxNTI4Nw==", "bodyText": "Rebased to the master, and using the suite-level metastore", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r503715287", "createdAt": "2020-10-13T07:10:47Z", "author": {"login": "pvary"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+    Assert.assertEquals(Collections.singletonMap(\"dummy\", \"test\"), icebergTable.properties());\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzgzOTYxMg=="}, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjY3NTU4OnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjo0MToxMlrOHaxydQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNDoxMjoxNFrOHbMV4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0MDc1Nw==", "bodyText": "Should this be done in the case where the table can work without Hive? The if statement seems backward to me.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497840757", "createdAt": "2020-09-30T22:41:12Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+    Assert.assertEquals(Collections.singletonMap(\"dummy\", \"test\"), icebergTable.properties());\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;\n+    try {\n+      client = new HiveMetaStoreClient(metastore.hiveConf());\n+      hmsTable = client.getTable(\"default\", \"customers\");\n+    } finally {\n+      if (client != null) {\n+        client.close();\n+      }\n+    }\n+\n+    Map<String, String> hmsParams = hmsTable.getParameters();\n+\n+    // This is only set for HiveCatalog based tables. Check the value, then remove it so the other checks can be general\n+    if (needToCheckSnapshotLocation()) {\n+      Assert.assertTrue(hmsParams.get(BaseMetastoreTableOperations.METADATA_LOCATION_PROP)\n+          .startsWith(icebergTable.location()));\n+      hmsParams.remove(BaseMetastoreTableOperations.METADATA_LOCATION_PROP);\n+    }\n+\n+    // General metadata checks\n+    Assert.assertEquals(6, hmsParams.size());\n+    Assert.assertEquals(\"test\", hmsParams.get(\"dummy\"));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(InputFormatConfig.EXTERNAL_TABLE_PURGE));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(\"EXTERNAL\"));\n+    Assert.assertNotNull(hmsParams.get(hive_metastoreConstants.DDL_TIME));\n+    Assert.assertEquals(HiveIcebergStorageHandler.class.getName(),\n+        hmsTable.getParameters().get(hive_metastoreConstants.META_TABLE_STORAGE));\n+    Assert.assertEquals(BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase(),\n+        hmsTable.getParameters().get(BaseMetastoreTableOperations.TABLE_TYPE_PROP));\n+\n+    if (Catalogs.canWorkWithoutHive(shell.getHiveConf())) {\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if the table was really dropped even from the Catalog\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+    } else {\n+      // Check the HMS table parameters\n+      Path hmsTableLocation;\n+      try {\n+        client = new HiveMetaStoreClient(metastore.hiveConf());\n+        hmsTableLocation = new Path(client.getTable(\"default\", \"customers\").getSd().getLocation());\n+      } finally {\n+        if (client != null) {\n+          client.close();\n+        }\n+      }\n+\n+      // Drop the table\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if we drop an exception when trying to drop the table\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+\n+      // Check if the files are kept\n+      FileSystem fs = Util.getFs(hmsTableLocation, shell.getHiveConf());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI3NTgwOQ==", "bodyText": "Fixed comments \ud83d\ude04", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r498275809", "createdAt": "2020-10-01T14:12:14Z", "author": {"login": "pvary"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+    Assert.assertEquals(Collections.singletonMap(\"dummy\", \"test\"), icebergTable.properties());\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;\n+    try {\n+      client = new HiveMetaStoreClient(metastore.hiveConf());\n+      hmsTable = client.getTable(\"default\", \"customers\");\n+    } finally {\n+      if (client != null) {\n+        client.close();\n+      }\n+    }\n+\n+    Map<String, String> hmsParams = hmsTable.getParameters();\n+\n+    // This is only set for HiveCatalog based tables. Check the value, then remove it so the other checks can be general\n+    if (needToCheckSnapshotLocation()) {\n+      Assert.assertTrue(hmsParams.get(BaseMetastoreTableOperations.METADATA_LOCATION_PROP)\n+          .startsWith(icebergTable.location()));\n+      hmsParams.remove(BaseMetastoreTableOperations.METADATA_LOCATION_PROP);\n+    }\n+\n+    // General metadata checks\n+    Assert.assertEquals(6, hmsParams.size());\n+    Assert.assertEquals(\"test\", hmsParams.get(\"dummy\"));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(InputFormatConfig.EXTERNAL_TABLE_PURGE));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(\"EXTERNAL\"));\n+    Assert.assertNotNull(hmsParams.get(hive_metastoreConstants.DDL_TIME));\n+    Assert.assertEquals(HiveIcebergStorageHandler.class.getName(),\n+        hmsTable.getParameters().get(hive_metastoreConstants.META_TABLE_STORAGE));\n+    Assert.assertEquals(BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase(),\n+        hmsTable.getParameters().get(BaseMetastoreTableOperations.TABLE_TYPE_PROP));\n+\n+    if (Catalogs.canWorkWithoutHive(shell.getHiveConf())) {\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if the table was really dropped even from the Catalog\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+    } else {\n+      // Check the HMS table parameters\n+      Path hmsTableLocation;\n+      try {\n+        client = new HiveMetaStoreClient(metastore.hiveConf());\n+        hmsTableLocation = new Path(client.getTable(\"default\", \"customers\").getSd().getLocation());\n+      } finally {\n+        if (client != null) {\n+          client.close();\n+        }\n+      }\n+\n+      // Drop the table\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if we drop an exception when trying to drop the table\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+\n+      // Check if the files are kept\n+      FileSystem fs = Util.getFs(hmsTableLocation, shell.getHiveConf());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0MDc1Nw=="}, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 138}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExNjY3Njc3OnYy", "diffSide": "RIGHT", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0zMFQyMjo0MTo0NlrOHaxzKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wMVQxNzo0ODo0OFrOHbU9mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0MDkzOA==", "bodyText": "Should this be done with DROP TABLE IF EXISTS in a @Before method?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r497840938", "createdAt": "2020-09-30T22:41:46Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+    Assert.assertEquals(Collections.singletonMap(\"dummy\", \"test\"), icebergTable.properties());\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;\n+    try {\n+      client = new HiveMetaStoreClient(metastore.hiveConf());\n+      hmsTable = client.getTable(\"default\", \"customers\");\n+    } finally {\n+      if (client != null) {\n+        client.close();\n+      }\n+    }\n+\n+    Map<String, String> hmsParams = hmsTable.getParameters();\n+\n+    // This is only set for HiveCatalog based tables. Check the value, then remove it so the other checks can be general\n+    if (needToCheckSnapshotLocation()) {\n+      Assert.assertTrue(hmsParams.get(BaseMetastoreTableOperations.METADATA_LOCATION_PROP)\n+          .startsWith(icebergTable.location()));\n+      hmsParams.remove(BaseMetastoreTableOperations.METADATA_LOCATION_PROP);\n+    }\n+\n+    // General metadata checks\n+    Assert.assertEquals(6, hmsParams.size());\n+    Assert.assertEquals(\"test\", hmsParams.get(\"dummy\"));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(InputFormatConfig.EXTERNAL_TABLE_PURGE));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(\"EXTERNAL\"));\n+    Assert.assertNotNull(hmsParams.get(hive_metastoreConstants.DDL_TIME));\n+    Assert.assertEquals(HiveIcebergStorageHandler.class.getName(),\n+        hmsTable.getParameters().get(hive_metastoreConstants.META_TABLE_STORAGE));\n+    Assert.assertEquals(BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase(),\n+        hmsTable.getParameters().get(BaseMetastoreTableOperations.TABLE_TYPE_PROP));\n+\n+    if (Catalogs.canWorkWithoutHive(shell.getHiveConf())) {\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if the table was really dropped even from the Catalog\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+    } else {\n+      // Check the HMS table parameters\n+      Path hmsTableLocation;\n+      try {\n+        client = new HiveMetaStoreClient(metastore.hiveConf());\n+        hmsTableLocation = new Path(client.getTable(\"default\", \"customers\").getSd().getLocation());\n+      } finally {\n+        if (client != null) {\n+          client.close();\n+        }\n+      }\n+\n+      // Drop the table\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if we drop an exception when trying to drop the table\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+\n+      // Check if the files are kept\n+      FileSystem fs = Util.getFs(hmsTableLocation, shell.getHiveConf());\n+      // TODO: files should be deleted\n+      // Assert.assertEquals(0, fs.listStatus(hmsTableLocation).length);\n+    }\n+  }\n+\n+  @Test\n+  public void testCreateTableWithoutSpec() throws TException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table partition data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(PartitionSpecParser.toJson(SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;\n+    try {\n+      client = new HiveMetaStoreClient(metastore.hiveConf());\n+      hmsTable = client.getTable(\"default\", \"customers\");\n+    } finally {\n+      if (client != null) {\n+        client.close();\n+      }\n+    }\n+\n+    Map<String, String> hmsParams = hmsTable.getParameters();\n+\n+    // Just check that the PartitionSpec is not set in the metadata\n+    Assert.assertNull(hmsParams.get(InputFormatConfig.PARTITION_SPEC));\n+\n+    if (needToCheckSnapshotLocation()) {\n+      Assert.assertEquals(6, hmsParams.size());\n+    } else {\n+      Assert.assertEquals(5, hmsParams.size());\n+    }\n+\n+    shell.executeStatement(\"DROP TABLE customers\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODI4MDMzNg==", "bodyText": "If there is a problem with the HiveMetaHook and the DROP TABLE fails for whatever reason, then all of the tests would be failing. I would keep it this way until we are sure about the stability of the DROP TABLE. What do you think?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r498280336", "createdAt": "2020-10-01T14:18:03Z", "author": {"login": "pvary"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+    Assert.assertEquals(Collections.singletonMap(\"dummy\", \"test\"), icebergTable.properties());\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;\n+    try {\n+      client = new HiveMetaStoreClient(metastore.hiveConf());\n+      hmsTable = client.getTable(\"default\", \"customers\");\n+    } finally {\n+      if (client != null) {\n+        client.close();\n+      }\n+    }\n+\n+    Map<String, String> hmsParams = hmsTable.getParameters();\n+\n+    // This is only set for HiveCatalog based tables. Check the value, then remove it so the other checks can be general\n+    if (needToCheckSnapshotLocation()) {\n+      Assert.assertTrue(hmsParams.get(BaseMetastoreTableOperations.METADATA_LOCATION_PROP)\n+          .startsWith(icebergTable.location()));\n+      hmsParams.remove(BaseMetastoreTableOperations.METADATA_LOCATION_PROP);\n+    }\n+\n+    // General metadata checks\n+    Assert.assertEquals(6, hmsParams.size());\n+    Assert.assertEquals(\"test\", hmsParams.get(\"dummy\"));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(InputFormatConfig.EXTERNAL_TABLE_PURGE));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(\"EXTERNAL\"));\n+    Assert.assertNotNull(hmsParams.get(hive_metastoreConstants.DDL_TIME));\n+    Assert.assertEquals(HiveIcebergStorageHandler.class.getName(),\n+        hmsTable.getParameters().get(hive_metastoreConstants.META_TABLE_STORAGE));\n+    Assert.assertEquals(BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase(),\n+        hmsTable.getParameters().get(BaseMetastoreTableOperations.TABLE_TYPE_PROP));\n+\n+    if (Catalogs.canWorkWithoutHive(shell.getHiveConf())) {\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if the table was really dropped even from the Catalog\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+    } else {\n+      // Check the HMS table parameters\n+      Path hmsTableLocation;\n+      try {\n+        client = new HiveMetaStoreClient(metastore.hiveConf());\n+        hmsTableLocation = new Path(client.getTable(\"default\", \"customers\").getSd().getLocation());\n+      } finally {\n+        if (client != null) {\n+          client.close();\n+        }\n+      }\n+\n+      // Drop the table\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if we drop an exception when trying to drop the table\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+\n+      // Check if the files are kept\n+      FileSystem fs = Util.getFs(hmsTableLocation, shell.getHiveConf());\n+      // TODO: files should be deleted\n+      // Assert.assertEquals(0, fs.listStatus(hmsTableLocation).length);\n+    }\n+  }\n+\n+  @Test\n+  public void testCreateTableWithoutSpec() throws TException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table partition data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(PartitionSpecParser.toJson(SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;\n+    try {\n+      client = new HiveMetaStoreClient(metastore.hiveConf());\n+      hmsTable = client.getTable(\"default\", \"customers\");\n+    } finally {\n+      if (client != null) {\n+        client.close();\n+      }\n+    }\n+\n+    Map<String, String> hmsParams = hmsTable.getParameters();\n+\n+    // Just check that the PartitionSpec is not set in the metadata\n+    Assert.assertNull(hmsParams.get(InputFormatConfig.PARTITION_SPEC));\n+\n+    if (needToCheckSnapshotLocation()) {\n+      Assert.assertEquals(6, hmsParams.size());\n+    } else {\n+      Assert.assertEquals(5, hmsParams.size());\n+    }\n+\n+    shell.executeStatement(\"DROP TABLE customers\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0MDkzOA=="}, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODM5NTIxNA==", "bodyText": "I think it can fail here or in @After (which is what I meant to say) with the same effect. Putting it in an after method ensures that no matter what happens with the test, say a NullPointerException somewhere, the table is dropped for the next case.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r498395214", "createdAt": "2020-10-01T17:08:49Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+    Assert.assertEquals(Collections.singletonMap(\"dummy\", \"test\"), icebergTable.properties());\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;\n+    try {\n+      client = new HiveMetaStoreClient(metastore.hiveConf());\n+      hmsTable = client.getTable(\"default\", \"customers\");\n+    } finally {\n+      if (client != null) {\n+        client.close();\n+      }\n+    }\n+\n+    Map<String, String> hmsParams = hmsTable.getParameters();\n+\n+    // This is only set for HiveCatalog based tables. Check the value, then remove it so the other checks can be general\n+    if (needToCheckSnapshotLocation()) {\n+      Assert.assertTrue(hmsParams.get(BaseMetastoreTableOperations.METADATA_LOCATION_PROP)\n+          .startsWith(icebergTable.location()));\n+      hmsParams.remove(BaseMetastoreTableOperations.METADATA_LOCATION_PROP);\n+    }\n+\n+    // General metadata checks\n+    Assert.assertEquals(6, hmsParams.size());\n+    Assert.assertEquals(\"test\", hmsParams.get(\"dummy\"));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(InputFormatConfig.EXTERNAL_TABLE_PURGE));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(\"EXTERNAL\"));\n+    Assert.assertNotNull(hmsParams.get(hive_metastoreConstants.DDL_TIME));\n+    Assert.assertEquals(HiveIcebergStorageHandler.class.getName(),\n+        hmsTable.getParameters().get(hive_metastoreConstants.META_TABLE_STORAGE));\n+    Assert.assertEquals(BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase(),\n+        hmsTable.getParameters().get(BaseMetastoreTableOperations.TABLE_TYPE_PROP));\n+\n+    if (Catalogs.canWorkWithoutHive(shell.getHiveConf())) {\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if the table was really dropped even from the Catalog\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+    } else {\n+      // Check the HMS table parameters\n+      Path hmsTableLocation;\n+      try {\n+        client = new HiveMetaStoreClient(metastore.hiveConf());\n+        hmsTableLocation = new Path(client.getTable(\"default\", \"customers\").getSd().getLocation());\n+      } finally {\n+        if (client != null) {\n+          client.close();\n+        }\n+      }\n+\n+      // Drop the table\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if we drop an exception when trying to drop the table\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+\n+      // Check if the files are kept\n+      FileSystem fs = Util.getFs(hmsTableLocation, shell.getHiveConf());\n+      // TODO: files should be deleted\n+      // Assert.assertEquals(0, fs.listStatus(hmsTableLocation).length);\n+    }\n+  }\n+\n+  @Test\n+  public void testCreateTableWithoutSpec() throws TException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table partition data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(PartitionSpecParser.toJson(SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;\n+    try {\n+      client = new HiveMetaStoreClient(metastore.hiveConf());\n+      hmsTable = client.getTable(\"default\", \"customers\");\n+    } finally {\n+      if (client != null) {\n+        client.close();\n+      }\n+    }\n+\n+    Map<String, String> hmsParams = hmsTable.getParameters();\n+\n+    // Just check that the PartitionSpec is not set in the metadata\n+    Assert.assertNull(hmsParams.get(InputFormatConfig.PARTITION_SPEC));\n+\n+    if (needToCheckSnapshotLocation()) {\n+      Assert.assertEquals(6, hmsParams.size());\n+    } else {\n+      Assert.assertEquals(5, hmsParams.size());\n+    }\n+\n+    shell.executeStatement(\"DROP TABLE customers\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0MDkzOA=="}, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQxNDk5NQ==", "bodyText": "Does not hurt, and when we change to use a single metastore it will be good", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r498414995", "createdAt": "2020-10-01T17:44:53Z", "author": {"login": "pvary"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+    Assert.assertEquals(Collections.singletonMap(\"dummy\", \"test\"), icebergTable.properties());\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;\n+    try {\n+      client = new HiveMetaStoreClient(metastore.hiveConf());\n+      hmsTable = client.getTable(\"default\", \"customers\");\n+    } finally {\n+      if (client != null) {\n+        client.close();\n+      }\n+    }\n+\n+    Map<String, String> hmsParams = hmsTable.getParameters();\n+\n+    // This is only set for HiveCatalog based tables. Check the value, then remove it so the other checks can be general\n+    if (needToCheckSnapshotLocation()) {\n+      Assert.assertTrue(hmsParams.get(BaseMetastoreTableOperations.METADATA_LOCATION_PROP)\n+          .startsWith(icebergTable.location()));\n+      hmsParams.remove(BaseMetastoreTableOperations.METADATA_LOCATION_PROP);\n+    }\n+\n+    // General metadata checks\n+    Assert.assertEquals(6, hmsParams.size());\n+    Assert.assertEquals(\"test\", hmsParams.get(\"dummy\"));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(InputFormatConfig.EXTERNAL_TABLE_PURGE));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(\"EXTERNAL\"));\n+    Assert.assertNotNull(hmsParams.get(hive_metastoreConstants.DDL_TIME));\n+    Assert.assertEquals(HiveIcebergStorageHandler.class.getName(),\n+        hmsTable.getParameters().get(hive_metastoreConstants.META_TABLE_STORAGE));\n+    Assert.assertEquals(BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase(),\n+        hmsTable.getParameters().get(BaseMetastoreTableOperations.TABLE_TYPE_PROP));\n+\n+    if (Catalogs.canWorkWithoutHive(shell.getHiveConf())) {\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if the table was really dropped even from the Catalog\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+    } else {\n+      // Check the HMS table parameters\n+      Path hmsTableLocation;\n+      try {\n+        client = new HiveMetaStoreClient(metastore.hiveConf());\n+        hmsTableLocation = new Path(client.getTable(\"default\", \"customers\").getSd().getLocation());\n+      } finally {\n+        if (client != null) {\n+          client.close();\n+        }\n+      }\n+\n+      // Drop the table\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if we drop an exception when trying to drop the table\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+\n+      // Check if the files are kept\n+      FileSystem fs = Util.getFs(hmsTableLocation, shell.getHiveConf());\n+      // TODO: files should be deleted\n+      // Assert.assertEquals(0, fs.listStatus(hmsTableLocation).length);\n+    }\n+  }\n+\n+  @Test\n+  public void testCreateTableWithoutSpec() throws TException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table partition data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(PartitionSpecParser.toJson(SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;\n+    try {\n+      client = new HiveMetaStoreClient(metastore.hiveConf());\n+      hmsTable = client.getTable(\"default\", \"customers\");\n+    } finally {\n+      if (client != null) {\n+        client.close();\n+      }\n+    }\n+\n+    Map<String, String> hmsParams = hmsTable.getParameters();\n+\n+    // Just check that the PartitionSpec is not set in the metadata\n+    Assert.assertNull(hmsParams.get(InputFormatConfig.PARTITION_SPEC));\n+\n+    if (needToCheckSnapshotLocation()) {\n+      Assert.assertEquals(6, hmsParams.size());\n+    } else {\n+      Assert.assertEquals(5, hmsParams.size());\n+    }\n+\n+    shell.executeStatement(\"DROP TABLE customers\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0MDkzOA=="}, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5ODQxNzA1MA==", "bodyText": "Yeah, this was mainly assuming that the metastore was shared.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r498417050", "createdAt": "2020-10-01T17:48:48Z", "author": {"login": "rdblue"}, "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java", "diffHunk": "@@ -157,6 +177,282 @@ public void testJoinTables() throws IOException {\n     Assert.assertArrayEquals(new Object[] {1L, \"Bob\", 102L, 33.33d}, rows.get(2));\n   }\n \n+  @Test\n+  public void testCreateDropTable() throws TException, IOException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"', \" +\n+        \"'\" + InputFormatConfig.PARTITION_SPEC + \"'='\" + PartitionSpecParser.toJson(IDENTITY_SPEC) + \"', \" +\n+        \"'dummy'='test')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(SchemaParser.toJson(CUSTOMER_SCHEMA), SchemaParser.toJson(icebergTable.schema()));\n+    Assert.assertEquals(PartitionSpecParser.toJson(IDENTITY_SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+    Assert.assertEquals(Collections.singletonMap(\"dummy\", \"test\"), icebergTable.properties());\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;\n+    try {\n+      client = new HiveMetaStoreClient(metastore.hiveConf());\n+      hmsTable = client.getTable(\"default\", \"customers\");\n+    } finally {\n+      if (client != null) {\n+        client.close();\n+      }\n+    }\n+\n+    Map<String, String> hmsParams = hmsTable.getParameters();\n+\n+    // This is only set for HiveCatalog based tables. Check the value, then remove it so the other checks can be general\n+    if (needToCheckSnapshotLocation()) {\n+      Assert.assertTrue(hmsParams.get(BaseMetastoreTableOperations.METADATA_LOCATION_PROP)\n+          .startsWith(icebergTable.location()));\n+      hmsParams.remove(BaseMetastoreTableOperations.METADATA_LOCATION_PROP);\n+    }\n+\n+    // General metadata checks\n+    Assert.assertEquals(6, hmsParams.size());\n+    Assert.assertEquals(\"test\", hmsParams.get(\"dummy\"));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(InputFormatConfig.EXTERNAL_TABLE_PURGE));\n+    Assert.assertEquals(\"TRUE\", hmsParams.get(\"EXTERNAL\"));\n+    Assert.assertNotNull(hmsParams.get(hive_metastoreConstants.DDL_TIME));\n+    Assert.assertEquals(HiveIcebergStorageHandler.class.getName(),\n+        hmsTable.getParameters().get(hive_metastoreConstants.META_TABLE_STORAGE));\n+    Assert.assertEquals(BaseMetastoreTableOperations.ICEBERG_TABLE_TYPE_VALUE.toUpperCase(),\n+        hmsTable.getParameters().get(BaseMetastoreTableOperations.TABLE_TYPE_PROP));\n+\n+    if (Catalogs.canWorkWithoutHive(shell.getHiveConf())) {\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if the table was really dropped even from the Catalog\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+    } else {\n+      // Check the HMS table parameters\n+      Path hmsTableLocation;\n+      try {\n+        client = new HiveMetaStoreClient(metastore.hiveConf());\n+        hmsTableLocation = new Path(client.getTable(\"default\", \"customers\").getSd().getLocation());\n+      } finally {\n+        if (client != null) {\n+          client.close();\n+        }\n+      }\n+\n+      // Drop the table\n+      shell.executeStatement(\"DROP TABLE customers\");\n+\n+      // Check if we drop an exception when trying to drop the table\n+      AssertHelpers.assertThrows(\"should throw exception\", NoSuchTableException.class,\n+          \"Table does not exist\", () -> {\n+            Catalogs.loadTable(shell.getHiveConf(), properties);\n+          }\n+      );\n+\n+      // Check if the files are kept\n+      FileSystem fs = Util.getFs(hmsTableLocation, shell.getHiveConf());\n+      // TODO: files should be deleted\n+      // Assert.assertEquals(0, fs.listStatus(hmsTableLocation).length);\n+    }\n+  }\n+\n+  @Test\n+  public void testCreateTableWithoutSpec() throws TException {\n+    // We need the location for HadoopTable based tests only\n+    String location = locationForCreateTable(temp.getRoot().getPath(), \"customers\");\n+    shell.executeStatement(\"CREATE EXTERNAL TABLE customers \" +\n+        \"STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \" +\n+        (location != null ? \"LOCATION '\" + location + \"' \" : \"\") +\n+        \"TBLPROPERTIES ('\" + InputFormatConfig.TABLE_SCHEMA + \"'='\" + SchemaParser.toJson(CUSTOMER_SCHEMA) + \"')\");\n+\n+    Properties properties = new Properties();\n+    properties.put(Catalogs.NAME, TableIdentifier.of(\"default\", \"customers\").toString());\n+    if (location != null) {\n+      properties.put(Catalogs.LOCATION, location);\n+    }\n+\n+    // Check the Iceberg table partition data\n+    org.apache.iceberg.Table icebergTable = Catalogs.loadTable(shell.getHiveConf(), properties);\n+    Assert.assertEquals(PartitionSpecParser.toJson(SPEC), PartitionSpecParser.toJson(icebergTable.spec()));\n+\n+    // Check the HMS table parameters\n+    IMetaStoreClient client = null;\n+    org.apache.hadoop.hive.metastore.api.Table hmsTable;\n+    try {\n+      client = new HiveMetaStoreClient(metastore.hiveConf());\n+      hmsTable = client.getTable(\"default\", \"customers\");\n+    } finally {\n+      if (client != null) {\n+        client.close();\n+      }\n+    }\n+\n+    Map<String, String> hmsParams = hmsTable.getParameters();\n+\n+    // Just check that the PartitionSpec is not set in the metadata\n+    Assert.assertNull(hmsParams.get(InputFormatConfig.PARTITION_SPEC));\n+\n+    if (needToCheckSnapshotLocation()) {\n+      Assert.assertEquals(6, hmsParams.size());\n+    } else {\n+      Assert.assertEquals(5, hmsParams.size());\n+    }\n+\n+    shell.executeStatement(\"DROP TABLE customers\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Nzg0MDkzOA=="}, "originalCommit": {"oid": "10127f34eaf2780ad44a385a045be8a656fc1421"}, "originalPosition": 186}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzODAzMjEzOnYy", "diffSide": "RIGHT", "path": "build.gradle", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNjo0MjowMlrOHd8P4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QyMDowNDo1MlrOHeDhig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE1Nzg1OA==", "bodyText": "This is the fix for flaky tests? What was the heap size before?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r501157858", "createdAt": "2020-10-07T16:42:02Z", "author": {"login": "rdblue"}, "path": "build.gradle", "diffHunk": "@@ -470,6 +470,11 @@ project(':iceberg-mr') {\n       exclude group: 'org.apache.calcite.avatica'\n     }\n   }\n+\n+  test {\n+    // testJoinTables / testScanTable\n+    maxHeapSize '1500m'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20367c90e3ac1f9e4ecfc4870074aad2b3f35972"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTI3NzA2Ng==", "bodyText": "The heap size was not set before. So I assume it was the default.\nBased on this doc, the default is 512MB\nI have checked the other tests, and found we use 1500/2500 in some cases. I figured that 1.5G should be enough as a first try, and it looks like it solved the flakiness \ud83d\ude04", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r501277066", "createdAt": "2020-10-07T20:04:52Z", "author": {"login": "pvary"}, "path": "build.gradle", "diffHunk": "@@ -470,6 +470,11 @@ project(':iceberg-mr') {\n       exclude group: 'org.apache.calcite.avatica'\n     }\n   }\n+\n+  test {\n+    // testJoinTables / testScanTable\n+    maxHeapSize '1500m'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE1Nzg1OA=="}, "originalCommit": {"oid": "20367c90e3ac1f9e4ecfc4870074aad2b3f35972"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzODA1NTM3OnYy", "diffSide": "RIGHT", "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/HiveCreateReplaceTableTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QxNjo0Nzo0NVrOHd8eGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQxODowMDo1OVrOHeqOvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE2MTQ5OQ==", "bodyText": "I don't think this is correct. Looks like this is happening because we always try to load the table, so the create path (the previous else case after checking base != null) is not taken.\nI think the logic should be: if the table exists, then check whether we are trying to create it (base is null). If we are trying to create it, validate that it is in a state that can be created: the table should have type \"iceberg\", but should not have a metadata location. If it has a metadata location, then the table already exists and it should throw the original AlreadyExistsException.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r501161499", "createdAt": "2020-10-07T16:47:45Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/HiveCreateReplaceTableTest.java", "diffHunk": "@@ -104,8 +105,8 @@ public void testCreateTableTxnTableCreatedConcurrently() {\n \n     AssertHelpers.assertThrows(\n         \"Create table txn should fail\",\n-        AlreadyExistsException.class,\n-        \"Table already exists: hivedb.tbl\",\n+        CommitFailedException.class,\n+        \"is not same as the current table metadata\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20367c90e3ac1f9e4ecfc4870074aad2b3f35972"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTkxMTIzMQ==", "bodyText": "Reverted to the original behavior based on your suggestion.\nHad to refactor stuff to reduce complexity", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r501911231", "createdAt": "2020-10-08T18:00:59Z", "author": {"login": "pvary"}, "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/HiveCreateReplaceTableTest.java", "diffHunk": "@@ -104,8 +105,8 @@ public void testCreateTableTxnTableCreatedConcurrently() {\n \n     AssertHelpers.assertThrows(\n         \"Create table txn should fail\",\n-        AlreadyExistsException.class,\n-        \"Table already exists: hivedb.tbl\",\n+        CommitFailedException.class,\n+        \"is not same as the current table metadata\",", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMTE2MTQ5OQ=="}, "originalCommit": {"oid": "20367c90e3ac1f9e4ecfc4870074aad2b3f35972"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MzY3NTE5OnYy", "diffSide": "RIGHT", "path": "mr/hs_err_pid63322.log", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMTo1NTozMFrOHexxHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwNzoxMTo1MFrOHgYX5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAzNDcxOQ==", "bodyText": "This file was probably added by mistake?", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r502034719", "createdAt": "2020-10-08T21:55:30Z", "author": {"login": "rdblue"}, "path": "mr/hs_err_pid63322.log", "diffHunk": "@@ -0,0 +1,811 @@\n+#", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6115ad1c676b644f786382ec9b852c5350433261"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzcxNTgxMw==", "bodyText": "Removed.\nThanks for noticing!", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r503715813", "createdAt": "2020-10-13T07:11:50Z", "author": {"login": "pvary"}, "path": "mr/hs_err_pid63322.log", "diffHunk": "@@ -0,0 +1,811 @@\n+#", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAzNDcxOQ=="}, "originalCommit": {"oid": "6115ad1c676b644f786382ec9b852c5350433261"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0MzY3ODg5OnYy", "diffSide": "RIGHT", "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOFQyMTo1Njo1MFrOHexzQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxMjowMzowNVrOHf70rw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAzNTI2Nw==", "bodyText": "I'd recommend taking a look at other classes based on this and copying their use of a client pool, rather than creating a client here.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r502035267", "createdAt": "2020-10-08T21:56:50Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java", "diffHunk": "@@ -66,26 +75,31 @@\n   private ExecutorService executorService;\n   private TServer server;\n   private HiveMetaStore.HMSHandler baseHandler;\n+  private IMetaStoreClient client = null;\n \n   public void start() {\n     try {\n-      hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n+      this.hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n       File derbyLogFile = new File(hiveLocalDir, \"derby.log\");\n       System.setProperty(\"derby.stream.error.file\", derbyLogFile.getAbsolutePath());\n       setupMetastoreDB(\"jdbc:derby:\" + getDerbyPath() + \";create=true\");\n \n       TServerSocket socket = new TServerSocket(0);\n       int port = socket.getServerSocket().getLocalPort();\n-      hiveConf = newHiveConf(port);\n-      server = newThriftServer(socket, hiveConf);\n-      executorService = Executors.newSingleThreadExecutor();\n-      executorService.submit(() -> server.serve());\n+      this.hiveConf = newHiveConf(port);\n+      this.server = newThriftServer(socket, hiveConf);\n+      this.executorService = Executors.newSingleThreadExecutor();\n+      this.executorService.submit(() -> server.serve());\n+      this.client = new HiveMetaStoreClient(hiveConf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6115ad1c676b644f786382ec9b852c5350433261"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjA3Mjc0MQ==", "bodyText": "This might fix the JDK 11 tests, which appear to be failing because of connections to the Hive metastore. My guess is that the \"write error\" is that it times out because the metastore ran out of thread -- and we don't want to increase those threads.\nJDK 8 tests are failing because of a constructor is being used that accepts a HiveConf. That needs to be done using reflection now.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r502072741", "createdAt": "2020-10-08T23:21:21Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java", "diffHunk": "@@ -66,26 +75,31 @@\n   private ExecutorService executorService;\n   private TServer server;\n   private HiveMetaStore.HMSHandler baseHandler;\n+  private IMetaStoreClient client = null;\n \n   public void start() {\n     try {\n-      hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n+      this.hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n       File derbyLogFile = new File(hiveLocalDir, \"derby.log\");\n       System.setProperty(\"derby.stream.error.file\", derbyLogFile.getAbsolutePath());\n       setupMetastoreDB(\"jdbc:derby:\" + getDerbyPath() + \";create=true\");\n \n       TServerSocket socket = new TServerSocket(0);\n       int port = socket.getServerSocket().getLocalPort();\n-      hiveConf = newHiveConf(port);\n-      server = newThriftServer(socket, hiveConf);\n-      executorService = Executors.newSingleThreadExecutor();\n-      executorService.submit(() -> server.serve());\n+      this.hiveConf = newHiveConf(port);\n+      this.server = newThriftServer(socket, hiveConf);\n+      this.executorService = Executors.newSingleThreadExecutor();\n+      this.executorService.submit(() -> server.serve());\n+      this.client = new HiveMetaStoreClient(hiveConf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAzNTI2Nw=="}, "originalCommit": {"oid": "6115ad1c676b644f786382ec9b852c5350433261"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjIxMDAxNg==", "bodyText": "Using a clientPool fixed these issues, but it seems that we do not have enough worker in the HMS and this causes failing tests. Need to investigate why the  clients are not closed.\nThanks for taking a look!", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r502210016", "createdAt": "2020-10-09T06:15:47Z", "author": {"login": "pvary"}, "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java", "diffHunk": "@@ -66,26 +75,31 @@\n   private ExecutorService executorService;\n   private TServer server;\n   private HiveMetaStore.HMSHandler baseHandler;\n+  private IMetaStoreClient client = null;\n \n   public void start() {\n     try {\n-      hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n+      this.hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n       File derbyLogFile = new File(hiveLocalDir, \"derby.log\");\n       System.setProperty(\"derby.stream.error.file\", derbyLogFile.getAbsolutePath());\n       setupMetastoreDB(\"jdbc:derby:\" + getDerbyPath() + \";create=true\");\n \n       TServerSocket socket = new TServerSocket(0);\n       int port = socket.getServerSocket().getLocalPort();\n-      hiveConf = newHiveConf(port);\n-      server = newThriftServer(socket, hiveConf);\n-      executorService = Executors.newSingleThreadExecutor();\n-      executorService.submit(() -> server.serve());\n+      this.hiveConf = newHiveConf(port);\n+      this.server = newThriftServer(socket, hiveConf);\n+      this.executorService = Executors.newSingleThreadExecutor();\n+      this.executorService.submit(() -> server.serve());\n+      this.client = new HiveMetaStoreClient(hiveConf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAzNTI2Nw=="}, "originalCommit": {"oid": "6115ad1c676b644f786382ec9b852c5350433261"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzNTg1NQ==", "bodyText": "I know that it's annoying to track down, but that's exactly why we want to limit the worker threads. Much better to catch leaked connections here rather than trying to fix them for customers at runtime!", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r502835855", "createdAt": "2020-10-10T21:48:32Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java", "diffHunk": "@@ -66,26 +75,31 @@\n   private ExecutorService executorService;\n   private TServer server;\n   private HiveMetaStore.HMSHandler baseHandler;\n+  private IMetaStoreClient client = null;\n \n   public void start() {\n     try {\n-      hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n+      this.hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n       File derbyLogFile = new File(hiveLocalDir, \"derby.log\");\n       System.setProperty(\"derby.stream.error.file\", derbyLogFile.getAbsolutePath());\n       setupMetastoreDB(\"jdbc:derby:\" + getDerbyPath() + \";create=true\");\n \n       TServerSocket socket = new TServerSocket(0);\n       int port = socket.getServerSocket().getLocalPort();\n-      hiveConf = newHiveConf(port);\n-      server = newThriftServer(socket, hiveConf);\n-      executorService = Executors.newSingleThreadExecutor();\n-      executorService.submit(() -> server.serve());\n+      this.hiveConf = newHiveConf(port);\n+      this.server = newThriftServer(socket, hiveConf);\n+      this.executorService = Executors.newSingleThreadExecutor();\n+      this.executorService.submit(() -> server.serve());\n+      this.client = new HiveMetaStoreClient(hiveConf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAzNTI2Nw=="}, "originalCommit": {"oid": "6115ad1c676b644f786382ec9b852c5350433261"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI0ODA0Nw==", "bodyText": "This was fun \ud83d\ude04\nUsing System.properties to configure the HMS made the ClientPool use the wrong URI.\nMoved every metastore related stuff to the TestMetaStore", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r503248047", "createdAt": "2020-10-12T12:03:05Z", "author": {"login": "pvary"}, "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java", "diffHunk": "@@ -66,26 +75,31 @@\n   private ExecutorService executorService;\n   private TServer server;\n   private HiveMetaStore.HMSHandler baseHandler;\n+  private IMetaStoreClient client = null;\n \n   public void start() {\n     try {\n-      hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n+      this.hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n       File derbyLogFile = new File(hiveLocalDir, \"derby.log\");\n       System.setProperty(\"derby.stream.error.file\", derbyLogFile.getAbsolutePath());\n       setupMetastoreDB(\"jdbc:derby:\" + getDerbyPath() + \";create=true\");\n \n       TServerSocket socket = new TServerSocket(0);\n       int port = socket.getServerSocket().getLocalPort();\n-      hiveConf = newHiveConf(port);\n-      server = newThriftServer(socket, hiveConf);\n-      executorService = Executors.newSingleThreadExecutor();\n-      executorService.submit(() -> server.serve());\n+      this.hiveConf = newHiveConf(port);\n+      this.server = newThriftServer(socket, hiveConf);\n+      this.executorService = Executors.newSingleThreadExecutor();\n+      this.executorService.submit(() -> server.serve());\n+      this.client = new HiveMetaStoreClient(hiveConf);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjAzNTI2Nw=="}, "originalCommit": {"oid": "6115ad1c676b644f786382ec9b852c5350433261"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MzYzNjY5OnYy", "diffSide": "RIGHT", "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxOTozNTowOVrOHgKzEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNjowNjo1M1rOHguYtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ5MzM5NA==", "bodyText": "Why is this needed? It may be nice to use this for applications, but for testing it seems like this allows us to pass the configuration incorrectly and still have tests pass. If it isn't needed, I'd rather remove it so that we must pass configuration properly.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r503493394", "createdAt": "2020-10-12T19:35:09Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java", "diffHunk": "@@ -66,26 +72,35 @@\n   private ExecutorService executorService;\n   private TServer server;\n   private HiveMetaStore.HMSHandler baseHandler;\n+  private HiveClientPool clientPool;\n \n   public void start() {\n     try {\n-      hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n+      this.hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n       File derbyLogFile = new File(hiveLocalDir, \"derby.log\");\n       System.setProperty(\"derby.stream.error.file\", derbyLogFile.getAbsolutePath());\n       setupMetastoreDB(\"jdbc:derby:\" + getDerbyPath() + \";create=true\");\n \n       TServerSocket socket = new TServerSocket(0);\n       int port = socket.getServerSocket().getLocalPort();\n-      hiveConf = newHiveConf(port);\n-      server = newThriftServer(socket, hiveConf);\n-      executorService = Executors.newSingleThreadExecutor();\n-      executorService.submit(() -> server.serve());\n+      this.hiveConf = newHiveConf(port);\n+      this.server = newThriftServer(socket, hiveConf);\n+      this.executorService = Executors.newSingleThreadExecutor();\n+      this.executorService.submit(() -> server.serve());\n+\n+      // in Hive3, setting this as a system prop ensures that it will be picked up whenever a new HiveConf is created\n+      System.setProperty(HiveConf.ConfVars.METASTOREURIS.varname, hiveConf.getVar(HiveConf.ConfVars.METASTOREURIS));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f798bed7101bf2b4eceb079c4b6dc96ab87a77d"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzUwOTc0Mg==", "bodyText": "The Hive3 patch brought this in. If I understood @marton-bod correctly this was needed because in the HiveRunner there are some places where the config is not passed down, and this was the only way to fix the HMS client misery.\nWhat I did here was only move this from the test class to the TestHiveMetastore class so it will affect the ClientPools as well", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r503509742", "createdAt": "2020-10-12T20:11:00Z", "author": {"login": "pvary"}, "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java", "diffHunk": "@@ -66,26 +72,35 @@\n   private ExecutorService executorService;\n   private TServer server;\n   private HiveMetaStore.HMSHandler baseHandler;\n+  private HiveClientPool clientPool;\n \n   public void start() {\n     try {\n-      hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n+      this.hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n       File derbyLogFile = new File(hiveLocalDir, \"derby.log\");\n       System.setProperty(\"derby.stream.error.file\", derbyLogFile.getAbsolutePath());\n       setupMetastoreDB(\"jdbc:derby:\" + getDerbyPath() + \";create=true\");\n \n       TServerSocket socket = new TServerSocket(0);\n       int port = socket.getServerSocket().getLocalPort();\n-      hiveConf = newHiveConf(port);\n-      server = newThriftServer(socket, hiveConf);\n-      executorService = Executors.newSingleThreadExecutor();\n-      executorService.submit(() -> server.serve());\n+      this.hiveConf = newHiveConf(port);\n+      this.server = newThriftServer(socket, hiveConf);\n+      this.executorService = Executors.newSingleThreadExecutor();\n+      this.executorService.submit(() -> server.serve());\n+\n+      // in Hive3, setting this as a system prop ensures that it will be picked up whenever a new HiveConf is created\n+      System.setProperty(HiveConf.ConfVars.METASTOREURIS.varname, hiveConf.getVar(HiveConf.ConfVars.METASTOREURIS));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ5MzM5NA=="}, "originalCommit": {"oid": "6f798bed7101bf2b4eceb079c4b6dc96ab87a77d"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzcyODc2OQ==", "bodyText": "That's correct. The rationale is outlined here: #1478 (comment)\nAs you mentioned, when using Hive3, not all threads spawned during HiveRunner initialization receive our custom properties we set on the HiveShell, therefore using the system props is there to ensure they're picked up during HiveConf construction. This is needed to get the tests working properly on Hive3, without the PersistenceManager-related flaky tests.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r503728769", "createdAt": "2020-10-13T07:33:42Z", "author": {"login": "marton-bod"}, "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java", "diffHunk": "@@ -66,26 +72,35 @@\n   private ExecutorService executorService;\n   private TServer server;\n   private HiveMetaStore.HMSHandler baseHandler;\n+  private HiveClientPool clientPool;\n \n   public void start() {\n     try {\n-      hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n+      this.hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n       File derbyLogFile = new File(hiveLocalDir, \"derby.log\");\n       System.setProperty(\"derby.stream.error.file\", derbyLogFile.getAbsolutePath());\n       setupMetastoreDB(\"jdbc:derby:\" + getDerbyPath() + \";create=true\");\n \n       TServerSocket socket = new TServerSocket(0);\n       int port = socket.getServerSocket().getLocalPort();\n-      hiveConf = newHiveConf(port);\n-      server = newThriftServer(socket, hiveConf);\n-      executorService = Executors.newSingleThreadExecutor();\n-      executorService.submit(() -> server.serve());\n+      this.hiveConf = newHiveConf(port);\n+      this.server = newThriftServer(socket, hiveConf);\n+      this.executorService = Executors.newSingleThreadExecutor();\n+      this.executorService.submit(() -> server.serve());\n+\n+      // in Hive3, setting this as a system prop ensures that it will be picked up whenever a new HiveConf is created\n+      System.setProperty(HiveConf.ConfVars.METASTOREURIS.varname, hiveConf.getVar(HiveConf.ConfVars.METASTOREURIS));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ5MzM5NA=="}, "originalCommit": {"oid": "6f798bed7101bf2b4eceb079c4b6dc96ab87a77d"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA3NjQ3MA==", "bodyText": "Okay, thanks for the reasoning. I guess I missed that this required a global setting in the other PR. I think this is fairly safe, since we always want to override the metastore URI using an option from the catalog's config.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r504076470", "createdAt": "2020-10-13T16:06:53Z", "author": {"login": "rdblue"}, "path": "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java", "diffHunk": "@@ -66,26 +72,35 @@\n   private ExecutorService executorService;\n   private TServer server;\n   private HiveMetaStore.HMSHandler baseHandler;\n+  private HiveClientPool clientPool;\n \n   public void start() {\n     try {\n-      hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n+      this.hiveLocalDir = createTempDirectory(\"hive\", asFileAttribute(fromString(\"rwxrwxrwx\"))).toFile();\n       File derbyLogFile = new File(hiveLocalDir, \"derby.log\");\n       System.setProperty(\"derby.stream.error.file\", derbyLogFile.getAbsolutePath());\n       setupMetastoreDB(\"jdbc:derby:\" + getDerbyPath() + \";create=true\");\n \n       TServerSocket socket = new TServerSocket(0);\n       int port = socket.getServerSocket().getLocalPort();\n-      hiveConf = newHiveConf(port);\n-      server = newThriftServer(socket, hiveConf);\n-      executorService = Executors.newSingleThreadExecutor();\n-      executorService.submit(() -> server.serve());\n+      this.hiveConf = newHiveConf(port);\n+      this.server = newThriftServer(socket, hiveConf);\n+      this.executorService = Executors.newSingleThreadExecutor();\n+      this.executorService.submit(() -> server.serve());\n+\n+      // in Hive3, setting this as a system prop ensures that it will be picked up whenever a new HiveConf is created\n+      System.setProperty(HiveConf.ConfVars.METASTOREURIS.varname, hiveConf.getVar(HiveConf.ConfVars.METASTOREURIS));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ5MzM5NA=="}, "originalCommit": {"oid": "6f798bed7101bf2b4eceb079c4b6dc96ab87a77d"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MzY0MjAxOnYy", "diffSide": "RIGHT", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxOTozNzowMVrOHgK2Jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwNzozMDoxN1rOHgZC-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ5NDE4Mw==", "bodyText": "Minor: this seems like a good candidate for else if (serdeProperties...).", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r503494183", "createdAt": "2020-10-12T19:37:01Z", "author": {"login": "rdblue"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "diffHunk": "@@ -29,25 +29,42 @@\n import org.apache.hadoop.io.Writable;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.SchemaParser;\n-import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n import org.apache.iceberg.mr.Catalogs;\n import org.apache.iceberg.mr.InputFormatConfig;\n import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergObjectInspector;\n import org.apache.iceberg.mr.mapred.Container;\n \n public class HiveIcebergSerDe extends AbstractSerDe {\n-\n   private ObjectInspector inspector;\n \n   @Override\n   public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    // HiveIcebergSerDe.initialize is called multiple places in Hive code:\n+    // - When we are trying to create a table - HiveDDL data is stored at the serDeProperties, but no Iceberg table\n+    // is created yet.\n+    // - When we are compiling the Hive query on HiveServer2 side - We only have table information (location/name),\n+    // and we have to read the schema using the table data. This is called multiple times so there is room for\n+    // optimizing here.\n+    // - When we are executing the Hive query in the execution engine - We do not want to load the table data on every\n+    // executor, but serDeProperties are populated by HiveIcebergStorageHandler.configureInputJobProperties() and\n+    // the resulting properties are serialized and distributed to the executors\n+\n     Schema tableSchema;\n     if (configuration.get(InputFormatConfig.TABLE_SCHEMA) != null) {\n       tableSchema = SchemaParser.fromJson(configuration.get(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      Table table = Catalogs.loadTable(configuration, serDeProperties);\n-      tableSchema = table.schema();\n+      if (serDeProperties.get(InputFormatConfig.TABLE_SCHEMA) != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6f798bed7101bf2b4eceb079c4b6dc96ab87a77d"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzcyNjg0MQ==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/1495#discussion_r503726841", "createdAt": "2020-10-13T07:30:17Z", "author": {"login": "pvary"}, "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergSerDe.java", "diffHunk": "@@ -29,25 +29,42 @@\n import org.apache.hadoop.io.Writable;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.SchemaParser;\n-import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n import org.apache.iceberg.mr.Catalogs;\n import org.apache.iceberg.mr.InputFormatConfig;\n import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergObjectInspector;\n import org.apache.iceberg.mr.mapred.Container;\n \n public class HiveIcebergSerDe extends AbstractSerDe {\n-\n   private ObjectInspector inspector;\n \n   @Override\n   public void initialize(@Nullable Configuration configuration, Properties serDeProperties) throws SerDeException {\n+    // HiveIcebergSerDe.initialize is called multiple places in Hive code:\n+    // - When we are trying to create a table - HiveDDL data is stored at the serDeProperties, but no Iceberg table\n+    // is created yet.\n+    // - When we are compiling the Hive query on HiveServer2 side - We only have table information (location/name),\n+    // and we have to read the schema using the table data. This is called multiple times so there is room for\n+    // optimizing here.\n+    // - When we are executing the Hive query in the execution engine - We do not want to load the table data on every\n+    // executor, but serDeProperties are populated by HiveIcebergStorageHandler.configureInputJobProperties() and\n+    // the resulting properties are serialized and distributed to the executors\n+\n     Schema tableSchema;\n     if (configuration.get(InputFormatConfig.TABLE_SCHEMA) != null) {\n       tableSchema = SchemaParser.fromJson(configuration.get(InputFormatConfig.TABLE_SCHEMA));\n     } else {\n-      Table table = Catalogs.loadTable(configuration, serDeProperties);\n-      tableSchema = table.schema();\n+      if (serDeProperties.get(InputFormatConfig.TABLE_SCHEMA) != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ5NDE4Mw=="}, "originalCommit": {"oid": "6f798bed7101bf2b4eceb079c4b6dc96ab87a77d"}, "originalPosition": 33}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3519, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}