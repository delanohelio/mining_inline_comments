{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3MTU1NTM3", "number": 1189, "title": "Spark: Support ORC vectorized reads", "bodyText": "Most of the new code is added in VectorizedSparkOrcReaders\nReplaced NullValuesColumnVector with ConstantColumnVector which can support any constant (including nulls)\nMoved the Iceberg to Spark constant conversion logic from RowDataReader into BaseDataReader so that it can be reused in BatchDataReader\nModified many of the Spark read test cases to test for both vectorized and non vectorized codepaths\nModified TestFilteredScan in Spark3 to use IcebergGenerics instead of Avro just like in Spark2. This enables use to test ORC reads/writes which do not have Avro GenericRecord writer.\n\nBenchmark results:\nTests reads of 10 files with 5M records each\nBenchmark                                                                          Mode  Cnt    Score    Error  Units\nIcebergSourceFlatORCDataReadBenchmark.readFileSourceNonVectorized                    ss    5   42.789 \u00b1  3.294   s/op\nIcebergSourceFlatORCDataReadBenchmark.readFileSourceVectorized                       ss    5   18.566 \u00b1  1.450   s/op\nIcebergSourceFlatORCDataReadBenchmark.readIcebergNonVectorized                       ss    5   30.186 \u00b1  1.007   s/op\nIcebergSourceFlatORCDataReadBenchmark.readIcebergVectorized                          ss    5   18.835 \u00b1  0.818   s/op\nIcebergSourceFlatORCDataReadBenchmark.readWithProjectionFileSourceNonVectorized      ss    5    8.935 \u00b1  0.801   s/op\nIcebergSourceFlatORCDataReadBenchmark.readWithProjectionFileSourceVectorized         ss    5    2.387 \u00b1  0.195   s/op\nIcebergSourceFlatORCDataReadBenchmark.readWithProjectionIcebergNonVectorized         ss    5   10.691 \u00b1  0.603   s/op\nIcebergSourceFlatORCDataReadBenchmark.readWithProjectionIcebergVectorized            ss    5    2.653 \u00b1  0.511   s/op\nIcebergSourceNestedORCDataReadBenchmark.readFileSourceNonVectorized                  ss    5  118.318 \u00b1  1.583   s/op\nIcebergSourceNestedORCDataReadBenchmark.readIcebergNonVectorized                     ss    5   18.943 \u00b1  1.305   s/op\nIcebergSourceNestedORCDataReadBenchmark.readIcebergVectorized                        ss    5    9.330 \u00b1  0.938   s/op\nIcebergSourceNestedORCDataReadBenchmark.readWithProjectionFileSourceNonVectorized    ss    5   86.136 \u00b1  1.139   s/op\nIcebergSourceNestedORCDataReadBenchmark.readWithProjectionIcebergNonVectorized       ss    5   16.671 \u00b1  0.855   s/op\nIcebergSourceNestedORCDataReadBenchmark.readWithProjectionIcebergVectorized          ss    5    6.710 \u00b1  0.679   s/op", "createdAt": "2020-07-10T00:16:37Z", "url": "https://github.com/apache/iceberg/pull/1189", "merged": true, "mergeCommit": {"oid": "6fab8f57bdb7e5fe7eadc3ff41558581338e1b69"}, "closed": true, "closedAt": "2020-07-13T21:27:37Z", "author": {"login": "shardulm94"}, "timelineItems": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABczsPfpAFqTQ0Njc0OTA4OA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc0n3VggH2gAyNDQ3MTU1NTM3OjYyYmIzMjdlYWRlZGYyNTFmOTMwYmE0OTVmNzlmMDgyMTE5OGVhOTc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2NzQ5MDg4", "url": "https://github.com/apache/iceberg/pull/1189#pullrequestreview-446749088", "createdAt": "2020-07-10T23:28:58Z", "commit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzoyODo1OFrOGwIQWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzoyODo1OFrOGwIQWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMDA4OA==", "bodyText": "This should validate that createReaderFunc hasn't also been called. And that function should validate that this one hasn't been called. That way there is no ambiguous behavior and the user gets an error when they try to set up both batch and row readers.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453120088", "createdAt": "2020-07-10T23:28:58Z", "author": {"login": "rdblue"}, "path": "orc/src/main/java/org/apache/iceberg/orc/ORC.java", "diffHunk": "@@ -177,9 +179,20 @@ public ReadBuilder filter(Expression newFilter) {\n       return this;\n     }\n \n+    public ReadBuilder createBatchedReaderFunc(Function<TypeDescription, OrcBatchReader<?>> batchReaderFunction) {\n+      this.batchedReaderFunc = batchReaderFunction;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 14}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2NzQ5ODYw", "url": "https://github.com/apache/iceberg/pull/1189#pullrequestreview-446749860", "createdAt": "2020-07-10T23:32:23Z", "commit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzozMjoyM1rOGwITZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzozMjoyM1rOGwITZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMDg3MQ==", "bodyText": "I like the changes here and in SparkOrcReader, but they aren't really needed for this PR and increase the number of files that are touched. In general, I prefer keeping changes like this separate to avoid unnecessary conflicts, and so that we can revert commits without removing unrelated clean-up.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453120871", "createdAt": "2020-07-10T23:32:23Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java", "diffHunk": "@@ -42,19 +42,26 @@\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.unsafe.types.UTF8String;\n \n-\n-class SparkOrcValueReaders {\n+public class SparkOrcValueReaders {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2NzUxNDI0", "url": "https://github.com/apache/iceberg/pull/1189#pullrequestreview-446751424", "createdAt": "2020-07-10T23:39:49Z", "commit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzozOTo0OVrOGwIZOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzozOTo0OVrOGwIZOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMjM2MQ==", "bodyText": "This needs to be a CloseableIterator to avoid leaking an open file. I'm also wondering if we could replace this implementation with a method like CloseableIterable.transform for CloseableIterator. All this is doing is calling a method on the result of another Iterator.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453122361", "createdAt": "2020-07-10T23:39:49Z", "author": {"login": "rdblue"}, "path": "orc/src/main/java/org/apache/iceberg/orc/OrcIterable.java", "diffHunk": "@@ -130,4 +140,25 @@ public T next() {\n     }\n   }\n \n+  private static class OrcBatchIterator<T> implements Iterator<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 79}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2NzUyNzQw", "url": "https://github.com/apache/iceberg/pull/1189#pullrequestreview-446752740", "createdAt": "2020-07-10T23:46:34Z", "commit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzo0NjozNFrOGwIedg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzo0NjozNFrOGwIedg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyMzcwMg==", "bodyText": "Nit: we would normally align this with the start of Schema on the previous line.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453123702", "createdAt": "2020-07-10T23:46:34Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkOrcReaders.java", "diffHunk": "@@ -0,0 +1,415 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.orc.OrcBatchReader;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n+import org.apache.iceberg.orc.OrcValueReader;\n+import org.apache.iceberg.orc.OrcValueReaders;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.SparkOrcValueReaders;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class VectorizedSparkOrcReaders {\n+\n+  private VectorizedSparkOrcReaders() {\n+  }\n+\n+  public static OrcBatchReader<ColumnarBatch> buildReader(Schema expectedSchema, TypeDescription fileSchema,\n+      Map<Integer, ?> idToConstant) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 52}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2NzUzNTMw", "url": "https://github.com/apache/iceberg/pull/1189#pullrequestreview-446753530", "createdAt": "2020-07-10T23:50:51Z", "commit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzo1MDo1MVrOGwIhkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzo1MDo1MVrOGwIhkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyNDQ5Ng==", "bodyText": "I will change this to not project constant columns to avoid materializing a ColumnVector similar to what we did in #1191", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453124496", "createdAt": "2020-07-10T23:50:51Z", "author": {"login": "shardulm94"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "diffHunk": "@@ -75,6 +103,16 @@\n       }\n \n       iter = builder.build();\n+    } else if (task.file().format() == FileFormat.ORC) {\n+      iter = ORC.read(location)\n+          .project(expectedSchema)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 62}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ2NzU0MzY4", "url": "https://github.com/apache/iceberg/pull/1189#pullrequestreview-446754368", "createdAt": "2020-07-10T23:55:56Z", "commit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzo1NTo1NlrOGwIk_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQyMzo1NTo1NlrOGwIk_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEyNTM3NQ==", "bodyText": "Why this extra check just to add a default value? If it is okay to return a default value, then getInt should never be called for a rowId where the value is null. And if this is never called when the value is null, then I'd rather directly cast. That way, a NullPointerException is thrown if the method contract is violated instead of silently returning the wrong value.", "url": "https://github.com/apache/iceberg/pull/1189#discussion_r453125375", "createdAt": "2020-07-10T23:55:56Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkOrcReaders.java", "diffHunk": "@@ -0,0 +1,415 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.IntStream;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.orc.OrcBatchReader;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n+import org.apache.iceberg.orc.OrcValueReader;\n+import org.apache.iceberg.orc.OrcValueReaders;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.SparkOrcValueReaders;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class VectorizedSparkOrcReaders {\n+\n+  private VectorizedSparkOrcReaders() {\n+  }\n+\n+  public static OrcBatchReader<ColumnarBatch> buildReader(Schema expectedSchema, TypeDescription fileSchema,\n+      Map<Integer, ?> idToConstant) {\n+    Converter converter = OrcSchemaWithTypeVisitor.visit(expectedSchema, fileSchema, new ReadBuilder(idToConstant));\n+\n+    return batch -> {\n+      BaseOrcColumnVector cv = (BaseOrcColumnVector) converter.convert(new StructColumnVector(batch.size, batch.cols),\n+          batch.size);\n+      ColumnarBatch columnarBatch = new ColumnarBatch(IntStream.range(0, expectedSchema.columns().size())\n+          .mapToObj(cv::getChild)\n+          .toArray(ColumnVector[]::new));\n+      columnarBatch.setNumRows(batch.size);\n+      return columnarBatch;\n+    };\n+  }\n+\n+  private interface Converter {\n+    ColumnVector convert(org.apache.orc.storage.ql.exec.vector.ColumnVector columnVector, int batchSize);\n+  }\n+\n+  private static class ReadBuilder extends OrcSchemaWithTypeVisitor<Converter> {\n+    private final Map<Integer, ?> idToConstant;\n+\n+    private ReadBuilder(Map<Integer, ?> idToConstant) {\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public Converter record(Types.StructType iStruct, TypeDescription record, List<String> names,\n+        List<Converter> fields) {\n+      return new StructConverter(iStruct, fields, idToConstant);\n+    }\n+\n+    @Override\n+    public Converter list(Types.ListType iList, TypeDescription array, Converter element) {\n+      return new ArrayConverter(iList, element);\n+    }\n+\n+    @Override\n+    public Converter map(Types.MapType iMap, TypeDescription map, Converter key, Converter value) {\n+      return new MapConverter(iMap, key, value);\n+    }\n+\n+    @Override\n+    public Converter primitive(Type.PrimitiveType iPrimitive, TypeDescription primitive) {\n+      final OrcValueReader<?> primitiveValueReader;\n+      switch (primitive.getCategory()) {\n+        case BOOLEAN:\n+          primitiveValueReader = OrcValueReaders.booleans();\n+          break;\n+        case BYTE:\n+          // Iceberg does not have a byte type. Use int\n+        case SHORT:\n+          // Iceberg does not have a short type. Use int\n+        case DATE:\n+        case INT:\n+          primitiveValueReader = OrcValueReaders.ints();\n+          break;\n+        case LONG:\n+          primitiveValueReader = OrcValueReaders.longs();\n+          break;\n+        case FLOAT:\n+          primitiveValueReader = OrcValueReaders.floats();\n+          break;\n+        case DOUBLE:\n+          primitiveValueReader = OrcValueReaders.doubles();\n+          break;\n+        case TIMESTAMP_INSTANT:\n+          primitiveValueReader = SparkOrcValueReaders.timestampTzs();\n+          break;\n+        case DECIMAL:\n+          primitiveValueReader = SparkOrcValueReaders.decimals(primitive.getPrecision(), primitive.getScale());\n+          break;\n+        case CHAR:\n+        case VARCHAR:\n+        case STRING:\n+          primitiveValueReader = SparkOrcValueReaders.utf8String();\n+          break;\n+        case BINARY:\n+          primitiveValueReader = OrcValueReaders.bytes();\n+          break;\n+        default:\n+          throw new IllegalArgumentException(\"Unhandled type \" + primitive);\n+      }\n+      return (columnVector, batchSize) ->\n+          new PrimitiveOrcColumnVector(iPrimitive, batchSize, columnVector, primitiveValueReader);\n+    }\n+  }\n+\n+  private abstract static class BaseOrcColumnVector extends ColumnVector {\n+    private final org.apache.orc.storage.ql.exec.vector.ColumnVector vector;\n+    private final int batchSize;\n+    private Integer numNulls;\n+\n+    BaseOrcColumnVector(Type type, int batchSize, org.apache.orc.storage.ql.exec.vector.ColumnVector vector) {\n+      super(SparkSchemaUtil.convert(type));\n+      this.vector = vector;\n+      this.batchSize = batchSize;\n+    }\n+\n+    @Override\n+    public void close() {\n+    }\n+\n+    @Override\n+    public boolean hasNull() {\n+      return !vector.noNulls;\n+    }\n+\n+    @Override\n+    public int numNulls() {\n+      if (numNulls == null) {\n+        numNulls = numNullsHelper();\n+      }\n+      return numNulls;\n+    }\n+\n+    private int numNullsHelper() {\n+      if (vector.isRepeating) {\n+        if (vector.isNull[0]) {\n+          return batchSize;\n+        } else {\n+          return 0;\n+        }\n+      } else if (vector.noNulls) {\n+        return 0;\n+      } else {\n+        int count = 0;\n+        for (int i = 0; i < batchSize; i++) {\n+          if (vector.isNull[i]) {\n+            count++;\n+          }\n+        }\n+        return count;\n+      }\n+    }\n+\n+    protected int getRowIndex(int rowId) {\n+      return vector.isRepeating ? 0 : rowId;\n+    }\n+\n+    @Override\n+    public boolean isNullAt(int rowId) {\n+      return vector.isNull[getRowIndex(rowId)];\n+    }\n+\n+    @Override\n+    public boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public ColumnarMap getMap(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public ColumnVector getChild(int ordinal) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  private static class PrimitiveOrcColumnVector extends BaseOrcColumnVector {\n+    private final org.apache.orc.storage.ql.exec.vector.ColumnVector vector;\n+    private final OrcValueReader<?> primitiveValueReader;\n+\n+    PrimitiveOrcColumnVector(Type type, int batchSize, org.apache.orc.storage.ql.exec.vector.ColumnVector vector,\n+        OrcValueReader<?> primitiveValueReader) {\n+      super(type, batchSize, vector);\n+      this.vector = vector;\n+      this.primitiveValueReader = primitiveValueReader;\n+    }\n+\n+    @Override\n+    public boolean getBoolean(int rowId) {\n+      Boolean value = (Boolean) primitiveValueReader.read(vector, rowId);\n+      return value != null ? value : false;\n+    }\n+\n+    @Override\n+    public int getInt(int rowId) {\n+      Integer value = (Integer) primitiveValueReader.read(vector, rowId);\n+      return value != null ? value : 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d"}, "originalPosition": 282}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "08855ca0d7516b23a87a74a4b8d7f9f42c566483", "author": {"user": {"login": "shardulm94", "name": "Shardul Mahadik"}}, "url": "https://github.com/apache/iceberg/commit/08855ca0d7516b23a87a74a4b8d7f9f42c566483", "committedDate": "2020-07-11T00:22:20Z", "message": "Spark: Support ORC vectorized reads"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "413902a25caef97957797b96ee0db302bad777a6", "author": {"user": {"login": "shardulm94", "name": "Shardul Mahadik"}}, "url": "https://github.com/apache/iceberg/commit/413902a25caef97957797b96ee0db302bad777a6", "committedDate": "2020-07-11T00:22:20Z", "message": "Fix test failure"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a2578fc0101c4d098ede4e90b7302f1612650fc0", "author": {"user": {"login": "shardulm94", "name": "Shardul Mahadik"}}, "url": "https://github.com/apache/iceberg/commit/a2578fc0101c4d098ede4e90b7302f1612650fc0", "committedDate": "2020-07-11T00:22:20Z", "message": "Rename StructReader to StructConverter"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "faeffa368d4bc7766ea2008b351acddd792ad7ef", "author": {"user": {"login": "shardulm94", "name": "Shardul Mahadik"}}, "url": "https://github.com/apache/iceberg/commit/faeffa368d4bc7766ea2008b351acddd792ad7ef", "committedDate": "2020-07-11T00:22:20Z", "message": "Fix method param alignment"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d13c3662ac5d7417c647ad5c6c5b22685341dc46", "author": {"user": {"login": "shardulm94", "name": "Shardul Mahadik"}}, "url": "https://github.com/apache/iceberg/commit/d13c3662ac5d7417c647ad5c6c5b22685341dc46", "committedDate": "2020-07-11T00:22:20Z", "message": "Validate that createReaderFunc and createBatchedReaderFunc are not called together"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "420e6e7ea797c4d580d406658c08e07d5f7b8b15", "author": {"user": {"login": "shardulm94", "name": "Shardul Mahadik"}}, "url": "https://github.com/apache/iceberg/commit/420e6e7ea797c4d580d406658c08e07d5f7b8b15", "committedDate": "2020-07-11T00:22:20Z", "message": "Remove vectorized ORC file source benchmarks for nested data since file source falls back to reading row by row"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9c2af8d5038b8dd478c39f57729370f7244816a7", "author": {"user": {"login": "shardulm94", "name": "Shardul Mahadik"}}, "url": "https://github.com/apache/iceberg/commit/9c2af8d5038b8dd478c39f57729370f7244816a7", "committedDate": "2020-07-11T02:00:33Z", "message": "Do not project constant columns from ORC reader"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7d0cf1cafec9ff42d8e4cb5019185af00ff80d58", "author": {"user": {"login": "shardulm94", "name": "Shardul Mahadik"}}, "url": "https://github.com/apache/iceberg/commit/7d0cf1cafec9ff42d8e4cb5019185af00ff80d58", "committedDate": "2020-07-11T02:00:33Z", "message": "Add CloseableIterator.transform and use CloseableIterators correctly"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6a5e8c53d3950922b8f6bd753329b7d943ac423d", "author": {"user": {"login": "shardulm94", "name": "Shardul Mahadik"}}, "url": "https://github.com/apache/iceberg/commit/6a5e8c53d3950922b8f6bd753329b7d943ac423d", "committedDate": "2020-07-10T01:12:47Z", "message": "Rename StructReader to StructConverter"}, "afterCommit": {"oid": "7d0cf1cafec9ff42d8e4cb5019185af00ff80d58", "author": {"user": {"login": "shardulm94", "name": "Shardul Mahadik"}}, "url": "https://github.com/apache/iceberg/commit/7d0cf1cafec9ff42d8e4cb5019185af00ff80d58", "committedDate": "2020-07-11T02:00:33Z", "message": "Add CloseableIterator.transform and use CloseableIterators correctly"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "62bb327eadedf251f930ba495f79f0821198ea97", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/62bb327eadedf251f930ba495f79f0821198ea97", "committedDate": "2020-07-13T20:56:53Z", "message": "Merge branch 'master' into orc-vectorized-new"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4247, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}