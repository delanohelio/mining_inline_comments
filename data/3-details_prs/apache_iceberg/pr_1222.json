{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDUzODY0ODgy", "number": 1222, "title": "Add Avro row position reader", "bodyText": "This adds an Avro ValueReader that returns the position of a row within a file.\nThe position reader's initial position is set from a callback that returns the starting row position of the split that is being read. The callback is passed to classes that implement a new interface, SupportsRowPosition. This uses a callback so that if there is no position reader, the starting row position does not need to be calculated, which is expensive.\nFinding the row position at the start of a split requires scanning through an Avro file stream. AvroIO now includes a utility method that keeps track of the number of rows in each Avro block and seeks past the block content until the next block is after the given split starting point. This validates Avro sync bytes to ensure the count is accurate.\nFixes #1019.", "createdAt": "2020-07-20T21:42:03Z", "url": "https://github.com/apache/iceberg/pull/1222", "merged": true, "mergeCommit": {"oid": "f17879c426c2e3c5fa40f17dfe58633bc866f1c9"}, "closed": true, "closedAt": "2020-07-23T21:18:51Z", "author": {"login": "rdblue"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABc24fpzgH2gAyNDUzODY0ODgyOjE4Y2JlZjk4N2Y2NDAyNjM0NTA2N2YxODE2MzI2ZGUzY2U1Njc2NTQ=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc31knogFqTQ1NDQ3NjgzOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "18cbef987f64026345067f1816326de3ce567654", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/18cbef987f64026345067f1816326de3ce567654", "committedDate": "2020-07-20T21:27:15Z", "message": "Add Avro row position reader."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2b15b522e039429da9e7fed5cd67da3e9125a463", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/2b15b522e039429da9e7fed5cd67da3e9125a463", "committedDate": "2020-07-21T00:41:46Z", "message": "Suppress new checkstyle problem in BuildAvroProjection."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUyMDUzMzY0", "url": "https://github.com/apache/iceberg/pull/1222#pullrequestreview-452053364", "createdAt": "2020-07-21T00:52:25Z", "commit": {"oid": "2b15b522e039429da9e7fed5cd67da3e9125a463"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMDo1MjoyNVrOG0kMpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMDo1MjoyNVrOG0kMpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzc3MjE5Ng==", "bodyText": "Should remove this before committing.", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r457772196", "createdAt": "2020-07-21T00:52:25Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/data/avro/DataReader.java", "diffHunk": "@@ -117,6 +126,7 @@ private ReadBuilder(Map<Integer, ?> idToConstant) {\n     @Override\n     public ValueReader<?> record(Types.StructType struct, Schema record,\n                                  List<String> names, List<ValueReader<?>> fields) {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b15b522e039429da9e7fed5cd67da3e9125a463"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUzODY0MDk2", "url": "https://github.com/apache/iceberg/pull/1222#pullrequestreview-453864096", "createdAt": "2020-07-23T06:32:12Z", "commit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwNjozMjoxMlrOG19vfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwNjo1NTowMFrOG1-QXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIzOTI5Mg==", "bodyText": "I don't understand this comment line\n\nonly if the position reader is set\n\nreader is set where?\n\nand this is a top-level field\n\nI don't see where the top-level field restriction comes from", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459239292", "createdAt": "2020-07-23T06:32:12Z", "author": {"login": "shardulm94"}, "path": "core/src/main/java/org/apache/iceberg/avro/ValueReaders.java", "diffHunk": "@@ -582,13 +585,37 @@ protected StructReader(List<ValueReader<?>> readers, Types.StructType struct, Ma\n         if (idToConstant.containsKey(field.fieldId())) {\n           positionList.add(pos);\n           constantList.add(idToConstant.get(field.fieldId()));\n+        } else if (field.fieldId() == MetadataColumns.ROW_POSITION.fieldId()) {\n+          // replace the _pos field reader with a position reader\n+          // only if the position reader is set and this is a top-level field", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0NzcxMA==", "bodyText": "Not sure if this valid start state, but I think there can be an edge case here\nrow-count|compressed-size-in-bytes|block-bytes|sync|EOF\n                                                 ^\n                                                 |\n                                               start\n\nIn this case it will seek and read the sync and then continue to read next block row count even after EOF\nSo should probably be while (nextSyncPos + 16 < start)?", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459247710", "createdAt": "2020-07-23T06:55:00Z", "author": {"login": "shardulm94"}, "path": "core/src/main/java/org/apache/iceberg/avro/AvroIO.java", "diffHunk": "@@ -131,4 +144,55 @@ public boolean markSupported() {\n       return stream.markSupported();\n     }\n   }\n+\n+  static long findStartingRowPos(Supplier<SeekableInputStream> open, long start) {\n+    try (SeekableInputStream in = open.get()) {\n+      // use a direct decoder that will not buffer so the position of the input stream is accurate\n+      BinaryDecoder decoder = DecoderFactory.get().directBinaryDecoder(in, null);\n+\n+      // an Avro file's layout looks like this:\n+      //   header|block|block|...\n+      // the header contains:\n+      //   magic|string-map|sync\n+      // each block consists of:\n+      //   row-count|compressed-size-in-bytes|block-bytes|sync\n+\n+      // it is necessary to read the header here because this is the only way to get the expected file sync bytes\n+      byte[] magic = MAGIC_READER.read(decoder, null);\n+      if (!Arrays.equals(AVRO_MAGIC, magic)) {\n+        throw new InvalidAvroMagicException(\"Not an Avro file\");\n+      }\n+\n+      META_READER.read(decoder, null); // ignore the file metadata, it isn't needed\n+      byte[] fileSync = SYNC_READER.read(decoder, null);\n+\n+      // the while loop reads row counts and seeks past the block bytes until the next sync pos is >= start, which\n+      // indicates that the next sync is the start of the split.\n+      byte[] blockSync = new byte[16];\n+      long totalRows = 0;\n+      long nextSyncPos = in.getPos();\n+\n+      while (nextSyncPos < start) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU0MzYwNjEx", "url": "https://github.com/apache/iceberg/pull/1222#pullrequestreview-454360611", "createdAt": "2020-07-23T17:45:24Z", "commit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNzo0NToyNFrOG2VFCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNzo0NToyNFrOG2VFCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYyMTY0MA==", "bodyText": "nit: This is deprecated in favor of UncheckedIOException", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459621640", "createdAt": "2020-07-23T17:45:24Z", "author": {"login": "rdsr"}, "path": "core/src/main/java/org/apache/iceberg/avro/AvroIO.java", "diffHunk": "@@ -131,4 +144,55 @@ public boolean markSupported() {\n       return stream.markSupported();\n     }\n   }\n+\n+  static long findStartingRowPos(Supplier<SeekableInputStream> open, long start) {\n+    try (SeekableInputStream in = open.get()) {\n+      // use a direct decoder that will not buffer so the position of the input stream is accurate\n+      BinaryDecoder decoder = DecoderFactory.get().directBinaryDecoder(in, null);\n+\n+      // an Avro file's layout looks like this:\n+      //   header|block|block|...\n+      // the header contains:\n+      //   magic|string-map|sync\n+      // each block consists of:\n+      //   row-count|compressed-size-in-bytes|block-bytes|sync\n+\n+      // it is necessary to read the header here because this is the only way to get the expected file sync bytes\n+      byte[] magic = MAGIC_READER.read(decoder, null);\n+      if (!Arrays.equals(AVRO_MAGIC, magic)) {\n+        throw new InvalidAvroMagicException(\"Not an Avro file\");\n+      }\n+\n+      META_READER.read(decoder, null); // ignore the file metadata, it isn't needed\n+      byte[] fileSync = SYNC_READER.read(decoder, null);\n+\n+      // the while loop reads row counts and seeks past the block bytes until the next sync pos is >= start, which\n+      // indicates that the next sync is the start of the split.\n+      byte[] blockSync = new byte[16];\n+      long totalRows = 0;\n+      long nextSyncPos = in.getPos();\n+\n+      while (nextSyncPos < start) {\n+        if (nextSyncPos != in.getPos()) {\n+          in.seek(nextSyncPos);\n+          SYNC_READER.read(decoder, blockSync);\n+\n+          if (!Arrays.equals(fileSync, blockSync)) {\n+            throw new RuntimeIOException(\"Invalid sync at %s\", nextSyncPos);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c"}, "originalPosition": 65}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU0NDI2NjU2", "url": "https://github.com/apache/iceberg/pull/1222#pullrequestreview-454426656", "createdAt": "2020-07-23T19:19:11Z", "commit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c"}, "state": "COMMENTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6f1fc7139c4e03973550e1da371ec8da85056b8b", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/6f1fc7139c4e03973550e1da371ec8da85056b8b", "committedDate": "2020-07-23T20:32:14Z", "message": "Handle EOF."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1adf3cfdd27c91456089be7d0d20740c6af4444b", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/1adf3cfdd27c91456089be7d0d20740c6af4444b", "committedDate": "2020-07-23T20:34:18Z", "message": "Update comment."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "df1cae9a872567e9efaa012aaa3d13234fda73b5", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/df1cae9a872567e9efaa012aaa3d13234fda73b5", "committedDate": "2020-07-23T20:35:37Z", "message": "Remove unnecessary whitespace change in DataReader."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/d107f778da2ea0997d9eda087982c20ed1e1f95c", "committedDate": "2020-07-22T21:52:01Z", "message": "Update DataReader.java"}, "afterCommit": {"oid": "df1cae9a872567e9efaa012aaa3d13234fda73b5", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/df1cae9a872567e9efaa012aaa3d13234fda73b5", "committedDate": "2020-07-23T20:35:37Z", "message": "Remove unnecessary whitespace change in DataReader."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU0NDc2ODM4", "url": "https://github.com/apache/iceberg/pull/1222#pullrequestreview-454476838", "createdAt": "2020-07-23T20:36:53Z", "commit": {"oid": "df1cae9a872567e9efaa012aaa3d13234fda73b5"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDozNjo1M1rOG2argA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDozNjo1M1rOG2argA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxMzQwOA==", "bodyText": "@shardulm94, here's a test for the EOF case.", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459713408", "createdAt": "2020-07-23T20:36:53Z", "author": {"login": "rdblue"}, "path": "core/src/test/java/org/apache/iceberg/avro/TestAvroFileSplit.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.avro;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.UUID;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.avro.DataWriter;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.types.Types.NestedField;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestAvroFileSplit {\n+  private static final Schema SCHEMA = new Schema(\n+      NestedField.required(1, \"id\", Types.LongType.get()),\n+      NestedField.required(2, \"data\", Types.StringType.get()));\n+\n+  private static final int NUM_RECORDS = 100_000;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  public List<Record> expected = null;\n+  public InputFile file = null;\n+\n+  @Before\n+  public void writeDataFile() throws IOException {\n+    this.expected = Lists.newArrayList();\n+\n+    OutputFile out = Files.localOutput(temp.newFile());\n+\n+    try (FileAppender<Object> writer = Avro.write(out)\n+        .set(TableProperties.AVRO_COMPRESSION, \"uncompressed\")\n+        .createWriterFunc(DataWriter::create)\n+        .schema(SCHEMA)\n+        .overwrite()\n+        .build()) {\n+\n+      Record record = GenericRecord.create(SCHEMA);\n+      for (long i = 0; i < NUM_RECORDS; i += 1) {\n+        Record next = record.copy(ImmutableMap.of(\n+            \"id\", i,\n+            \"data\", UUID.randomUUID().toString()));\n+        expected.add(next);\n+        writer.add(next);\n+      }\n+    }\n+\n+    this.file = out.toInputFile();\n+  }\n+\n+  @Test\n+  public void testSplitDataSkipping() throws IOException {\n+    long end = file.getLength();\n+    long splitLocation = end / 2;\n+\n+    List<Record> firstHalf = readAvro(file, SCHEMA, 0, splitLocation);\n+    Assert.assertNotEquals(\"First split should not be empty\", 0, firstHalf.size());\n+\n+    List<Record> secondHalf = readAvro(file, SCHEMA, splitLocation + 1, end - splitLocation - 1);\n+    Assert.assertNotEquals(\"Second split should not be empty\", 0, secondHalf.size());\n+\n+    Assert.assertEquals(\"Total records should match expected\",\n+        expected.size(), firstHalf.size() + secondHalf.size());\n+\n+    for (int i = 0; i < firstHalf.size(); i += 1) {\n+      Assert.assertEquals(expected.get(i), firstHalf.get(i));\n+    }\n+\n+    for (int i = 0; i < secondHalf.size(); i += 1) {\n+      Assert.assertEquals(expected.get(firstHalf.size() + i), secondHalf.get(i));\n+    }\n+  }\n+\n+  @Test\n+  public void testPosField() throws IOException {\n+    Schema projection = new Schema(\n+        SCHEMA.columns().get(0),\n+        MetadataColumns.ROW_POSITION,\n+        SCHEMA.columns().get(1));\n+\n+    List<Record> records = readAvro(file, projection, 0, file.getLength());\n+\n+    for (int i = 0; i < expected.size(); i += 1) {\n+      Assert.assertEquals(\"Field _pos should match\",\n+          (long) i, records.get(i).getField(MetadataColumns.ROW_POSITION.name()));\n+      Assert.assertEquals(\"Field id should match\",\n+          expected.get(i).getField(\"id\"), records.get(i).getField(\"id\"));\n+      Assert.assertEquals(\"Field data should match\",\n+          expected.get(i).getField(\"data\"), records.get(i).getField(\"data\"));\n+    }\n+  }\n+\n+  @Test\n+  public void testPosFieldWithSplits() throws IOException {\n+    Schema projection = new Schema(\n+        SCHEMA.columns().get(0),\n+        MetadataColumns.ROW_POSITION,\n+        SCHEMA.columns().get(1));\n+\n+    long end = file.getLength();\n+    long splitLocation = end / 2;\n+\n+    List<Record> secondHalf = readAvro(file, projection, splitLocation + 1, end - splitLocation - 1);\n+    Assert.assertNotEquals(\"Second split should not be empty\", 0, secondHalf.size());\n+\n+    List<Record> firstHalf = readAvro(file, projection, 0, splitLocation);\n+    Assert.assertNotEquals(\"First split should not be empty\", 0, firstHalf.size());\n+\n+    Assert.assertEquals(\"Total records should match expected\",\n+        expected.size(), firstHalf.size() + secondHalf.size());\n+\n+    for (int i = 0; i < firstHalf.size(); i += 1) {\n+      Assert.assertEquals(\"Field _pos should match\",\n+          (long) i, firstHalf.get(i).getField(MetadataColumns.ROW_POSITION.name()));\n+      Assert.assertEquals(\"Field id should match\",\n+          expected.get(i).getField(\"id\"), firstHalf.get(i).getField(\"id\"));\n+      Assert.assertEquals(\"Field data should match\",\n+          expected.get(i).getField(\"data\"), firstHalf.get(i).getField(\"data\"));\n+    }\n+\n+    for (int i = 0; i < secondHalf.size(); i += 1) {\n+      Assert.assertEquals(\"Field _pos should match\",\n+          (long) (firstHalf.size() + i), secondHalf.get(i).getField(MetadataColumns.ROW_POSITION.name()));\n+      Assert.assertEquals(\"Field id should match\",\n+          expected.get(firstHalf.size() + i).getField(\"id\"), secondHalf.get(i).getField(\"id\"));\n+      Assert.assertEquals(\"Field data should match\",\n+          expected.get(firstHalf.size() + i).getField(\"data\"), secondHalf.get(i).getField(\"data\"));\n+    }\n+  }\n+\n+  @Test\n+  public void testPosWithEOFSplit() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df1cae9a872567e9efaa012aaa3d13234fda73b5"}, "originalPosition": 166}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4294, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}