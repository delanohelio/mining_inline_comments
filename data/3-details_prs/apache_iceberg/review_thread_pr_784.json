{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzcyMDUxODU0", "number": 784, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMDo0NTo1MlrODeqFhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMDo0ODozMlrODeqHOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDczNDE0OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "isResolved": true, "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMDo0NTo1MlrOFn6tcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMlQxOTozMDoxM1rOFo7uVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMDY5MA==", "bodyText": "This needs to return the Configuration that was passed in when creating the HadoopInputFile, if there was one. The file system's Configuration may not contain the same configuration properties as the one that was passed in, and this Configuration is needed in some paths.", "url": "https://github.com/apache/iceberg/pull/784#discussion_r377400690", "createdAt": "2020-02-11T00:45:52Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "diffHunk": "@@ -133,7 +143,7 @@ public SeekableInputStream newStream() {\n   }\n \n   public Configuration getConf() {\n-    return conf;\n+    return fs.getConf();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b8e6e4e66742568829f87dc71ad4d76738e60bf"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzY1NDQ5OQ==", "bodyText": "@rdblue I made the change but would like to clarify. If HadoopInputFile conf may not be the same as FileSystem conf, will it be better to make it more explicit by implementing Configurable or extending from Configured?", "url": "https://github.com/apache/iceberg/pull/784#discussion_r377654499", "createdAt": "2020-02-11T14:07:09Z", "author": {"login": "vrozov"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "diffHunk": "@@ -133,7 +143,7 @@ public SeekableInputStream newStream() {\n   }\n \n   public Configuration getConf() {\n-    return conf;\n+    return fs.getConf();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMDY5MA=="}, "originalCommit": {"oid": "6b8e6e4e66742568829f87dc71ad4d76738e60bf"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk2MjE3Nw==", "bodyText": "I don't think we want to add complexity to how Configuration objects are handled. A Configuration is used to instantiate a catalog, and it gets passed down to FileIO from there. We don't need to be able to change that configuration, we just need to make sure the one passed to create the catalog is used by parts like ORC that currently use a Configuration.\nAnd if a Configuration is not passed, it is fine to use new Configuration() or the config from a file system that was passed. We don't make guarantees in that case.", "url": "https://github.com/apache/iceberg/pull/784#discussion_r377962177", "createdAt": "2020-02-11T23:31:59Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "diffHunk": "@@ -133,7 +143,7 @@ public SeekableInputStream newStream() {\n   }\n \n   public Configuration getConf() {\n-    return conf;\n+    return fs.getConf();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMDY5MA=="}, "originalCommit": {"oid": "6b8e6e4e66742568829f87dc71ad4d76738e60bf"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk4MDcyOA==", "bodyText": "I don't see how extending from Configured will add complexity. The behavior of HadoopInputFile or HadoopOutputFile won't change.", "url": "https://github.com/apache/iceberg/pull/784#discussion_r377980728", "createdAt": "2020-02-12T00:31:54Z", "author": {"login": "vrozov"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "diffHunk": "@@ -133,7 +143,7 @@ public SeekableInputStream newStream() {\n   }\n \n   public Configuration getConf() {\n-    return conf;\n+    return fs.getConf();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMDY5MA=="}, "originalCommit": {"oid": "6b8e6e4e66742568829f87dc71ad4d76738e60bf"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzk4MzgyMg==", "bodyText": "It's not a big change for Iceberg, but it would mean that there are multiple paths for passing a Configuration to parts that use it. I'd rather avoid that so that there is one way configurations are passed through code to avoid callers hijacking parts and updating the configuration held by a HadoopOutputFile.", "url": "https://github.com/apache/iceberg/pull/784#discussion_r377983822", "createdAt": "2020-02-12T00:43:15Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "diffHunk": "@@ -133,7 +143,7 @@ public SeekableInputStream newStream() {\n   }\n \n   public Configuration getConf() {\n-    return conf;\n+    return fs.getConf();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMDY5MA=="}, "originalCommit": {"oid": "6b8e6e4e66742568829f87dc71ad4d76738e60bf"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODAwNDYyNA==", "bodyText": "I don't see much difference between\nConfiguration conf = new Configuration();\nPath path = new Path(...);\nHadoopInputFile file = HadoopInputFile.fromPath(path, conf);\n...\nconf = new Configuration();\nfile.setConf(conf);\n\nand\nConfiguration conf = new Configuration();\nPath path = new Path(...);\nHadoopInputFile file = HadoopInputFile.fromPath(path, conf);\n...\nconf.set();\n\nif I correctly understand your concern with incorrectly updating the configuration. It would be necessary to copy passed configuration object to prevent hijacking.", "url": "https://github.com/apache/iceberg/pull/784#discussion_r378004624", "createdAt": "2020-02-12T02:04:54Z", "author": {"login": "vrozov"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "diffHunk": "@@ -133,7 +143,7 @@ public SeekableInputStream newStream() {\n   }\n \n   public Configuration getConf() {\n-    return conf;\n+    return fs.getConf();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMDY5MA=="}, "originalCommit": {"oid": "6b8e6e4e66742568829f87dc71ad4d76738e60bf"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODQyNTI2MA==", "bodyText": "You can get input and output files by constructing them yourself, but the recommended way is to use the factory methods on a table's FileIO so that the catalog/table implementation can inject what it needs to at this layer, like using a different implementation for an object store.\nThe Hadoop variants of input and output file accept a configuration so that HadoopFileIO can pass it, but changing that Configuration is not a part of the API. We should avoid cases where implementations attempt to change the configuration that an InputFile or OutputFile uses. The right way to pass a configuration is to pass it in when creating a catalog.", "url": "https://github.com/apache/iceberg/pull/784#discussion_r378425260", "createdAt": "2020-02-12T18:13:14Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "diffHunk": "@@ -133,7 +143,7 @@ public SeekableInputStream newStream() {\n   }\n \n   public Configuration getConf() {\n-    return conf;\n+    return fs.getConf();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMDY5MA=="}, "originalCommit": {"oid": "6b8e6e4e66742568829f87dc71ad4d76738e60bf"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3ODQ2NTg3Ng==", "bodyText": "Still the same applies to FileIO implementation. Without a deep copy of Configuration inside HadoopInputFile, it is possible to cache Configuration object on a class that implements FileIO and expose methods that modify it. I do not suggest that it needs to be handled as part of this PR, just pointing out that even though it is not possible to change reference, the returned object is not immutable. Assuming that there are no other issues to address, can you please approve the PR?", "url": "https://github.com/apache/iceberg/pull/784#discussion_r378465876", "createdAt": "2020-02-12T19:30:13Z", "author": {"login": "vrozov"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "diffHunk": "@@ -133,7 +143,7 @@ public SeekableInputStream newStream() {\n   }\n \n   public Configuration getConf() {\n-    return conf;\n+    return fs.getConf();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMDY5MA=="}, "originalCommit": {"oid": "6b8e6e4e66742568829f87dc71ad4d76738e60bf"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDczNTM3OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMDo0NjozNFrOFn6uJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMDo0NjozNFrOFn6uJA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMDg2OA==", "bodyText": "This should include an error message that explains what went wrong, like \"Invalid input file length: %s\"", "url": "https://github.com/apache/iceberg/pull/784#discussion_r377400868", "createdAt": "2020-02-11T00:46:34Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "diffHunk": "@@ -54,51 +55,60 @@ public static HadoopInputFile fromLocation(CharSequence location, long length,\n     return fromPath(path, length, conf);\n   }\n \n+  public static HadoopInputFile fromLocation(CharSequence location, FileSystem fs) {\n+    Path path = new Path(location.toString());\n+    return fromPath(path, fs);\n+  }\n+\n+  public static HadoopInputFile fromLocation(CharSequence location, long length,\n+                                             FileSystem fs) {\n+    Path path = new Path(location.toString());\n+    return fromPath(path, length, fs);\n+  }\n+\n   public static HadoopInputFile fromPath(Path path, Configuration conf) {\n-    try {\n-      FileSystem fs = path.getFileSystem(conf);\n-      return new HadoopInputFile(fs, path, conf);\n-    } catch (IOException e) {\n-      throw new RuntimeIOException(e, \"Failed to get file system for path: %s\", path);\n-    }\n+    FileSystem fs = Util.getFs(path, conf);\n+    return fromPath(path, fs);\n   }\n \n   public static HadoopInputFile fromPath(Path path, long length, Configuration conf) {\n-    try {\n-      FileSystem fs = path.getFileSystem(conf);\n-      return new HadoopInputFile(fs, path, length, conf);\n-    } catch (IOException e) {\n-      throw new RuntimeIOException(e, \"Failed to get file system for path: %s\", path);\n-    }\n+    FileSystem fs = Util.getFs(path, conf);\n+    return fromPath(path, length, fs);\n+  }\n+\n+  public static HadoopInputFile fromPath(Path path, FileSystem fs) {\n+    return new HadoopInputFile(fs, path);\n+  }\n+\n+  public static HadoopInputFile fromPath(Path path, long length, FileSystem fs) {\n+    return new HadoopInputFile(fs, path, length);\n   }\n \n   public static HadoopInputFile fromStatus(FileStatus stat, Configuration conf) {\n-    try {\n-      FileSystem fs = stat.getPath().getFileSystem(conf);\n-      return new HadoopInputFile(fs, stat, conf);\n-    } catch (IOException e) {\n-      throw new RuntimeIOException(e, \"Failed to get file system for path: %s\", stat.getPath());\n-    }\n+    FileSystem fs = Util.getFs(stat.getPath(), conf);\n+    return fromStatus(stat, fs);\n+  }\n+\n+  public static HadoopInputFile fromStatus(FileStatus stat, FileSystem fs) {\n+    return new HadoopInputFile(fs, stat);\n   }\n \n-  private HadoopInputFile(FileSystem fs, Path path, Configuration conf) {\n+  private HadoopInputFile(FileSystem fs, Path path) {\n     this.fs = fs;\n     this.path = path;\n-    this.conf = conf;\n   }\n \n-  private HadoopInputFile(FileSystem fs, Path path, long length, Configuration conf) {\n+  private HadoopInputFile(FileSystem fs, Path path, long length) {\n+    checkArgument(length >= 0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b8e6e4e66742568829f87dc71ad4d76738e60bf"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDczNjcyOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMDo0NzoyNlrOFn6vCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQxOTozNTo1NVrOFoWYOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMTA5Ng==", "bodyText": "The project's style avoids static imports. Can you use Preconditions.checkArgument instead of just the method name where this is called?", "url": "https://github.com/apache/iceberg/pull/784#discussion_r377401096", "createdAt": "2020-02-11T00:47:26Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "diffHunk": "@@ -30,6 +30,8 @@\n import org.apache.iceberg.io.InputFile;\n import org.apache.iceberg.io.SeekableInputStream;\n \n+import static com.google.common.base.Preconditions.checkArgument;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b8e6e4e66742568829f87dc71ad4d76738e60bf"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzY2MDUyNw==", "bodyText": "Will it be good to remove com.google.common.base.Preconditions.* exclusion from AvoidStaticImport rule in the project checkstyle?", "url": "https://github.com/apache/iceberg/pull/784#discussion_r377660527", "createdAt": "2020-02-11T14:17:21Z", "author": {"login": "vrozov"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "diffHunk": "@@ -30,6 +30,8 @@\n import org.apache.iceberg.io.InputFile;\n import org.apache.iceberg.io.SeekableInputStream;\n \n+import static com.google.common.base.Preconditions.checkArgument;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMTA5Ng=="}, "originalCommit": {"oid": "6b8e6e4e66742568829f87dc71ad4d76738e60bf"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzczMTY2NQ==", "bodyText": "I am +1 on removing it from checkstyle in a separate PR as we technically allow such imports but try to avoid them.", "url": "https://github.com/apache/iceberg/pull/784#discussion_r377731665", "createdAt": "2020-02-11T16:03:09Z", "author": {"login": "aokolnychyi"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "diffHunk": "@@ -30,6 +30,8 @@\n import org.apache.iceberg.io.InputFile;\n import org.apache.iceberg.io.SeekableInputStream;\n \n+import static com.google.common.base.Preconditions.checkArgument;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMTA5Ng=="}, "originalCommit": {"oid": "6b8e6e4e66742568829f87dc71ad4d76738e60bf"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Nzg1NDAwOA==", "bodyText": "Yes. This should be caught by checkstyle.", "url": "https://github.com/apache/iceberg/pull/784#discussion_r377854008", "createdAt": "2020-02-11T19:35:55Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java", "diffHunk": "@@ -30,6 +30,8 @@\n import org.apache.iceberg.io.InputFile;\n import org.apache.iceberg.io.SeekableInputStream;\n \n+import static com.google.common.base.Preconditions.checkArgument;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMTA5Ng=="}, "originalCommit": {"oid": "6b8e6e4e66742568829f87dc71ad4d76738e60bf"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjMzNDczODUxOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopOutputFile.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMDo0ODozMlrOFn6wGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMDo0ODozMlrOFn6wGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMTM3MQ==", "bodyText": "This also needs to return the Configuration that was used to create the output file, if there was one.", "url": "https://github.com/apache/iceberg/pull/784#discussion_r377401371", "createdAt": "2020-02-11T00:48:32Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/hadoop/HadoopOutputFile.java", "diffHunk": "@@ -73,7 +87,7 @@ public Path getPath() {\n   }\n \n   public Configuration getConf() {\n-    return conf;\n+    return fs.getConf();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6b8e6e4e66742568829f87dc71ad4d76738e60bf"}, "originalPosition": 56}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3018, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}