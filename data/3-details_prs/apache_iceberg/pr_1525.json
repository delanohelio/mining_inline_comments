{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk0NDA5Nzcy", "number": 1525, "title": "Provide API and Implementation for Creating Iceberg Tables from Spark", "bodyText": "Previously the SparkUtil class provided an importSparkTable command but\nthis command suffered from a few shortcomings. It had a difficult api and\ndid not work directly with DSV2 Iceberg Catalogs. To provide a simpler interface\nthat shares a similar pattern to currently available Spark Actions we introduce\nCreateActions and MigrateAction.\nThrough this command we can both migrate a Table as well as make a snapshot\nof an existing table.\nGeneral usage:\nCreateActions\n.createIcebergTable(spark, dest)\n.fromSourceTable(source)\n.withNewTableLocation(location)\n.execute()", "createdAt": "2020-09-28T21:23:17Z", "url": "https://github.com/apache/iceberg/pull/1525", "merged": true, "mergeCommit": {"oid": "2512e7f0a1e9d1e4bc34098634d3cca0fe8ebea9"}, "closed": true, "closedAt": "2020-12-10T20:03:01Z", "author": {"login": "RussellSpitzer"}, "timelineItems": {"totalCount": 135, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdkhp3JAH2gAyNDk0NDA5NzcyOmVmMTI0NDU1NDJhYjkzNzY0ZTU0YmVhMzZmNjFmZDk4OTBkNGI0NDk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdk4dC1gFqTU0OTUyOTE0NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "ef12445542ab93764e54bea36f61fd9890d4b449", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/ef12445542ab93764e54bea36f61fd9890d4b449", "committedDate": "2020-12-09T16:51:06Z", "message": "Refactor out Spark 3 Migration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "af9ed97fa4e0047f1dd6ae8b6d979d5f67447e2e", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/af9ed97fa4e0047f1dd6ae8b6d979d5f67447e2e", "committedDate": "2020-12-09T16:51:06Z", "message": "Fixup from Rebase"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bd330688dd052df82f3f52dfdb10d667ec407127", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/bd330688dd052df82f3f52dfdb10d667ec407127", "committedDate": "2020-12-09T16:51:06Z", "message": "Fix CheckStyle Errors"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f4e456ed4fdf42fba99dbc095a21d900b468c8df", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/f4e456ed4fdf42fba99dbc095a21d900b468c8df", "committedDate": "2020-12-09T16:51:06Z", "message": "Add GC_ENABLED false Flag to Snapshot Tables"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a8898ccffa97466d16d0eede6303f52aa4644c7a", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/a8898ccffa97466d16d0eede6303f52aa4644c7a", "committedDate": "2020-12-09T16:51:06Z", "message": "Fix Source Catalog Requirements"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f25e4731774e3ff1587a984fcfd067f434a49cfa", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/f25e4731774e3ff1587a984fcfd067f434a49cfa", "committedDate": "2020-12-09T16:51:06Z", "message": "Various Reviewer Comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/bf50967bb9ab9344b3c242c274cc9fab58c8b115", "committedDate": "2020-12-09T19:34:36Z", "message": "Review Comments and Changes"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "committedDate": "2020-12-04T21:01:07Z", "message": "Various Reviewer Comments"}, "afterCommit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/bf50967bb9ab9344b3c242c274cc9fab58c8b115", "committedDate": "2020-12-09T19:34:36Z", "message": "Review Comments and Changes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ4NzQ1MjI4", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-548745228", "createdAt": "2020-12-10T01:17:47Z", "commit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMToxNzo0N1rOICw8Gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMToxNzo0N1rOICw8Gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc2OTg4Mg==", "bodyText": "Nit: indentation is off.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539769882", "createdAt": "2020-12-10T01:17:47Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String LOCATION = \"location\";\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =\n+      ImmutableList.of(\"path\", \"transient_lastDdlTime\", \"serialization.format\");\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final TableCatalog sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private final Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                     CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) this.sourceCatalog.loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected TableCatalog sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableIdent() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableIdent() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private void validateSourceTable() {\n+    String sourceTableProvider = sourceCatalogTable.provider().get().toLowerCase(Locale.ROOT);\n+    Preconditions.checkArgument(ALLOWED_SOURCES.contains(sourceTableProvider),\n+          \"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115"}, "originalPosition": 144}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ4NzQ1NzU0", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-548745754", "createdAt": "2020-12-10T01:19:25Z", "commit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMToxOToyNVrOICw-CA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMToxOToyNVrOICw-CA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3MDM3Ng==", "bodyText": "I thought this was going to be changed to the Iceberg NoSuchNamespaceException? Same with the other one. Can we throw the right Iceberg exception so that callers can catch them?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539770376", "createdAt": "2020-12-10T01:19:25Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String LOCATION = \"location\";\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =\n+      ImmutableList.of(\"path\", \"transient_lastDdlTime\", \"serialization.format\");\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final TableCatalog sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private final Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                     CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) this.sourceCatalog.loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected TableCatalog sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableIdent() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableIdent() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private void validateSourceTable() {\n+    String sourceTableProvider = sourceCatalogTable.provider().get().toLowerCase(Locale.ROOT);\n+    Preconditions.checkArgument(ALLOWED_SOURCES.contains(sourceTableProvider),\n+          \"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider);\n+    Preconditions.checkArgument(!sourceCatalogTable.storage().locationUri().isEmpty(),\n+        \"Cannot create an Iceberg table from a source without an explicit location\");\n+  }\n+\n+  private StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {\n+    Preconditions.checkArgument(catalog instanceof SparkSessionCatalog || catalog instanceof SparkCatalog,\n+        \"Cannot create Iceberg table in non Iceberg Catalog. \" +\n+            \"Catalog %s was of class %s but %s or %s are required\",\n+        catalog.name(), catalog.getClass(), SparkSessionCatalog.class.getName(), SparkCatalog.class.getName());\n+\n+    return (StagingTableCatalog) catalog;\n+  }\n+\n+  protected StagedTable stageDestTable() {\n+    try {\n+      Map<String, String> props = targetTableProps();\n+      StructType schema = sourceTable.schema();\n+      Transform[] partitioning = sourceTable.partitioning();\n+      return destCatalog.stageCreate(destTableIdent, schema, partitioning, props);\n+    } catch (NoSuchNamespaceException e) {\n+      throw new IllegalArgumentException(\"Cannot create a new table in a namespace which does not exist\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115"}, "originalPosition": 165}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ4NzQ2MDQ2", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-548746046", "createdAt": "2020-12-10T01:20:09Z", "commit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMToyMDowOVrOICw_QQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMToyMDowOVrOICw_QQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3MDY4OQ==", "bodyText": "Same here. I think these should be the right Iceberg exceptions. And, the exception message here should include the table identifier that could not be found.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539770689", "createdAt": "2020-12-10T01:20:09Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableIdent().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableIdent().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableIdent(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115"}, "originalPosition": 69}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ4NzQ2MjI1", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-548746225", "createdAt": "2020-12-10T01:20:39Z", "commit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMToyMDozOVrOICw_7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMToyMDozOVrOICw_7Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3MDg2MQ==", "bodyText": "This should similarly include the name of the table that already exists.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539770861", "createdAt": "2020-12-10T01:20:39Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableIdent().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableIdent().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableIdent(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115"}, "originalPosition": 71}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ4NzQ2NDE1", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-548746415", "createdAt": "2020-12-10T01:21:10Z", "commit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMToyMToxMFrOICxAog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMToyMToxMFrOICxAog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3MTA0Mg==", "bodyText": "Please include the destination table name in the error message when this is updated, too.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539771042", "createdAt": "2020-12-10T01:21:10Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String LOCATION = \"location\";\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =\n+      ImmutableList.of(\"path\", \"transient_lastDdlTime\", \"serialization.format\");\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final TableCatalog sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private final Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                     CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) this.sourceCatalog.loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected TableCatalog sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableIdent() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableIdent() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private void validateSourceTable() {\n+    String sourceTableProvider = sourceCatalogTable.provider().get().toLowerCase(Locale.ROOT);\n+    Preconditions.checkArgument(ALLOWED_SOURCES.contains(sourceTableProvider),\n+          \"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider);\n+    Preconditions.checkArgument(!sourceCatalogTable.storage().locationUri().isEmpty(),\n+        \"Cannot create an Iceberg table from a source without an explicit location\");\n+  }\n+\n+  private StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {\n+    Preconditions.checkArgument(catalog instanceof SparkSessionCatalog || catalog instanceof SparkCatalog,\n+        \"Cannot create Iceberg table in non Iceberg Catalog. \" +\n+            \"Catalog %s was of class %s but %s or %s are required\",\n+        catalog.name(), catalog.getClass(), SparkSessionCatalog.class.getName(), SparkCatalog.class.getName());\n+\n+    return (StagingTableCatalog) catalog;\n+  }\n+\n+  protected StagedTable stageDestTable() {\n+    try {\n+      Map<String, String> props = targetTableProps();\n+      StructType schema = sourceTable.schema();\n+      Transform[] partitioning = sourceTable.partitioning();\n+      return destCatalog.stageCreate(destTableIdent, schema, partitioning, props);\n+    } catch (NoSuchNamespaceException e) {\n+      throw new IllegalArgumentException(\"Cannot create a new table in a namespace which does not exist\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Destination table already exists\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115"}, "originalPosition": 167}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ4NzQ3MTcz", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-548747173", "createdAt": "2020-12-10T01:23:10Z", "commit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMToyMzoxMFrOICxD0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMToyMzoxMFrOICxD0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3MTg1OQ==", "bodyText": "The return value of this needs to be a SparkTable for both uses, so I think it should return a SparkTable to avoid unchecked casts in the callers.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539771859", "createdAt": "2020-12-10T01:23:10Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String LOCATION = \"location\";\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =\n+      ImmutableList.of(\"path\", \"transient_lastDdlTime\", \"serialization.format\");\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final TableCatalog sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private final Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                     CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) this.sourceCatalog.loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected TableCatalog sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableIdent() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableIdent() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private void validateSourceTable() {\n+    String sourceTableProvider = sourceCatalogTable.provider().get().toLowerCase(Locale.ROOT);\n+    Preconditions.checkArgument(ALLOWED_SOURCES.contains(sourceTableProvider),\n+          \"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider);\n+    Preconditions.checkArgument(!sourceCatalogTable.storage().locationUri().isEmpty(),\n+        \"Cannot create an Iceberg table from a source without an explicit location\");\n+  }\n+\n+  private StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {\n+    Preconditions.checkArgument(catalog instanceof SparkSessionCatalog || catalog instanceof SparkCatalog,\n+        \"Cannot create Iceberg table in non Iceberg Catalog. \" +\n+            \"Catalog %s was of class %s but %s or %s are required\",\n+        catalog.name(), catalog.getClass(), SparkSessionCatalog.class.getName(), SparkCatalog.class.getName());\n+\n+    return (StagingTableCatalog) catalog;\n+  }\n+\n+  protected StagedTable stageDestTable() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115"}, "originalPosition": 158}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ4NzQ5MTky", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-548749192", "createdAt": "2020-12-10T01:28:43Z", "commit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMToyODo0NFrOICxLtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMToyODo0NFrOICxLtg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3Mzg3OA==", "bodyText": "Migrate catches any exceptions from the abort and logs them, but allows the original exception to propagate. I think this should do the same.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539773878", "createdAt": "2020-12-10T01:28:44Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableIdent, CatalogPlugin destCatalog,\n+                       Identifier destTableIdent) {\n+    super(spark, sourceCatalog, sourceTableIdent, destCatalog, destTableIdent);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable = stageDestTable();\n+    Table icebergTable = ((SparkTable) stagedTable).table();\n+    // TODO Check table location here against source location\n+\n+    ensureNameMappingPresent(icebergTable);\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = getMetadataLocation(icebergTable);\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableIdent(), destTableIdent(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting to commit snapshot changes, rolling back\");\n+        stagedTable.abortStagedChanges();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115"}, "originalPosition": 78}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ4NzQ5NTU3", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-548749557", "createdAt": "2020-12-10T01:29:41Z", "commit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMToyOTo0MVrOICxNUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwMToyOTo0MVrOICxNUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc3NDI4OA==", "bodyText": "The checks here should be Preconditions checks as well.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r539774288", "createdAt": "2020-12-10T01:29:41Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,139 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableIdent, CatalogPlugin destCatalog,\n+                       Identifier destTableIdent) {\n+    super(spark, sourceCatalog, sourceTableIdent, destCatalog, destTableIdent);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable = stageDestTable();\n+    Table icebergTable = ((SparkTable) stagedTable).table();\n+    // TODO Check table location here against source location\n+\n+    ensureNameMappingPresent(icebergTable);\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = getMetadataLocation(icebergTable);\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableIdent(), destTableIdent(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting to commit snapshot changes, rolling back\");\n+        stagedTable.abortStagedChanges();\n+      }\n+    }\n+\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    long numMigratedFiles = Long.parseLong(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+    LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    return numMigratedFiles;\n+  }\n+\n+  @Override\n+  protected Map<String, String> targetTableProps() {\n+    Map<String, String> properties = Maps.newHashMap();\n+\n+    // Remove any possible location properties from origin properties\n+    properties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+    properties.remove(LOCATION);\n+    properties.remove(TableProperties.WRITE_METADATA_LOCATION);\n+    properties.remove(TableProperties.WRITE_NEW_DATA_LOCATION);\n+\n+    EXCLUDED_PROPERTIES.forEach(properties::remove);\n+    properties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+    properties.put(TableProperties.GC_ENABLED, \"false\");\n+    properties.put(\"snapshot\", \"true\");\n+    properties.putAll(additionalProperties());\n+\n+    // Don't use the default location for the destination table if an alternate has be set\n+    if (destTableLocation != null) {\n+      properties.put(LOCATION, destTableLocation);\n+    }\n+\n+    return properties;\n+  }\n+\n+  @Override\n+  protected TableCatalog checkSourceCatalog(CatalogPlugin catalog) {\n+    // Currently the Import code relies on being able to look up the table in the session code\n+    if (!(catalog.name().equals(\"spark_catalog\"))) {\n+      throw new IllegalArgumentException(String.format(\n+          \"Cannot snapshot a table that isn't in spark_catalog, the session catalog. \" +\n+              \"Found source catalog %s\", catalog.name()));\n+    }\n+\n+    if (!(catalog instanceof TableCatalog)) {\n+      throw new IllegalArgumentException(String.format(\n+          \"Cannot snapshot a table from a non-table catalog %s. Catalog has class of %s.\", catalog.name(),\n+          catalog.getClass().toString()\n+      ));\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bf50967bb9ab9344b3c242c274cc9fab58c8b115"}, "originalPosition": 126}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a99c8216da0f2adc74bc8b2498599fd4228bee63", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/a99c8216da0f2adc74bc8b2498599fd4228bee63", "committedDate": "2020-12-10T15:56:21Z", "message": "Change Exception Handling"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fd6ca55717e5f4c74988bf6f51c074978155094e", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/fd6ca55717e5f4c74988bf6f51c074978155094e", "committedDate": "2020-12-10T15:51:35Z", "message": "Change Exception Handling"}, "afterCommit": {"oid": "a99c8216da0f2adc74bc8b2498599fd4228bee63", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/a99c8216da0f2adc74bc8b2498599fd4228bee63", "committedDate": "2020-12-10T15:56:21Z", "message": "Change Exception Handling"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5MzUwMDY5", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-549350069", "createdAt": "2020-12-10T16:02:40Z", "commit": {"oid": "a99c8216da0f2adc74bc8b2498599fd4228bee63"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjowMjo0MFrOIDQqPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjowMjo0MFrOIDQqPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDI4OTU5Ng==", "bodyText": "nit: this on the right side seems redundant", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540289596", "createdAt": "2020-12-10T16:02:40Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String LOCATION = \"location\";\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =\n+      ImmutableList.of(\"path\", \"transient_lastDdlTime\", \"serialization.format\");\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final TableCatalog sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private final Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                     CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) this.sourceCatalog.loadTable(sourceTableIdent);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a99c8216da0f2adc74bc8b2498599fd4228bee63"}, "originalPosition": 85}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c746b86385bae9f8ecfb847bf87612e377672978", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/c746b86385bae9f8ecfb847bf87612e377672978", "committedDate": "2020-12-10T16:16:13Z", "message": "Use StagedSparkTable"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5MzYzNDkw", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-549363490", "createdAt": "2020-12-10T16:16:30Z", "commit": {"oid": "a99c8216da0f2adc74bc8b2498599fd4228bee63"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjoxNjozMFrOIDRUBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjoxNjozMFrOIDRUBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMwMDI5Mw==", "bodyText": "Do we also exclude Iceberg related props? WRITE_NEW_DATA_LOCATION and WRITE_METADATA_LOCATION?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540300293", "createdAt": "2020-12-10T16:16:30Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String LOCATION = \"location\";\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a99c8216da0f2adc74bc8b2498599fd4228bee63"}, "originalPosition": 56}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5MzcwOTgx", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-549370981", "createdAt": "2020-12-10T16:22:39Z", "commit": {"oid": "c746b86385bae9f8ecfb847bf87612e377672978"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjoyMjo0MFrOIDRpXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjoyMjo0MFrOIDRpXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMwNTc1Ng==", "bodyText": "nit: Do we add quotes around names or no? I am fine either way but having it consistent is good.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540305756", "createdAt": "2020-12-10T16:22:40Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String LOCATION = \"location\";\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =\n+      ImmutableList.of(\"path\", \"transient_lastDdlTime\", \"serialization.format\");\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final TableCatalog sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private final Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                     CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) this.sourceCatalog.loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (org.apache.spark.sql.catalyst.analysis.NoSuchTableException e) {\n+      throw new NoSuchTableException(\"Cannot not find source table %s\", sourceTableIdent);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected TableCatalog sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableIdent() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableIdent() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private void validateSourceTable() {\n+    String sourceTableProvider = sourceCatalogTable.provider().get().toLowerCase(Locale.ROOT);\n+    Preconditions.checkArgument(ALLOWED_SOURCES.contains(sourceTableProvider),\n+        \"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider);\n+    Preconditions.checkArgument(!sourceCatalogTable.storage().locationUri().isEmpty(),\n+        \"Cannot create an Iceberg table from a source without an explicit location\");\n+  }\n+\n+  private StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {\n+    Preconditions.checkArgument(catalog instanceof SparkSessionCatalog || catalog instanceof SparkCatalog,\n+        \"Cannot create Iceberg table in non Iceberg Catalog. Catalog %s was of class %s but %s or %s are required\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c746b86385bae9f8ecfb847bf87612e377672978"}, "originalPosition": 151}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5Mzc2MjEw", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-549376210", "createdAt": "2020-12-10T16:26:43Z", "commit": {"oid": "c746b86385bae9f8ecfb847bf87612e377672978"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjoyNjo0M1rOIDR4rQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjoyNjo0M1rOIDR4rQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMwOTY3Nw==", "bodyText": "nit: catalog.getClass() seems to be missing getName", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540309677", "createdAt": "2020-12-10T16:26:43Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+import org.apache.spark.sql.types.StructType;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String LOCATION = \"location\";\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  protected static final List<String> EXCLUDED_PROPERTIES =\n+      ImmutableList.of(\"path\", \"transient_lastDdlTime\", \"serialization.format\");\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final TableCatalog sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private final Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                     CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) this.sourceCatalog.loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (org.apache.spark.sql.catalyst.analysis.NoSuchTableException e) {\n+      throw new NoSuchTableException(\"Cannot not find source table %s\", sourceTableIdent);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected TableCatalog sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableIdent() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableIdent() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private void validateSourceTable() {\n+    String sourceTableProvider = sourceCatalogTable.provider().get().toLowerCase(Locale.ROOT);\n+    Preconditions.checkArgument(ALLOWED_SOURCES.contains(sourceTableProvider),\n+        \"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider);\n+    Preconditions.checkArgument(!sourceCatalogTable.storage().locationUri().isEmpty(),\n+        \"Cannot create an Iceberg table from a source without an explicit location\");\n+  }\n+\n+  private StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {\n+    Preconditions.checkArgument(catalog instanceof SparkSessionCatalog || catalog instanceof SparkCatalog,\n+        \"Cannot create Iceberg table in non Iceberg Catalog. Catalog %s was of class %s but %s or %s are required\",\n+        catalog.name(), catalog.getClass(), SparkSessionCatalog.class.getName(), SparkCatalog.class.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c746b86385bae9f8ecfb847bf87612e377672978"}, "originalPosition": 152}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5MzgxNTAw", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-549381500", "createdAt": "2020-12-10T16:30:53Z", "commit": {"oid": "c746b86385bae9f8ecfb847bf87612e377672978"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjozMDo1M1rOIDSHRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjozMDo1M1rOIDSHRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMxMzQxNA==", "bodyText": "nit: should we define this one closer to the block where it is used? Like next to stagedTable?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540313414", "createdAt": "2020-12-10T16:30:53Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c746b86385bae9f8ecfb847bf87612e377672978"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5Mzg0NTQx", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-549384541", "createdAt": "2020-12-10T16:33:15Z", "commit": {"oid": "c746b86385bae9f8ecfb847bf87612e377672978"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjozMzoxNVrOIDSOMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjozMzoxNVrOIDSOMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMxNTE4Nw==", "bodyText": "nit: missing quotes", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540315187", "createdAt": "2020-12-10T16:33:15Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableIdent().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableIdent().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableIdent(), backupIdentifier);\n+    } catch (org.apache.spark.sql.catalyst.analysis.NoSuchTableException e) {\n+      throw new NoSuchTableException(\"Cannot find table '%s' to migrate\", sourceTableIdent());\n+    } catch (org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException e) {\n+      throw new AlreadyExistsException(\"Cannot rename migration source '%s' to backup name '%s'.\" +\n+          \" Backup table already exists.\", sourceTableIdent(), backupIdentifier);\n+    }\n+\n+    StagedSparkTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      stagedTable = stageDestTable();\n+      icebergTable = stagedTable.table();\n+\n+      ensureNameMappingPresent(icebergTable);\n+\n+      String stagingLocation = getMetadataLocation(icebergTable);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableIdent(), stagingLocation);\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting perform migration changes, aborting table creation and restoring backup.\");\n+\n+        try {\n+          stagedTable.abortStagedChanges();\n+        } catch (Exception abortException) {\n+          LOG.error(\"Cannot abort staged changes\", abortException);\n+        }\n+\n+        try {\n+          destCatalog().renameTable(backupIdentifier, sourceTableIdent());\n+        } catch (org.apache.spark.sql.catalyst.analysis.NoSuchTableException nstException) {\n+          throw new NoSuchTableException(\"Cannot restore backup '%s', the backup cannot be found\", backupIdentifier);\n+        } catch (org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException taeException) {\n+          throw new AlreadyExistsException(\"Cannot restore backup, a table with the original name \" +\n+              \"exists. The backup can be found with the name %s\", backupIdentifier);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c746b86385bae9f8ecfb847bf87612e377672978"}, "originalPosition": 107}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5Mzg3NDI5", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-549387429", "createdAt": "2020-12-10T16:36:18Z", "commit": {"oid": "c746b86385bae9f8ecfb847bf87612e377672978"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjozNjoxOFrOIDSZfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjozNjoxOFrOIDSZfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMxODA3OQ==", "bodyText": "Why use threw and finally instead of catch? Do we also re-throw the original exception?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540318079", "createdAt": "2020-12-10T16:36:18Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableIdent().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableIdent().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableIdent(), backupIdentifier);\n+    } catch (org.apache.spark.sql.catalyst.analysis.NoSuchTableException e) {\n+      throw new NoSuchTableException(\"Cannot find table '%s' to migrate\", sourceTableIdent());\n+    } catch (org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException e) {\n+      throw new AlreadyExistsException(\"Cannot rename migration source '%s' to backup name '%s'.\" +\n+          \" Backup table already exists.\", sourceTableIdent(), backupIdentifier);\n+    }\n+\n+    StagedSparkTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      stagedTable = stageDestTable();\n+      icebergTable = stagedTable.table();\n+\n+      ensureNameMappingPresent(icebergTable);\n+\n+      String stagingLocation = getMetadataLocation(icebergTable);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableIdent(), stagingLocation);\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c746b86385bae9f8ecfb847bf87612e377672978"}, "originalPosition": 92}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5Mzg5NjQ4", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-549389648", "createdAt": "2020-12-10T16:38:33Z", "commit": {"oid": "c746b86385bae9f8ecfb847bf87612e377672978"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjozODozM1rOIDSijw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjozODozM1rOIDSijw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMyMDM5OQ==", "bodyText": "nit: quotes?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540320399", "createdAt": "2020-12-10T16:38:33Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.exceptions.NoSuchTableException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableIdent().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableIdent().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableIdent(), backupIdentifier);\n+    } catch (org.apache.spark.sql.catalyst.analysis.NoSuchTableException e) {\n+      throw new NoSuchTableException(\"Cannot find table '%s' to migrate\", sourceTableIdent());\n+    } catch (org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException e) {\n+      throw new AlreadyExistsException(\"Cannot rename migration source '%s' to backup name '%s'.\" +\n+          \" Backup table already exists.\", sourceTableIdent(), backupIdentifier);\n+    }\n+\n+    StagedSparkTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      stagedTable = stageDestTable();\n+      icebergTable = stagedTable.table();\n+\n+      ensureNameMappingPresent(icebergTable);\n+\n+      String stagingLocation = getMetadataLocation(icebergTable);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableIdent(), stagingLocation);\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting perform migration changes, aborting table creation and restoring backup.\");\n+\n+        try {\n+          stagedTable.abortStagedChanges();\n+        } catch (Exception abortException) {\n+          LOG.error(\"Cannot abort staged changes\", abortException);\n+        }\n+\n+        try {\n+          destCatalog().renameTable(backupIdentifier, sourceTableIdent());\n+        } catch (org.apache.spark.sql.catalyst.analysis.NoSuchTableException nstException) {\n+          throw new NoSuchTableException(\"Cannot restore backup '%s', the backup cannot be found\", backupIdentifier);\n+        } catch (org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException taeException) {\n+          throw new AlreadyExistsException(\"Cannot restore backup, a table with the original name \" +\n+              \"exists. The backup can be found with the name %s\", backupIdentifier);\n+        }\n+      }\n+    }\n+\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    long numMigratedFiles = Long.parseLong(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+    LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    return numMigratedFiles;\n+  }\n+\n+  @Override\n+  protected Map<String, String> targetTableProps() {\n+    Map<String, String> properties = Maps.newHashMap();\n+    properties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+    EXCLUDED_PROPERTIES.forEach(properties::remove);\n+    properties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+    properties.put(\"migrated\", \"true\");\n+    properties.putAll(additionalProperties());\n+    return properties;\n+  }\n+\n+  @Override\n+  protected TableCatalog checkSourceCatalog(CatalogPlugin catalog) {\n+    // Currently the Import code relies on being able to look up the table in the session code\n+    Preconditions.checkArgument(catalog instanceof SparkSessionCatalog,\n+        \"Cannot migrate a table from a non-Iceberg Spark Session Catalog. Found %s of class %s as the source catalog.\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c746b86385bae9f8ecfb847bf87612e377672978"}, "originalPosition": 133}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5MzkyODcw", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-549392870", "createdAt": "2020-12-10T16:41:54Z", "commit": {"oid": "c746b86385bae9f8ecfb847bf87612e377672978"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjo0MTo1NFrOIDSvng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxNjo0MTo1NFrOIDSvng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDMyMzc0Mg==", "bodyText": "+1", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r540323742", "createdAt": "2020-12-10T16:41:54Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.StagedSparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableIdent, CatalogPlugin destCatalog,\n+                       Identifier destTableIdent) {\n+    super(spark, sourceCatalog, sourceTableIdent, destCatalog, destTableIdent);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedSparkTable stagedTable = stageDestTable();\n+    Table icebergTable = stagedTable.table();\n+    // TODO Check table location here against source location\n+\n+    ensureNameMappingPresent(icebergTable);\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = getMetadataLocation(icebergTable);\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableIdent(), destTableIdent(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting to commit snapshot changes, rolling back\");\n+\n+        try {\n+          stagedTable.abortStagedChanges();\n+        } catch (Exception abortException) {\n+          LOG.error(\"Cannot abort staged changes\", abortException);\n+        }\n+      }\n+    }\n+\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    long numMigratedFiles = Long.parseLong(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+    LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    return numMigratedFiles;\n+  }\n+\n+  @Override\n+  protected Map<String, String> targetTableProps() {\n+    Map<String, String> properties = Maps.newHashMap();\n+\n+    // Remove any possible location properties from origin properties\n+    properties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c746b86385bae9f8ecfb847bf87612e377672978"}, "originalPosition": 97}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NDQ0NDk0", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-549444494", "createdAt": "2020-12-10T17:36:39Z", "commit": {"oid": "c746b86385bae9f8ecfb847bf87612e377672978"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a4bbc7af123e2f003941f3b3fb6aab3fc39821b3", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/a4bbc7af123e2f003941f3b3fb6aab3fc39821b3", "committedDate": "2020-12-10T18:45:23Z", "message": "Final review nits"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ5NTI5MTQ0", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-549529144", "createdAt": "2020-12-10T19:24:55Z", "commit": {"oid": "a4bbc7af123e2f003941f3b3fb6aab3fc39821b3"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ff541764d283b4990e3ccc8201d63ef7f73eb500", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/ff541764d283b4990e3ccc8201d63ef7f73eb500", "committedDate": "2020-09-28T21:22:23Z", "message": "Provide API and Implementation for Creating Iceberg Tables from Spark\n\nPreviously the SparkUtil class provided an importSparkTable command but\nthis command suffered from a few shortcomings. It had a difficult api and\ndid not work directly with DSV2 Iceberg Catalogs. To provide a simpler interface\nthat shares a similar pattern to currently available Spark Actions we introduce\nCreateActions and MigrateAction.\n\nThrough this command we can both migrate a Table as well as make a snapshot\nof an existing table.\n\nGeneral usage:\n\n  CreateActions\n    .createIcebergTable(spark, dest)\n    .fromSourceTable(source)\n    .withNewTableLocation(location)\n    .execute()"}, "afterCommit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/a23fa89b2d5a2be61af72836656e4bf3322c9c39", "committedDate": "2020-09-28T22:11:42Z", "message": "Provide API and Implementation for Creating Iceberg Tables from Spark\n\nPreviously the SparkUtil class provided an importSparkTable command but\nthis command suffered from a few shortcomings. It had a difficult api and\ndid not work directly with DSV2 Iceberg Catalogs. To provide a simpler interface\nthat shares a similar pattern to currently available Spark Actions we introduce\nCreateActions and MigrateAction.\n\nThrough this command we can both migrate a Table as well as make a snapshot\nof an existing table.\n\nGeneral usage:\n\n  CreateActions\n    .createIcebergTable(spark, dest)\n    .fromSourceTable(source)\n    .withNewTableLocation(location)\n    .execute()"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4MDM4NjUw", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-498038650", "createdAt": "2020-09-29T03:27:42Z", "commit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwMzoyNzo0MlrOHZXMSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwNDowMjozMFrOHZXsxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM1NjQyNQ==", "bodyText": "EDIT: I goofed. We're creating iceberg tables from Spark tables. So my 1st suggestion should be ignored entirely. Of course spark tables won't have hidden partition specs as that's an iceberg concept and we're using this to convert tables from spark to iceberg \ud83e\udd26 . Is it possible to re-use a spark table's bucket based partitioning? I've never personally used it due to the small files problem it can generate, but are our hash functions for bucketing so different from the Spark bucketing (or some other issue) that we can't make it work - like, in a follow up PR?\n--- Original ---\nMy understanding is that we're only searching for non-hidden partitions here because that's all that's possible to create from a table derived from catalyst's TableIdentifier which has no knowledge of hidden partitions and uses Hive style partitioning instead. That being said, I do still think that the JavaDoc could use an update emphasizing this. How about something like this...\n/**\n * Given a Spark table identifier, determine the PartitionSpec, which will be either\n * an identity or unpartitioned PartitionSpec based on the original table's hive-style\n * partition columns.\n */\n\nI'd love to hear other suggestions, but to me the JavaDoc seems ... somehow missing something. But I'm not quite sure what it is. \ud83e\udd14", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r496356425", "createdAt": "2020-09-29T03:27:42Z", "author": {"login": "kbendick"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkSchemaUtil.java", "diffHunk": "@@ -63,6 +65,21 @@ public static Schema schemaForTable(SparkSession spark, String name) {\n     return new Schema(converted.asNestedType().asStructType().fields());\n   }\n \n+  /**\n+   * Given a Spark table identifier, determine the PartitionSpec.\n+   * @param spark the SparkSession which contains the identifier\n+   * @param table a TableIdentifier, if the namespace is left blank the catalog().currentDatabase() will be used\n+   * @return a IcebergPartitionSpec representing the partitioning of the Spark table\n+   * @throws AnalysisException if thrown by the Spark catalog\n+   */\n+  public static PartitionSpec specForTable(SparkSession spark, TableIdentifier table) throws AnalysisException {\n+    String db = table.database().nonEmpty() ? table.database().get() : spark.catalog().currentDatabase();\n+    PartitionSpec spec = identitySpec(\n+        schemaForTable(spark, table.unquotedString()),\n+        spark.catalog().listColumns(db, table.table()).collectAsList());\n+    return spec == null ? PartitionSpec.unpartitioned() : spec;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM2NDc0MA==", "bodyText": "I think that you can get the table DDL via a utility function in catalyst, and then there's a function that returns a schema from DDL. However, if this is returning an Iceberg schema then do please ignore me :)", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r496364740", "createdAt": "2020-09-29T04:02:30Z", "author": {"login": "kbendick"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkSchemaUtil.java", "diffHunk": "@@ -63,6 +65,21 @@ public static Schema schemaForTable(SparkSession spark, String name) {\n     return new Schema(converted.asNestedType().asStructType().fields());\n   }\n \n+  /**\n+   * Given a Spark table identifier, determine the PartitionSpec.\n+   * @param spark the SparkSession which contains the identifier\n+   * @param table a TableIdentifier, if the namespace is left blank the catalog().currentDatabase() will be used\n+   * @return a IcebergPartitionSpec representing the partitioning of the Spark table\n+   * @throws AnalysisException if thrown by the Spark catalog\n+   */\n+  public static PartitionSpec specForTable(SparkSession spark, TableIdentifier table) throws AnalysisException {\n+    String db = table.database().nonEmpty() ? table.database().get() : spark.catalog().currentDatabase();\n+    PartitionSpec spec = identitySpec(\n+        schemaForTable(spark, table.unquotedString()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "originalPosition": 26}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4MDg1NTcx", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-498085571", "createdAt": "2020-09-29T04:23:40Z", "commit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwNDoyMzo0MFrOHZYawA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQwNDoyMzo0MFrOHZYawA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjM3NjUxMg==", "bodyText": "Can we create a separate PR for this and merge it in sooner? I imagine this PR will take longer to merge because of its large scope, but this seems useful now.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r496376512", "createdAt": "2020-09-29T04:23:40Z", "author": {"login": "kbendick"}, "path": "spark/src/test/java/org/apache/iceberg/spark/SparkTestBase.java", "diffHunk": "@@ -63,7 +63,9 @@ public static void startMetastoreAndSpark() {\n \n   @AfterClass\n   public static void stopMetastoreAndSpark() {\n-    catalog.close();\n+    if (catalog != null) {\n+      catalog.close();\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "originalPosition": 7}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAzNDQ3Njg5", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-503447689", "createdAt": "2020-10-07T00:28:06Z", "commit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDoyODowNlrOHdecVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDoyODowNlrOHdecVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY2OTUyNA==", "bodyText": "I don't think that we should leak the v1 Spark API (TableIdentifier) in a util class like this. What about passing database and name separately? Then the caller is responsible for adding database, which avoids the need to use v1 spark.catalog() to default it.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500669524", "createdAt": "2020-10-07T00:28:06Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkSchemaUtil.java", "diffHunk": "@@ -63,6 +65,21 @@ public static Schema schemaForTable(SparkSession spark, String name) {\n     return new Schema(converted.asNestedType().asStructType().fields());\n   }\n \n+  /**\n+   * Given a Spark table identifier, determine the PartitionSpec.\n+   * @param spark the SparkSession which contains the identifier\n+   * @param table a TableIdentifier, if the namespace is left blank the catalog().currentDatabase() will be used\n+   * @return a IcebergPartitionSpec representing the partitioning of the Spark table\n+   * @throws AnalysisException if thrown by the Spark catalog\n+   */\n+  public static PartitionSpec specForTable(SparkSession spark, TableIdentifier table) throws AnalysisException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "originalPosition": 23}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAzNDQ4NzYz", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-503448763", "createdAt": "2020-10-07T00:31:47Z", "commit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDozMTo0N1rOHdegVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDozMTo0N1rOHdegVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3MDU1MA==", "bodyText": "I don't think that spark.catalog().currentDatabase() is correct. I thought that spark.catalog() always returns the built-in v1 catalog.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500670550", "createdAt": "2020-10-07T00:31:47Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/CreateActions.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+\n+public class CreateActions {\n+  private final Identifier newTableName;\n+  private final SparkSession spark;\n+\n+  private CreateActions(SparkSession spark, Identifier newTableName) {\n+    this.spark = spark;\n+    this.newTableName = newTableName;\n+  }\n+\n+  /**\n+   * Create a new Iceberg table in the Iceberg SparkSession catalog based on an\n+   * existing non-iceberg table.\n+   * @param spark the session to use for interacting with Spark\n+   * @param newTableName the string representation of the multipart identifier for new table's name\n+   */\n+  public static CreateActions createIcebergTable(SparkSession spark, String newTableName) {\n+    Identifier identifier = Spark3Util.parseIdentifier(spark, newTableName);\n+    if (identifier.namespace() == null || identifier.namespace().length == 0) {\n+      identifier = Identifier.of(new String[] {spark.catalog().currentDatabase()}, identifier.name());\n+    }\n+    return createIcebergTable(spark, identifier);\n+  }\n+\n+  /**\n+   * Create a new Iceberg table in the Iceberg SparkSession catalog based on an\n+   * existing non-iceberg table.\n+   * @param spark the session to use for interacting with Spark\n+   * @param newTableName the Spark catalog Identifier for new table's name\n+   */\n+  public static CreateActions createIcebergTable(SparkSession spark, Identifier newTableName) {\n+    return new CreateActions(spark, newTableName);\n+  }\n+\n+  /**\n+   * Existing table to be used for creating the Iceberg Table\n+   * @param existingTable the string representation of an existing table\n+   * @return a Spark Action to perform the migration\n+   */\n+  public MigrateAction fromSourceTable(String existingTable) {\n+    Identifier identifier = Spark3Util.parseIdentifier(spark, existingTable);\n+    if (identifier.namespace() == null || identifier.namespace().length == 0) {\n+      identifier = Identifier.of(new String[] {spark.catalog().currentDatabase()}, identifier.name());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "originalPosition": 66}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAzNDQ5NTIw", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-503449520", "createdAt": "2020-10-07T00:34:21Z", "commit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDozNDoyMlrOHdei_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDozNDoyMlrOHdei_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3MTIyOQ==", "bodyText": "Is it possible to use the default V2SessionCatalog and Iceberg's non-session catalog?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500671229", "createdAt": "2020-10-07T00:34:22Z", "author": {"login": "rdblue"}, "path": "spark3/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.CreateActions;\n+import org.apache.iceberg.spark.MigrateAction;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+\n+  // Only valid for IcebergV2Catalog\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAzNDQ5NzAx", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-503449701", "createdAt": "2020-10-07T00:34:57Z", "commit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDozNDo1N1rOHdejrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDozNDo1N1rOHdejrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3MTQwNA==", "bodyText": "This doesn't look qualified?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500671404", "createdAt": "2020-10-07T00:34:57Z", "author": {"login": "rdblue"}, "path": "spark3/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.CreateActions;\n+import org.apache.iceberg.spark.MigrateAction;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+\n+  // Only valid for IcebergV2Catalog\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )},\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hadoop\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )}\n+    };\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  String qualifiedTableName = \"baseTable\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "originalPosition": 73}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAzNDUyMTI1", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-503452125", "createdAt": "2020-10-07T00:43:20Z", "commit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDo0MzoyMFrOHdesEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDo0MzoyMFrOHdesEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3MzU1Mw==", "bodyText": "I think this is referred to as \"snapshot\" in the tests, right? (From the SNAPSHOT TABLE command?)\nAnd the other one is a \"migrate\"?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500673553", "createdAt": "2020-10-07T00:43:20Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/MigrateAction.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.actions.Action;\n+import org.apache.iceberg.actions.ExpireSnapshotsAction;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "originalPosition": 61}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAzNDUyNDY4", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-503452468", "createdAt": "2020-10-07T00:44:26Z", "commit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDo0NDoyNlrOHdetKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDo0NDoyNlrOHdetKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3MzgzNA==", "bodyText": "We normally put constructors before public methods. Minor, but this was harder to find than needed.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500673834", "createdAt": "2020-10-07T00:44:26Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/MigrateAction.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.actions.Action;\n+import org.apache.iceberg.actions.ExpireSnapshotsAction;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table with the same name\n+ *   This pathway will create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+public class MigrateAction implements Action<Long> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final SparkSessionCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  /**\n+   * Creates an Iceberg Location at a given location instead of using the location\n+   * provided by the source table. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * Use this if you would like to experiment with Iceberg without changing\n+   * your original files.\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  public MigrateAction withNewTableLocation(String newLocation) {\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  public MigrateAction withAdditionalProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties\n+   * with the same key name will be overwritten.\n+   * @param key the key of the property to add\n+   * @param value the value of the property to add\n+   */\n+  public MigrateAction withAdditionalProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  public MigrateAction(SparkSession spark, Identifier destTableName, Identifier sourceTableName) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "originalPosition": 131}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAzNDU0Mjk1", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-503454295", "createdAt": "2020-10-07T00:50:47Z", "commit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDo1MDo0N1rOHdezpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDo1MDo0N1rOHdezpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3NTQ5NQ==", "bodyText": "I'm finding it hard to follow when this would migrate a table vs snapshot a table, and it also requires a separate class.\nDid you consider an API using verbs directly? I'm imagining something like this:\nActions.migrate(\"db.table\").execute();\nActions.snapshot(\"db.table\").as(\"db.table_iceberg\").execute();\n\n// maybe even create like?\nActions.createTable(\"db.table\").like(\"db.table_hive\").execute()\nWe could add methods as well, but those verbs correspond to the SQL that we expose and we haven't needed anything more complicated than that.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500675495", "createdAt": "2020-10-07T00:50:47Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/CreateActions.java", "diffHunk": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+\n+public class CreateActions {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAzNDU1Mjg5", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-503455289", "createdAt": "2020-10-07T00:54:17Z", "commit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDo1NDoxN1rOHde3Dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDo1NDoxN1rOHde3Dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3NjM2Ng==", "bodyText": "Does this mean that only Spark tables are supported and not Hive tables? I don't think that Hive tables have providers.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500676366", "createdAt": "2020-10-07T00:54:17Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/MigrateAction.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.actions.Action;\n+import org.apache.iceberg.actions.ExpireSnapshotsAction;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table with the same name\n+ *   This pathway will create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+public class MigrateAction implements Action<Long> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final SparkSessionCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  /**\n+   * Creates an Iceberg Location at a given location instead of using the location\n+   * provided by the source table. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * Use this if you would like to experiment with Iceberg without changing\n+   * your original files.\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  public MigrateAction withNewTableLocation(String newLocation) {\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  public MigrateAction withAdditionalProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties\n+   * with the same key name will be overwritten.\n+   * @param key the key of the property to add\n+   * @param value the value of the property to add\n+   */\n+  public MigrateAction withAdditionalProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  public MigrateAction(SparkSession spark, Identifier destTableName, Identifier sourceTableName) {\n+    this.spark = spark;\n+    this.sourceTableName = sourceTableName;\n+    this.destTableName = destTableName;\n+    this.destCatalog = getSparkSessionCatalogOrFail(spark, destTableName);\n+\n+    try {\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable =\n+        spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable, ALLOWED_SOURCES);\n+\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+    this.sessionCatalogReplacement = !sourceTableProvider.equals(\"iceberg\") && sourceTableName.equals(destTableName);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable, Set<String> supportedSourceTableProviders) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "originalPosition": 160}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAzNDU2MDIw", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-503456020", "createdAt": "2020-10-07T00:56:58Z", "commit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDo1Njo1OFrOHde5vA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDo1Njo1OFrOHde5vA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3NzA1Mg==", "bodyText": "Style nit: Both \"Spark\" and \"OrFail\" are implied, so I think they are just making this method name longer. It could be getSessionCatalog.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500677052", "createdAt": "2020-10-07T00:56:58Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/MigrateAction.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.actions.Action;\n+import org.apache.iceberg.actions.ExpireSnapshotsAction;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table with the same name\n+ *   This pathway will create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+public class MigrateAction implements Action<Long> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final SparkSessionCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  /**\n+   * Creates an Iceberg Location at a given location instead of using the location\n+   * provided by the source table. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * Use this if you would like to experiment with Iceberg without changing\n+   * your original files.\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  public MigrateAction withNewTableLocation(String newLocation) {\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  public MigrateAction withAdditionalProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties\n+   * with the same key name will be overwritten.\n+   * @param key the key of the property to add\n+   * @param value the value of the property to add\n+   */\n+  public MigrateAction withAdditionalProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  public MigrateAction(SparkSession spark, Identifier destTableName, Identifier sourceTableName) {\n+    this.spark = spark;\n+    this.sourceTableName = sourceTableName;\n+    this.destTableName = destTableName;\n+    this.destCatalog = getSparkSessionCatalogOrFail(spark, destTableName);\n+\n+    try {\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable =\n+        spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable, ALLOWED_SOURCES);\n+\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+    this.sessionCatalogReplacement = !sourceTableProvider.equals(\"iceberg\") && sourceTableName.equals(destTableName);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable, Set<String> supportedSourceTableProviders) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!supportedSourceTableProviders.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  private static Map<String, String> extraIcebergTableProps(String tableLocation, String metadataLocation) {\n+    return ImmutableMap.of(\n+        TableProperties.WRITE_METADATA_LOCATION, metadataLocation,\n+        TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation,\n+        \"migrated\", \"true\");\n+  }\n+\n+  private static SparkSessionCatalog getSparkSessionCatalogOrFail(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "originalPosition": 178}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTAzNDU2NDUy", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-503456452", "createdAt": "2020-10-07T00:58:34Z", "commit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDo1ODozNFrOHde7Ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wN1QwMDo1ODozNFrOHde7Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMDY3NzQzMA==", "bodyText": "I think everything after the staged table is created should be in the try block so that any exception thrown will roll back any changes that were made.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r500677430", "createdAt": "2020-10-07T00:58:34Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/MigrateAction.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.actions.Action;\n+import org.apache.iceberg.actions.ExpireSnapshotsAction;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogManager;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table with the same name\n+ *   This pathway will create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+public class MigrateAction implements Action<Long> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final SparkSessionCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  /**\n+   * Creates an Iceberg Location at a given location instead of using the location\n+   * provided by the source table. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * Use this if you would like to experiment with Iceberg without changing\n+   * your original files.\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  public MigrateAction withNewTableLocation(String newLocation) {\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  public MigrateAction withAdditionalProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties\n+   * with the same key name will be overwritten.\n+   * @param key the key of the property to add\n+   * @param value the value of the property to add\n+   */\n+  public MigrateAction withAdditionalProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  public MigrateAction(SparkSession spark, Identifier destTableName, Identifier sourceTableName) {\n+    this.spark = spark;\n+    this.sourceTableName = sourceTableName;\n+    this.destTableName = destTableName;\n+    this.destCatalog = getSparkSessionCatalogOrFail(spark, destTableName);\n+\n+    try {\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable =\n+        spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable, ALLOWED_SOURCES);\n+\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+    this.sessionCatalogReplacement = !sourceTableProvider.equals(\"iceberg\") && sourceTableName.equals(destTableName);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable, Set<String> supportedSourceTableProviders) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!supportedSourceTableProviders.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  private static Map<String, String> extraIcebergTableProps(String tableLocation, String metadataLocation) {\n+    return ImmutableMap.of(\n+        TableProperties.WRITE_METADATA_LOCATION, metadataLocation,\n+        TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation,\n+        \"migrated\", \"true\");\n+  }\n+\n+  private static SparkSessionCatalog getSparkSessionCatalogOrFail(\n+      SparkSession spark,\n+      Identifier destTableName) {\n+    CatalogPlugin sessionCat;\n+    String[] destNamespace = destTableName.namespace();\n+    if (destNamespace != null && destNamespace.length > 0 &&\n+        spark.sessionState().catalogManager().isCatalogRegistered(destNamespace[0])) {\n+      sessionCat = spark.sessionState().catalogManager().catalog(destTableName.namespace()[0]);\n+    } else {\n+      sessionCat = spark.sessionState().catalogManager().catalog(CatalogManager.SESSION_CATALOG_NAME());\n+    }\n+    if (!(sessionCat instanceof SparkSessionCatalog)) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non Iceberg Catalog. Catalog %s was of class\" +\n+          \" %s but %s is required\", sessionCat.name(), sessionCat.getClass(), SparkSessionCatalog.class.getName()));\n+    }\n+    return (SparkSessionCatalog) sessionCat;\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+        .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+        .putAll(JavaConverters.mapAsJavaMapConverter(sourceTable.properties()).asJava())\n+        .putAll(extraIcebergTableProps(destDataLocation, destMetadataLocation))\n+        .putAll(additionalProperties)\n+        .build();\n+\n+    StagedTable stagedTable;\n+    try {\n+      if (sessionCatalogReplacement) {\n+        /*\n+         * Spark Session Catalog cannot stage a replacement of a Session table with an Iceberg Table.\n+         * To workaround this we create a replcamement table which is renamed after it\n+         * is successfully constructed.\n+         */\n+        stagedTable = destCatalog.stageCreateOrReplace(Identifier.of(\n+            destTableName.namespace(),\n+            destTableName.name() + REPLACEMENT_NAME),\n+            sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      } else {\n+        stagedTable = destCatalog.stageCreateOrReplace(destTableName, sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      }\n+    } catch (NoSuchNamespaceException e) {\n+      throw new IllegalArgumentException(\"Cannot create a new table in a namespace which does not exist\", e);\n+    }\n+\n+    String stagingLocation = destMetadataLocation;\n+    Table icebergTable = ((SparkTable) stagedTable).table();\n+\n+    LOG.info(\"Beginning migration of {} to {}\", sourceTableName, destTableName);\n+    SparkTableUtil.importSparkTable(spark, Spark3Util.toTableIdentifier(sourceTableName), icebergTable,\n+        stagingLocation);\n+\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    long numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+\n+    try {\n+      stagedTable.commitStagedChanges();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23fa89b2d5a2be61af72836656e4bf3322c9c39"}, "originalPosition": 237}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7df39052feec72c6cc251ade373c30dbb819c8f5", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/7df39052feec72c6cc251ade373c30dbb819c8f5", "committedDate": "2020-10-13T19:27:18Z", "message": "Address Reviewer Comments\n\nRedo api\n\nLock public api down to\nAction.migrate\nAction.snapshot\n\nPublic api on the Create Action limited to\n\"As\" changing the destination name\n\"with property/properties\" adding table properties"}, "afterCommit": {"oid": "85112ca46e2439b586b5970eb2f4bf09079eec24", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/85112ca46e2439b586b5970eb2f4bf09079eec24", "committedDate": "2020-10-13T22:31:12Z", "message": "Fix tests\n\nHadoop Catalog may be fundamentally incompatible with our actions here\nsince it requires for both migrate and snapshot since it requires a specific\ndirectory structure matching with the identifier. It's also difficult to\nexplicitly detect when a Hadoop catalog is in use because that information\nis private in both of our Catalog Implementations."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3ODc3MTM3", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-507877137", "createdAt": "2020-10-13T22:38:33Z", "commit": {"oid": "85112ca46e2439b586b5970eb2f4bf09079eec24"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMjozODozM1rOHg77lQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QyMjozODozM1rOHg77lQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDI5ODM4OQ==", "bodyText": "Base class for Actions that are common to both Spark 2 and 3", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r504298389", "createdAt": "2020-10-13T22:38:33Z", "author": {"login": "RussellSpitzer"}, "path": "spark/src/main/java/org/apache/iceberg/actions/CommonActions.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import org.apache.iceberg.Table;\n+import org.apache.spark.sql.SparkSession;\n+\n+public abstract class CommonActions {\n+\n+  private SparkSession spark;\n+  private Table table;\n+\n+  protected CommonActions(SparkSession spark, Table table) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "85112ca46e2439b586b5970eb2f4bf09079eec24"}, "originalPosition": 25}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "85112ca46e2439b586b5970eb2f4bf09079eec24", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/85112ca46e2439b586b5970eb2f4bf09079eec24", "committedDate": "2020-10-13T22:31:12Z", "message": "Fix tests\n\nHadoop Catalog may be fundamentally incompatible with our actions here\nsince it requires for both migrate and snapshot since it requires a specific\ndirectory structure matching with the identifier. It's also difficult to\nexplicitly detect when a Hadoop catalog is in use because that information\nis private in both of our Catalog Implementations."}, "afterCommit": {"oid": "d250de2c822bb43f16ff3c133fffe2e43c85d8d2", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/d250de2c822bb43f16ff3c133fffe2e43c85d8d2", "committedDate": "2020-10-13T22:39:59Z", "message": "Fix tests\n\nHadoop Catalog may be fundamentally incompatible with our actions here\nsince it requires for both migrate and snapshot since it requires a specific\ndirectory structure matching with the identifier. It's also difficult to\nexplicitly detect when a Hadoop catalog is in use because that information\nis private in both of our Catalog Implementations."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "03c54e4f7dbcd7d0b7cab3ba57c0976dd8a3c944", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/03c54e4f7dbcd7d0b7cab3ba57c0976dd8a3c944", "committedDate": "2020-10-13T23:39:16Z", "message": "Add overrides for IcebergSourceHadoop"}, "afterCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8", "committedDate": "2020-10-23T17:46:41Z", "message": "Implement new Actions on reflection based Actions"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc1NDc2", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521275476", "createdAt": "2020-11-01T23:46:18Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMzo0NjoxOFrOHrzMqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMzo0NjoxOFrOHrzMqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY4OTY0MQ==", "bodyText": "Typo: Should be \"migrate\" not \"snapshot\".", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515689641", "createdAt": "2020-11-01T23:46:18Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/Actions.java", "diffHunk": "@@ -77,6 +82,81 @@ public ExpireSnapshotsAction expireSnapshots() {\n     return new ExpireSnapshotsAction(spark, table);\n   }\n \n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @return Action to perform migration\n+   */\n+  public static CreateAction migrate(String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), String.class).buildStaticChecked()\n+          .invoke(tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc1NzY2", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521275766", "createdAt": "2020-11-01T23:49:37Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMzo0OTozN1rOHrzN5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMzo0OTozN1rOHrzN5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY4OTk1Nw==", "bodyText": "The doc here is really specific to Spark, but I don't think there is a need for it. How about something like \"The table will no longer be accessible using the previous implementation\"?\nAlso, new data will be written to the data directory to avoid breaking any copies of the table. When we migrate, we leave a db.table_hive copy that can be renamed back in place to roll back the operation. The user is responsible for dropping the _hive copy or doing renames to rollback. Not sure how much of that applies to this PR though.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515689957", "createdAt": "2020-11-01T23:49:37Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/Actions.java", "diffHunk": "@@ -77,6 +82,81 @@ public ExpireSnapshotsAction expireSnapshots() {\n     return new ExpireSnapshotsAction(spark, table);\n   }\n \n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 29}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc1ODYw", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521275860", "createdAt": "2020-11-01T23:50:28Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMzo1MDoyOFrOHrzOWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMzo1MDoyOFrOHrzOWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MDA3Mg==", "bodyText": "Same here, should be \"migrate\".", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515690072", "createdAt": "2020-11-01T23:50:28Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/Actions.java", "diffHunk": "@@ -77,6 +82,81 @@ public ExpireSnapshotsAction expireSnapshots() {\n     return new ExpireSnapshotsAction(spark, table);\n   }\n \n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @return Action to perform migration\n+   */\n+  public static CreateAction migrate(String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), String.class).buildStaticChecked()\n+          .invoke(tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @param spark     Spark session to use for looking up table\n+   * @return Action to perform migration\n+   */\n+  public static CreateAction migrate(SparkSession spark, String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), SparkSession.class, String.class).buildStaticChecked()\n+          .invoke(spark, tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 58}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc2MTYz", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521276163", "createdAt": "2020-11-01T23:54:05Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMzo1NDowNVrOHrzPrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMzo1NDowNVrOHrzPrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MDQxMg==", "bodyText": "The wording in this Javadoc and the argument names seem a little confusing to me. I think it should be \"Creates a new Iceberg table that is a snapshot of the given source table\", then \"The new table can be altered, . . .\". Referring to the table that gets created as a table should help users understand what is happening a bit better than referring to it as a \"snapshot\", which has a conflicting meaning when used as a noun. Here, I think we want to stick to using it as a verb.\nSince the \"source\" and \"dest\" are tables, I'd use \"sourceName\" or \"sourceTable\" and \"destName\" or \"destTable\".", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515690412", "createdAt": "2020-11-01T23:54:05Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/Actions.java", "diffHunk": "@@ -77,6 +82,81 @@ public ExpireSnapshotsAction expireSnapshots() {\n     return new ExpireSnapshotsAction(spark, table);\n   }\n \n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @return Action to perform migration\n+   */\n+  public static CreateAction migrate(String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), String.class).buildStaticChecked()\n+          .invoke(tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @param spark     Spark session to use for looking up table\n+   * @return Action to perform migration\n+   */\n+  public static CreateAction migrate(SparkSession spark, String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), SparkSession.class, String.class).buildStaticChecked()\n+          .invoke(spark, tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Creates an independent Iceberg snapshot of a given table. The Snapshot can be altered, appended or deleted without\n+   * causing any change to the original table the snapshot is based on. New data and metadata will be created in the\n+   * location passed to this method.\n+   *\n+   * @param sourceId Snapshot's source data\n+   * @param destId   Name of the new snapshot", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 68}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc2MjE3", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521276217", "createdAt": "2020-11-01T23:54:35Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMzo1NDozNVrOHrzP9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMzo1NDozNVrOHrzP9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MDQ4Nw==", "bodyText": "It may be helpful to link to the CreateAction API rather than just calling out that it is an Action.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515690487", "createdAt": "2020-11-01T23:54:35Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/Actions.java", "diffHunk": "@@ -77,6 +82,81 @@ public ExpireSnapshotsAction expireSnapshots() {\n     return new ExpireSnapshotsAction(spark, table);\n   }\n \n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @return Action to perform migration\n+   */\n+  public static CreateAction migrate(String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), String.class).buildStaticChecked()\n+          .invoke(tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @param spark     Spark session to use for looking up table\n+   * @return Action to perform migration\n+   */\n+  public static CreateAction migrate(SparkSession spark, String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), SparkSession.class, String.class).buildStaticChecked()\n+          .invoke(spark, tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Creates an independent Iceberg snapshot of a given table. The Snapshot can be altered, appended or deleted without\n+   * causing any change to the original table the snapshot is based on. New data and metadata will be created in the\n+   * location passed to this method.\n+   *\n+   * @param sourceId Snapshot's source data\n+   * @param destId   Name of the new snapshot\n+   * @param location Location for metadata and new data for the Snapshot\n+   * @return Action to perform snapshot", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 70}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc2NDgx", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521276481", "createdAt": "2020-11-01T23:57:46Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMzo1Nzo0NlrOHrzRTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMzo1Nzo0NlrOHrzRTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MDgzMA==", "bodyText": "Actions.migrate returns a CreateAction, but as seems to change the operation from an in-place migration to something unclear. Is the original table retained if the name doesn't conflict?\nSimilarly, I don't think there is a need for this with Actions.snapshot because snapshot accepts a destination table name. When I suggested as, my intent was to use it as a way to pass the destination table name for snapshot. But if you can't create a SnapshotAction without a table name we don't need this.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515690830", "createdAt": "2020-11-01T23:57:46Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  CreateAction withAdditionalProperties(Map<String, String> properties);\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties\n+   * with the same key name will be overwritten.\n+   * @param key the key of the property to add\n+   * @param value the value of the property to add\n+   * @return this for chaining\n+   */\n+  CreateAction withAdditionalProperty(String key, String value);\n+\n+  /**\n+   * Changes the name of the new Iceberg table we are making to the name passed\n+   * to this method.\n+   * @param newName The new name of the table to be made\n+   * @return this for chaining\n+   */\n+  CreateAction as(String newName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 49}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc2NTUy", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521276552", "createdAt": "2020-11-01T23:58:46Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMzo1ODo0NlrOHrzRqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMVQyMzo1ODo0NlrOHrzRqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MDkyMQ==", "bodyText": "Why not set and setAll? Is \"additional\" more clear?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515690921", "createdAt": "2020-11-01T23:58:46Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  CreateAction withAdditionalProperties(Map<String, String> properties);\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties\n+   * with the same key name will be overwritten.\n+   * @param key the key of the property to add\n+   * @param value the value of the property to add\n+   * @return this for chaining\n+   */\n+  CreateAction withAdditionalProperty(String key, String value);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc2ODgx", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521276881", "createdAt": "2020-11-02T00:02:02Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDowMjowM1rOHrzTNw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDowMjowM1rOHrzTNw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTMxOQ==", "bodyText": "Can we avoid Scala in this API to avoid breaking changes? I think converting to List immediately and then calling this method is going to be better.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515691319", "createdAt": "2020-11-02T00:02:03Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java", "diffHunk": "@@ -543,4 +550,63 @@ private static String sqlString(org.apache.iceberg.expressions.Literal<?> lit) {\n       }\n     }\n   }\n+\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, String name) {\n+    try {\n+      return catalogAndIdentifier(spark, spark.sessionState().sqlParser().parseMultipartIdentifier(name));\n+    } catch (ParseException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot parse identifier %s\", name), e);\n+    }\n+  }\n+\n+  /**\n+   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply\n+   * Attempts to find the catalog and identifier a multipart identifier represents\n+   * @param spark Spark session to use for resolution\n+   * @param nameParts Multipart identifier representing a table\n+   * @return The CatalogPlugin and Identifier for the table\n+   */\n+  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, Seq<String> nameParts) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc3MTA4", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521277108", "createdAt": "2020-11-02T00:04:25Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDowNDoyNVrOHrzULg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDowNDoyNVrOHrzULg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTU2Ng==", "bodyText": "Why is this needed?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515691566", "createdAt": "2020-11-02T00:04:25Z", "author": {"login": "rdblue"}, "path": "spark3/src/test/java/org/apache/iceberg/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+\n+  // Only valid for IcebergV2Catalog - Hadoop Catalog does not support Staged Tables\n+  @Parameterized.Parameters(name = \"Catalog Name {0} - Options {2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )},\n+        new Object[] { \"testhive\", SparkCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\"\n+        )}\n+    };\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  String baseTableName = \"baseTable\";\n+  File tableDir;\n+  String tableLocation;\n+  final String implementation;\n+  final String type;\n+  final TableCatalog catalog;\n+\n+  public TestCreateActions(\n+      String catalogName,\n+      String implementation,\n+      Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+    this.catalog = (TableCatalog) spark.sessionState().catalogManager().catalog(catalogName);\n+    this.implementation = implementation;\n+    this.type = config.get(\"type\");\n+  }\n+\n+  @Before\n+  public void before() {\n+    try {\n+      this.tableDir = temp.newFolder();\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+    this.tableLocation = tableDir.toURI().toString();\n+\n+    spark.conf().set(\"hive.exec.dynamic.partition\", \"true\");\n+    spark.conf().set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+\n+\n+    List<SimpleRecord> expected = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\"),\n+        new SimpleRecord(3, \"c\")\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, SimpleRecord.class);\n+\n+    df.select(\"id\", \"data\").orderBy(\"data\").write()\n+        .mode(\"append\")\n+        .option(\"path\", tableLocation)\n+        .saveAsTable(baseTableName);\n+  }\n+\n+  @After\n+  public void after() throws IOException {\n+    // Drop the hive table.\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+  }\n+\n+\n+  @Test\n+  public void testMigratePartitioned() throws Exception {\n+    String dest = uniqueName(\"iceberg_migrate_partitioned\");\n+    String source = uniqueName(\"test_migrate_partitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet PARTITIONED BY (id) location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.migrate(source).as(dest),\n+        3);\n+  }\n+\n+  @Test\n+  public void testMigrateUnpartitioned() throws Exception {\n+    String dest = uniqueName(\"iceberg_migrate_unpartitioned\");\n+    String source = uniqueName(\"test_migrate_unpartitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.migrate(source).as(dest),\n+        2);\n+  }\n+\n+  @Test\n+  public void testMigrateReplace() throws Exception {\n+    // We can't do a replacement unless we have overridden the spark_catalog\n+    if (catalog.name().equals(\"spark_catalog\")) {\n+      String source = uniqueName(uniqueName(\"iceberg_migrate_replace\"));\n+      testCreate(source,\n+          source,\n+          \"CREATE TABLE %s using parquet PARTITIONED BY (id) location '%s' AS SELECT * FROM %s\",\n+          () -> Actions.migrate(source),\n+          3);\n+    }\n+  }\n+\n+  @Test\n+  public void testSnapshotPartitioned() throws Exception {\n+    File location = temp.newFolder();\n+    String dest = uniqueName(\"iceberg_snapshot_partitioned\");\n+    String source = uniqueName(\"test_snapshot_partitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet PARTITIONED BY (id) location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.snapshot(source, dest, location.toString()),\n+        3);\n+    testIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testSnapshotUnpartitioned() throws Exception {\n+    File location = temp.newFolder();\n+    String dest = uniqueName(\"iceberg_snapshot_unpartitioned\");\n+    String source = uniqueName(\"test_snapshot_unpartitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.snapshot(source, dest, location.toString()),\n+        2);\n+    testIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testSnapshotHiveTable() throws Exception {\n+    File location = temp.newFolder();\n+    String dest = uniqueName(\"iceberg_snapshot_hive_table\");\n+    String source = uniqueName(\"snapshot_hive_table\");\n+    testCreate(source,\n+        dest,\n+        String.format(\"CREATE EXTERNAL TABLE %s (id Int, data String) STORED AS parquet LOCATION '%s'\", source,\n+            tableLocation),\n+        () -> Actions.snapshot(source, dest, location.toString()),\n+        3);\n+    testIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testProperties() throws Exception {\n+    String dest = uniqueName(\"iceberg_properties\");\n+    String source = uniqueName(\"test_properties_table\");\n+    Map<String, String> props = Maps.newHashMap();\n+    props.put(\"city\", \"New Orleans\");\n+    props.put(\"note\", \"Jazz\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.migrate(source).as(dest)\n+        .withAdditionalProperty(\"dogs\", \"sundance\")\n+        .withAdditionalProperties(props),\n+        2);\n+\n+    SparkTable table = loadTable(dest);\n+\n+\n+    Map<String, String> expectedProps = Maps.newHashMap();\n+    expectedProps.putAll(props);\n+    expectedProps.put(\"dogs\", \"sundance\");\n+\n+    for (Map.Entry<String, String> entry : expectedProps.entrySet()) {\n+      Assert.assertTrue(\n+          \"Created table missing property \" + entry.getKey(),\n+          table.properties().containsKey(entry.getKey()));\n+      Assert.assertEquals(\"Property value is not the expected value\",\n+          entry.getValue(), table.properties().get(entry.getKey()));\n+    }\n+  }\n+\n+  private SparkTable loadTable(String name) throws NoSuchTableException {\n+    return (SparkTable) catalog.loadTable(Spark3Util.catalogAndIdentifier(spark, name).identifier());\n+  }\n+\n+  // Creates a table, runs the migration command and checks the results.\n+  private void testCreate(String source, String dest, String sqlCreate, Supplier<CreateAction> action,\n+      long expectedMigratedFiles) throws\n+      Exception {\n+\n+    File location = temp.newFolder();\n+\n+    spark.sql(String.format(sqlCreate, source, location, baseTableName));\n+\n+    long migratedFiles = action.get().execute();\n+\n+    SparkTable table = loadTable(dest);\n+\n+    Assert.assertEquals(\"Provider should be iceberg\", \"iceberg\",\n+        table.properties().get(TableCatalog.PROP_PROVIDER));\n+    Assert.assertEquals(\"Expected number of migrated files\", expectedMigratedFiles, migratedFiles);\n+    Assert.assertEquals(\"Expected rows in table \", 3, spark.table(dest).count());\n+  }\n+\n+  // Inserts records into the destination, makes sure those records exist and source table is unchanged\n+  private void testIsolatedSnapshot(String source, String dest) {\n+    List<Row> expected = spark.sql(String.format(\"SELECT * FROM %s\", source)).collectAsList();\n+\n+    List<SimpleRecord> extraData = Lists.newArrayList(\n+        new SimpleRecord(4, \"d\")\n+    );\n+    Dataset<Row> df = spark.createDataFrame(extraData, SimpleRecord.class);\n+    df.write().format(\"iceberg\").mode(\"append\").saveAsTable(dest);\n+\n+    List<Row> result = spark.sql(String.format(\"SELECT * FROM %s\", source)).collectAsList();\n+    Assert.assertEquals(\"No additional rows should be added to the original table\", expected.size(),\n+        result.size());\n+\n+    List<Row> snapshot = spark.sql(String.format(\"SELECT * FROM %s WHERE id = 4 AND data = 'd'\", dest)).collectAsList();\n+    Assert.assertEquals(\"Added row not found in snapshot\", 1, snapshot.size());\n+  }\n+\n+  private String uniqueName(String source) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 269}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc3MzE2", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521277316", "createdAt": "2020-11-02T00:06:32Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDowNjozMlrOHrzU_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDowNjozMlrOHrzU_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTc3NQ==", "bodyText": "What is IcebergV2Catalog? It looks like SparkCatalog supports staged tables with both Hadoop and Hive catalogs.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515691775", "createdAt": "2020-11-02T00:06:32Z", "author": {"login": "rdblue"}, "path": "spark3/src/test/java/org/apache/iceberg/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+\n+  // Only valid for IcebergV2Catalog - Hadoop Catalog does not support Staged Tables", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 50}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc3MzQw", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521277340", "createdAt": "2020-11-02T00:06:42Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDowNjo0M1rOHrzVGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDowNjo0M1rOHrzVGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTgwMw==", "bodyText": "Are these private?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515691803", "createdAt": "2020-11-02T00:06:43Z", "author": {"login": "rdblue"}, "path": "spark3/src/test/java/org/apache/iceberg/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+\n+  // Only valid for IcebergV2Catalog - Hadoop Catalog does not support Staged Tables\n+  @Parameterized.Parameters(name = \"Catalog Name {0} - Options {2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )},\n+        new Object[] { \"testhive\", SparkCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\"\n+        )}\n+    };\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  String baseTableName = \"baseTable\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 70}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc3Mzg1", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521277385", "createdAt": "2020-11-02T00:07:03Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDowNzowM1rOHrzVSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDowNzowM1rOHrzVSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTg1MQ==", "bodyText": "Nit: extra newline", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515691851", "createdAt": "2020-11-02T00:07:03Z", "author": {"login": "rdblue"}, "path": "spark3/src/test/java/org/apache/iceberg/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+\n+  // Only valid for IcebergV2Catalog - Hadoop Catalog does not support Staged Tables\n+  @Parameterized.Parameters(name = \"Catalog Name {0} - Options {2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )},\n+        new Object[] { \"testhive\", SparkCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\"\n+        )}\n+    };\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  String baseTableName = \"baseTable\";\n+  File tableDir;\n+  String tableLocation;\n+  final String implementation;\n+  final String type;\n+  final TableCatalog catalog;\n+\n+  public TestCreateActions(\n+      String catalogName,\n+      String implementation,\n+      Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+    this.catalog = (TableCatalog) spark.sessionState().catalogManager().catalog(catalogName);\n+    this.implementation = implementation;\n+    this.type = config.get(\"type\");\n+  }\n+\n+  @Before\n+  public void before() {\n+    try {\n+      this.tableDir = temp.newFolder();\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+    this.tableLocation = tableDir.toURI().toString();\n+\n+    spark.conf().set(\"hive.exec.dynamic.partition\", \"true\");\n+    spark.conf().set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 100}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc3NTA2", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521277506", "createdAt": "2020-11-02T00:08:16Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDowODoxN1rOHrzVrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDowODoxN1rOHrzVrw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MTk1MQ==", "bodyText": "Can you use Assume for this instead of if? That way it shows up as a skipped case.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515691951", "createdAt": "2020-11-02T00:08:17Z", "author": {"login": "rdblue"}, "path": "spark3/src/test/java/org/apache/iceberg/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+\n+  // Only valid for IcebergV2Catalog - Hadoop Catalog does not support Staged Tables\n+  @Parameterized.Parameters(name = \"Catalog Name {0} - Options {2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )},\n+        new Object[] { \"testhive\", SparkCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\"\n+        )}\n+    };\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  String baseTableName = \"baseTable\";\n+  File tableDir;\n+  String tableLocation;\n+  final String implementation;\n+  final String type;\n+  final TableCatalog catalog;\n+\n+  public TestCreateActions(\n+      String catalogName,\n+      String implementation,\n+      Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+    this.catalog = (TableCatalog) spark.sessionState().catalogManager().catalog(catalogName);\n+    this.implementation = implementation;\n+    this.type = config.get(\"type\");\n+  }\n+\n+  @Before\n+  public void before() {\n+    try {\n+      this.tableDir = temp.newFolder();\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+    this.tableLocation = tableDir.toURI().toString();\n+\n+    spark.conf().set(\"hive.exec.dynamic.partition\", \"true\");\n+    spark.conf().set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+\n+\n+    List<SimpleRecord> expected = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\"),\n+        new SimpleRecord(3, \"c\")\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, SimpleRecord.class);\n+\n+    df.select(\"id\", \"data\").orderBy(\"data\").write()\n+        .mode(\"append\")\n+        .option(\"path\", tableLocation)\n+        .saveAsTable(baseTableName);\n+  }\n+\n+  @After\n+  public void after() throws IOException {\n+    // Drop the hive table.\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+  }\n+\n+\n+  @Test\n+  public void testMigratePartitioned() throws Exception {\n+    String dest = uniqueName(\"iceberg_migrate_partitioned\");\n+    String source = uniqueName(\"test_migrate_partitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet PARTITIONED BY (id) location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.migrate(source).as(dest),\n+        3);\n+  }\n+\n+  @Test\n+  public void testMigrateUnpartitioned() throws Exception {\n+    String dest = uniqueName(\"iceberg_migrate_unpartitioned\");\n+    String source = uniqueName(\"test_migrate_unpartitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.migrate(source).as(dest),\n+        2);\n+  }\n+\n+  @Test\n+  public void testMigrateReplace() throws Exception {\n+    // We can't do a replacement unless we have overridden the spark_catalog\n+    if (catalog.name().equals(\"spark_catalog\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 147}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc4MzEw", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521278310", "createdAt": "2020-11-02T00:15:40Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDoxNTo0MVrOHrzZGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDoxNTo0MVrOHrzZGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MjgyNQ==", "bodyText": "I like the code reuse that these tests have, but I don't think that the pattern of a single method like this is very readable. For example, it isn't clear what expectedMigratedFiles is until you read this, but it differs between test cases.\nI think a better pattern for reuse is to use separate methods that are well named. These tests have a great example of what I'm talking about with testIsolatedSnapshot. You could add a boolean to testCreate for whether this method should call testIsolatedSnapshot, but it is more readable to simply put the call in the test case directly.\nI think a similar option, assertMigratedFileCount, would be an improvement over passing an unlabelled number. Similarly, building a method to create and initialize the source table with a name rather than passing a create statement would help readability in the tests.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515692825", "createdAt": "2020-11-02T00:15:41Z", "author": {"login": "rdblue"}, "path": "spark3/src/test/java/org/apache/iceberg/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,272 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+\n+  // Only valid for IcebergV2Catalog - Hadoop Catalog does not support Staged Tables\n+  @Parameterized.Parameters(name = \"Catalog Name {0} - Options {2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )},\n+        new Object[] { \"testhive\", SparkCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\"\n+        )}\n+    };\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  String baseTableName = \"baseTable\";\n+  File tableDir;\n+  String tableLocation;\n+  final String implementation;\n+  final String type;\n+  final TableCatalog catalog;\n+\n+  public TestCreateActions(\n+      String catalogName,\n+      String implementation,\n+      Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+    this.catalog = (TableCatalog) spark.sessionState().catalogManager().catalog(catalogName);\n+    this.implementation = implementation;\n+    this.type = config.get(\"type\");\n+  }\n+\n+  @Before\n+  public void before() {\n+    try {\n+      this.tableDir = temp.newFolder();\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+    this.tableLocation = tableDir.toURI().toString();\n+\n+    spark.conf().set(\"hive.exec.dynamic.partition\", \"true\");\n+    spark.conf().set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+\n+\n+    List<SimpleRecord> expected = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\"),\n+        new SimpleRecord(3, \"c\")\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, SimpleRecord.class);\n+\n+    df.select(\"id\", \"data\").orderBy(\"data\").write()\n+        .mode(\"append\")\n+        .option(\"path\", tableLocation)\n+        .saveAsTable(baseTableName);\n+  }\n+\n+  @After\n+  public void after() throws IOException {\n+    // Drop the hive table.\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+  }\n+\n+\n+  @Test\n+  public void testMigratePartitioned() throws Exception {\n+    String dest = uniqueName(\"iceberg_migrate_partitioned\");\n+    String source = uniqueName(\"test_migrate_partitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet PARTITIONED BY (id) location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.migrate(source).as(dest),\n+        3);\n+  }\n+\n+  @Test\n+  public void testMigrateUnpartitioned() throws Exception {\n+    String dest = uniqueName(\"iceberg_migrate_unpartitioned\");\n+    String source = uniqueName(\"test_migrate_unpartitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.migrate(source).as(dest),\n+        2);\n+  }\n+\n+  @Test\n+  public void testMigrateReplace() throws Exception {\n+    // We can't do a replacement unless we have overridden the spark_catalog\n+    if (catalog.name().equals(\"spark_catalog\")) {\n+      String source = uniqueName(uniqueName(\"iceberg_migrate_replace\"));\n+      testCreate(source,\n+          source,\n+          \"CREATE TABLE %s using parquet PARTITIONED BY (id) location '%s' AS SELECT * FROM %s\",\n+          () -> Actions.migrate(source),\n+          3);\n+    }\n+  }\n+\n+  @Test\n+  public void testSnapshotPartitioned() throws Exception {\n+    File location = temp.newFolder();\n+    String dest = uniqueName(\"iceberg_snapshot_partitioned\");\n+    String source = uniqueName(\"test_snapshot_partitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet PARTITIONED BY (id) location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.snapshot(source, dest, location.toString()),\n+        3);\n+    testIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testSnapshotUnpartitioned() throws Exception {\n+    File location = temp.newFolder();\n+    String dest = uniqueName(\"iceberg_snapshot_unpartitioned\");\n+    String source = uniqueName(\"test_snapshot_unpartitioned_table\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.snapshot(source, dest, location.toString()),\n+        2);\n+    testIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testSnapshotHiveTable() throws Exception {\n+    File location = temp.newFolder();\n+    String dest = uniqueName(\"iceberg_snapshot_hive_table\");\n+    String source = uniqueName(\"snapshot_hive_table\");\n+    testCreate(source,\n+        dest,\n+        String.format(\"CREATE EXTERNAL TABLE %s (id Int, data String) STORED AS parquet LOCATION '%s'\", source,\n+            tableLocation),\n+        () -> Actions.snapshot(source, dest, location.toString()),\n+        3);\n+    testIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testProperties() throws Exception {\n+    String dest = uniqueName(\"iceberg_properties\");\n+    String source = uniqueName(\"test_properties_table\");\n+    Map<String, String> props = Maps.newHashMap();\n+    props.put(\"city\", \"New Orleans\");\n+    props.put(\"note\", \"Jazz\");\n+    testCreate(source,\n+        dest,\n+        \"CREATE TABLE %s using parquet location '%s' AS SELECT * FROM %s\",\n+        () -> Actions.migrate(source).as(dest)\n+        .withAdditionalProperty(\"dogs\", \"sundance\")\n+        .withAdditionalProperties(props),\n+        2);\n+\n+    SparkTable table = loadTable(dest);\n+\n+\n+    Map<String, String> expectedProps = Maps.newHashMap();\n+    expectedProps.putAll(props);\n+    expectedProps.put(\"dogs\", \"sundance\");\n+\n+    for (Map.Entry<String, String> entry : expectedProps.entrySet()) {\n+      Assert.assertTrue(\n+          \"Created table missing property \" + entry.getKey(),\n+          table.properties().containsKey(entry.getKey()));\n+      Assert.assertEquals(\"Property value is not the expected value\",\n+          entry.getValue(), table.properties().get(entry.getKey()));\n+    }\n+  }\n+\n+  private SparkTable loadTable(String name) throws NoSuchTableException {\n+    return (SparkTable) catalog.loadTable(Spark3Util.catalogAndIdentifier(spark, name).identifier());\n+  }\n+\n+  // Creates a table, runs the migration command and checks the results.\n+  private void testCreate(String source, String dest, String sqlCreate, Supplier<CreateAction> action,\n+      long expectedMigratedFiles) throws", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 234}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc4ODA3", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521278807", "createdAt": "2020-11-02T00:20:08Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDoyMDowOFrOHrzbPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDoyMDowOFrOHrzbPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5MzM3Mg==", "bodyText": "Session catalog?\nDo we have a way around this restriction? In our implementation, we load the source table using our Hive or Spark table implementation and check that it is what we expect. Then we use that implementation to load the partitions. Would we similarly require a v2 table implementation to make this catalog agnostic?\n(This isn't a blocker, just curious to hear your ideas.)", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515693372", "createdAt": "2020-11-02T00:20:08Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table with the same name\n+ *   This pathway will create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+class Spark3CreateAction implements CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3CreateAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final CatalogPlugin destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = destCatalog;\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable, ALLOWED_SOURCES);\n+\n+    this.sessionCatalogReplacement = isSessionCatalogReplacement();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private boolean isSessionCatalogReplacement() {\n+    boolean sourceIceberg = sourceTable.provider().get().toLowerCase(Locale.ROOT).equals(\"iceberg\");\n+    boolean sameCatalog = sourceCatalog == destCatalog;\n+    boolean sameIdentifier = sourceTableName.name().equals(destTableName.name()) &&\n+        Arrays.equals(sourceTableName.namespace(), destTableName.namespace());\n+    return !sourceIceberg && sameCatalog && sameIdentifier;\n+  }\n+\n+\n+  /**\n+   * Creates the Iceberg data and metadata at a given location instead of the source table\n+   * location. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * Use this if you would like to experiment with Iceberg without changing\n+   * your original files.\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  CreateAction asSnapshotAtLocation(String newLocation) {\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagingTableCatalog stagingCatalog = checkDestinationCatalog(destCatalog);\n+    Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+        .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+        .putAll(JavaConverters.mapAsJavaMapConverter(sourceTable.properties()).asJava())\n+        .putAll(extraIcebergTableProps(destDataLocation, destMetadataLocation))\n+        .putAll(additionalProperties)\n+        .build();\n+\n+    StagedTable stagedTable;\n+    try {\n+      if (sessionCatalogReplacement) {\n+        /*\n+         * Spark Session Catalog cannot stage a replacement of a Session table with an Iceberg Table.\n+         * To workaround this we create a replacement table which is renamed after it\n+         * is successfully constructed.\n+         */\n+        stagedTable = stagingCatalog.stageCreate(Identifier.of(\n+            destTableName.namespace(),\n+            destTableName.name() + REPLACEMENT_NAME),\n+            sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      } else {\n+        stagedTable = stagingCatalog.stageCreate(destTableName, sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      }\n+    } catch (NoSuchNamespaceException e) {\n+      throw new IllegalArgumentException(\"Cannot create a new table in a namespace which does not exist\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Destination table already exists\", e);\n+    }\n+\n+    String stagingLocation = destMetadataLocation;\n+    Table icebergTable = ((SparkTable) stagedTable).table();\n+\n+    LOG.info(\"Beginning migration of {} to {}\", sourceTableName, destTableName);\n+    long numMigratedFiles = 0;\n+    try {\n+      SparkTableUtil.importSparkTable(spark, Spark3Util.toTableIdentifier(sourceTableName), icebergTable,\n+          stagingLocation);\n+\n+      Snapshot snapshot = icebergTable.currentSnapshot();\n+      numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+\n+      stagedTable.commitStagedChanges();\n+      LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    } catch (Exception e) {\n+      LOG.error(\"Error when attempting to commit migration changes, rolling back\", e);\n+      stagedTable.abortStagedChanges();\n+      throw e;\n+    }\n+\n+    if (sessionCatalogReplacement) {\n+      Identifier replacementTable = Identifier.of(destTableName.namespace(), destTableName.name() + REPLACEMENT_NAME);\n+      try {\n+        stagingCatalog.dropTable(destTableName);\n+        stagingCatalog.renameTable(replacementTable, destTableName);\n+      } catch (NoSuchTableException e) {\n+        LOG.error(\"Cannot migrate, replacement table is missing. Attempting to recreate source table\", e);\n+        try {\n+          stagingCatalog.createTable(sourceTableName, sourceTable.schema(),\n+              Spark3Util.toTransforms(sourcePartitionSpec), JavaConverters.mapAsJavaMap(sourceTable.properties()));\n+        } catch (TableAlreadyExistsException tableAlreadyExistsException) {\n+          Log.error(\"Cannot recreate source table. Source table has already been recreated\", e);\n+          throw new RuntimeException(e);\n+        } catch (NoSuchNamespaceException noSuchNamespaceException) {\n+          Log.error(\"Cannot recreate source table. Source namespace has been removed, cannot recreate\", e);\n+          throw new RuntimeException(e);\n+        }\n+      } catch (TableAlreadyExistsException e) {\n+        Log.error(\"Cannot migrate, Source table was recreated before replacement could be moved. \" +\n+            \"Attempting to remove replacement table.\", e);\n+        stagingCatalog.dropTable(replacementTable);\n+        stagedTable.abortStagedChanges();\n+      }\n+    }\n+\n+    return numMigratedFiles;\n+  }\n+\n+  @Override\n+  public CreateAction withAdditionalProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withAdditionalProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction as(String newName) {\n+    Spark3Util.CatalogAndIdentifier newDest = Spark3Util.catalogAndIdentifier(spark, newName);\n+    return new Spark3CreateAction(spark, sourceCatalog, sourceTableName, newDest.catalog(), newDest.identifier())\n+        .withAdditionalProperties(this.additionalProperties);\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable, Set<String> supportedSourceTableProviders) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!supportedSourceTableProviders.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  private static Map<String, String> extraIcebergTableProps(String tableLocation, String metadataLocation) {\n+    return ImmutableMap.of(\n+        TableProperties.WRITE_METADATA_LOCATION, metadataLocation,\n+        TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation,\n+        \"migrated\", \"true\");\n+  }\n+\n+  private static StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {\n+    if (!(catalog instanceof SparkSessionCatalog) && !(catalog instanceof SparkCatalog)) {\n+      throw new IllegalArgumentException(String.format(\"Cannot create Iceberg table in non Iceberg Catalog. \" +\n+              \"Catalog %s was of class %s but %s or %s are required\", catalog.name(), catalog.getClass(),\n+          SparkSessionCatalog.class.getName(), SparkCatalog.class.getName()));\n+    }\n+    return (StagingTableCatalog) catalog;\n+  }\n+\n+  private CatalogPlugin checkSourceCatalog(CatalogPlugin catalog) {\n+    // Currently the Import code relies on being able to look up the table in the session code", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 281}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc5MjIy", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521279222", "createdAt": "2020-11-02T00:23:53Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDoyMzo1M1rOHrzczQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDoyMzo1M1rOHrzczQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5Mzc3Mw==", "bodyText": "This excludes Hive tables that don't use Spark's provider, including those created with STORED AS parquet instead of USING parquet. I don't think that is necessary. Isn't the only requirement that all of the partitions are a supported format?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515693773", "createdAt": "2020-11-02T00:23:53Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table with the same name\n+ *   This pathway will create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+class Spark3CreateAction implements CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3CreateAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final CatalogPlugin destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = destCatalog;\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable, ALLOWED_SOURCES);\n+\n+    this.sessionCatalogReplacement = isSessionCatalogReplacement();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private boolean isSessionCatalogReplacement() {\n+    boolean sourceIceberg = sourceTable.provider().get().toLowerCase(Locale.ROOT).equals(\"iceberg\");\n+    boolean sameCatalog = sourceCatalog == destCatalog;\n+    boolean sameIdentifier = sourceTableName.name().equals(destTableName.name()) &&\n+        Arrays.equals(sourceTableName.namespace(), destTableName.namespace());\n+    return !sourceIceberg && sameCatalog && sameIdentifier;\n+  }\n+\n+\n+  /**\n+   * Creates the Iceberg data and metadata at a given location instead of the source table\n+   * location. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * Use this if you would like to experiment with Iceberg without changing\n+   * your original files.\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  CreateAction asSnapshotAtLocation(String newLocation) {\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagingTableCatalog stagingCatalog = checkDestinationCatalog(destCatalog);\n+    Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+        .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+        .putAll(JavaConverters.mapAsJavaMapConverter(sourceTable.properties()).asJava())\n+        .putAll(extraIcebergTableProps(destDataLocation, destMetadataLocation))\n+        .putAll(additionalProperties)\n+        .build();\n+\n+    StagedTable stagedTable;\n+    try {\n+      if (sessionCatalogReplacement) {\n+        /*\n+         * Spark Session Catalog cannot stage a replacement of a Session table with an Iceberg Table.\n+         * To workaround this we create a replacement table which is renamed after it\n+         * is successfully constructed.\n+         */\n+        stagedTable = stagingCatalog.stageCreate(Identifier.of(\n+            destTableName.namespace(),\n+            destTableName.name() + REPLACEMENT_NAME),\n+            sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      } else {\n+        stagedTable = stagingCatalog.stageCreate(destTableName, sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      }\n+    } catch (NoSuchNamespaceException e) {\n+      throw new IllegalArgumentException(\"Cannot create a new table in a namespace which does not exist\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Destination table already exists\", e);\n+    }\n+\n+    String stagingLocation = destMetadataLocation;\n+    Table icebergTable = ((SparkTable) stagedTable).table();\n+\n+    LOG.info(\"Beginning migration of {} to {}\", sourceTableName, destTableName);\n+    long numMigratedFiles = 0;\n+    try {\n+      SparkTableUtil.importSparkTable(spark, Spark3Util.toTableIdentifier(sourceTableName), icebergTable,\n+          stagingLocation);\n+\n+      Snapshot snapshot = icebergTable.currentSnapshot();\n+      numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+\n+      stagedTable.commitStagedChanges();\n+      LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    } catch (Exception e) {\n+      LOG.error(\"Error when attempting to commit migration changes, rolling back\", e);\n+      stagedTable.abortStagedChanges();\n+      throw e;\n+    }\n+\n+    if (sessionCatalogReplacement) {\n+      Identifier replacementTable = Identifier.of(destTableName.namespace(), destTableName.name() + REPLACEMENT_NAME);\n+      try {\n+        stagingCatalog.dropTable(destTableName);\n+        stagingCatalog.renameTable(replacementTable, destTableName);\n+      } catch (NoSuchTableException e) {\n+        LOG.error(\"Cannot migrate, replacement table is missing. Attempting to recreate source table\", e);\n+        try {\n+          stagingCatalog.createTable(sourceTableName, sourceTable.schema(),\n+              Spark3Util.toTransforms(sourcePartitionSpec), JavaConverters.mapAsJavaMap(sourceTable.properties()));\n+        } catch (TableAlreadyExistsException tableAlreadyExistsException) {\n+          Log.error(\"Cannot recreate source table. Source table has already been recreated\", e);\n+          throw new RuntimeException(e);\n+        } catch (NoSuchNamespaceException noSuchNamespaceException) {\n+          Log.error(\"Cannot recreate source table. Source namespace has been removed, cannot recreate\", e);\n+          throw new RuntimeException(e);\n+        }\n+      } catch (TableAlreadyExistsException e) {\n+        Log.error(\"Cannot migrate, Source table was recreated before replacement could be moved. \" +\n+            \"Attempting to remove replacement table.\", e);\n+        stagingCatalog.dropTable(replacementTable);\n+        stagedTable.abortStagedChanges();\n+      }\n+    }\n+\n+    return numMigratedFiles;\n+  }\n+\n+  @Override\n+  public CreateAction withAdditionalProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withAdditionalProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction as(String newName) {\n+    Spark3Util.CatalogAndIdentifier newDest = Spark3Util.catalogAndIdentifier(spark, newName);\n+    return new Spark3CreateAction(spark, sourceCatalog, sourceTableName, newDest.catalog(), newDest.identifier())\n+        .withAdditionalProperties(this.additionalProperties);\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable, Set<String> supportedSourceTableProviders) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 253}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxMjc5NDIy", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-521279422", "createdAt": "2020-11-02T00:25:33Z", "commit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDoyNTozM1rOHrzdsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQwMDoyNTozM1rOHrzdsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTY5NDAwMw==", "bodyText": "Should this use a static set of table providers instead of passing them in?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r515694003", "createdAt": "2020-11-02T00:25:33Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table with the same name\n+ *   This pathway will create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+class Spark3CreateAction implements CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3CreateAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final CatalogPlugin destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = destCatalog;\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable, ALLOWED_SOURCES);\n+\n+    this.sessionCatalogReplacement = isSessionCatalogReplacement();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private boolean isSessionCatalogReplacement() {\n+    boolean sourceIceberg = sourceTable.provider().get().toLowerCase(Locale.ROOT).equals(\"iceberg\");\n+    boolean sameCatalog = sourceCatalog == destCatalog;\n+    boolean sameIdentifier = sourceTableName.name().equals(destTableName.name()) &&\n+        Arrays.equals(sourceTableName.namespace(), destTableName.namespace());\n+    return !sourceIceberg && sameCatalog && sameIdentifier;\n+  }\n+\n+\n+  /**\n+   * Creates the Iceberg data and metadata at a given location instead of the source table\n+   * location. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * Use this if you would like to experiment with Iceberg without changing\n+   * your original files.\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  CreateAction asSnapshotAtLocation(String newLocation) {\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagingTableCatalog stagingCatalog = checkDestinationCatalog(destCatalog);\n+    Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+        .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+        .putAll(JavaConverters.mapAsJavaMapConverter(sourceTable.properties()).asJava())\n+        .putAll(extraIcebergTableProps(destDataLocation, destMetadataLocation))\n+        .putAll(additionalProperties)\n+        .build();\n+\n+    StagedTable stagedTable;\n+    try {\n+      if (sessionCatalogReplacement) {\n+        /*\n+         * Spark Session Catalog cannot stage a replacement of a Session table with an Iceberg Table.\n+         * To workaround this we create a replacement table which is renamed after it\n+         * is successfully constructed.\n+         */\n+        stagedTable = stagingCatalog.stageCreate(Identifier.of(\n+            destTableName.namespace(),\n+            destTableName.name() + REPLACEMENT_NAME),\n+            sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      } else {\n+        stagedTable = stagingCatalog.stageCreate(destTableName, sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      }\n+    } catch (NoSuchNamespaceException e) {\n+      throw new IllegalArgumentException(\"Cannot create a new table in a namespace which does not exist\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Destination table already exists\", e);\n+    }\n+\n+    String stagingLocation = destMetadataLocation;\n+    Table icebergTable = ((SparkTable) stagedTable).table();\n+\n+    LOG.info(\"Beginning migration of {} to {}\", sourceTableName, destTableName);\n+    long numMigratedFiles = 0;\n+    try {\n+      SparkTableUtil.importSparkTable(spark, Spark3Util.toTableIdentifier(sourceTableName), icebergTable,\n+          stagingLocation);\n+\n+      Snapshot snapshot = icebergTable.currentSnapshot();\n+      numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+\n+      stagedTable.commitStagedChanges();\n+      LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    } catch (Exception e) {\n+      LOG.error(\"Error when attempting to commit migration changes, rolling back\", e);\n+      stagedTable.abortStagedChanges();\n+      throw e;\n+    }\n+\n+    if (sessionCatalogReplacement) {\n+      Identifier replacementTable = Identifier.of(destTableName.namespace(), destTableName.name() + REPLACEMENT_NAME);\n+      try {\n+        stagingCatalog.dropTable(destTableName);\n+        stagingCatalog.renameTable(replacementTable, destTableName);\n+      } catch (NoSuchTableException e) {\n+        LOG.error(\"Cannot migrate, replacement table is missing. Attempting to recreate source table\", e);\n+        try {\n+          stagingCatalog.createTable(sourceTableName, sourceTable.schema(),\n+              Spark3Util.toTransforms(sourcePartitionSpec), JavaConverters.mapAsJavaMap(sourceTable.properties()));\n+        } catch (TableAlreadyExistsException tableAlreadyExistsException) {\n+          Log.error(\"Cannot recreate source table. Source table has already been recreated\", e);\n+          throw new RuntimeException(e);\n+        } catch (NoSuchNamespaceException noSuchNamespaceException) {\n+          Log.error(\"Cannot recreate source table. Source namespace has been removed, cannot recreate\", e);\n+          throw new RuntimeException(e);\n+        }\n+      } catch (TableAlreadyExistsException e) {\n+        Log.error(\"Cannot migrate, Source table was recreated before replacement could be moved. \" +\n+            \"Attempting to remove replacement table.\", e);\n+        stagingCatalog.dropTable(replacementTable);\n+        stagedTable.abortStagedChanges();\n+      }\n+    }\n+\n+    return numMigratedFiles;\n+  }\n+\n+  @Override\n+  public CreateAction withAdditionalProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withAdditionalProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction as(String newName) {\n+    Spark3Util.CatalogAndIdentifier newDest = Spark3Util.catalogAndIdentifier(spark, newName);\n+    return new Spark3CreateAction(spark, sourceCatalog, sourceTableName, newDest.catalog(), newDest.identifier())\n+        .withAdditionalProperties(this.additionalProperties);\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable, Set<String> supportedSourceTableProviders) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "216eb56a60a7ea3b4bdd9465ae92116ab44ef8b8"}, "originalPosition": 252}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "91a5d5c9155c7bff626d41026a517be8eb3a27e1", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/91a5d5c9155c7bff626d41026a517be8eb3a27e1", "committedDate": "2020-11-16T23:16:18Z", "message": "Reviewer Comments\n\nAdded Snapshot with default location\nRefactored tests for less reptition and clear reading\nAdd tests for hadoop backed tables, only supports snapshotting in the default table location\nAdd tests for managed hive tables\nChanged property api to set()\nRemoved \"as()\""}, "afterCommit": {"oid": "683b7e5265adf67b1e24a25c008e2f289f1d97ad", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/683b7e5265adf67b1e24a25c008e2f289f1d97ad", "committedDate": "2020-11-17T18:30:39Z", "message": "Provide API and Implementation for Creating Iceberg Tables from Spark\n\nPreviously the SparkUtil class provided an importSparkTable command but\nthis command suffered from a few shortcomings. It had a difficult api and\ndid not work directly with DSV2 Iceberg Catalogs. To provide a simpler interface\nthat shares a similar pattern to currently available Spark Actions we introduce\nActions.migrate and Actions.snapshot.\n\nThrough these commands we can both migrate a Table as well as make a snapshot\nof an existing table."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzOTI0MTk5", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-533924199", "createdAt": "2020-11-18T22:12:52Z", "commit": {"oid": "683b7e5265adf67b1e24a25c008e2f289f1d97ad"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjoxMjo1M1rOH2EbMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjoxMjo1M1rOH2EbMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ1NzY1MQ==", "bodyText": "Well, it seems we rely too much on reflection here. When we decided to use reflection to split actions, we did not have these static methods and it was reasonable since the scope of the change was smaller compared to introducing BaseActions. I am no longer sure reflection is a good idea here as making these methods work with reflection is more complicated than having BaseActions. On top, we don't have compile time checks. Users will call these methods in Spark 2 and will get runtime exceptions. I'd prefer to not expose those methods at all.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526457651", "createdAt": "2020-11-18T22:12:53Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/Actions.java", "diffHunk": "@@ -77,6 +82,120 @@ public ExpireSnapshotsAction expireSnapshots() {\n     return new ExpireSnapshotsAction(spark, table);\n   }\n \n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @return {@link CreateAction} to perform migration\n+   */\n+  public static CreateAction migrate(String tableName) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "683b7e5265adf67b1e24a25c008e2f289f1d97ad"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzOTQ5ODgz", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-533949883", "createdAt": "2020-11-18T22:57:14Z", "commit": {"oid": "683b7e5265adf67b1e24a25c008e2f289f1d97ad"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjo1NzoxNFrOH2Fueg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjo1NzoxNFrOH2Fueg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3ODk3MA==", "bodyText": "If we don't assign an explicit location while snapshotting, how are we going to validate the new location is different compared to the existing table location?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526478970", "createdAt": "2020-11-18T22:57:14Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table of the same name.\n+ *   This pathway will first create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+class Spark3CreateAction implements CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3CreateAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final CatalogPlugin destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = destCatalog;\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable);\n+\n+    this.sessionCatalogReplacement = isSessionCatalogReplacement();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private boolean isSessionCatalogReplacement() {\n+    boolean sourceIceberg = sourceTable.provider().get().toLowerCase(Locale.ROOT).equals(\"iceberg\");\n+    boolean sameCatalog = sourceCatalog == destCatalog;\n+    boolean sameIdentifier = sourceTableName.name().equals(destTableName.name()) &&\n+        Arrays.equals(sourceTableName.namespace(), destTableName.namespace());\n+    return !sourceIceberg && sameCatalog && sameIdentifier;\n+  }\n+\n+\n+  /**\n+   * Creates the Iceberg data and metadata at a given location instead of the source table\n+   * location. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  CreateAction asSnapshotAtLocation(String newLocation) {\n+    Preconditions.checkArgument(!newLocation.equals(sourceTableLocation), \"Cannot create a snapshot with the\" +\n+        \"same data location as the source table. To place new files in the source table directory use the migrate \" +\n+        \"command.\");\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  /**\n+   * Creates the Iceberg data and metadata at the catalog default location for the\n+   * new table.\n+   *\n+   * @return this for chaining\n+   */\n+  CreateAction asSnapshotAtDefaultLocation() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "683b7e5265adf67b1e24a25c008e2f289f1d97ad"}, "originalPosition": 160}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzOTUwMzQ3", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-533950347", "createdAt": "2020-11-18T22:58:07Z", "commit": {"oid": "683b7e5265adf67b1e24a25c008e2f289f1d97ad"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjo1ODowOFrOH2Fv_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjo1ODowOFrOH2Fv_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3OTM1OQ==", "bodyText": "I am not sure this is correct. Shouldn't we be assigning the table location, not data location?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526479359", "createdAt": "2020-11-18T22:58:08Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table of the same name.\n+ *   This pathway will first create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+class Spark3CreateAction implements CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3CreateAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final CatalogPlugin destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = destCatalog;\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable);\n+\n+    this.sessionCatalogReplacement = isSessionCatalogReplacement();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private boolean isSessionCatalogReplacement() {\n+    boolean sourceIceberg = sourceTable.provider().get().toLowerCase(Locale.ROOT).equals(\"iceberg\");\n+    boolean sameCatalog = sourceCatalog == destCatalog;\n+    boolean sameIdentifier = sourceTableName.name().equals(destTableName.name()) &&\n+        Arrays.equals(sourceTableName.namespace(), destTableName.namespace());\n+    return !sourceIceberg && sameCatalog && sameIdentifier;\n+  }\n+\n+\n+  /**\n+   * Creates the Iceberg data and metadata at a given location instead of the source table\n+   * location. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  CreateAction asSnapshotAtLocation(String newLocation) {\n+    Preconditions.checkArgument(!newLocation.equals(sourceTableLocation), \"Cannot create a snapshot with the\" +\n+        \"same data location as the source table. To place new files in the source table directory use the migrate \" +\n+        \"command.\");\n+    this.destDataLocation = newLocation;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "683b7e5265adf67b1e24a25c008e2f289f1d97ad"}, "originalPosition": 149}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzOTY4MjM3", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-533968237", "createdAt": "2020-11-18T23:37:35Z", "commit": {"oid": "683b7e5265adf67b1e24a25c008e2f289f1d97ad"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMzozNzozNVrOH2Gp_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMzozNzozNVrOH2Gp_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ5NDIwNw==", "bodyText": "setAll?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526494207", "createdAt": "2020-11-18T23:37:35Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  CreateAction set(Map<String, String> properties);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "683b7e5265adf67b1e24a25c008e2f289f1d97ad"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzOTc3MTYy", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-533977162", "createdAt": "2020-11-18T23:57:55Z", "commit": {"oid": "683b7e5265adf67b1e24a25c008e2f289f1d97ad"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMzo1Nzo1NlrOH2HH5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMzo1Nzo1NlrOH2HH5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjUwMTg2Mg==", "bodyText": "Is it enough to check if it is instance of BaseCatalog we introduced recently?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r526501862", "createdAt": "2020-11-18T23:57:56Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,310 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Arrays;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import jline.internal.Log;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * This action will migrate a known table in a Spark Catalog that is not an Iceberg table into an Iceberg table.\n+ * The created new table will be able to interact with and modify files in the original table.\n+ *\n+ * There are two main code paths\n+ *   - Creating a brand new iceberg table or replacing an existing Iceberg table\n+ *   This pathway will use a staged table to stage the creation or replacement, only committing after\n+ *   import has succeeded.\n+ *\n+ *   - Replacing a table in the Session Catalog with an Iceberg Table of the same name.\n+ *   This pathway will first create a temporary table with a different name. This replacement table will\n+ *   be committed upon a successful import. Then the original session catalog entry will be dropped\n+ *   and the new replacement table renamed to take its place.\n+ */\n+class Spark3CreateAction implements CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3CreateAction.class);\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  private static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+  private static final String REPLACEMENT_NAME = \"_REPLACEMENT_\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final CatalogTable sourceTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final Boolean sessionCatalogReplacement;\n+  private final CatalogPlugin destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private String destDataLocation;\n+  private String destMetadataLocation;\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = destCatalog;\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = spark.sessionState().catalog().getTableMetadata(Spark3Util.toTableIdentifier(sourceTableName));\n+    } catch (NoSuchTableException | NoSuchDatabaseException e) {\n+      throw new IllegalArgumentException(String.format(\"Could not find source table %s\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceTable);\n+\n+    this.sessionCatalogReplacement = isSessionCatalogReplacement();\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceTable.storage().locationUri().get());\n+    this.destDataLocation = sourceTableLocation;\n+    this.destMetadataLocation = sourceTableLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+  }\n+\n+  private boolean isSessionCatalogReplacement() {\n+    boolean sourceIceberg = sourceTable.provider().get().toLowerCase(Locale.ROOT).equals(\"iceberg\");\n+    boolean sameCatalog = sourceCatalog == destCatalog;\n+    boolean sameIdentifier = sourceTableName.name().equals(destTableName.name()) &&\n+        Arrays.equals(sourceTableName.namespace(), destTableName.namespace());\n+    return !sourceIceberg && sameCatalog && sameIdentifier;\n+  }\n+\n+\n+  /**\n+   * Creates the Iceberg data and metadata at a given location instead of the source table\n+   * location. New metadata and data files will be added to this\n+   * new location and further operations will not effect the source table.\n+   *\n+   * @param newLocation the base directory for the new Iceberg Table\n+   * @return this for chaining\n+   */\n+  CreateAction asSnapshotAtLocation(String newLocation) {\n+    Preconditions.checkArgument(!newLocation.equals(sourceTableLocation), \"Cannot create a snapshot with the\" +\n+        \"same data location as the source table. To place new files in the source table directory use the migrate \" +\n+        \"command.\");\n+    this.destDataLocation = newLocation;\n+    this.destMetadataLocation = newLocation + \"/\" + ICEBERG_METADATA_FOLDER;\n+    return this;\n+  }\n+\n+  /**\n+   * Creates the Iceberg data and metadata at the catalog default location for the\n+   * new table.\n+   *\n+   * @return this for chaining\n+   */\n+  CreateAction asSnapshotAtDefaultLocation() {\n+    this.destDataLocation = null;\n+    this.destMetadataLocation = null;\n+    return this;\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagingTableCatalog stagingCatalog = checkDestinationCatalog(destCatalog);\n+    Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+        .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+        .putAll(JavaConverters.mapAsJavaMapConverter(sourceTable.properties()).asJava())\n+        .putAll(extraIcebergTableProps(destDataLocation, destMetadataLocation))\n+        .putAll(additionalProperties)\n+        .build();\n+\n+    StagedTable stagedTable;\n+    try {\n+      if (sessionCatalogReplacement) {\n+        /*\n+         * Spark Session Catalog cannot stage a replacement of a Session table with an Iceberg Table.\n+         * To workaround this we create a replacement table which is renamed after it\n+         * is successfully constructed.\n+         */\n+        stagedTable = stagingCatalog.stageCreate(Identifier.of(\n+            destTableName.namespace(),\n+            destTableName.name() + REPLACEMENT_NAME),\n+            sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      } else {\n+        stagedTable = stagingCatalog.stageCreate(destTableName, sourceTable.schema(),\n+            Spark3Util.toTransforms(sourcePartitionSpec), newTableProperties);\n+      }\n+    } catch (NoSuchNamespaceException e) {\n+      throw new IllegalArgumentException(\"Cannot create a new table in a namespace which does not exist\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Destination table already exists\", e);\n+    }\n+\n+    Table icebergTable = ((SparkTable) stagedTable).table();\n+\n+    String stagingLocation;\n+    if (destMetadataLocation != null) {\n+      stagingLocation = destMetadataLocation;\n+    } else {\n+      stagingLocation = ((SparkTable) stagedTable).table().location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+    }\n+\n+    LOG.info(\"Beginning migration of {} to {} using metadata location {}\", sourceTableName, destTableName,\n+        stagingLocation);\n+\n+    long numMigratedFiles;\n+    try {\n+      SparkTableUtil.importSparkTable(spark, Spark3Util.toTableIdentifier(sourceTableName), icebergTable,\n+          stagingLocation);\n+\n+      Snapshot snapshot = icebergTable.currentSnapshot();\n+      numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+\n+      stagedTable.commitStagedChanges();\n+      LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    } catch (Exception e) {\n+      LOG.error(\"Error when attempting to commit migration changes, rolling back\", e);\n+      stagedTable.abortStagedChanges();\n+      throw e;\n+    }\n+\n+    if (sessionCatalogReplacement) {\n+      Identifier replacementTable = Identifier.of(destTableName.namespace(), destTableName.name() + REPLACEMENT_NAME);\n+      try {\n+        stagingCatalog.dropTable(destTableName);\n+        stagingCatalog.renameTable(replacementTable, destTableName);\n+      } catch (NoSuchTableException e) {\n+        LOG.error(\"Cannot migrate, replacement table is missing. Attempting to recreate source table\", e);\n+        try {\n+          stagingCatalog.createTable(sourceTableName, sourceTable.schema(),\n+              Spark3Util.toTransforms(sourcePartitionSpec), JavaConverters.mapAsJavaMap(sourceTable.properties()));\n+        } catch (TableAlreadyExistsException tableAlreadyExistsException) {\n+          Log.error(\"Cannot recreate source table. Source table has already been recreated\", e);\n+          throw new RuntimeException(e);\n+        } catch (NoSuchNamespaceException noSuchNamespaceException) {\n+          Log.error(\"Cannot recreate source table. Source namespace has been removed, cannot recreate\", e);\n+          throw new RuntimeException(e);\n+        }\n+      } catch (TableAlreadyExistsException e) {\n+        Log.error(\"Cannot migrate, Source table was recreated before replacement could be moved. \" +\n+            \"Attempting to remove replacement table.\", e);\n+        stagingCatalog.dropTable(replacementTable);\n+        stagedTable.abortStagedChanges();\n+      }\n+    }\n+\n+    return numMigratedFiles;\n+  }\n+\n+  @Override\n+  public CreateAction set(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction set(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  private static Map<String, String> extraIcebergTableProps(String tableLocation, String metadataLocation) {\n+    if (tableLocation != null && metadataLocation != null) {\n+      return ImmutableMap.of(\n+          TableProperties.WRITE_METADATA_LOCATION, metadataLocation,\n+          TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation,\n+          \"migrated\", \"true\"\n+      );\n+    } else {\n+      return ImmutableMap.of(\"migrated\", \"true\");\n+    }\n+  }\n+\n+  private static StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {\n+    if (!(catalog instanceof SparkSessionCatalog) && !(catalog instanceof SparkCatalog)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "683b7e5265adf67b1e24a25c008e2f289f1d97ad"}, "originalPosition": 292}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d36a7b427b2b56dcfee31d2c2a26ccc6e4030a72", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/d36a7b427b2b56dcfee31d2c2a26ccc6e4030a72", "committedDate": "2020-11-25T18:23:16Z", "message": "Get all Tests Working Again"}, "afterCommit": {"oid": "9d521f386c5cd6ca1e5b8b70d2c5cb411eddcae9", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/9d521f386c5cd6ca1e5b8b70d2c5cb411eddcae9", "committedDate": "2020-11-25T18:24:03Z", "message": "Get all Tests Working Again"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9d521f386c5cd6ca1e5b8b70d2c5cb411eddcae9", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/9d521f386c5cd6ca1e5b8b70d2c5cb411eddcae9", "committedDate": "2020-11-25T18:24:03Z", "message": "Get all Tests Working Again"}, "afterCommit": {"oid": "2e6e20a19fd81569bd296799b19b6b5ae612d19d", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/2e6e20a19fd81569bd296799b19b6b5ae612d19d", "committedDate": "2020-11-28T17:49:28Z", "message": "Fixup from Rebase"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9d823b97394d6d026b43208be1751b95ef466f88", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/9d823b97394d6d026b43208be1751b95ef466f88", "committedDate": "2020-12-03T16:27:20Z", "message": "Fix Source Catalog Requirements"}, "afterCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/09f329bc39e8a2a1b2b5f925c580a2177833510b", "committedDate": "2020-12-03T19:58:57Z", "message": "Fix Source Catalog Requirements"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NjAxNDc1", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-544601475", "createdAt": "2020-12-04T01:54:05Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMTo1NDowNVrOH-9RwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMTo1NDowNVrOH-9RwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3NzcyOA==", "bodyText": "specForTable is a bit hacky. At a minimum, it seems strange that this would create a string table name just for specForTable to split on . immediately.\nSince the source table is loaded as a v1 table just after this, why not use the schema and partition fields from the CatalogTable instead?\nWe should probably also deprecate specForTable if we are going to maintain this, since this can be a much better utility for conversion.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535777728", "createdAt": "2020-12-04T01:54:05Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 81}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NjAyMjUy", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-544602252", "createdAt": "2020-12-04T01:56:24Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMTo1NjoyNFrOH-9U5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMTo1NjoyNFrOH-9U5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3ODUzMg==", "bodyText": "Why not use TableCatalog?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535778532", "createdAt": "2020-12-04T01:56:24Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableName);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableName), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use a non-v1 table %s as a source\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction setAll(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction set(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 123}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NjAyNjcz", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-544602673", "createdAt": "2020-12-04T01:57:38Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMTo1NzozOFrOH-9WvA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMTo1NzozOFrOH-9WvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3OTAwNA==", "bodyText": "How is hive handled? Do we just assume that the partitions are a supported format?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535779004", "createdAt": "2020-12-04T01:57:38Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 50}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NjAzNzE3", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-544603717", "createdAt": "2020-12-04T02:00:50Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjowMDo1MFrOH-9bAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjowMDo1MFrOH-9bAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc4MDA5OQ==", "bodyText": "It would probably be better to use a mutable map because this one will reject duplicate properties instead of overwriting.\nYou can also use ImmutableMap.builder() instead of supplying the key and value types.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535780099", "createdAt": "2020-12-04T02:00:50Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    try {\n+      Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 76}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NjAzOTM1", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-544603935", "createdAt": "2020-12-04T02:01:26Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjowMToyN1rOH-9b5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjowMToyN1rOH-9b5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc4MDMyNA==", "bodyText": "Can't this use stagedTable properties instead?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535780324", "createdAt": "2020-12-04T02:01:27Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    try {\n+      Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+          .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+          .putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava())\n+          .putAll(tableLocationProperties(sourceTableLocation()))\n+          .putAll(additionalProperties())\n+          .build();\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 92}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NjA0MDc5", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-544604079", "createdAt": "2020-12-04T02:01:52Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjowMTo1MlrOH-9ckA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjowMTo1MlrOH-9ckA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc4MDQ5Ng==", "bodyText": "Since we know the properties are in the table, should we look up this location from properties?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535780496", "createdAt": "2020-12-04T02:01:52Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    try {\n+      Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+          .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+          .putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava())\n+          .putAll(tableLocationProperties(sourceTableLocation()))\n+          .putAll(additionalProperties())\n+          .build();\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        applyDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 96}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NjA0MzU1", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-544604355", "createdAt": "2020-12-04T02:02:42Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjowMjo0MlrOH-9dtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjowMjo0MlrOH-9dtw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc4MDc5MQ==", "bodyText": "This is another place where we might want to update our own API rather than creating a special instance (v2BackupIdentifier) to pass through.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535780791", "createdAt": "2020-12-04T02:02:42Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    try {\n+      Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+          .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+          .putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava())\n+          .putAll(tableLocationProperties(sourceTableLocation()))\n+          .putAll(additionalProperties())\n+          .build();\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        applyDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 102}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NjE1Njgw", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-544615680", "createdAt": "2020-12-04T02:37:00Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjozNzowMVrOH--K0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjozNzowMVrOH--K0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MjMzOQ==", "bodyText": "I think it is usually better to do work like this in a finally block using a threw boolean:\nboolean threw = true;\ntry {\n  // something that might fail\n  threw = false;\n} finally {\n  if (threw) {\n    // clean up\n  }\n}\nThat has the advantage that whatever happened in the block, you get the correct exception rather than needing to throw a generic RuntimeException which would make it difficult to use this action in a higher-level application. Also, there are technically throwables that are not Exception.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535792339", "createdAt": "2020-12-04T02:37:01Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    try {\n+      Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+          .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+          .putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava())\n+          .putAll(tableLocationProperties(sourceTableLocation()))\n+          .putAll(additionalProperties())\n+          .build();\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        applyDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+    } catch (Exception e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 105}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NjE1ODM5", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-544615839", "createdAt": "2020-12-04T02:37:34Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjozNzozNFrOH--LiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjozNzozNFrOH--LiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MjUyMA==", "bodyText": "Since the changes are only staged, this should happen after restoring the backup table, in case there is a failure in the abort.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535792520", "createdAt": "2020-12-04T02:37:34Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    try {\n+      Map<String, String> newTableProperties = new ImmutableMap.Builder<String, String>()\n+          .put(TableCatalog.PROP_PROVIDER, \"iceberg\")\n+          .putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava())\n+          .putAll(tableLocationProperties(sourceTableLocation()))\n+          .putAll(additionalProperties())\n+          .build();\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        applyDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+    } catch (Exception e) {\n+      LOG.error(\"Error when attempting perform migration changes, aborting table creation and restoring backup\", e);\n+\n+      try {\n+        if (stagedTable != null) {\n+          stagedTable.abortStagedChanges();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 110}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NjE2MjI0", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-544616224", "createdAt": "2020-12-04T02:38:54Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjozODo1NFrOH--NKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjozODo1NFrOH--NKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MjkzOA==", "bodyText": "I think of \"apply\" as using the name mapping. What about \"add\" instead because this creates it and adds it to the table?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535792938", "createdAt": "2020-12-04T02:38:54Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableName);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableName), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use a non-v1 table %s as a source\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction setAll(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction set(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableName;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {\n+    return destTableName;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  protected static Map<String, String> tableLocationProperties(String tableLocation) {\n+    return ImmutableMap.of(\n+        TableProperties.WRITE_METADATA_LOCATION, tableLocation + \"/\" + ICEBERG_METADATA_FOLDER,\n+        TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation\n+    );\n+  }\n+\n+  protected static void applyDefaultTableNameMapping(Table table) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 167}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NjE2NDY0", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-544616464", "createdAt": "2020-12-04T02:39:42Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjozOTo0MlrOH--OLw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjozOTo0MlrOH--OLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MzE5OQ==", "bodyText": "Here as well, this will fail if the properties conflict. Since we know that Iceberg won't modify them it is safe to use a mutable map.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535793199", "createdAt": "2020-12-04T02:39:42Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private final String destTableLocation;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+    // Cannot check if the table location that would be generated by the catalog will match the source\n+    this.destTableLocation = null;\n+  }\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName, String destTableLocation) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+    Preconditions.checkArgument(!sourceTableLocation().equals(destTableLocation),\n+        \"Cannot create snapshot where destination location is the same as the source location. This\" +\n+            \"would cause a mixing of original table created and snapshot created files.\");\n+    this.destTableLocation = destTableLocation;\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        applyDefaultTableNameMapping(icebergTable);\n+      }\n+    } catch (TableAlreadyExistsException taeException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because a table already exists with that name\",\n+          taeException);\n+    } catch (NoSuchNamespaceException nsnException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because the namespace given does not exist\",\n+          nsnException);\n+    }\n+\n+    try {\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableName(), destTableName(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+\n+    } catch (Exception e) {\n+      LOG.error(\"Error when attempting to commit snapshot changes, rolling back\", e);\n+      if (stagedTable != null) {\n+        stagedTable.abortStagedChanges();\n+      }\n+      throw e;\n+    }\n+\n+    long numMigratedFiles;\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+    LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    return numMigratedFiles;\n+  }\n+\n+  private Map<String, String> buildPropertyMap() {\n+    ImmutableMap.Builder<String, String> propBuilder = ImmutableMap.<String, String>builder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 117}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NjE2NzI2", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-544616726", "createdAt": "2020-12-04T02:40:32Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjo0MDozMlrOH--POQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjo0MDozMlrOH--POQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MzQ2NQ==", "bodyText": "Can you add more whitespace between control flow blocks in this PR?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535793465", "createdAt": "2020-12-04T02:40:32Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/SparkActions.java", "diffHunk": "@@ -20,10 +20,75 @@\n package org.apache.iceberg.actions;\n \n import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.Spark3Util;\n import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n \n-class SparkActions extends Actions {\n+public class SparkActions extends Actions {\n   protected SparkActions(SparkSession spark, Table table) {\n     super(spark, table);\n   }\n+\n+  public static CreateAction migrate(String tableName) {\n+    return migrate(SparkSession.active(), tableName);\n+  }\n+\n+  public static CreateAction migrate(SparkSession spark, String tableName) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+    try {\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, tableName);\n+    } catch (ParseException e) {\n+      throw new IllegalArgumentException(\"Cannot parse migrate target\", e);\n+    }\n+\n+    return new Spark3MigrateAction(spark, catalogAndIdentifier.catalog(), catalogAndIdentifier.identifier());\n+  }\n+\n+  public static CreateAction snapshot(String sourceId, String destId) {\n+    return snapshot(SparkSession.active(), sourceId, destId);\n+  }\n+\n+  public static CreateAction snapshot(SparkSession spark, String sourceId, String destId) {\n+    Spark3Util.CatalogAndIdentifier sourceIdent;\n+    try {\n+      sourceIdent = Spark3Util.catalogAndIdentifier(spark, sourceId);\n+    } catch (ParseException e) {\n+      throw new IllegalArgumentException(\"Cannot parse snapshot source\", e);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NjE3Mzk0", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-544617394", "createdAt": "2020-12-04T02:42:40Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjo0Mjo0MFrOH--SDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjo0Mjo0MFrOH--SDg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5NDE5MA==", "bodyText": "Why is the location passed to snapshot? That seems like something that should be set on the action instead because it isn't required.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535794190", "createdAt": "2020-12-04T02:42:40Z", "author": {"login": "rdblue"}, "path": "spark3/src/test/java/org/apache/iceberg/actions/TestCreateActions.java", "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.commons.io.FileUtils;\n+import org.apache.commons.io.filefilter.TrueFileFilter;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.source.SimpleRecord;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SaveMode;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runners.Parameterized;\n+import scala.Some;\n+import scala.collection.Seq;\n+\n+public class TestCreateActions extends SparkCatalogTestBase {\n+  private static final String CREATE_PARTITIONED_PARQUET = \"CREATE TABLE %s (id INT, data STRING) \" +\n+      \"using parquet PARTITIONED BY (id) LOCATION '%s'\";\n+  private static final String CREATE_PARQUET = \"CREATE TABLE %s (id INT, data STRING) \" +\n+      \"using parquet LOCATION '%s'\";\n+  private static final String CREATE_HIVE_EXTERNAL_PARQUET = \"CREATE EXTERNAL TABLE %s (data STRING) \" +\n+      \"PARTITIONED BY (id INT) STORED AS parquet LOCATION '%s'\";\n+  private static final String CREATE_HIVE_PARQUET = \"CREATE TABLE %s (data STRING) \" +\n+      \"PARTITIONED BY (id INT) STORED AS parquet\";\n+\n+  private static final String NAMESPACE = \"default\";\n+\n+  @Parameterized.Parameters(name = \"Catalog Name {0} - Options {2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )},\n+        new Object[] {\"spark_catalog\", SparkSessionCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hadoop\",\n+            \"default-namespace\", \"default\",\n+            \"parquet-enabled\", \"true\",\n+            \"cache-enabled\", \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+        )},\n+        new Object[] { \"testhive\", SparkCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hive\",\n+            \"default-namespace\", \"default\"\n+        )},\n+        new Object[] { \"testhadoop\", SparkCatalog.class.getName(), ImmutableMap.of(\n+            \"type\", \"hadoop\",\n+            \"default-namespace\", \"default\"\n+        )}\n+    };\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private String baseTableName = \"baseTable\";\n+  private File tableDir;\n+  private String tableLocation;\n+  private final String type;\n+  private final TableCatalog catalog;\n+\n+  public TestCreateActions(\n+      String catalogName,\n+      String implementation,\n+      Map<String, String> config) {\n+    super(catalogName, implementation, config);\n+    this.catalog = (TableCatalog) spark.sessionState().catalogManager().catalog(catalogName);\n+    this.type = config.get(\"type\");\n+  }\n+\n+  @Before\n+  public void before() {\n+    try {\n+      this.tableDir = temp.newFolder();\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+    this.tableLocation = tableDir.toURI().toString();\n+\n+    spark.conf().set(\"hive.exec.dynamic.partition\", \"true\");\n+    spark.conf().set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\");\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+\n+    List<SimpleRecord> expected = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\"),\n+        new SimpleRecord(3, \"c\")\n+    );\n+\n+    Dataset<Row> df = spark.createDataFrame(expected, SimpleRecord.class);\n+\n+    df.select(\"id\", \"data\").orderBy(\"data\").write()\n+        .mode(\"append\")\n+        .option(\"path\", tableLocation)\n+        .saveAsTable(baseTableName);\n+  }\n+\n+  @After\n+  public void after() throws IOException {\n+    // Drop the hive table.\n+    spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n+  }\n+\n+  @Test\n+  public void testMigratePartitioned() throws Exception {\n+    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n+    Assume.assumeTrue(\"Can only migrate from Spark Session Catalog\", catalog.name().equals(\"spark_catalog\"));\n+    String source = sourceName(\"test_migrate_partitioned_table\");\n+    String dest = source;\n+    createSourceTable(CREATE_PARTITIONED_PARQUET, source);\n+    assertMigratedFileCount(Actions.migrate(source), source, dest);\n+  }\n+\n+  @Test\n+  public void testMigrateUnpartitioned() throws Exception {\n+    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n+    Assume.assumeTrue(\"Can only migrate from Spark Session Catalog\", catalog.name().equals(\"spark_catalog\"));\n+    String source = sourceName(\"test_migrate_unpartitioned_table\");\n+    String dest = source;\n+    createSourceTable(CREATE_PARQUET, source);\n+    assertMigratedFileCount(Actions.migrate(source), source, dest);\n+  }\n+\n+  @Test\n+  public void testSnapshotPartitioned() throws Exception {\n+    Assume.assumeTrue(\"Cannot snapshot with arbitrary location in a hadoop based catalog\",\n+        !type.equals(\"hadoop\"));\n+    File location = temp.newFolder();\n+    String source = sourceName(\"test_snapshot_partitioned_table\");\n+    String dest = destName(\"iceberg_snapshot_partitioned\");\n+    createSourceTable(CREATE_PARTITIONED_PARQUET, source);\n+    assertMigratedFileCount(Actions.snapshot(source, dest, location.toString()), source, dest);\n+    assertIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testSnapshotUnpartitioned() throws Exception {\n+    Assume.assumeTrue(\"Cannot snapshot with arbitrary location in a hadoop based catalog\",\n+        !type.equals(\"hadoop\"));\n+    File location = temp.newFolder();\n+    String source = sourceName(\"test_snapshot_unpartitioned_table\");\n+    String dest = destName(\"iceberg_snapshot_unpartitioned\");\n+    createSourceTable(CREATE_PARQUET, source);\n+    assertMigratedFileCount(Actions.snapshot(source, dest, location.toString()), source, dest);\n+    assertIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testSnapshotHiveTable() throws Exception {\n+    Assume.assumeTrue(\"Cannot snapshot with arbitrary location in a hadoop based catalog\",\n+        !type.equals(\"hadoop\"));\n+    File location = temp.newFolder();\n+    String source = sourceName(\"snapshot_hive_table\");\n+    String dest = destName(\"iceberg_snapshot_hive_table\");\n+    createSourceTable(CREATE_HIVE_EXTERNAL_PARQUET, source);\n+    assertMigratedFileCount(Actions.snapshot(source, dest, location.toString()), source, dest);\n+    assertIsolatedSnapshot(source, dest);\n+  }\n+\n+  @Test\n+  public void testMigrateHiveTable() throws Exception {\n+    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n+    String source = sourceName(\"migrate_hive_table\");\n+    String dest = source;\n+    createSourceTable(CREATE_HIVE_EXTERNAL_PARQUET, source);\n+    assertMigratedFileCount(Actions.migrate(source), source, dest);\n+  }\n+\n+  @Test\n+  public void testSnapshotManagedHiveTable() throws Exception {\n+    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n+    File location = temp.newFolder();\n+    String source = sourceName(\"snapshot_managed_hive_table\");\n+    String dest = destName(\"iceberg_snapshot_managed_hive_table\");\n+    createSourceTable(CREATE_HIVE_PARQUET, source);\n+    assertMigratedFileCount(Actions.snapshot(source, dest, location.toString()), source, dest);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 222}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ0NjE3NzQx", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-544617741", "createdAt": "2020-12-04T02:43:37Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjo0MzozN1rOH--TUQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMjo0MzozN1rOH--TUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5NDUxMw==", "bodyText": "I would probably remove this. I don't see a need to supply location this way and it adds complexity with an additional dynamic method call.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r535794513", "createdAt": "2020-12-04T02:43:37Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/Actions.java", "diffHunk": "@@ -77,6 +82,120 @@ public ExpireSnapshotsAction expireSnapshots() {\n     return new ExpireSnapshotsAction(spark, table);\n   }\n \n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be represented by it's\n+   * previous provider in the session catalog and a new metadata directory will be created at the table's location.\n+   *\n+   * @param tableName Table to be converted\n+   * @return {@link CreateAction} to perform migration\n+   */\n+  public static CreateAction migrate(String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), String.class).buildStaticChecked()\n+          .invoke(tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Migrate is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Converts the provided table into an Iceberg table in place. The table will no longer be accesible by it's\n+   * previous implementation\n+   *\n+   * @param tableName Table to be converted\n+   * @param spark     Spark session to use for looking up table\n+   * @return {@link CreateAction} to perform migration\n+   */\n+  public static CreateAction migrate(SparkSession spark, String tableName) {\n+    try {\n+      return DynMethods.builder(\"migrate\")\n+          .impl(implClass(), SparkSession.class, String.class).buildStaticChecked()\n+          .invoke(spark, tableName);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Migrate is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Creates an independent Iceberg table based on a given table. The new Iceberg table can be altered, appended or\n+   * deleted without causing any change to the original. New data and metadata will be created in the default\n+   * location for tables of this name in the destination catalog.\n+   *\n+   * @param sourceTable Original table which is the basis for the new Iceberg table\n+   * @param destTable   New Iceberg table being created\n+   * @return {@link CreateAction} to perform snapshot\n+   */\n+  public static CreateAction snapshot(SparkSession spark, String sourceTable, String destTable) {\n+    try {\n+      return DynMethods.builder(\"snapshot\")\n+          .impl(implClass(), SparkSession.class, String.class, String.class).buildStaticChecked()\n+          .invoke(spark, sourceTable, destTable);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Creates an independent Iceberg table based on a given table. The new Iceberg table can be altered, appended or\n+   * deleted without causing any change to the original. New data and metadata will be created in the default\n+   * location for tables of this name in the destination catalog.\n+   *\n+   * @param sourceTable Original table which is the basis for the new Iceberg table\n+   * @param destTable   New Iceberg table being created\n+   * @return {@link CreateAction} to perform snapshot\n+   */\n+  public static CreateAction snapshot(String sourceTable, String destTable) {\n+    try {\n+      return DynMethods.builder(\"snapshot\")\n+          .impl(implClass(), String.class, String.class).buildStaticChecked()\n+          .invoke(sourceTable, destTable);\n+    } catch (NoSuchMethodException ex) {\n+      throw new UnsupportedOperationException(\"Snapshot is not implemented for this version of Spark\");\n+    }\n+  }\n+\n+  /**\n+   * Creates an independent Iceberg table based on a given table. The new Iceberg table can be altered, appended or\n+   * deleted without causing any change to the original. New data and metadata will be created in the\n+   * new location passed to this method.\n+   *\n+   * @param sourceTable Original table which is the basis for the new Iceberg table\n+   * @param destTable   New Iceberg table being created\n+   * @param location Location for metadata and new data for the new table\n+   * @return {@link CreateAction} to perform snapshot\n+   */\n+  public static CreateAction snapshot(String sourceTable, String destTable, String location) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 110}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ1MTYwOTMw", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-545160930", "createdAt": "2020-12-04T17:34:14Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxNzozNDoxNFrOH_a4cw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxNzozNDoxNFrOH_a4cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI2Mjc3MQ==", "bodyText": "Should it be withProperties like we have in Catalog.Builder?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536262771", "createdAt": "2020-12-04T17:34:14Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  CreateAction setAll(Map<String, String> properties);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ1MTYxMTQ2", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-545161146", "createdAt": "2020-12-04T17:34:32Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxNzozNDozMlrOH_a5Qw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxNzozNDozMlrOH_a5Qw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI2Mjk3OQ==", "bodyText": "And withProperty here?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536262979", "createdAt": "2020-12-04T17:34:32Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  CreateAction setAll(Map<String, String> properties);\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties\n+   * with the same key name will be overwritten.\n+   * @param key the key of the property to add\n+   * @param value the value of the property to add\n+   * @return this for chaining\n+   */\n+  CreateAction set(String key, String value);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ1MTcxODQ2", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-545171846", "createdAt": "2020-12-04T17:49:14Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxNzo0OToxNFrOH_bcDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxNzo0OToxNFrOH_bcDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI3MTg4Nw==", "bodyText": "nit: formatting is off", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536271887", "createdAt": "2020-12-04T17:49:14Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 71}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ1MTg3MTYz", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-545187163", "createdAt": "2020-12-04T18:10:31Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxODoxMDozMVrOH_cOQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxODoxMDozMVrOH_cOQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI4NDczNg==", "bodyText": "Seems like it will be possible to snapshot a non-Iceberg Hive table as Iceberg Hadoop table. I thought for a sec we should disable such cases but there is probably no good reason. We can snapshot tables from other catalogs.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536284736", "createdAt": "2020-12-04T18:10:31Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 64}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ1MTk0MzI1", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-545194325", "createdAt": "2020-12-04T18:21:12Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxODoyMToxMlrOH_cmXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxODoyMToxMlrOH_cmXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI5MDkxMA==", "bodyText": "This must always be TableCatalog, right? Should we validate this early and call checkSourceCatalog on it? The validation method can be void and can operate on TableCatalog.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536290910", "createdAt": "2020-12-04T18:21:12Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ1MTk2MjE1", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-545196215", "createdAt": "2020-12-04T18:23:59Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxODoyMzo1OVrOH_csiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxODoyMzo1OVrOH_csiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI5MjQ5MQ==", "bodyText": "nit: sourceTableIdent?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536292491", "createdAt": "2020-12-04T18:23:59Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 60}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ1MTk3Nzgx", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-545197781", "createdAt": "2020-12-04T18:26:12Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxODoyNjoxMlrOH_cxOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxODoyNjoxMlrOH_cxOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjI5MzY5MA==", "bodyText": "nit: destTableIdent?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536293690", "createdAt": "2020-12-04T18:26:12Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableName;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 65}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ1MjIyNDYw", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-545222460", "createdAt": "2020-12-04T19:03:32Z", "commit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxOTowMzozMlrOH_eDdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQxOTowMzozMlrOH_eDdQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjMxNDc0MQ==", "bodyText": "Why do we have this method in parent class? Isn't the location config specific to the operation type? We have to set the data location to the table location only in MIGRATE.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r536314741", "createdAt": "2020-12-04T19:03:32Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.AnalysisException;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableName;\n+  private final PartitionSpec sourcePartitionSpec;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableName;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName,\n+                       CatalogPlugin destCatalog,  Identifier destTableName) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableName = sourceTableName;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableName = destTableName;\n+\n+    try {\n+      String sourceString = String.join(\".\", sourceTableName.namespace()) + \".\" + sourceTableName.name();\n+      sourcePartitionSpec = SparkSchemaUtil.specForTable(spark, sourceString);\n+    } catch (AnalysisException e) {\n+      throw new IllegalArgumentException(\"Cannot determining partitioning of \" + sourceTableName.toString(), e);\n+    }\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableName);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableName), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use a non-v1 table %s as a source\", sourceTableName), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction setAll(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction set(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableName;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {\n+    return destTableName;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  protected static Map<String, String> tableLocationProperties(String tableLocation) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "09f329bc39e8a2a1b2b5f925c580a2177833510b"}, "originalPosition": 160}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f0be57cd8c89582495bd39caafacd0734510fd03", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/f0be57cd8c89582495bd39caafacd0734510fd03", "committedDate": "2020-12-04T20:39:16Z", "message": "Various Reviewer Comments"}, "afterCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/e5636c7804ffd321253aa0d3a775e0c2a7c10e68", "committedDate": "2020-12-04T21:01:07Z", "message": "Various Reviewer Comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MjMzOTAw", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547233900", "createdAt": "2020-12-08T13:30:23Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzozMDoyM1rOIBbgrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzozMDoyM1rOIBbgrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM3MDIyMA==", "bodyText": "I am fine with returning a number of imported files but if someone has any ideas about other useful info, we may consider adding a result class.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538370220", "createdAt": "2020-12-08T13:30:23Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 24}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MjM1NjIz", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547235623", "createdAt": "2020-12-08T13:32:25Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzozMjoyNVrOIBboww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzozMjoyNVrOIBboww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM3MjI5MQ==", "bodyText": "nit: extra line", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538372291", "createdAt": "2020-12-08T13:32:25Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  CreateAction withProperties(Map<String, String> properties);\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties\n+   * with the same key name will be overwritten.\n+   * @param key the key of the property to add\n+   * @param value the value of the property to add\n+   * @return this for chaining\n+   */\n+  CreateAction withProperty(String key, String value);\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 42}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MjM4MTQ0", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547238144", "createdAt": "2020-12-08T13:35:29Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzozNToyOVrOIBb06A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzozNToyOVrOIBb06A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM3NTQwMA==", "bodyText": "nit: Table -> table", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538375400", "createdAt": "2020-12-08T13:35:29Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 27}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MjM4MjIy", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547238222", "createdAt": "2020-12-08T13:35:34Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzozNTozNFrOIBb1TA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzozNTozNFrOIBb1TA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM3NTUwMA==", "bodyText": "nit: Table -> table", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538375500", "createdAt": "2020-12-08T13:35:34Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/CreateAction.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Map;\n+\n+interface CreateAction extends Action<Long> {\n+\n+  /**\n+   * Adds additional properties to the newly created Iceberg Table. Any properties with\n+   * the same key name will be overwritten.\n+   * @param properties a map of properties to be included\n+   * @return this for chaining\n+   */\n+  CreateAction withProperties(Map<String, String> properties);\n+\n+  /**\n+   * Adds an additional property to the newly created Iceberg Table. Any properties", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 35}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MjQwNTk2", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547240596", "createdAt": "2020-12-08T13:38:31Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzozODozMVrOIBcBig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzozODozMVrOIBcBig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM3ODYzNA==", "bodyText": "nit: formatting is off", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538378634", "createdAt": "2020-12-08T13:38:31Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 67}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MjQ0NDg1", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547244485", "createdAt": "2020-12-08T13:43:18Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzo0MzoxOFrOIBcUjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzo0MzoxOFrOIBcUjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM4MzUwMw==", "bodyText": "Can we mark it final?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538383503", "createdAt": "2020-12-08T13:43:18Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 64}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MjU1NjQ3", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547255647", "createdAt": "2020-12-08T13:56:20Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzo1NjoyMVrOIBdRFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzo1NjoyMVrOIBdRFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODM5ODk5Ng==", "bodyText": "It looks like having a separate method to check if it is an Iceberg catalog would make it more readable than having two negations.\nOr we could use ValidationException. It also gives formatting for free.\n    ValidationException.check(\n        catalog instanceof SparkSessionCatalog || catalog instanceof SparkCatalog,\n        \"Unsupported destination catalog. Catalog %s is %s but must be either %s or %s\",\n        catalog.name(), catalog.getClass().getName(), SparkSessionCatalog.class.getName(), SparkCatalog.class.getName());", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538398996", "createdAt": "2020-12-08T13:56:21Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  protected static Map<String, String> tableLocationProperties(String tableLocation) {\n+    return ImmutableMap.of(\n+        TableProperties.WRITE_METADATA_LOCATION, tableLocation + \"/\" + ICEBERG_METADATA_FOLDER,\n+        TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation\n+    );\n+  }\n+\n+  protected static void assignDefaultTableNameMapping(Table table) {\n+    NameMapping nameMapping = MappingUtil.create(table.schema());\n+    String nameMappingJson = NameMappingParser.toJson(nameMapping);\n+    table.updateProperties().set(TableProperties.DEFAULT_NAME_MAPPING, nameMappingJson).commit();\n+  }\n+\n+  private static StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {\n+    if (!(catalog instanceof SparkSessionCatalog) && !(catalog instanceof SparkCatalog)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 163}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MjU2OTQy", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547256942", "createdAt": "2020-12-08T13:57:46Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzo1Nzo0NlrOIBdYSg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzo1Nzo0NlrOIBdYSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQwMDg0Mg==", "bodyText": "Why static?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538400842", "createdAt": "2020-12-08T13:57:46Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  protected static Map<String, String> tableLocationProperties(String tableLocation) {\n+    return ImmutableMap.of(\n+        TableProperties.WRITE_METADATA_LOCATION, tableLocation + \"/\" + ICEBERG_METADATA_FOLDER,\n+        TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation\n+    );\n+  }\n+\n+  protected static void assignDefaultTableNameMapping(Table table) {\n+    NameMapping nameMapping = MappingUtil.create(table.schema());\n+    String nameMappingJson = NameMappingParser.toJson(nameMapping);\n+    table.updateProperties().set(TableProperties.DEFAULT_NAME_MAPPING, nameMappingJson).commit();\n+  }\n+\n+  private static StagingTableCatalog checkDestinationCatalog(CatalogPlugin catalog) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 162}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MjU3Mzgx", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547257381", "createdAt": "2020-12-08T13:58:15Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzo1ODoxNVrOIBdaXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxMzo1ODoxNVrOIBdaXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQwMTM3NQ==", "bodyText": "Same here. Why static?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538401375", "createdAt": "2020-12-08T13:58:15Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+\n+    if (sourceTable.storage().locationUri().isEmpty()) {\n+      throw new IllegalArgumentException(\"Cannot create an Iceberg table from a source without an explicit location\");\n+    }\n+  }\n+\n+  protected static Map<String, String> tableLocationProperties(String tableLocation) {\n+    return ImmutableMap.of(\n+        TableProperties.WRITE_METADATA_LOCATION, tableLocation + \"/\" + ICEBERG_METADATA_FOLDER,\n+        TableProperties.WRITE_NEW_DATA_LOCATION, tableLocation\n+    );\n+  }\n+\n+  protected static void assignDefaultTableNameMapping(Table table) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 156}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MjYyMDc3", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547262077", "createdAt": "2020-12-08T14:03:34Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNDowMzozNFrOIBdvOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNDowMzozNFrOIBdvOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQwNjcxNA==", "bodyText": "I think it will be more readable to use Preconditions here.\n    Preconditions.checkArgument(ALLOWED_SOURCES.contains(sourceTableProvider),\n        \"Cannot create an Iceberg table from source provider: %s\",\n        sourceTableProvider);", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538406714", "createdAt": "2020-12-08T14:03:34Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 139}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MjYyMjIx", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547262221", "createdAt": "2020-12-08T14:03:43Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNDowMzo0M1rOIBdv2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNDowMzo0M1rOIBdv2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQwNjg3NQ==", "bodyText": "same here", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538406875", "createdAt": "2020-12-08T14:03:43Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));\n+    }\n+\n+    if (sourceTable.storage().locationUri().isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 144}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MzE4MzM3", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547318337", "createdAt": "2020-12-08T15:01:29Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNTowMTozMFrOIBhjwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNTowMTozMFrOIBhjwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ2OTMxMg==", "bodyText": "It looks like the staging part is same in both. Can we factor this out into the parent class?\n  protected StagedTable stageDestTable() {\n    try {\n      Map<String, String> props = targetTableProps();\n      StructType schema = sourceTable.schema();\n      Transform[] partitioning = sourceTable.partitioning();\n      return destCatalog.stageCreate(destTableIdent, schema, partitioning, props);\n    } catch (NoSuchNamespaceException e) {\n      throw new IllegalArgumentException(\"Cannot create a new table in a namespace which does not exist\", e);\n    } catch (TableAlreadyExistsException e) {\n      throw new IllegalArgumentException(\"Destination table already exists\", e);\n    }\n  }\n\n  protected abstract Map<String, String> targetTableProps();\n\n\nThen we can override targetTableProps in each subclass and make sure we add extra metadata like migrated or snapshot flags if we want to.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538469312", "createdAt": "2020-12-08T15:01:30Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      Map<String, String> newTableProperties = new HashMap<>();\n+      newTableProperties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+      newTableProperties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+      newTableProperties.putAll(tableLocationProperties(sourceTableLocation()));\n+      newTableProperties.putAll(additionalProperties());\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 84}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MzI0ODc3", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547324877", "createdAt": "2020-12-08T15:07:59Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNTowODowMFrOIBh_GQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNTowODowMFrOIBh_GQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ3NjMxMw==", "bodyText": "I don't think this is correct. I think we should allow custom data and metadata folders in SNAPSHOT.\nHere is how get the metadata location internally:\n  def getMetadataLocation(table: Table): String = {\n    table.properties.getOrDefault(\n      TableProperties.WRITE_METADATA_LOCATION,\n      table.location + \"/\" + ICEBERG_METADATA_FOLDER)\n  }\n\nI believe we should do the same here.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538476313", "createdAt": "2020-12-08T15:08:00Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+    } catch (TableAlreadyExistsException taeException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because a table already exists with that name\",\n+          taeException);\n+    } catch (NoSuchNamespaceException nsnException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because the namespace given does not exist\",\n+          nsnException);\n+    }\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MzI2Nzc3", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547326777", "createdAt": "2020-12-08T15:09:50Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNTowOTo1MFrOIBiHbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNTowOTo1MFrOIBiHbg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ3ODQ0Ng==", "bodyText": "How about we add a check that the table location, as well as metadata and data locations, do not interfere with the original table? This way we don't have to know the default table location the catalog will assign.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538478446", "createdAt": "2020-12-08T15:09:50Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 67}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MzI4Njg2", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547328686", "createdAt": "2020-12-08T15:11:43Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToxMTo0M1rOIBiPgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToxMTo0M1rOIBiPgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4MDUxNQ==", "bodyText": "I think stagedTable is never null and this check is redundant.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538480515", "createdAt": "2020-12-08T15:11:43Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+    } catch (TableAlreadyExistsException taeException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because a table already exists with that name\",\n+          taeException);\n+    } catch (NoSuchNamespaceException nsnException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because the namespace given does not exist\",\n+          nsnException);\n+    }\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableName(), destTableName(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting to commit snapshot changes, rolling back\");\n+        if (stagedTable != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 93}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MzI5NzQ3", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547329747", "createdAt": "2020-12-08T15:12:50Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToxMjo1MFrOIBiUQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToxMjo1MFrOIBiUQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4MTcyOQ==", "bodyText": "Why cannot we just do the rollback in a catch clause? This seems a bit too complicated to me.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538481729", "createdAt": "2020-12-08T15:12:50Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+    } catch (TableAlreadyExistsException taeException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because a table already exists with that name\",\n+          taeException);\n+    } catch (NoSuchNamespaceException nsnException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because the namespace given does not exist\",\n+          nsnException);\n+    }\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableName(), destTableName(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 91}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MzMwNTE3", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547330517", "createdAt": "2020-12-08T15:13:35Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToxMzozNlrOIBiXew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToxMzozNlrOIBiXew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4MjU1NQ==", "bodyText": "I think it can be Long.parseLong to avoid boxing.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538482555", "createdAt": "2020-12-08T15:13:36Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+    } catch (TableAlreadyExistsException taeException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because a table already exists with that name\",\n+          taeException);\n+    } catch (NoSuchNamespaceException nsnException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because the namespace given does not exist\",\n+          nsnException);\n+    }\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableName(), destTableName(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting to commit snapshot changes, rolling back\");\n+        if (stagedTable != null) {\n+          stagedTable.abortStagedChanges();\n+        }\n+      }\n+    }\n+\n+    long numMigratedFiles;\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 101}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MzMxMDgz", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547331083", "createdAt": "2020-12-08T15:14:09Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToxNDowOVrOIBiZ2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToxNDowOVrOIBiZ2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4MzE2MQ==", "bodyText": "Can we define the var on the same line? It should fit.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538483161", "createdAt": "2020-12-08T15:14:09Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+    } catch (TableAlreadyExistsException taeException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because a table already exists with that name\",\n+          taeException);\n+    } catch (NoSuchNamespaceException nsnException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because the namespace given does not exist\",\n+          nsnException);\n+    }\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableName(), destTableName(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting to commit snapshot changes, rolling back\");\n+        if (stagedTable != null) {\n+          stagedTable.abortStagedChanges();\n+        }\n+      }\n+    }\n+\n+    long numMigratedFiles;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 99}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MzMyODM0", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547332834", "createdAt": "2020-12-08T15:15:53Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToxNTo1M1rOIBihDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToxNTo1M1rOIBihDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4NTAwNQ==", "bodyText": "I think we miss   at the end of this line. I'd also split it in between sentences.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538485005", "createdAt": "2020-12-08T15:15:53Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+    } catch (TableAlreadyExistsException taeException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because a table already exists with that name\",\n+          taeException);\n+    } catch (NoSuchNamespaceException nsnException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because the namespace given does not exist\",\n+          nsnException);\n+    }\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableName(), destTableName(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting to commit snapshot changes, rolling back\");\n+        if (stagedTable != null) {\n+          stagedTable.abortStagedChanges();\n+        }\n+      }\n+    }\n+\n+    long numMigratedFiles;\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+    LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    return numMigratedFiles;\n+  }\n+\n+  private Map<String, String> buildPropertyMap() {\n+    Map<String, String> properties = new HashMap<>();\n+    properties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+    properties.put(TableProperties.GC_ENABLED, \"false\");\n+    properties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+    properties.putAll(additionalProperties());\n+\n+    // Don't use the default location for the destination table if an alternate has be set\n+    if (destTableLocation != null) {\n+      properties.putAll(tableLocationProperties(destTableLocation));\n+    }\n+\n+    return properties;\n+  }\n+\n+  @Override\n+  protected TableCatalog checkSourceCatalog(CatalogPlugin catalog) {\n+    // Currently the Import code relies on being able to look up the table in the session code\n+    if (!(catalog.name().equals(\"spark_catalog\"))) {\n+      throw new IllegalArgumentException(String.format(\n+          \"Cannot snapshot a table that isn't in spark_catalog, the session catalog. \" +\n+              \"Found source catalog %s\", catalog.name()));\n+    }\n+\n+    if (!(catalog instanceof TableCatalog)) {\n+      throw new IllegalArgumentException(String.format(\n+          \"Cannot snapshot a table from a non-table catalog %s. Catalog has class of %s.\", catalog.name(),\n+          catalog.getClass().toString()\n+      ));\n+    }\n+\n+    return (TableCatalog) catalog;\n+  }\n+\n+  @Override\n+  public SnapshotAction withLocation(String location) {\n+    Preconditions.checkArgument(!sourceTableLocation().equals(location),\n+        \"Cannot create snapshot where destination location is the same as the source location. This\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 143}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MzM3NTU4", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547337558", "createdAt": "2020-12-08T15:20:40Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyMDo0MFrOIBi0ZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyMDo0MFrOIBi0ZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ4OTk1Nw==", "bodyText": "Do we need to remove any properties, @rdblue? I think we remove at least path internally.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538489957", "createdAt": "2020-12-08T15:20:40Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      Map<String, String> newTableProperties = new HashMap<>();\n+      newTableProperties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 79}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MzQwNTIx", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547340521", "createdAt": "2020-12-08T15:23:43Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyMzo0M1rOIBjBlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyMzo0M1rOIBjBlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ5MzMzMg==", "bodyText": "Do we always have a namespace?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538493332", "createdAt": "2020-12-08T15:23:43Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      Map<String, String> newTableProperties = new HashMap<>();\n+      newTableProperties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+      newTableProperties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+      newTableProperties.putAll(tableLocationProperties(sourceTableLocation()));\n+      newTableProperties.putAll(additionalProperties());\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!stagedTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = stagedTable.properties().get(TableProperties.WRITE_METADATA_LOCATION);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 101}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MzQxNjcw", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547341670", "createdAt": "2020-12-08T15:24:51Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyNDo1MlrOIBjGwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyNDo1MlrOIBjGwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ5NDY1Ng==", "bodyText": "nit: sourceTableIdent and destTableIdent", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538494656", "createdAt": "2020-12-08T15:24:52Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 55}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MzQyMjgz", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547342283", "createdAt": "2020-12-08T15:25:27Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyNToyOFrOIBjJjw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyNToyOFrOIBjJjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ5NTM3NQ==", "bodyText": "nit: destTableIdent?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538495375", "createdAt": "2020-12-08T15:25:28Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 128}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MzQyNTE4", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547342518", "createdAt": "2020-12-08T15:25:42Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyNTo0MlrOIBjKoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyNTo0MlrOIBjKoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ5NTY0OA==", "bodyText": "nit: sourceTableIdent?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538495648", "createdAt": "2020-12-08T15:25:42Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 116}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MzQzNDM4", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547343438", "createdAt": "2020-12-08T15:26:39Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyNjo0MFrOIBjOUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyNjo0MFrOIBjOUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ5NjU5NA==", "bodyText": "If we move staging to the parent class, this would also be simplified. It is a bit hard to follow now.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538496594", "createdAt": "2020-12-08T15:26:40Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      Map<String, String> newTableProperties = new HashMap<>();\n+      newTableProperties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+      newTableProperties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+      newTableProperties.putAll(tableLocationProperties(sourceTableLocation()));\n+      newTableProperties.putAll(additionalProperties());\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!stagedTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = stagedTable.properties().get(TableProperties.WRITE_METADATA_LOCATION);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+\n+    } catch (TableAlreadyExistsException tableAlreadyExistsException) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 108}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MzQ0MTM0", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547344134", "createdAt": "2020-12-08T15:27:21Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyNzoyMVrOIBjRpQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyNzoyMVrOIBjRpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODQ5NzQ0NQ==", "bodyText": "nit: parseLong", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538497445", "createdAt": "2020-12-08T15:27:21Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      Map<String, String> newTableProperties = new HashMap<>();\n+      newTableProperties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+      newTableProperties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+      newTableProperties.putAll(tableLocationProperties(sourceTableLocation()));\n+      newTableProperties.putAll(additionalProperties());\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!stagedTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = stagedTable.properties().get(TableProperties.WRITE_METADATA_LOCATION);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+\n+    } catch (TableAlreadyExistsException tableAlreadyExistsException) {\n+      throw new IllegalArgumentException(\"Cannot migrate because a table was created with the same name as the \" +\n+          \"original was created after we renamed the original table to a backup identifier.\",\n+          tableAlreadyExistsException);\n+    } catch (NoSuchNamespaceException noSuchNamespaceException) {\n+      throw new IllegalArgumentException(\"Cannot migrate because the namespace for the table no longer exists after\" +\n+          \"we renamed the original table to a backup identifier.\",\n+          noSuchNamespaceException);\n+    } finally {\n+\n+      if (threw) {\n+        LOG.error(\"Error when attempting perform migration changes, aborting table creation and restoring backup.\");\n+        try {\n+          destCatalog().renameTable(backupIdentifier, sourceTableName());\n+        } catch (NoSuchTableException nstException) {\n+          throw new IllegalArgumentException(\"Cannot restore backup, the backup cannot be found\", nstException);\n+        } catch (TableAlreadyExistsException taeException) {\n+          throw new IllegalArgumentException(String.format(\"Cannot restore backup, a table with the original name \" +\n+              \"exists. The backup can be found with the name %s\", backupIdentifier.toString()), taeException);\n+        }\n+\n+        try {\n+          if (stagedTable != null) {\n+            stagedTable.abortStagedChanges();\n+          }\n+        } catch (Exception abortException) {\n+          LOG.error(\"Unable to abort staged changes\", abortException);\n+        }\n+      }\n+    }\n+\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    long numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 140}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3MzQ2NzQ1", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547346745", "createdAt": "2020-12-08T15:29:51Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyOTo1MVrOIBjczQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNToyOTo1MVrOIBjczQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODUwMDMwMQ==", "bodyText": "Do we really want to throw a new exception rather than rethrwoing the original exception?\nI kind of liked our internal utility tryWithSafeFinallyAndFailureCallbacks that we copied from Spark for such things.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538500301", "createdAt": "2020-12-08T15:29:51Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      Map<String, String> newTableProperties = new HashMap<>();\n+      newTableProperties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+      newTableProperties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+      newTableProperties.putAll(tableLocationProperties(sourceTableLocation()));\n+      newTableProperties.putAll(additionalProperties());\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!stagedTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = stagedTable.properties().get(TableProperties.WRITE_METADATA_LOCATION);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+\n+    } catch (TableAlreadyExistsException tableAlreadyExistsException) {\n+      throw new IllegalArgumentException(\"Cannot migrate because a table was created with the same name as the \" +\n+          \"original was created after we renamed the original table to a backup identifier.\",\n+          tableAlreadyExistsException);\n+    } catch (NoSuchNamespaceException noSuchNamespaceException) {\n+      throw new IllegalArgumentException(\"Cannot migrate because the namespace for the table no longer exists after\" +\n+          \"we renamed the original table to a backup identifier.\",\n+          noSuchNamespaceException);\n+    } finally {\n+\n+      if (threw) {\n+        LOG.error(\"Error when attempting perform migration changes, aborting table creation and restoring backup.\");\n+        try {\n+          destCatalog().renameTable(backupIdentifier, sourceTableName());\n+        } catch (NoSuchTableException nstException) {\n+          throw new IllegalArgumentException(\"Cannot restore backup, the backup cannot be found\", nstException);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 123}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3NTU2OTg1", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547556985", "createdAt": "2020-12-08T19:23:33Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxOToyMzozM1rOIByQ7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxOToyMzozM1rOIByQ7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc0MzAyMw==", "bodyText": "Not a blocker, but we will probably want to relax this constraint when we support direct access to Hive catalogs. Provider is specific to Spark, so if we were to load directly nothing would detect the provider. And in any case, if the provider is \"hive\", it's basically delegating to the partition-level checks. This is nice to catch problems early, though.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538743023", "createdAt": "2020-12-08T19:23:33Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3CreateAction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.mapping.MappingUtil;\n+import org.apache.iceberg.mapping.NameMapping;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.SparkCatalog;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.catalog.CatalogTable;\n+import org.apache.spark.sql.catalyst.catalog.CatalogUtils;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagingTableCatalog;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.apache.spark.sql.connector.catalog.V1Table;\n+import org.apache.spark.sql.connector.expressions.Transform;\n+\n+abstract class Spark3CreateAction implements CreateAction {\n+  private static final Set<String> ALLOWED_SOURCES = ImmutableSet.of(\"parquet\", \"avro\", \"orc\", \"hive\");\n+  protected static final String ICEBERG_METADATA_FOLDER = \"metadata\";\n+\n+  private final SparkSession spark;\n+\n+  // Source Fields\n+  private final V1Table sourceTable;\n+  private final CatalogTable sourceCatalogTable;\n+  private final String sourceTableLocation;\n+  private final CatalogPlugin sourceCatalog;\n+  private final Identifier sourceTableIdent;\n+\n+  // Destination Fields\n+  private final StagingTableCatalog destCatalog;\n+  private final Identifier destTableIdent;\n+\n+  // Optional Parameters for destination\n+  private Map<String, String> additionalProperties = Maps.newHashMap();\n+\n+  Spark3CreateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableIdent,\n+                       CatalogPlugin destCatalog, Identifier destTableIdent) {\n+\n+    this.spark = spark;\n+    this.sourceCatalog = checkSourceCatalog(sourceCatalog);\n+    this.sourceTableIdent = sourceTableIdent;\n+    this.destCatalog = checkDestinationCatalog(destCatalog);\n+    this.destTableIdent = destTableIdent;\n+\n+    try {\n+      this.sourceTable = (V1Table) ((TableCatalog) sourceCatalog).loadTable(sourceTableIdent);\n+      this.sourceCatalogTable = sourceTable.v1Table();\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot not find source table %s\", sourceTableIdent), e);\n+    } catch (ClassCastException e) {\n+      throw new IllegalArgumentException(String.format(\"Cannot use non-v1 table %s as a source\", sourceTableIdent), e);\n+    }\n+    validateSourceTable(sourceCatalogTable);\n+\n+    this.sourceTableLocation = CatalogUtils.URIToString(sourceCatalogTable.storage().locationUri().get());\n+  }\n+\n+  @Override\n+  public CreateAction withProperties(Map<String, String> properties) {\n+    this.additionalProperties.putAll(properties);\n+    return this;\n+  }\n+\n+  @Override\n+  public CreateAction withProperty(String key, String value) {\n+    this.additionalProperties.put(key, value);\n+    return this;\n+  }\n+\n+  protected SparkSession spark() {\n+    return spark;\n+  }\n+\n+  protected String sourceTableLocation() {\n+    return sourceTableLocation;\n+  }\n+\n+  protected CatalogTable v1SourceTable() {\n+    return sourceCatalogTable;\n+  }\n+\n+  protected CatalogPlugin sourceCatalog() {\n+    return sourceCatalog;\n+  }\n+\n+  protected Identifier sourceTableName() {\n+    return sourceTableIdent;\n+  }\n+\n+  protected Transform[] sourcePartitionSpec() {\n+    return sourceTable.partitioning();\n+  }\n+\n+  protected StagingTableCatalog destCatalog() {\n+    return destCatalog;\n+  }\n+\n+  protected Identifier destTableName() {\n+    return destTableIdent;\n+  }\n+\n+  protected Map<String, String> additionalProperties() {\n+    return additionalProperties;\n+  }\n+\n+  private static void validateSourceTable(CatalogTable sourceTable) {\n+    String sourceTableProvider = sourceTable.provider().get().toLowerCase(Locale.ROOT);\n+\n+    if (!ALLOWED_SOURCES.contains(sourceTableProvider)) {\n+      throw new IllegalArgumentException(\n+          String.format(\"Cannot create an Iceberg table from source provider: %s\", sourceTableProvider));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 141}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3NTY2MjM4", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547566238", "createdAt": "2020-12-08T19:36:04Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxOTozNjowNFrOIBywRA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxOTozNjowNFrOIBywRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc1MTA0NA==", "bodyText": "Instead of throwing IllegalArgumentException, what about throwing the Iceberg exception for these cases?", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538751044", "createdAt": "2020-12-08T19:36:04Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      Map<String, String> newTableProperties = new HashMap<>();\n+      newTableProperties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+      newTableProperties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+      newTableProperties.putAll(tableLocationProperties(sourceTableLocation()));\n+      newTableProperties.putAll(additionalProperties());\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!stagedTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = stagedTable.properties().get(TableProperties.WRITE_METADATA_LOCATION);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+\n+    } catch (TableAlreadyExistsException tableAlreadyExistsException) {\n+      throw new IllegalArgumentException(\"Cannot migrate because a table was created with the same name as the \" +\n+          \"original was created after we renamed the original table to a backup identifier.\",\n+          tableAlreadyExistsException);\n+    } catch (NoSuchNamespaceException noSuchNamespaceException) {\n+      throw new IllegalArgumentException(\"Cannot migrate because the namespace for the table no longer exists after\" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 113}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3NTY4MDUy", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547568052", "createdAt": "2020-12-08T19:38:29Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxOTozODoyOVrOIBy2VQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxOTozODoyOVrOIBy2VQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc1MjU5Nw==", "bodyText": "Precondition here as well.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538752597", "createdAt": "2020-12-08T19:38:29Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Some;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Takes a Spark table in the sourceCatalog and attempts to transform it into an Iceberg\n+ * Table in the same location with the same identifier. Once complete the identifier which\n+ * previously referred to a non-iceberg table will refer to the newly migrated iceberg\n+ * table.\n+ */\n+class Spark3MigrateAction extends Spark3CreateAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3MigrateAction.class);\n+  private static final String BACKUP_SUFFIX = \"_BACKUP_\";\n+\n+  Spark3MigrateAction(SparkSession spark, CatalogPlugin sourceCatalog, Identifier sourceTableName) {\n+    super(spark, sourceCatalog, sourceTableName, sourceCatalog, sourceTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    Table icebergTable;\n+\n+    // Move source table to a new name, halting all modifications and allowing us to stage\n+    // the creation of a new Iceberg table in its place\n+    String backupName = sourceTableName().name() + BACKUP_SUFFIX;\n+    Identifier backupIdentifier = Identifier.of(sourceTableName().namespace(), backupName);\n+    try {\n+      destCatalog().renameTable(sourceTableName(), backupIdentifier);\n+    } catch (NoSuchTableException e) {\n+      throw new IllegalArgumentException(\"Cannot find table to migrate\", e);\n+    } catch (TableAlreadyExistsException e) {\n+      throw new IllegalArgumentException(\"Cannot rename migration source to backup name\", e);\n+    }\n+\n+    StagedTable stagedTable = null;\n+    boolean threw = true;\n+    try {\n+      Map<String, String> newTableProperties = new HashMap<>();\n+      newTableProperties.put(TableCatalog.PROP_PROVIDER, \"iceberg\");\n+      newTableProperties.putAll(JavaConverters.mapAsJavaMapConverter(v1SourceTable().properties()).asJava());\n+      newTableProperties.putAll(tableLocationProperties(sourceTableLocation()));\n+      newTableProperties.putAll(additionalProperties());\n+\n+      stagedTable = destCatalog().stageCreate(Identifier.of(\n+          destTableName().namespace(),\n+          destTableName().name()),\n+          v1SourceTable().schema(),\n+          sourcePartitionSpec(),\n+          newTableProperties);\n+\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!stagedTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+\n+      String stagingLocation = stagedTable.properties().get(TableProperties.WRITE_METADATA_LOCATION);\n+\n+      LOG.info(\"Beginning migration of {} using metadata location {}\", sourceTableName(), stagingLocation);\n+\n+      Some<String> backupNamespace = Some.apply(backupIdentifier.namespace()[0]);\n+      TableIdentifier v1BackupIdentifier = new TableIdentifier(backupIdentifier.name(), backupNamespace);\n+      SparkTableUtil.importSparkTable(spark(), v1BackupIdentifier, icebergTable, stagingLocation);\n+\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+\n+    } catch (TableAlreadyExistsException tableAlreadyExistsException) {\n+      throw new IllegalArgumentException(\"Cannot migrate because a table was created with the same name as the \" +\n+          \"original was created after we renamed the original table to a backup identifier.\",\n+          tableAlreadyExistsException);\n+    } catch (NoSuchNamespaceException noSuchNamespaceException) {\n+      throw new IllegalArgumentException(\"Cannot migrate because the namespace for the table no longer exists after\" +\n+          \"we renamed the original table to a backup identifier.\",\n+          noSuchNamespaceException);\n+    } finally {\n+\n+      if (threw) {\n+        LOG.error(\"Error when attempting perform migration changes, aborting table creation and restoring backup.\");\n+        try {\n+          destCatalog().renameTable(backupIdentifier, sourceTableName());\n+        } catch (NoSuchTableException nstException) {\n+          throw new IllegalArgumentException(\"Cannot restore backup, the backup cannot be found\", nstException);\n+        } catch (TableAlreadyExistsException taeException) {\n+          throw new IllegalArgumentException(String.format(\"Cannot restore backup, a table with the original name \" +\n+              \"exists. The backup can be found with the name %s\", backupIdentifier.toString()), taeException);\n+        }\n+\n+        try {\n+          if (stagedTable != null) {\n+            stagedTable.abortStagedChanges();\n+          }\n+        } catch (Exception abortException) {\n+          LOG.error(\"Unable to abort staged changes\", abortException);\n+        }\n+      }\n+    }\n+\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    long numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+    LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    return numMigratedFiles;\n+  }\n+\n+  @Override\n+  protected TableCatalog checkSourceCatalog(CatalogPlugin catalog) {\n+    // Currently the Import code relies on being able to look up the table in the session code\n+    if (!(catalog instanceof SparkSessionCatalog)) {\n+      throw new IllegalArgumentException(String.format(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 149}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3NTcwMzA0", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547570304", "createdAt": "2020-12-08T19:41:34Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxOTo0MTozNFrOIBy9tg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxOTo0MTozNFrOIBy9tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc1NDQ4Ng==", "bodyText": "Since this logic is the same in both places, this could be moved into the assign function as ensureNameMappingPresent or something.", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538754486", "createdAt": "2020-12-08T19:41:34Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 69}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3NTcxODYw", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547571860", "createdAt": "2020-12-08T19:43:45Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxOTo0Mzo0NVrOIBzDHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxOTo0Mzo0NVrOIBzDHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc1NTg2OQ==", "bodyText": "Nit: prefer Maps.newHashMap().", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538755869", "createdAt": "2020-12-08T19:43:45Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/Spark3SnapshotAction.java", "diffHunk": "@@ -0,0 +1,148 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.SnapshotSummary;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.SparkTableUtil;\n+import org.apache.iceberg.spark.source.SparkTable;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.TableIdentifier;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException;\n+import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n+import org.apache.spark.sql.connector.catalog.CatalogPlugin;\n+import org.apache.spark.sql.connector.catalog.Identifier;\n+import org.apache.spark.sql.connector.catalog.StagedTable;\n+import org.apache.spark.sql.connector.catalog.TableCatalog;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.collection.JavaConverters;\n+\n+/**\n+ * Creates a new Iceberg table based on a source Spark table. The new Iceberg table will\n+ * have a different data and metadata directory allowing it to exist independently of the\n+ * source table.\n+ */\n+class Spark3SnapshotAction extends Spark3CreateAction implements SnapshotAction {\n+  private static final Logger LOG = LoggerFactory.getLogger(Spark3SnapshotAction.class);\n+\n+  private String destTableLocation = null;\n+\n+  Spark3SnapshotAction(SparkSession spark, CatalogPlugin sourceCatalog,\n+                       Identifier sourceTableName, CatalogPlugin destCatalog,\n+                       Identifier destTableName) {\n+    super(spark, sourceCatalog, sourceTableName, destCatalog, destTableName);\n+  }\n+\n+  @Override\n+  public Long execute() {\n+    StagedTable stagedTable;\n+    Table icebergTable;\n+\n+    try {\n+      stagedTable = destCatalog().stageCreate(destTableName(), v1SourceTable().schema(),\n+          sourcePartitionSpec(), buildPropertyMap());\n+      icebergTable = ((SparkTable) stagedTable).table();\n+\n+      if (!icebergTable.properties().containsKey(TableProperties.DEFAULT_NAME_MAPPING)) {\n+        assignDefaultTableNameMapping(icebergTable);\n+      }\n+    } catch (TableAlreadyExistsException taeException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because a table already exists with that name\",\n+          taeException);\n+    } catch (NoSuchNamespaceException nsnException) {\n+      throw new IllegalArgumentException(\"Cannot create snapshot because the namespace given does not exist\",\n+          nsnException);\n+    }\n+\n+    boolean threw = true;\n+    try {\n+      String stagingLocation = icebergTable.location() + \"/\" + ICEBERG_METADATA_FOLDER;\n+      LOG.info(\"Beginning snapshot of {} to {} using metadata location {}\", sourceTableName(), destTableName(),\n+          stagingLocation);\n+\n+      TableIdentifier v1TableIdentifier = v1SourceTable().identifier();\n+      SparkTableUtil.importSparkTable(spark(), v1TableIdentifier, icebergTable, stagingLocation);\n+      stagedTable.commitStagedChanges();\n+      threw = false;\n+    } finally {\n+      if (threw) {\n+        LOG.error(\"Error when attempting to commit snapshot changes, rolling back\");\n+        if (stagedTable != null) {\n+          stagedTable.abortStagedChanges();\n+        }\n+      }\n+    }\n+\n+    long numMigratedFiles;\n+    Snapshot snapshot = icebergTable.currentSnapshot();\n+    numMigratedFiles = Long.valueOf(snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP));\n+    LOG.info(\"Successfully loaded Iceberg metadata for {} files\", numMigratedFiles);\n+    return numMigratedFiles;\n+  }\n+\n+  private Map<String, String> buildPropertyMap() {\n+    Map<String, String> properties = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 107}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3NTc1MDg3", "url": "https://github.com/apache/iceberg/pull/1525#pullrequestreview-547575087", "createdAt": "2020-12-08T19:48:27Z", "commit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxOTo0ODoyN1rOIBzOQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxOTo0ODoyN1rOIBzOQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc1ODcyMQ==", "bodyText": "There are quite a few try/catch blocks for this. Maybe add a version that doesn't throw ParseException and pass in a context string to form the error message. Also, the error message should include the table name that failed parsing:\npublic static CatalogAndIdentifier catalogAndIdentifier(String description, SparkSession spark, String tableName) {\n  try {\n    return catalogAndIdentifier(spark, tableName);\n  } catch (ParseException e) {\n    throw new IllegalArgumentException(\"Cannot parse \" + description + \": \" + tableName, e);\n  }\n}", "url": "https://github.com/apache/iceberg/pull/1525#discussion_r538758721", "createdAt": "2020-12-08T19:48:27Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/iceberg/actions/SparkActions.java", "diffHunk": "@@ -20,10 +20,54 @@\n package org.apache.iceberg.actions;\n \n import org.apache.iceberg.Table;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.spark.Spark3Util;\n import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n \n-class SparkActions extends Actions {\n+public class SparkActions extends Actions {\n   protected SparkActions(SparkSession spark, Table table) {\n     super(spark, table);\n   }\n+\n+  public static CreateAction migrate(String tableName) {\n+    return migrate(SparkSession.active(), tableName);\n+  }\n+\n+  public static CreateAction migrate(SparkSession spark, String tableName) {\n+    Spark3Util.CatalogAndIdentifier catalogAndIdentifier;\n+    try {\n+      catalogAndIdentifier = Spark3Util.catalogAndIdentifier(spark, tableName);\n+    } catch (ParseException e) {\n+      throw new IllegalArgumentException(\"Cannot parse migrate target\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5636c7804ffd321253aa0d3a775e0c2a7c10e68"}, "originalPosition": 24}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a6b02d26dae1a1c3e6182b86646c832dd2a37640", "author": {"user": {"login": "RussellSpitzer", "name": "Russell Spitzer"}}, "url": "https://github.com/apache/iceberg/commit/a6b02d26dae1a1c3e6182b86646c832dd2a37640", "committedDate": "2020-12-09T16:51:06Z", "message": "Provide API and Implementation for Creating Iceberg Tables from Spark\n\nPreviously the SparkUtil class provided an importSparkTable command but\nthis command suffered from a few shortcomings. It had a difficult api and\ndid not work directly with DSV2 Iceberg Catalogs. To provide a simpler interface\nthat shares a similar pattern to currently available Spark Actions we introduce\nActions.migrate and Actions.snapshot.\n\nThrough these commands we can both migrate a Table as well as make a snapshot\nof an existing table."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3863, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}