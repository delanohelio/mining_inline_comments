{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY5MTAwNDMx", "number": 1352, "title": "Support row-level deletes with IcebergGenerics", "bodyText": "This adds support for applying row-level deletes when reading with IcebergGenerics.\nThis also refactors the classes used by the IcebergGenerics reader so that generics can be used more easily in other applications. It adds GenericReader that will open tasks (either file tasks or combined tasks) and will automatically build a row-based CloseableIterator<Record> that removes deleted rows and applies task residual filters.", "createdAt": "2020-08-17T22:33:27Z", "url": "https://github.com/apache/iceberg/pull/1352", "merged": true, "mergeCommit": {"oid": "0b9d9940dc49602da585fd6bdc937f3cb1df70c0"}, "closed": true, "closedAt": "2020-08-24T20:58:11Z", "author": {"login": "rdblue"}, "timelineItems": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdAHdEHgFqTQ2OTQ4MjY4Nw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdCHXBfAFqTQ3Mzc3NTM1Mg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NDgyNjg3", "url": "https://github.com/apache/iceberg/pull/1352#pullrequestreview-469482687", "createdAt": "2020-08-18T13:58:19Z", "commit": {"oid": "b48d9043704978653c138a42ac65fd400b321275"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMzo1ODoxOVrOHCV5Ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMzo1ODoxOVrOHCV5Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIxNzkxMA==", "bodyText": "Shouldn't this be 2?", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472217910", "createdAt": "2020-08-18T13:58:19Z", "author": {"login": "RussellSpitzer"}, "path": "api/src/main/java/org/apache/iceberg/data/Record.java", "diffHunk": "@@ -35,4 +36,26 @@\n   Record copy();\n \n   Record copy(Map<String, Object> overwriteValues);\n+\n+  default Record copy(String field, Object value) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);\n+    overwriteValues.put(field, value);\n+    return copy(overwriteValues);\n+  }\n+\n+  default Record copy(String field1, Object value1, String field2, Object value2) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b48d9043704978653c138a42ac65fd400b321275"}, "originalPosition": 20}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NDgyODAy", "url": "https://github.com/apache/iceberg/pull/1352#pullrequestreview-469482802", "createdAt": "2020-08-18T13:58:26Z", "commit": {"oid": "b48d9043704978653c138a42ac65fd400b321275"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMzo1ODoyN1rOHCV5nA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxMzo1ODoyN1rOHCV5nA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIxODAxMg==", "bodyText": "And this 3?", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472218012", "createdAt": "2020-08-18T13:58:27Z", "author": {"login": "RussellSpitzer"}, "path": "api/src/main/java/org/apache/iceberg/data/Record.java", "diffHunk": "@@ -35,4 +36,26 @@\n   Record copy();\n \n   Record copy(Map<String, Object> overwriteValues);\n+\n+  default Record copy(String field, Object value) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);\n+    overwriteValues.put(field, value);\n+    return copy(overwriteValues);\n+  }\n+\n+  default Record copy(String field1, Object value1, String field2, Object value2) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);\n+    overwriteValues.put(field1, value1);\n+    overwriteValues.put(field2, value2);\n+    return copy(overwriteValues);\n+  }\n+\n+  default Record copy(String field1, Object value1, String field2, Object value2, String field3, Object value3) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b48d9043704978653c138a42ac65fd400b321275"}, "originalPosition": 27}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NTA4NDcw", "url": "https://github.com/apache/iceberg/pull/1352#pullrequestreview-469508470", "createdAt": "2020-08-18T14:25:00Z", "commit": {"oid": "b48d9043704978653c138a42ac65fd400b321275"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNDoyNTowMFrOHCXFFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNDoyNTowMFrOHCXFFQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIzNzMzMw==", "bodyText": "Should we have a Precondition that sortedDeletesByPartition is not null in the constructor?", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472237333", "createdAt": "2020-08-18T14:25:00Z", "author": {"login": "RussellSpitzer"}, "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -78,6 +82,10 @@\n     this.sortedDeletesByPartition = sortedDeletesByPartition;\n   }\n \n+  public boolean isEmpty() {\n+    return (globalDeletes == null || globalDeletes.length == 0) && sortedDeletesByPartition.isEmpty();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b48d9043704978653c138a42ac65fd400b321275"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NTIzNjc3", "url": "https://github.com/apache/iceberg/pull/1352#pullrequestreview-469523677", "createdAt": "2020-08-18T14:40:09Z", "commit": {"oid": "b48d9043704978653c138a42ac65fd400b321275"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNDo0MDowOVrOHCXx8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNDo0MDowOVrOHCXx8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI0ODgxOQ==", "bodyText": "Is this different than just lexicographically comparing the bytes of the path with the byte buffer? Just wondering if converting the path to bytes may be cheaper than than buffer to string. Not a big deal though and probably has no real perf impacts at this location.", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472248819", "createdAt": "2020-08-18T14:40:09Z", "author": {"login": "RussellSpitzer"}, "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +104,157 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, specsById.get(file.specId()).schema()))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        return canContainPosDeletesForFile(dataFile, deleteFile);\n+\n+      case EQUALITY_DELETES:\n+        return canContainEqDeletesForFile(dataFile, deleteFile, schema);\n+    }\n+\n+    return true;\n+  }\n+\n+  private static boolean canContainPosDeletesForFile(DataFile dataFile, DeleteFile deleteFile) {\n+    // check that the delete file can contain the data file's file_path\n+    Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+    if (lowers == null || uppers == null) {\n+      return true;\n+    }\n+\n+    Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+    int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+    ByteBuffer lower = lowers.get(pathId);\n+    if (lower != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b48d9043704978653c138a42ac65fd400b321275"}, "originalPosition": 97}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NTM1MTM3", "url": "https://github.com/apache/iceberg/pull/1352#pullrequestreview-469535137", "createdAt": "2020-08-18T14:51:34Z", "commit": {"oid": "b48d9043704978653c138a42ac65fd400b321275"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNDo1MTozNVrOHCYUyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNDo1MTozNVrOHCYUyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI1NzczOQ==", "bodyText": "This is a rather complicated function and It would be great help if we had a few comments about the kind of checks we are doing.", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472257739", "createdAt": "2020-08-18T14:51:35Z", "author": {"login": "RussellSpitzer"}, "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +104,157 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, specsById.get(file.specId()).schema()))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        return canContainPosDeletesForFile(dataFile, deleteFile);\n+\n+      case EQUALITY_DELETES:\n+        return canContainEqDeletesForFile(dataFile, deleteFile, schema);\n+    }\n+\n+    return true;\n+  }\n+\n+  private static boolean canContainPosDeletesForFile(DataFile dataFile, DeleteFile deleteFile) {\n+    // check that the delete file can contain the data file's file_path\n+    Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+    if (lowers == null || uppers == null) {\n+      return true;\n+    }\n+\n+    Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+    int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+    ByteBuffer lower = lowers.get(pathId);\n+    if (lower != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {\n+      return false;\n     }\n+\n+    ByteBuffer upper = uppers.get(pathId);\n+    if (upper != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, upper)) > 0) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private static boolean canContainEqDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b48d9043704978653c138a42ac65fd400b321275"}, "originalPosition": 111}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NTM3NTA4", "url": "https://github.com/apache/iceberg/pull/1352#pullrequestreview-469537508", "createdAt": "2020-08-18T14:53:52Z", "commit": {"oid": "b48d9043704978653c138a42ac65fd400b321275"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNDo1Mzo1MlrOHCYbtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNDo1Mzo1MlrOHCYbtw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI1OTUxMQ==", "bodyText": "Just checking but in this map we will never have a 0 right? Just making sure we aren't ignore nullValueCount.get(X) == 0", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472259511", "createdAt": "2020-08-18T14:53:52Z", "author": {"login": "RussellSpitzer"}, "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +104,157 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, specsById.get(file.specId()).schema()))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        return canContainPosDeletesForFile(dataFile, deleteFile);\n+\n+      case EQUALITY_DELETES:\n+        return canContainEqDeletesForFile(dataFile, deleteFile, schema);\n+    }\n+\n+    return true;\n+  }\n+\n+  private static boolean canContainPosDeletesForFile(DataFile dataFile, DeleteFile deleteFile) {\n+    // check that the delete file can contain the data file's file_path\n+    Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+    if (lowers == null || uppers == null) {\n+      return true;\n+    }\n+\n+    Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+    int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+    ByteBuffer lower = lowers.get(pathId);\n+    if (lower != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {\n+      return false;\n     }\n+\n+    ByteBuffer upper = uppers.get(pathId);\n+    if (upper != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, upper)) > 0) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private static boolean canContainEqDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    if (dataFile.lowerBounds() == null || dataFile.upperBounds() == null ||\n+        deleteFile.lowerBounds() == null || deleteFile.upperBounds() == null) {\n+      return true;\n+    }\n+\n+    Map<Integer, ByteBuffer> dataLowers = dataFile.lowerBounds();\n+    Map<Integer, ByteBuffer> dataUppers = dataFile.upperBounds();\n+    Map<Integer, ByteBuffer> deleteLowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> deleteUppers = deleteFile.upperBounds();\n+\n+    Map<Integer, Long> dataNullCounts = dataFile.nullValueCounts();\n+    Map<Integer, Long> dataValueCounts = dataFile.valueCounts();\n+    Map<Integer, Long> deleteNullCounts = deleteFile.nullValueCounts();\n+    Map<Integer, Long> deleteValueCounts = deleteFile.valueCounts();\n+\n+    for (int id : deleteFile.equalityFieldIds()) {\n+      Types.NestedField field = schema.findField(id);\n+      if (!field.type().isPrimitiveType()) {\n+        return true;\n+      }\n+\n+      if (allNull(dataNullCounts, dataValueCounts, field) && allNonNull(deleteNullCounts, field)) {\n+        return false;\n+      }\n+\n+      if (allNull(deleteNullCounts, deleteValueCounts, field) && allNonNull(dataNullCounts, field)) {\n+        return false;\n+      }\n+\n+      ByteBuffer dataLower = dataLowers.get(id);\n+      ByteBuffer dataUpper = dataUppers.get(id);\n+      ByteBuffer deleteLower = deleteLowers.get(id);\n+      ByteBuffer deleteUpper = deleteUppers.get(id);\n+      if (dataLower == null || dataUpper == null || deleteLower == null || deleteUpper == null) {\n+        return true;\n+      }\n+\n+      if (!rangesOverlap(field.type().asPrimitiveType(), dataLower, dataUpper, deleteLower, deleteUpper)) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  private static <T> boolean rangesOverlap(Type.PrimitiveType type,\n+                                           ByteBuffer dataLowerBuf, ByteBuffer dataUpperBuf,\n+                                           ByteBuffer deleteLowerBuf, ByteBuffer deleteUpperBuf) {\n+    Comparator<T> comparator = Comparators.forType(type);\n+    T dataLower = Conversions.fromByteBuffer(type, dataLowerBuf);\n+    T dataUpper = Conversions.fromByteBuffer(type, dataUpperBuf);\n+    T deleteLower = Conversions.fromByteBuffer(type, deleteLowerBuf);\n+    T deleteUpper = Conversions.fromByteBuffer(type, deleteUpperBuf);\n+\n+    return comparator.compare(deleteLower, dataUpper) <= 0 && comparator.compare(dataLower, deleteUpper) <= 0;\n+  }\n+\n+  private static boolean allNonNull(Map<Integer, Long> nullValueCounts, Types.NestedField field) {\n+    if (field.isRequired()) {\n+      return true;\n+    }\n+\n+    if (nullValueCounts == null) {\n+      return false;\n+    }\n+\n+    Long nullValueCount = nullValueCounts.get(field.fieldId());\n+    if (nullValueCount == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b48d9043704978653c138a42ac65fd400b321275"}, "originalPosition": 179}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY5NTQwMTA4", "url": "https://github.com/apache/iceberg/pull/1352#pullrequestreview-469540108", "createdAt": "2020-08-18T14:56:25Z", "commit": {"oid": "b48d9043704978653c138a42ac65fd400b321275"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNDo1NjoyNVrOHCYj0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQxNDo1NjoyNVrOHCYj0Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI2MTU4NQ==", "bodyText": "Ah I see we do a 0 check here as well", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472261585", "createdAt": "2020-08-18T14:56:25Z", "author": {"login": "RussellSpitzer"}, "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +104,157 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, specsById.get(file.specId()).schema()))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        return canContainPosDeletesForFile(dataFile, deleteFile);\n+\n+      case EQUALITY_DELETES:\n+        return canContainEqDeletesForFile(dataFile, deleteFile, schema);\n+    }\n+\n+    return true;\n+  }\n+\n+  private static boolean canContainPosDeletesForFile(DataFile dataFile, DeleteFile deleteFile) {\n+    // check that the delete file can contain the data file's file_path\n+    Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+    if (lowers == null || uppers == null) {\n+      return true;\n+    }\n+\n+    Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+    int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+    ByteBuffer lower = lowers.get(pathId);\n+    if (lower != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {\n+      return false;\n     }\n+\n+    ByteBuffer upper = uppers.get(pathId);\n+    if (upper != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, upper)) > 0) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private static boolean canContainEqDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    if (dataFile.lowerBounds() == null || dataFile.upperBounds() == null ||\n+        deleteFile.lowerBounds() == null || deleteFile.upperBounds() == null) {\n+      return true;\n+    }\n+\n+    Map<Integer, ByteBuffer> dataLowers = dataFile.lowerBounds();\n+    Map<Integer, ByteBuffer> dataUppers = dataFile.upperBounds();\n+    Map<Integer, ByteBuffer> deleteLowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> deleteUppers = deleteFile.upperBounds();\n+\n+    Map<Integer, Long> dataNullCounts = dataFile.nullValueCounts();\n+    Map<Integer, Long> dataValueCounts = dataFile.valueCounts();\n+    Map<Integer, Long> deleteNullCounts = deleteFile.nullValueCounts();\n+    Map<Integer, Long> deleteValueCounts = deleteFile.valueCounts();\n+\n+    for (int id : deleteFile.equalityFieldIds()) {\n+      Types.NestedField field = schema.findField(id);\n+      if (!field.type().isPrimitiveType()) {\n+        return true;\n+      }\n+\n+      if (allNull(dataNullCounts, dataValueCounts, field) && allNonNull(deleteNullCounts, field)) {\n+        return false;\n+      }\n+\n+      if (allNull(deleteNullCounts, deleteValueCounts, field) && allNonNull(dataNullCounts, field)) {\n+        return false;\n+      }\n+\n+      ByteBuffer dataLower = dataLowers.get(id);\n+      ByteBuffer dataUpper = dataUppers.get(id);\n+      ByteBuffer deleteLower = deleteLowers.get(id);\n+      ByteBuffer deleteUpper = deleteUppers.get(id);\n+      if (dataLower == null || dataUpper == null || deleteLower == null || deleteUpper == null) {\n+        return true;\n+      }\n+\n+      if (!rangesOverlap(field.type().asPrimitiveType(), dataLower, dataUpper, deleteLower, deleteUpper)) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  private static <T> boolean rangesOverlap(Type.PrimitiveType type,\n+                                           ByteBuffer dataLowerBuf, ByteBuffer dataUpperBuf,\n+                                           ByteBuffer deleteLowerBuf, ByteBuffer deleteUpperBuf) {\n+    Comparator<T> comparator = Comparators.forType(type);\n+    T dataLower = Conversions.fromByteBuffer(type, dataLowerBuf);\n+    T dataUpper = Conversions.fromByteBuffer(type, dataUpperBuf);\n+    T deleteLower = Conversions.fromByteBuffer(type, deleteLowerBuf);\n+    T deleteUpper = Conversions.fromByteBuffer(type, deleteUpperBuf);\n+\n+    return comparator.compare(deleteLower, dataUpper) <= 0 && comparator.compare(dataLower, deleteUpper) <= 0;\n+  }\n+\n+  private static boolean allNonNull(Map<Integer, Long> nullValueCounts, Types.NestedField field) {\n+    if (field.isRequired()) {\n+      return true;\n+    }\n+\n+    if (nullValueCounts == null) {\n+      return false;\n+    }\n+\n+    Long nullValueCount = nullValueCounts.get(field.fieldId());\n+    if (nullValueCount == null) {\n+      return false;\n+    }\n+\n+    return nullValueCount <= 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b48d9043704978653c138a42ac65fd400b321275"}, "originalPosition": 183}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b48d9043704978653c138a42ac65fd400b321275", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/b48d9043704978653c138a42ac65fd400b321275", "committedDate": "2020-08-17T22:27:56Z", "message": "Apply row-level deletes when reading with IcebergGenerics."}, "afterCommit": {"oid": "fe86cc83cbe46ad95a925a2e486f9292ccc9b1b6", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/fe86cc83cbe46ad95a925a2e486f9292ccc9b1b6", "committedDate": "2020-08-18T21:57:36Z", "message": "Apply row-level deletes when reading with IcebergGenerics."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aaa13ae2a69ae3653e8774b7cf9c520488eb0154", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/aaa13ae2a69ae3653e8774b7cf9c520488eb0154", "committedDate": "2020-08-18T22:34:45Z", "message": "Apply row-level deletes when reading with IcebergGenerics."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fe86cc83cbe46ad95a925a2e486f9292ccc9b1b6", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/fe86cc83cbe46ad95a925a2e486f9292ccc9b1b6", "committedDate": "2020-08-18T21:57:36Z", "message": "Apply row-level deletes when reading with IcebergGenerics."}, "afterCommit": {"oid": "aaa13ae2a69ae3653e8774b7cf9c520488eb0154", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/aaa13ae2a69ae3653e8774b7cf9c520488eb0154", "committedDate": "2020-08-18T22:34:45Z", "message": "Apply row-level deletes when reading with IcebergGenerics."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c91af88e6083129be9eca4687c6cdf658be43f8a", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/c91af88e6083129be9eca4687c6cdf658be43f8a", "committedDate": "2020-08-18T23:28:28Z", "message": "Fix checkstyle problems."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcwMjMwNTY0", "url": "https://github.com/apache/iceberg/pull/1352#pullrequestreview-470230564", "createdAt": "2020-08-19T08:21:13Z", "commit": {"oid": "c91af88e6083129be9eca4687c6cdf658be43f8a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQwODoyMToxNFrOHC8hPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQwODoyMToxNFrOHC8hPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg1MDc1MA==", "bodyText": "For equality deletes, wouldn't we need the file projection to also include equality field ids if not already present?", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472850750", "createdAt": "2020-08-19T08:21:14Z", "author": {"login": "shardulm94"}, "path": "data/src/main/java/org/apache/iceberg/data/GenericReader.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.orc.GenericOrcReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Evaluator;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableGroup;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PartitionUtil;\n+import org.apache.iceberg.util.ProjectStructLike;\n+import org.apache.iceberg.util.StructLikeSet;\n+\n+class GenericReader implements Serializable {\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final FileIO io;\n+  private final Schema projection;\n+  private final boolean caseSensitive;\n+  private final boolean reuseContainers;\n+\n+  GenericReader(TableScan scan, boolean reuseContainers) {\n+    this.io = scan.table().io();\n+    this.projection = scan.schema();\n+    this.caseSensitive = scan.isCaseSensitive();\n+    this.reuseContainers = reuseContainers;\n+  }\n+\n+  CloseableIterator<Record> open(CloseableIterable<CombinedScanTask> tasks) {\n+    Iterable<FileScanTask> fileTasks = Iterables.concat(Iterables.transform(tasks, CombinedScanTask::files));\n+    return CloseableIterable.concat(Iterables.transform(fileTasks, this::open)).iterator();\n+  }\n+\n+  public CloseableIterable<Record> open(CombinedScanTask task) {\n+    return new CombinedTaskIterable(task);\n+  }\n+\n+  public CloseableIterable<Record> open(FileScanTask task) {\n+    List<DeleteFile> posDeletes = Lists.newArrayList();\n+    List<DeleteFile> eqDeletes = Lists.newArrayList();\n+    for (DeleteFile delete : task.deletes()) {\n+      switch (delete.content()) {\n+        case POSITION_DELETES:\n+          posDeletes.add(delete);\n+          break;\n+        case EQUALITY_DELETES:\n+          eqDeletes.add(delete);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n+      }\n+    }\n+\n+    Schema fileProjection = fileProjection(!posDeletes.isEmpty());\n+\n+    CloseableIterable<Record> records = openFile(task, fileProjection);\n+    records = applyPosDeletes(records, fileProjection, task.file().path(), posDeletes, task.file());\n+    records = applyEqDeletes(records, fileProjection, eqDeletes, task.file());\n+    records = applyResidual(records, fileProjection, task.residual());\n+\n+    return records;\n+  }\n+\n+  private Schema fileProjection(boolean hasPosDeletes) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c91af88e6083129be9eca4687c6cdf658be43f8a"}, "originalPosition": 117}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcwMjQwOTQw", "url": "https://github.com/apache/iceberg/pull/1352#pullrequestreview-470240940", "createdAt": "2020-08-19T08:34:47Z", "commit": {"oid": "c91af88e6083129be9eca4687c6cdf658be43f8a"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQwODozNDo0N1rOHC9BVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQwODo0NDo0N1rOHC9ZhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg1ODk2Ng==", "bodyText": "Does this mean equality deletes can only currently support deletes on top level fields?", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472858966", "createdAt": "2020-08-19T08:34:47Z", "author": {"login": "shardulm94"}, "path": "data/src/main/java/org/apache/iceberg/data/GenericReader.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.orc.GenericOrcReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Evaluator;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableGroup;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PartitionUtil;\n+import org.apache.iceberg.util.ProjectStructLike;\n+import org.apache.iceberg.util.StructLikeSet;\n+\n+class GenericReader implements Serializable {\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final FileIO io;\n+  private final Schema projection;\n+  private final boolean caseSensitive;\n+  private final boolean reuseContainers;\n+\n+  GenericReader(TableScan scan, boolean reuseContainers) {\n+    this.io = scan.table().io();\n+    this.projection = scan.schema();\n+    this.caseSensitive = scan.isCaseSensitive();\n+    this.reuseContainers = reuseContainers;\n+  }\n+\n+  CloseableIterator<Record> open(CloseableIterable<CombinedScanTask> tasks) {\n+    Iterable<FileScanTask> fileTasks = Iterables.concat(Iterables.transform(tasks, CombinedScanTask::files));\n+    return CloseableIterable.concat(Iterables.transform(fileTasks, this::open)).iterator();\n+  }\n+\n+  public CloseableIterable<Record> open(CombinedScanTask task) {\n+    return new CombinedTaskIterable(task);\n+  }\n+\n+  public CloseableIterable<Record> open(FileScanTask task) {\n+    List<DeleteFile> posDeletes = Lists.newArrayList();\n+    List<DeleteFile> eqDeletes = Lists.newArrayList();\n+    for (DeleteFile delete : task.deletes()) {\n+      switch (delete.content()) {\n+        case POSITION_DELETES:\n+          posDeletes.add(delete);\n+          break;\n+        case EQUALITY_DELETES:\n+          eqDeletes.add(delete);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n+      }\n+    }\n+\n+    Schema fileProjection = fileProjection(!posDeletes.isEmpty());\n+\n+    CloseableIterable<Record> records = openFile(task, fileProjection);\n+    records = applyPosDeletes(records, fileProjection, task.file().path(), posDeletes, task.file());\n+    records = applyEqDeletes(records, fileProjection, eqDeletes, task.file());\n+    records = applyResidual(records, fileProjection, task.residual());\n+\n+    return records;\n+  }\n+\n+  private Schema fileProjection(boolean hasPosDeletes) {\n+    if (hasPosDeletes) {\n+      List<Types.NestedField> columns = Lists.newArrayList(projection.columns());\n+      columns.add(MetadataColumns.ROW_POSITION);\n+      return new Schema(columns);\n+    }\n+\n+    return projection;\n+  }\n+\n+  private CloseableIterable<Record> applyResidual(CloseableIterable<Record> records, Schema recordSchema,\n+                                                  Expression residual) {\n+    if (residual != null && residual != Expressions.alwaysTrue()) {\n+      InternalRecordWrapper wrapper = new InternalRecordWrapper(recordSchema.asStruct());\n+      Evaluator filter = new Evaluator(recordSchema.asStruct(), residual, caseSensitive);\n+      return CloseableIterable.filter(records, record -> filter.eval(wrapper.wrap(record)));\n+    }\n+\n+    return records;\n+  }\n+\n+  private CloseableIterable<Record> applyEqDeletes(CloseableIterable<Record> records, Schema recordSchema,\n+                                                   List<DeleteFile> eqDeletes, DataFile dataFile) {\n+    if (eqDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    Multimap<Set<Integer>, DeleteFile> filesByDeleteIds = Multimaps.newMultimap(Maps.newHashMap(), Lists::newArrayList);\n+    for (DeleteFile delete : eqDeletes) {\n+      filesByDeleteIds.put(Sets.newHashSet(delete.equalityFieldIds()), delete);\n+    }\n+\n+    CloseableIterable<Record> filteredRecords = records;\n+    for (Map.Entry<Set<Integer>, Collection<DeleteFile>> entry : filesByDeleteIds.asMap().entrySet()) {\n+      Set<Integer> ids = entry.getKey();\n+      Iterable<DeleteFile> deletes = entry.getValue();\n+\n+      Schema deleteSchema = TypeUtil.select(recordSchema, ids);\n+      int[] orderedIds = deleteSchema.columns().stream().mapToInt(Types.NestedField::fieldId).toArray();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c91af88e6083129be9eca4687c6cdf658be43f8a"}, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg2NDI5Mw==", "bodyText": "The schema returned Record here will also include metadata columns and/or columns for equality deletes on top of the schema requested by the user. Do we want to apply a projection on top to match the schema requested?", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472864293", "createdAt": "2020-08-19T08:43:24Z", "author": {"login": "shardulm94"}, "path": "data/src/main/java/org/apache/iceberg/data/GenericReader.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.orc.GenericOrcReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Evaluator;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableGroup;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PartitionUtil;\n+import org.apache.iceberg.util.ProjectStructLike;\n+import org.apache.iceberg.util.StructLikeSet;\n+\n+class GenericReader implements Serializable {\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final FileIO io;\n+  private final Schema projection;\n+  private final boolean caseSensitive;\n+  private final boolean reuseContainers;\n+\n+  GenericReader(TableScan scan, boolean reuseContainers) {\n+    this.io = scan.table().io();\n+    this.projection = scan.schema();\n+    this.caseSensitive = scan.isCaseSensitive();\n+    this.reuseContainers = reuseContainers;\n+  }\n+\n+  CloseableIterator<Record> open(CloseableIterable<CombinedScanTask> tasks) {\n+    Iterable<FileScanTask> fileTasks = Iterables.concat(Iterables.transform(tasks, CombinedScanTask::files));\n+    return CloseableIterable.concat(Iterables.transform(fileTasks, this::open)).iterator();\n+  }\n+\n+  public CloseableIterable<Record> open(CombinedScanTask task) {\n+    return new CombinedTaskIterable(task);\n+  }\n+\n+  public CloseableIterable<Record> open(FileScanTask task) {\n+    List<DeleteFile> posDeletes = Lists.newArrayList();\n+    List<DeleteFile> eqDeletes = Lists.newArrayList();\n+    for (DeleteFile delete : task.deletes()) {\n+      switch (delete.content()) {\n+        case POSITION_DELETES:\n+          posDeletes.add(delete);\n+          break;\n+        case EQUALITY_DELETES:\n+          eqDeletes.add(delete);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n+      }\n+    }\n+\n+    Schema fileProjection = fileProjection(!posDeletes.isEmpty());\n+\n+    CloseableIterable<Record> records = openFile(task, fileProjection);\n+    records = applyPosDeletes(records, fileProjection, task.file().path(), posDeletes, task.file());\n+    records = applyEqDeletes(records, fileProjection, eqDeletes, task.file());\n+    records = applyResidual(records, fileProjection, task.residual());\n+\n+    return records;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c91af88e6083129be9eca4687c6cdf658be43f8a"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg2NTE1Nw==", "bodyText": "Was this removed to ignore the extra columns coming from the file projection?", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472865157", "createdAt": "2020-08-19T08:44:47Z", "author": {"login": "shardulm94"}, "path": "core/src/main/java/org/apache/iceberg/util/StructLikeWrapper.java", "diffHunk": "@@ -73,11 +73,6 @@ public boolean equals(Object other) {\n       return false;\n     }\n \n-    int len = struct.size();\n-    if (len != that.struct.size()) {\n-      return false;\n-    }\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c91af88e6083129be9eca4687c6cdf658be43f8a"}, "originalPosition": 8}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyMzE4MzU2", "url": "https://github.com/apache/iceberg/pull/1352#pullrequestreview-472318356", "createdAt": "2020-08-21T08:52:07Z", "commit": {"oid": "c91af88e6083129be9eca4687c6cdf658be43f8a"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQwODo1MjowOFrOHEjugg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQwODo1MjowOFrOHEjugg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU0MTY5OA==", "bodyText": "Can we make this configurable? For example, let users choose how much memory they want to use for this.", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r474541698", "createdAt": "2020-08-21T08:52:08Z", "author": {"login": "chenjunjiedada"}, "path": "data/src/main/java/org/apache/iceberg/data/GenericReader.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.orc.GenericOrcReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Evaluator;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableGroup;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PartitionUtil;\n+import org.apache.iceberg.util.ProjectStructLike;\n+import org.apache.iceberg.util.StructLikeSet;\n+\n+class GenericReader implements Serializable {\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final FileIO io;\n+  private final Schema projection;\n+  private final boolean caseSensitive;\n+  private final boolean reuseContainers;\n+\n+  GenericReader(TableScan scan, boolean reuseContainers) {\n+    this.io = scan.table().io();\n+    this.projection = scan.schema();\n+    this.caseSensitive = scan.isCaseSensitive();\n+    this.reuseContainers = reuseContainers;\n+  }\n+\n+  CloseableIterator<Record> open(CloseableIterable<CombinedScanTask> tasks) {\n+    Iterable<FileScanTask> fileTasks = Iterables.concat(Iterables.transform(tasks, CombinedScanTask::files));\n+    return CloseableIterable.concat(Iterables.transform(fileTasks, this::open)).iterator();\n+  }\n+\n+  public CloseableIterable<Record> open(CombinedScanTask task) {\n+    return new CombinedTaskIterable(task);\n+  }\n+\n+  public CloseableIterable<Record> open(FileScanTask task) {\n+    List<DeleteFile> posDeletes = Lists.newArrayList();\n+    List<DeleteFile> eqDeletes = Lists.newArrayList();\n+    for (DeleteFile delete : task.deletes()) {\n+      switch (delete.content()) {\n+        case POSITION_DELETES:\n+          posDeletes.add(delete);\n+          break;\n+        case EQUALITY_DELETES:\n+          eqDeletes.add(delete);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n+      }\n+    }\n+\n+    Schema fileProjection = fileProjection(!posDeletes.isEmpty());\n+\n+    CloseableIterable<Record> records = openFile(task, fileProjection);\n+    records = applyPosDeletes(records, fileProjection, task.file().path(), posDeletes, task.file());\n+    records = applyEqDeletes(records, fileProjection, eqDeletes, task.file());\n+    records = applyResidual(records, fileProjection, task.residual());\n+\n+    return records;\n+  }\n+\n+  private Schema fileProjection(boolean hasPosDeletes) {\n+    if (hasPosDeletes) {\n+      List<Types.NestedField> columns = Lists.newArrayList(projection.columns());\n+      columns.add(MetadataColumns.ROW_POSITION);\n+      return new Schema(columns);\n+    }\n+\n+    return projection;\n+  }\n+\n+  private CloseableIterable<Record> applyResidual(CloseableIterable<Record> records, Schema recordSchema,\n+                                                  Expression residual) {\n+    if (residual != null && residual != Expressions.alwaysTrue()) {\n+      InternalRecordWrapper wrapper = new InternalRecordWrapper(recordSchema.asStruct());\n+      Evaluator filter = new Evaluator(recordSchema.asStruct(), residual, caseSensitive);\n+      return CloseableIterable.filter(records, record -> filter.eval(wrapper.wrap(record)));\n+    }\n+\n+    return records;\n+  }\n+\n+  private CloseableIterable<Record> applyEqDeletes(CloseableIterable<Record> records, Schema recordSchema,\n+                                                   List<DeleteFile> eqDeletes, DataFile dataFile) {\n+    if (eqDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    Multimap<Set<Integer>, DeleteFile> filesByDeleteIds = Multimaps.newMultimap(Maps.newHashMap(), Lists::newArrayList);\n+    for (DeleteFile delete : eqDeletes) {\n+      filesByDeleteIds.put(Sets.newHashSet(delete.equalityFieldIds()), delete);\n+    }\n+\n+    CloseableIterable<Record> filteredRecords = records;\n+    for (Map.Entry<Set<Integer>, Collection<DeleteFile>> entry : filesByDeleteIds.asMap().entrySet()) {\n+      Set<Integer> ids = entry.getKey();\n+      Iterable<DeleteFile> deletes = entry.getValue();\n+\n+      Schema deleteSchema = TypeUtil.select(recordSchema, ids);\n+      int[] orderedIds = deleteSchema.columns().stream().mapToInt(Types.NestedField::fieldId).toArray();\n+\n+      // a wrapper to translate from generic objects to internal representations\n+      InternalRecordWrapper asStructLike = new InternalRecordWrapper(recordSchema.asStruct());\n+\n+      // a projection to select and reorder fields of the file schema to match the delete rows\n+      ProjectStructLike projectRow = ProjectStructLike.of(recordSchema, orderedIds);\n+\n+      Iterable<CloseableIterable<Record>> deleteRecords = Iterables.transform(deletes,\n+          delete -> openDeletes(delete, dataFile, deleteSchema));\n+      StructLikeSet deleteSet = Deletes.toEqualitySet(\n+          // copy the delete records because they will be held in a set\n+          CloseableIterable.transform(CloseableIterable.concat(deleteRecords), Record::copy),\n+          deleteSchema.asStruct());\n+\n+      filteredRecords = Deletes.filter(filteredRecords,\n+          record -> projectRow.wrap(asStructLike.wrap(record)), deleteSet);\n+    }\n+\n+    return filteredRecords;\n+  }\n+\n+  private CloseableIterable<Record> applyPosDeletes(CloseableIterable<Record> records, Schema recordSchema,\n+                                                    CharSequence file, List<DeleteFile> posDeletes, DataFile dataFile) {\n+    if (posDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    Accessor<StructLike> posAccessor = recordSchema.accessorForField(MetadataColumns.ROW_POSITION.fieldId());\n+    Function<Record, Long> posGetter = record -> (Long) posAccessor.get(record);\n+    List<CloseableIterable<StructLike>> deletes = Lists.transform(posDeletes,\n+        delete -> openPosDeletes(delete, dataFile));\n+\n+    // if there are fewer deletes than a reasonable number to keep in memory, use a set\n+    if (posDeletes.stream().mapToLong(DeleteFile::recordCount).sum() < 100_000L) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c91af88e6083129be9eca4687c6cdf658be43f8a"}, "originalPosition": 189}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ddbecd0eac02e786631af043e565a8aec83e843d", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/ddbecd0eac02e786631af043e565a8aec83e843d", "committedDate": "2020-08-23T00:44:18Z", "message": "Fix problems from review."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fc1f3452f74d4cb9a6f7f9d344a91bfc16afdf65", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/fc1f3452f74d4cb9a6f7f9d344a91bfc16afdf65", "committedDate": "2020-08-23T00:55:34Z", "message": "Remove accidental additions."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9607b5958aac84e8d89f17cbabbe2c82124410bd", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/9607b5958aac84e8d89f17cbabbe2c82124410bd", "committedDate": "2020-08-23T00:56:17Z", "message": "Fix license header."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c4b3a9b79025cf68789acea82b454125e10442fe", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/c4b3a9b79025cf68789acea82b454125e10442fe", "committedDate": "2020-08-23T00:56:57Z", "message": "Fix checkstyle."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDczMTAxMTI1", "url": "https://github.com/apache/iceberg/pull/1352#pullrequestreview-473101125", "createdAt": "2020-08-24T03:41:43Z", "commit": {"oid": "ddbecd0eac02e786631af043e565a8aec83e843d"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwMzo0MTo0M1rOHFTlHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwMzo0MTo0M1rOHFTlHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMyNTcyNA==", "bodyText": "Interesting constraint here that a column cannot be deleted if there are live equality delete files depending on the column. Maybe we should be checking this while deleting columns?", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r475325724", "createdAt": "2020-08-24T03:41:43Z", "author": {"login": "shardulm94"}, "path": "data/src/main/java/org/apache/iceberg/data/GenericReader.java", "diffHunk": "@@ -114,14 +117,40 @@\n     return records;\n   }\n \n-  private Schema fileProjection(boolean hasPosDeletes) {\n-    if (hasPosDeletes) {\n-      List<Types.NestedField> columns = Lists.newArrayList(projection.columns());\n+  private Schema fileProjection(List<DeleteFile> posDeletes, List<DeleteFile> eqDeletes) {\n+    Set<Integer> requiredIds = Sets.newLinkedHashSet();\n+    if (!posDeletes.isEmpty()) {\n+      requiredIds.add(MetadataColumns.ROW_POSITION.fieldId());\n+    }\n+\n+    for (DeleteFile eqDelete : eqDeletes) {\n+      requiredIds.addAll(eqDelete.equalityFieldIds());\n+    }\n+\n+    Set<Integer> missingIds = Sets.newLinkedHashSet(Sets.difference(requiredIds, TypeUtil.getProjectedIds(projection)));\n+\n+    if (missingIds.isEmpty()) {\n+      return projection;\n+    }\n+\n+    // TODO: support adding nested columns. this will currently fail when finding nested columns to add\n+    List<Types.NestedField> columns = Lists.newArrayList(projection.columns());\n+    for (int fieldId : missingIds) {\n+      if (fieldId == MetadataColumns.ROW_POSITION.fieldId()) {\n+        continue; // add _pos at the end\n+      }\n+\n+      Types.NestedField field = tableSchema.asStruct().field(fieldId);\n+      Preconditions.checkArgument(field != null, \"Cannot find required field for ID %s\", fieldId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ddbecd0eac02e786631af043e565a8aec83e843d"}, "originalPosition": 66}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDczMTcxNDAx", "url": "https://github.com/apache/iceberg/pull/1352#pullrequestreview-473171401", "createdAt": "2020-08-24T07:11:09Z", "commit": {"oid": "c4b3a9b79025cf68789acea82b454125e10442fe"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwNzoxMTowOVrOHFXOCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNFQwNzoxMTowOVrOHFXOCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM4NTM1NQ==", "bodyText": "I saw the upper layer will need the CloseableIterable<StructLike> and CloseableIteratable<Record>,   How about marking the parameterized T as T extends StructLike here", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r475385355", "createdAt": "2020-08-24T07:11:09Z", "author": {"login": "openinx"}, "path": "data/src/main/java/org/apache/iceberg/data/GenericReader.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.orc.GenericOrcReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Evaluator;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableGroup;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PartitionUtil;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.apache.parquet.Preconditions;\n+\n+class GenericReader implements Serializable {\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final FileIO io;\n+  private final Schema tableSchema;\n+  private final Schema projection;\n+  private final boolean caseSensitive;\n+  private final boolean reuseContainers;\n+\n+  GenericReader(TableScan scan, boolean reuseContainers) {\n+    this.io = scan.table().io();\n+    this.tableSchema = scan.table().schema();\n+    this.projection = scan.schema();\n+    this.caseSensitive = scan.isCaseSensitive();\n+    this.reuseContainers = reuseContainers;\n+  }\n+\n+  CloseableIterator<Record> open(CloseableIterable<CombinedScanTask> tasks) {\n+    Iterable<FileScanTask> fileTasks = Iterables.concat(Iterables.transform(tasks, CombinedScanTask::files));\n+    return CloseableIterable.concat(Iterables.transform(fileTasks, this::open)).iterator();\n+  }\n+\n+  public CloseableIterable<Record> open(CombinedScanTask task) {\n+    return new CombinedTaskIterable(task);\n+  }\n+\n+  public CloseableIterable<Record> open(FileScanTask task) {\n+    List<DeleteFile> posDeletes = Lists.newArrayList();\n+    List<DeleteFile> eqDeletes = Lists.newArrayList();\n+    for (DeleteFile delete : task.deletes()) {\n+      switch (delete.content()) {\n+        case POSITION_DELETES:\n+          posDeletes.add(delete);\n+          break;\n+        case EQUALITY_DELETES:\n+          eqDeletes.add(delete);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n+      }\n+    }\n+\n+    Schema fileProjection = fileProjection(posDeletes, eqDeletes);\n+\n+    CloseableIterable<Record> records = openFile(task, fileProjection);\n+    records = applyPosDeletes(records, fileProjection, task.file().path(), posDeletes, task.file());\n+    records = applyEqDeletes(records, fileProjection, eqDeletes, task.file());\n+    records = applyResidual(records, fileProjection, task.residual());\n+\n+    return records;\n+  }\n+\n+  private Schema fileProjection(List<DeleteFile> posDeletes, List<DeleteFile> eqDeletes) {\n+    Set<Integer> requiredIds = Sets.newLinkedHashSet();\n+    if (!posDeletes.isEmpty()) {\n+      requiredIds.add(MetadataColumns.ROW_POSITION.fieldId());\n+    }\n+\n+    for (DeleteFile eqDelete : eqDeletes) {\n+      requiredIds.addAll(eqDelete.equalityFieldIds());\n+    }\n+\n+    Set<Integer> missingIds = Sets.newLinkedHashSet(Sets.difference(requiredIds, TypeUtil.getProjectedIds(projection)));\n+\n+    if (missingIds.isEmpty()) {\n+      return projection;\n+    }\n+\n+    // TODO: support adding nested columns. this will currently fail when finding nested columns to add\n+    List<Types.NestedField> columns = Lists.newArrayList(projection.columns());\n+    for (int fieldId : missingIds) {\n+      if (fieldId == MetadataColumns.ROW_POSITION.fieldId()) {\n+        continue; // add _pos at the end\n+      }\n+\n+      Types.NestedField field = tableSchema.asStruct().field(fieldId);\n+      Preconditions.checkArgument(field != null, \"Cannot find required field for ID %s\", fieldId);\n+\n+      columns.add(field);\n+    }\n+\n+    if (requiredIds.contains(MetadataColumns.ROW_POSITION.fieldId())) {\n+      columns.add(MetadataColumns.ROW_POSITION);\n+    }\n+\n+    return new Schema(columns);\n+  }\n+\n+  private CloseableIterable<Record> applyResidual(CloseableIterable<Record> records, Schema recordSchema,\n+                                                  Expression residual) {\n+    if (residual != null && residual != Expressions.alwaysTrue()) {\n+      InternalRecordWrapper wrapper = new InternalRecordWrapper(recordSchema.asStruct());\n+      Evaluator filter = new Evaluator(recordSchema.asStruct(), residual, caseSensitive);\n+      return CloseableIterable.filter(records, record -> filter.eval(wrapper.wrap(record)));\n+    }\n+\n+    return records;\n+  }\n+\n+  private CloseableIterable<Record> applyEqDeletes(CloseableIterable<Record> records, Schema recordSchema,\n+                                                   List<DeleteFile> eqDeletes, DataFile dataFile) {\n+    if (eqDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    Multimap<Set<Integer>, DeleteFile> filesByDeleteIds = Multimaps.newMultimap(Maps.newHashMap(), Lists::newArrayList);\n+    for (DeleteFile delete : eqDeletes) {\n+      filesByDeleteIds.put(Sets.newHashSet(delete.equalityFieldIds()), delete);\n+    }\n+\n+    CloseableIterable<Record> filteredRecords = records;\n+    for (Map.Entry<Set<Integer>, Collection<DeleteFile>> entry : filesByDeleteIds.asMap().entrySet()) {\n+      Set<Integer> ids = entry.getKey();\n+      Iterable<DeleteFile> deletes = entry.getValue();\n+\n+      Schema deleteSchema = TypeUtil.select(recordSchema, ids);\n+\n+      // a wrapper to translate from generic objects to internal representations\n+      InternalRecordWrapper asStructLike = new InternalRecordWrapper(recordSchema.asStruct());\n+\n+      // a projection to select and reorder fields of the file schema to match the delete rows\n+      StructProjection projectRow = StructProjection.create(recordSchema, deleteSchema);\n+\n+      Iterable<CloseableIterable<Record>> deleteRecords = Iterables.transform(deletes,\n+          delete -> openDeletes(delete, dataFile, deleteSchema));\n+      StructLikeSet deleteSet = Deletes.toEqualitySet(\n+          // copy the delete records because they will be held in a set\n+          CloseableIterable.transform(CloseableIterable.concat(deleteRecords), Record::copy),\n+          deleteSchema.asStruct());\n+\n+      filteredRecords = Deletes.filter(filteredRecords,\n+          record -> projectRow.wrap(asStructLike.wrap(record)), deleteSet);\n+    }\n+\n+    return filteredRecords;\n+  }\n+\n+  private CloseableIterable<Record> applyPosDeletes(CloseableIterable<Record> records, Schema recordSchema,\n+                                                    CharSequence file, List<DeleteFile> posDeletes, DataFile dataFile) {\n+    if (posDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    Accessor<StructLike> posAccessor = recordSchema.accessorForField(MetadataColumns.ROW_POSITION.fieldId());\n+    Function<Record, Long> posGetter = record -> (Long) posAccessor.get(record);\n+    List<CloseableIterable<StructLike>> deletes = Lists.transform(posDeletes,\n+        delete -> openPosDeletes(delete, dataFile));\n+\n+    // if there are fewer deletes than a reasonable number to keep in memory, use a set\n+    if (posDeletes.stream().mapToLong(DeleteFile::recordCount).sum() < 100_000L) {\n+      return Deletes.filter(records, posGetter, Deletes.toPositionSet(file, CloseableIterable.concat(deletes)));\n+    }\n+\n+    return Deletes.streamingFilter(records, posGetter, Deletes.deletePositions(file, deletes));\n+  }\n+\n+  private CloseableIterable<StructLike> openPosDeletes(DeleteFile file, DataFile dataFile) {\n+    return openDeletes(file, dataFile, POS_DELETE_SCHEMA);\n+  }\n+\n+  private <T> CloseableIterable<T> openDeletes(DeleteFile deleteFile, DataFile dataFile, Schema deleteSchema) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c4b3a9b79025cf68789acea82b454125e10442fe"}, "originalPosition": 228}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "49f4bf5d729e24bed221b626711081fff6965916", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/49f4bf5d729e24bed221b626711081fff6965916", "committedDate": "2020-08-24T16:30:46Z", "message": "Fix checkstyle failures in tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4d3731a3d20faf7998614b80dcbc812d8412e8dd", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/4d3731a3d20faf7998614b80dcbc812d8412e8dd", "committedDate": "2020-08-24T16:48:24Z", "message": "Fix types in delete helper methods."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDczNzc1MzUy", "url": "https://github.com/apache/iceberg/pull/1352#pullrequestreview-473775352", "createdAt": "2020-08-24T18:59:34Z", "commit": {"oid": "4d3731a3d20faf7998614b80dcbc812d8412e8dd"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4072, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}