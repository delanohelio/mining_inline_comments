{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI4MjAwNDQ4", "number": 1095, "title": "Add HadoopCatalog example to create/load tables in the Iceberg User docs", "bodyText": "This PR propose to add HadoopCatalog example to create/load tables in the Iceberg User docs.", "createdAt": "2020-06-05T03:31:58Z", "url": "https://github.com/apache/iceberg/pull/1095", "merged": true, "mergeCommit": {"oid": "f3c54a810ad06ab2ad7c42ca63eb5d8607690d0a"}, "closed": true, "closedAt": "2020-06-17T17:56:09Z", "author": {"login": "hzfanxinxin"}, "timelineItems": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcoKC-OgH2gAyNDI4MjAwNDQ4OjE0NzMwZmRiODBlY2NkZTI0NWI4MGI1MGI2MTc1NjA5OTIxYWQ0OTI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcsNq7IAH2gAyNDI4MjAwNDQ4OjFhNDg1OWJlZGNhYTMxOTRiOGRkZTQ2NGQ1OTIyMTEyOTcxODY4ODg=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492", "author": {"user": {"login": "fanxinxin", "name": null}}, "url": "https://github.com/apache/iceberg/commit/14730fdb80eccde245b80b50b6175609921ad492", "committedDate": "2020-06-05T03:25:21Z", "message": "Add HadoopCatalog example to create/load tables in the Iceberg User docs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4MzE5ODE2", "url": "https://github.com/apache/iceberg/pull/1095#pullrequestreview-428319816", "createdAt": "2020-06-10T18:25:16Z", "commit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoyNToxNlrOGiBQRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoyNToxNlrOGiBQRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMyNTMxOA==", "bodyText": "This needs to note the disadvantages of using HadoopCatalog so that users are informed about the problems. Specifically, this can only be used with HDFS. Can you add that to the first sentence, like this?\n\nA Hadoop catalog doesn't need to connect to a Hive MetaStore, but can only be used with HDFS or similar file systems that support atomic rename.", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r438325318", "createdAt": "2020-06-10T18:25:16Z", "author": {"login": "rdblue"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+The Hadoop catalog doesn't need to connects to a Hive MetaStore. To get a Hadoop catalog see:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "originalPosition": 6}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4MzIwNzEz", "url": "https://github.com/apache/iceberg/pull/1095#pullrequestreview-428320713", "createdAt": "2020-06-10T18:26:29Z", "commit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoyNjozMFrOGiBTAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoyNjozMFrOGiBTAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMyNjAxNw==", "bodyText": "This doesn't use the right URI format, schema://authority/path. This URI would use warehouse_path for the HDFS host in the authority section.", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r438326017", "createdAt": "2020-06-10T18:26:30Z", "author": {"login": "rdblue"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+The Hadoop catalog doesn't need to connects to a Hive MetaStore. To get a Hadoop catalog see:\n+\n+```scala\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+\n+val conf = new Configuration();\n+val warehousePath = \"hdfs://warehouse_path\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "originalPosition": 13}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4MzIwOTY3", "url": "https://github.com/apache/iceberg/pull/1095#pullrequestreview-428320967", "createdAt": "2020-06-10T18:26:49Z", "commit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoyNjo0OVrOGiBTuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoyNjo0OVrOGiBTuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMyNjIwMA==", "bodyText": "I don't think rename is implemented, is it?", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r438326200", "createdAt": "2020-06-10T18:26:49Z", "author": {"login": "rdblue"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+The Hadoop catalog doesn't need to connects to a Hive MetaStore. To get a Hadoop catalog see:\n+\n+```scala\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+\n+val conf = new Configuration();\n+val warehousePath = \"hdfs://warehouse_path\";\n+val catalog = new HadoopCatalog(conf, warehousePath);\n+```\n+\n+Like Hive catalog, Hadoop catalog implements the interface `Catalog`. So it also contains methods for working with tables, like createTable, loadTable, renameTable, and dropTable.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "originalPosition": 17}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDI4MzIxNTQ4", "url": "https://github.com/apache/iceberg/pull/1095#pullrequestreview-428321548", "createdAt": "2020-06-10T18:27:37Z", "commit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoyNzozN1rOGiBVgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoyNzozN1rOGiBVgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMyNjY1Ng==", "bodyText": "This is strange because it actually loads the table using HadoopTables and not the HadoopCatalog. The URI passed here must match the one created by the catalog.", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r438326656", "createdAt": "2020-06-10T18:27:37Z", "author": {"login": "rdblue"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+The Hadoop catalog doesn't need to connects to a Hive MetaStore. To get a Hadoop catalog see:\n+\n+```scala\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+\n+val conf = new Configuration();\n+val warehousePath = \"hdfs://warehouse_path\";\n+val catalog = new HadoopCatalog(conf, warehousePath);\n+```\n+\n+Like Hive catalog, Hadoop catalog implements the interface `Catalog`. So it also contains methods for working with tables, like createTable, loadTable, renameTable, and dropTable.\n+                                                                                       \n+This example create a table with Hadoop catalog:\n+\n+```scala\n+val name = TableIdentifier.of(\"logging\", \"logs\")\n+val table = catalog.createTable(name, schema, spec)\n+\n+// write into the new logs table with Spark 2.4\n+logsDF.write\n+    .format(\"iceberg\")\n+    .mode(\"append\")\n+    .save(\"hdfs://warehouse_path/logging/logs\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "originalPosition": 29}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6200ebc3b56970eab5cb914b9d509ae921d7afda", "author": {"user": {"login": "fanxinxin", "name": null}}, "url": "https://github.com/apache/iceberg/commit/6200ebc3b56970eab5cb914b9d509ae921d7afda", "committedDate": "2020-06-17T07:40:27Z", "message": "Addressing the comments https://github.com/apache/iceberg/pull/1095"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4f8130778fd4e0eec7e35d45d4549e17a2040417", "author": {"user": {"login": "fanxinxin", "name": null}}, "url": "https://github.com/apache/iceberg/commit/4f8130778fd4e0eec7e35d45d4549e17a2040417", "committedDate": "2020-06-17T07:53:40Z", "message": "Addressing the comments 14730fdb80eccde245b80b50b6175609921ad492"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDMyNjM1MDk4", "url": "https://github.com/apache/iceberg/pull/1095#pullrequestreview-432635098", "createdAt": "2020-06-17T17:48:18Z", "commit": {"oid": "4f8130778fd4e0eec7e35d45d4549e17a2040417"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNzo0ODoxOFrOGlQi-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNzo0ODoxOFrOGlQi-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcyMTU5Mw==", "bodyText": "This location also needs to be fixed to avoid using warehouse_path as the authority section of the URI.", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r441721593", "createdAt": "2020-06-17T17:48:18Z", "author": {"login": "rdblue"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+A Hadoop catalog doesn't need to connect to a Hive MetaStore, but can only be used with HDFS or similar file systems that support atomic rename. To get a Hadoop catalog see:\n+\n+```scala\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+\n+val conf = new Configuration();\n+val warehousePath = \"hdfs://host:8020/warehouse_path\";\n+val catalog = new HadoopCatalog(conf, warehousePath);\n+```\n+\n+Like Hive catalog, Hadoop catalog implements the interface `Catalog`. So it also contains methods for working with tables, like createTable, loadTable, and dropTable.\n+                                                                                       \n+This example create a table with Hadoop catalog:\n+\n+```scala\n+val name = TableIdentifier.of(\"logging\", \"logs\")\n+val table = catalog.createTable(name, schema, spec)\n+\n+// write into the new logs table with Spark 2.4\n+logsDF.write\n+    .format(\"iceberg\")\n+    .mode(\"append\")\n+    .save(\"hdfs://warehouse_path/logging/logs\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4f8130778fd4e0eec7e35d45d4549e17a2040417"}, "originalPosition": 29}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0b6d3e1a9de2deffec92cb9e822cacee576237d3", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/0b6d3e1a9de2deffec92cb9e822cacee576237d3", "committedDate": "2020-06-17T17:53:18Z", "message": "Update api-quickstart.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1a4859bedcaa3194b8dde464d592211297186888", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/1a4859bedcaa3194b8dde464d592211297186888", "committedDate": "2020-06-17T17:54:24Z", "message": "Update java-api-quickstart.md"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4480, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}