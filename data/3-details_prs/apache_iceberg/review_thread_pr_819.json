{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzc4NTY2OTEx", "number": 819, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNzoyMzo0MFrODpNUbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNjoxNzozN1rODsv-wA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0NTM2NDI5OnYy", "diffSide": "RIGHT", "path": "core/src/test/java/org/apache/iceberg/TestSplitPlanning.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNzoyMzo0MFrOF4PYzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNzoyMzo0MFrOF4PYzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDUxNjY4Ng==", "bodyText": "Nit: unnecessary empty line.", "url": "https://github.com/apache/iceberg/pull/819#discussion_r394516686", "createdAt": "2020-03-18T17:23:40Z", "author": {"login": "rdblue"}, "path": "core/src/test/java/org/apache/iceberg/TestSplitPlanning.java", "diffHunk": "@@ -147,18 +178,23 @@ private void appendFiles(Iterable<DataFile> files) {\n     appendFiles.commit();\n   }\n \n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "af1afd0060748dde3b9fcc72e9ad5ac08612fa7e"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ0NTM4MTExOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/BaseTableScan.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQxNzoyNzo0OFrOF4Pj4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yMVQwNzoxODozM1rOF5oEyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDUxOTUyMA==", "bodyText": "I don't think this is necessary. When options are used to set the split size in the Spark reader, it uses TableProperties.SPLIT_SIZE to pass it here. That should work for metadata tables as well, right?\nThe situation that we need to handle in this PR is setting the default, like you had before. We just want to use a metadata split size so you can set it and not have the same split size used (by default) for both metadata and data scans.", "url": "https://github.com/apache/iceberg/pull/819#discussion_r394519520", "createdAt": "2020-03-18T17:27:48Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/BaseTableScan.java", "diffHunk": "@@ -225,6 +225,8 @@ public Expression filter() {\n     long splitSize;\n     if (options.containsKey(TableProperties.SPLIT_SIZE)) {\n       splitSize = Long.parseLong(options.get(TableProperties.SPLIT_SIZE));\n+    } else if (options.containsKey(TableProperties.METADATA_SPLIT_SIZE)) {\n+      splitSize = Long.parseLong(options.get(TableProperties.METADATA_SPLIT_SIZE));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "af1afd0060748dde3b9fcc72e9ad5ac08612fa7e"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTk2OTczNg==", "bodyText": "@rdblue Thanks for the comments and updated the PR accordingly.", "url": "https://github.com/apache/iceberg/pull/819#discussion_r395969736", "createdAt": "2020-03-21T07:18:33Z", "author": {"login": "jun-he"}, "path": "core/src/main/java/org/apache/iceberg/BaseTableScan.java", "diffHunk": "@@ -225,6 +225,8 @@ public Expression filter() {\n     long splitSize;\n     if (options.containsKey(TableProperties.SPLIT_SIZE)) {\n       splitSize = Long.parseLong(options.get(TableProperties.SPLIT_SIZE));\n+    } else if (options.containsKey(TableProperties.METADATA_SPLIT_SIZE)) {\n+      splitSize = Long.parseLong(options.get(TableProperties.METADATA_SPLIT_SIZE));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDUxOTUyMA=="}, "originalCommit": {"oid": "af1afd0060748dde3b9fcc72e9ad5ac08612fa7e"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1NTgwOTMyOnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/TableProperties.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QwMDowMTozM1rOF5z5bw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QwMDowMTozM1rOF5z5bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE2MzQzOQ==", "bodyText": "Can we move this up by the other split size property? Also, the other name is read.split.target-size so let's use the same prefix. How about changing this to read.split.metadata-target-size?", "url": "https://github.com/apache/iceberg/pull/819#discussion_r396163439", "createdAt": "2020-03-23T00:01:33Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/TableProperties.java", "diffHunk": "@@ -106,6 +106,8 @@ private TableProperties() {}\n   public static final String DEFAULT_WRITE_METRICS_MODE = \"write.metadata.metrics.default\";\n   public static final String DEFAULT_WRITE_METRICS_MODE_DEFAULT = \"truncate(16)\";\n \n+  public static final String METADATA_SPLIT_SIZE = \"read.metadata.split-size\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8dc705aa11be87a6739d3ce157f7f76662701931"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ4MjUwMDQ4OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNjoxNzozN1rOF9xmcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0zMFQxNjoxNzozN1rOF9xmcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMDMyMDExNQ==", "bodyText": "nit: I think it would be a bit more readable if we split this logic into multiple lines in all such places. However, it does not matter that much.\nTable table = tables.create(...);\n\ntable.refresh();\n\nList<ManifestFile> manifests = table.currentSnapshot().manifests();\nint expectedSplitNum = (manifests.get(0).length() + splitSize -1) / splitSize;", "url": "https://github.com/apache/iceberg/pull/819#discussion_r400320115", "createdAt": "2020-03-30T16:17:37Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java", "diffHunk": "@@ -284,4 +284,103 @@ public void testIncrementalScanOptions() throws IOException {\n         .collectAsList();\n     Assert.assertEquals(\"Records should match\", expectedRecords.subList(2, 3), result1);\n   }\n+\n+  @Test\n+  public void testMetadataSplitSizeOptionOverrideTableProperties() throws IOException {\n+    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+    int splitSize = 2 * 1024;\n+\n+    HadoopTables tables = new HadoopTables(CONF);\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Map<String, String> options = Maps.newHashMap();\n+    options.put(TableProperties.SPLIT_SIZE, String.valueOf(128L * 1024 * 1024)); // 128Mb\n+    options.put(TableProperties.METADATA_SPLIT_SIZE, String.valueOf(32L * 1024 * 1024)); // 32MB\n+    tables.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<SimpleRecord> expectedRecords = Lists.newArrayList(\n+        new SimpleRecord(1, \"a\"),\n+        new SimpleRecord(2, \"b\")\n+    );\n+    Dataset<Row> originalDf = spark.createDataFrame(expectedRecords, SimpleRecord.class);\n+    originalDf.select(\"id\", \"data\").write()\n+        .format(\"iceberg\")\n+        .mode(\"append\")\n+        .save(tableLocation);\n+\n+    int expectedSplits = ((int) tables.load(tableLocation + \"#entries\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3faa9d5b592b29acc729ddef0e770cf01a933277"}, "originalPosition": 27}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2820, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}