{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQyMDI1MDky", "number": 1145, "title": "Implement the flink stream writer to accept the row data and emit the complete data files event to downstream", "bodyText": "", "createdAt": "2020-06-30T13:22:02Z", "url": "https://github.com/apache/iceberg/pull/1145", "merged": true, "mergeCommit": {"oid": "7101c25b91287578e387f8c8cdc7f5547310171e"}, "closed": true, "closedAt": "2020-08-07T16:43:15Z", "author": {"login": "openinx"}, "timelineItems": {"totalCount": 37, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcwuYoXgFqTQ0MTA5NzA0Nw==", "endCursor": "Y3Vyc29yOnYyOpPPAAABc8nMNHAFqTQ2MzQ1MzU2MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMDk3MDQ3", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-441097047", "createdAt": "2020-07-01T18:16:59Z", "commit": {"oid": "4d277a0115f9521c82806fc35d71c1640bad20a4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxODoxNjo1OVrOGrwkpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxODoxNjo1OVrOGrwkpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUzNzc2Ng==", "bodyText": "Why make this modifiable?", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r448537766", "createdAt": "2020-07-01T18:16:59Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -360,7 +360,7 @@ public ByteBuffer keyMetadata() {\n     if (list != null) {\n       List<E> copy = Lists.newArrayListWithExpectedSize(list.size());\n       copy.addAll(list);\n-      return Collections.unmodifiableList(copy);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4d277a0115f9521c82806fc35d71c1640bad20a4"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyMTI3NDc1", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442127475", "createdAt": "2020-07-03T03:37:45Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QwMzozNzo0NVrOGsiwUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QwNDo0NToxOVrOGsjjbA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM1OTk1Mg==", "bodyText": "Operator in Flink will be serialized into byte[] whatever. So I think it is true about set all the non-serializable members to be null. You don't need add this comment.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449359952", "createdAt": "2020-07-03T03:37:45Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.java.ClosureCleaner;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.OutputFileFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<SerializableDataFile>\n+    implements OneInputStreamOperator<Row, SerializableDataFile> {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final SerializableConfiguration conf;\n+  private Schema readSchema;\n+\n+  private transient Table table;\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+\n+  /**\n+   * Be careful to do the initialization in this constructor, because in {@link DataStream#addSink(SinkFunction)}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 77}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM2MTI0OQ==", "bodyText": "createFileFormat ?", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449361249", "createdAt": "2020-07-03T03:45:01Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.java.ClosureCleaner;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.OutputFileFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<SerializableDataFile>\n+    implements OneInputStreamOperator<Row, SerializableDataFile> {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final SerializableConfiguration conf;\n+  private Schema readSchema;\n+\n+  private transient Table table;\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+\n+  /**\n+   * Be careful to do the initialization in this constructor, because in {@link DataStream#addSink(SinkFunction)}\n+   * it will call {@link ClosureCleaner#clean(Object, ExecutionConfig.ClosureCleanerLevel, boolean)} to set all the\n+   * non-serializable members to be null.\n+   *\n+   * @param tablePath  The base path of the iceberg table.\n+   * @param readSchema The schema of source data.\n+   * @param conf       The hadoop's configuration.\n+   */\n+  private IcebergStreamWriter(String tablePath, Schema readSchema, Configuration conf) {\n+    this.tablePath = tablePath;\n+    this.conf = new SerializableConfiguration(conf);\n+    this.readSchema = readSchema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.table = TableUtil.findTable(tablePath, conf.get());\n+    if (this.readSchema != null) {\n+      // reassign ids to match the existing table schema\n+      readSchema = TypeUtil.reassignIds(readSchema, table.schema());\n+      TypeUtil.validateWriteSchema(readSchema, table.schema(), true, true);\n+    }\n+\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(table);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(table, fileFormat, subTaskId);\n+    this.writer = TaskWriterFactory.createTaskWriter(table.spec(),\n+        appenderFactory,\n+        outputFileFactory,\n+        getTargetFileSizeBytes(),\n+        fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer {} subtask {} begin preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+    for (DataFile dataFile : writer.getCompleteFiles()) {\n+      emit(dataFile);\n+    }\n+    // Remember to clear the writer's cached complete files.\n+    writer.reset();\n+    LOG.info(\"Iceberg writer {} subtask {} completed preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<Row> element) throws Exception {\n+    Row value = element.getValue();\n+    writer.append(value);\n+\n+    // Emit the data file entries to downstream committer operator if there exist any complete files.\n+    List<DataFile> completeFiles = writer.getCompleteFiles();\n+    if (!completeFiles.isEmpty()) {\n+      completeFiles.forEach(this::emit);\n+      // Remember to clear the writer's cached complete files.\n+      writer.reset();\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws Exception {\n+    if (writer != null) {\n+      writer.close();\n+      writer = null;\n+    }\n+  }\n+\n+  private void emit(DataFile dataFile) {\n+    output.collect(new StreamRecord<>(new SerializableDataFile(dataFile)));\n+  }\n+\n+  private FileFormat getFileFormat() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM2MzY3Mg==", "bodyText": "A reset in TaskWriter is just clear complete files?", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449363672", "createdAt": "2020-07-03T03:57:36Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/writer/TaskWriter.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+\n+/**\n+ * The writer interface which could accept records and provide the generated data files.\n+ *\n+ * @param <T> to indicate the record data type.\n+ */\n+public interface TaskWriter<T> {\n+  int ROW_DIVISOR = 1000;\n+\n+  /**\n+   * Append the row into the data files.\n+   */\n+  void append(T record) throws IOException;\n+\n+  /**\n+   * Close the writer.\n+   */\n+  void close() throws IOException;\n+\n+  /**\n+   * To get the full list of complete files, we should call this method after {@link TaskWriter#close()} because the\n+   * close method will close all the opening data files and build {@link DataFile} to the return array list.\n+   *\n+   * @return the cached completed data files of this task writer.\n+   */\n+  List<DataFile> getCompleteFiles();\n+\n+  /**\n+   * Reset to clear all the cached complete files.\n+   */\n+  void reset();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM2ODY5MQ==", "bodyText": "I can see always a reset after getCompleteFiles. Can just provides a pollCompleteFiles?", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449368691", "createdAt": "2020-07-03T04:24:18Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.java.ClosureCleaner;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.OutputFileFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<SerializableDataFile>\n+    implements OneInputStreamOperator<Row, SerializableDataFile> {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final SerializableConfiguration conf;\n+  private Schema readSchema;\n+\n+  private transient Table table;\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+\n+  /**\n+   * Be careful to do the initialization in this constructor, because in {@link DataStream#addSink(SinkFunction)}\n+   * it will call {@link ClosureCleaner#clean(Object, ExecutionConfig.ClosureCleanerLevel, boolean)} to set all the\n+   * non-serializable members to be null.\n+   *\n+   * @param tablePath  The base path of the iceberg table.\n+   * @param readSchema The schema of source data.\n+   * @param conf       The hadoop's configuration.\n+   */\n+  private IcebergStreamWriter(String tablePath, Schema readSchema, Configuration conf) {\n+    this.tablePath = tablePath;\n+    this.conf = new SerializableConfiguration(conf);\n+    this.readSchema = readSchema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.table = TableUtil.findTable(tablePath, conf.get());\n+    if (this.readSchema != null) {\n+      // reassign ids to match the existing table schema\n+      readSchema = TypeUtil.reassignIds(readSchema, table.schema());\n+      TypeUtil.validateWriteSchema(readSchema, table.schema(), true, true);\n+    }\n+\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(table);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(table, fileFormat, subTaskId);\n+    this.writer = TaskWriterFactory.createTaskWriter(table.spec(),\n+        appenderFactory,\n+        outputFileFactory,\n+        getTargetFileSizeBytes(),\n+        fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer {} subtask {} begin preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+    for (DataFile dataFile : writer.getCompleteFiles()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM3MTQyOA==", "bodyText": "computeIfAbsent?", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449371428", "createdAt": "2020-07-03T04:37:43Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/writer/PartitionWriter.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.PartitionKey;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The PartitionFanoutWriter will open a writing data file for each partition and route the given record to the\n+ * corresponding data file in the correct partition.\n+ *\n+ * @param <T> defines the data type of record to write.\n+ */\n+class PartitionWriter<T> extends BaseTaskWriter<T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(PartitionWriter.class);\n+\n+  private final PartitionSpec spec;\n+  private final FileAppenderFactory<T> factory;\n+  private final Function<PartitionKey, EncryptedOutputFile> outputFileGetter;\n+  private final Function<T, PartitionKey> keyGetter;\n+  private final Map<PartitionKey, WrappedFileAppender<T>> writers;\n+  private final long targetFileSize;\n+  private final FileFormat fileFormat;\n+  private final List<DataFile> completeDataFiles;\n+\n+  PartitionWriter(PartitionSpec spec,\n+                  FileAppenderFactory<T> factory,\n+                  Function<PartitionKey, EncryptedOutputFile> outputFileGetter,\n+                  Function<T, PartitionKey> keyGetter,\n+                  long targetFileSize,\n+                  FileFormat fileFormat) {\n+    this.spec = spec;\n+    this.factory = factory;\n+    this.outputFileGetter = outputFileGetter;\n+    this.keyGetter = keyGetter;\n+    this.writers = Maps.newHashMap();\n+    this.targetFileSize = targetFileSize;\n+    this.fileFormat = fileFormat;\n+    this.completeDataFiles = Lists.newArrayList();\n+  }\n+\n+  @Override\n+  public void append(T record) throws IOException {\n+    PartitionKey partitionKey = keyGetter.apply(record);\n+    Preconditions.checkArgument(partitionKey != null, \"Partition key shouldn't be null\");\n+\n+    WrappedFileAppender<T> writer = writers.get(partitionKey);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM3MTYxNw==", "bodyText": "We don't need store partitionKey, it is already in map?", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449371617", "createdAt": "2020-07-03T04:38:46Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/writer/PartitionWriter.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.PartitionKey;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The PartitionFanoutWriter will open a writing data file for each partition and route the given record to the\n+ * corresponding data file in the correct partition.\n+ *\n+ * @param <T> defines the data type of record to write.\n+ */\n+class PartitionWriter<T> extends BaseTaskWriter<T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(PartitionWriter.class);\n+\n+  private final PartitionSpec spec;\n+  private final FileAppenderFactory<T> factory;\n+  private final Function<PartitionKey, EncryptedOutputFile> outputFileGetter;\n+  private final Function<T, PartitionKey> keyGetter;\n+  private final Map<PartitionKey, WrappedFileAppender<T>> writers;\n+  private final long targetFileSize;\n+  private final FileFormat fileFormat;\n+  private final List<DataFile> completeDataFiles;\n+\n+  PartitionWriter(PartitionSpec spec,\n+                  FileAppenderFactory<T> factory,\n+                  Function<PartitionKey, EncryptedOutputFile> outputFileGetter,\n+                  Function<T, PartitionKey> keyGetter,\n+                  long targetFileSize,\n+                  FileFormat fileFormat) {\n+    this.spec = spec;\n+    this.factory = factory;\n+    this.outputFileGetter = outputFileGetter;\n+    this.keyGetter = keyGetter;\n+    this.writers = Maps.newHashMap();\n+    this.targetFileSize = targetFileSize;\n+    this.fileFormat = fileFormat;\n+    this.completeDataFiles = Lists.newArrayList();\n+  }\n+\n+  @Override\n+  public void append(T record) throws IOException {\n+    PartitionKey partitionKey = keyGetter.apply(record);\n+    Preconditions.checkArgument(partitionKey != null, \"Partition key shouldn't be null\");\n+\n+    WrappedFileAppender<T> writer = writers.get(partitionKey);\n+    if (writer == null) {\n+      writer = createWrappedFileAppender(partitionKey);\n+      writers.put(partitionKey, writer);\n+    }\n+    writer.fileAppender.add(record);\n+\n+    // Roll the writer if reach the target file size.\n+    writer.currentRows++;\n+    if (writer.currentRows % ROW_DIVISOR == 0 && writer.fileAppender.length() >= targetFileSize) {\n+      closeCurrentWriter(writer);\n+      writers.remove(partitionKey);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    for (WrappedFileAppender<T> wrap : writers.values()) {\n+      closeCurrentWriter(wrap);\n+      LOG.debug(\"Close file appender: {}, completeDataFiles: {}\",\n+          wrap.encryptedOutputFile.encryptingOutputFile().location(),\n+          completeDataFiles.size());\n+    }\n+    this.writers.clear();\n+  }\n+\n+  @Override\n+  public List<DataFile> getCompleteFiles() {\n+    if (completeDataFiles.size() > 0) {\n+      return ImmutableList.copyOf(this.completeDataFiles);\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  @Override\n+  public void reset() {\n+    this.completeDataFiles.clear();\n+  }\n+\n+  private WrappedFileAppender<T> createWrappedFileAppender(PartitionKey partitionKey) {\n+    EncryptedOutputFile outputFile = outputFileGetter.apply(partitionKey);\n+    FileAppender<T> appender = factory.newAppender(outputFile.encryptingOutputFile(), fileFormat);\n+    return new WrappedFileAppender<>(partitionKey, outputFile, appender);\n+  }\n+\n+  private void closeCurrentWriter(WrappedFileAppender<T> wrap) throws IOException {\n+    DataFile dataFile = closeFileAppender(wrap.fileAppender, wrap.encryptedOutputFile, spec, wrap.partitionKey);\n+    completeDataFiles.add(dataFile);\n+  }\n+\n+  private static class WrappedFileAppender<T> {\n+    private final PartitionKey partitionKey;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM3MjE1Ng==", "bodyText": "spec.fields().size() == 0 ? null : partitionKey just be partitionKey?", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449372156", "createdAt": "2020-07-03T04:41:17Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/writer/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+\n+  protected DataFile closeFileAppender(FileAppender<T> fileAppender, EncryptedOutputFile currentFile,\n+                                       PartitionSpec spec, StructLike partitionKey) throws IOException {\n+    // Close the file appender firstly.\n+    fileAppender.close();\n+\n+    // metrics are only valid after the appender is closed.\n+    Metrics metrics = fileAppender.metrics();\n+    long fileSizeInBytes = fileAppender.length();\n+    List<Long> splitOffsets = fileAppender.splitOffsets();\n+\n+    // Construct the DataFile and add it into the completeDataFiles.\n+    return DataFiles.builder(spec)\n+        .withEncryptedOutputFile(currentFile)\n+        .withPath(currentFile.encryptingOutputFile().location())\n+        .withFileSizeInBytes(fileSizeInBytes)\n+        .withPartition(spec.fields().size() == 0 ? null : partitionKey)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM3MjY5MA==", "bodyText": "add a method in this class: add(...), you can increment currentRows here.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449372690", "createdAt": "2020-07-03T04:43:47Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/writer/PartitionWriter.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.PartitionKey;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The PartitionFanoutWriter will open a writing data file for each partition and route the given record to the\n+ * corresponding data file in the correct partition.\n+ *\n+ * @param <T> defines the data type of record to write.\n+ */\n+class PartitionWriter<T> extends BaseTaskWriter<T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(PartitionWriter.class);\n+\n+  private final PartitionSpec spec;\n+  private final FileAppenderFactory<T> factory;\n+  private final Function<PartitionKey, EncryptedOutputFile> outputFileGetter;\n+  private final Function<T, PartitionKey> keyGetter;\n+  private final Map<PartitionKey, WrappedFileAppender<T>> writers;\n+  private final long targetFileSize;\n+  private final FileFormat fileFormat;\n+  private final List<DataFile> completeDataFiles;\n+\n+  PartitionWriter(PartitionSpec spec,\n+                  FileAppenderFactory<T> factory,\n+                  Function<PartitionKey, EncryptedOutputFile> outputFileGetter,\n+                  Function<T, PartitionKey> keyGetter,\n+                  long targetFileSize,\n+                  FileFormat fileFormat) {\n+    this.spec = spec;\n+    this.factory = factory;\n+    this.outputFileGetter = outputFileGetter;\n+    this.keyGetter = keyGetter;\n+    this.writers = Maps.newHashMap();\n+    this.targetFileSize = targetFileSize;\n+    this.fileFormat = fileFormat;\n+    this.completeDataFiles = Lists.newArrayList();\n+  }\n+\n+  @Override\n+  public void append(T record) throws IOException {\n+    PartitionKey partitionKey = keyGetter.apply(record);\n+    Preconditions.checkArgument(partitionKey != null, \"Partition key shouldn't be null\");\n+\n+    WrappedFileAppender<T> writer = writers.get(partitionKey);\n+    if (writer == null) {\n+      writer = createWrappedFileAppender(partitionKey);\n+      writers.put(partitionKey, writer);\n+    }\n+    writer.fileAppender.add(record);\n+\n+    // Roll the writer if reach the target file size.\n+    writer.currentRows++;\n+    if (writer.currentRows % ROW_DIVISOR == 0 && writer.fileAppender.length() >= targetFileSize) {\n+      closeCurrentWriter(writer);\n+      writers.remove(partitionKey);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    for (WrappedFileAppender<T> wrap : writers.values()) {\n+      closeCurrentWriter(wrap);\n+      LOG.debug(\"Close file appender: {}, completeDataFiles: {}\",\n+          wrap.encryptedOutputFile.encryptingOutputFile().location(),\n+          completeDataFiles.size());\n+    }\n+    this.writers.clear();\n+  }\n+\n+  @Override\n+  public List<DataFile> getCompleteFiles() {\n+    if (completeDataFiles.size() > 0) {\n+      return ImmutableList.copyOf(this.completeDataFiles);\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  @Override\n+  public void reset() {\n+    this.completeDataFiles.clear();\n+  }\n+\n+  private WrappedFileAppender<T> createWrappedFileAppender(PartitionKey partitionKey) {\n+    EncryptedOutputFile outputFile = outputFileGetter.apply(partitionKey);\n+    FileAppender<T> appender = factory.newAppender(outputFile.encryptingOutputFile(), fileFormat);\n+    return new WrappedFileAppender<>(partitionKey, outputFile, appender);\n+  }\n+\n+  private void closeCurrentWriter(WrappedFileAppender<T> wrap) throws IOException {\n+    DataFile dataFile = closeFileAppender(wrap.fileAppender, wrap.encryptedOutputFile, spec, wrap.partitionKey);\n+    completeDataFiles.add(dataFile);\n+  }\n+\n+  private static class WrappedFileAppender<T> {\n+    private final PartitionKey partitionKey;\n+    private final EncryptedOutputFile encryptedOutputFile;\n+    private final FileAppender<T> fileAppender;\n+\n+    private long currentRows = 0;\n+\n+    WrappedFileAppender(PartitionKey partitionKey,\n+                        EncryptedOutputFile encryptedOutputFile,\n+                        FileAppender<T> fileAppender) {\n+      this.partitionKey = partitionKey;\n+      this.encryptedOutputFile = encryptedOutputFile;\n+      this.fileAppender = fileAppender;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM3Mjg2Mw==", "bodyText": "Can you add comments to explain why need align to ROW_DIVISOR?", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449372863", "createdAt": "2020-07-03T04:44:41Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/writer/UnpartitionedWriter.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+\n+class UnpartitionedWriter<T> extends BaseTaskWriter<T> {\n+\n+  private final FileAppenderFactory<T> factory;\n+  private final Supplier<EncryptedOutputFile> outputFileSupplier;\n+  private final long targetFileSize;\n+  private final FileFormat fileFormat;\n+  private final List<DataFile> completeDataFiles;\n+\n+  private long currentRows = 0;\n+  private EncryptedOutputFile currentOutputFile;\n+  private FileAppender<T> currentAppender = null;\n+\n+  UnpartitionedWriter(FileAppenderFactory<T> factory,\n+                      Supplier<EncryptedOutputFile> outputFileSupplier,\n+                      long targetFileSize,\n+                      FileFormat fileFormat) {\n+    this.factory = factory;\n+    this.outputFileSupplier = outputFileSupplier;\n+    this.targetFileSize = targetFileSize;\n+    this.fileFormat = fileFormat;\n+    this.completeDataFiles = new ArrayList<>();\n+  }\n+\n+  @Override\n+  public void append(T record) throws IOException {\n+    if (currentAppender == null) {\n+      currentOutputFile = outputFileSupplier.get();\n+      currentAppender = factory.newAppender(currentOutputFile.encryptingOutputFile(), fileFormat);\n+    }\n+    currentAppender.add(record);\n+\n+    // Roll the writer if reach the target file size.\n+    currentRows++;\n+    if (currentRows % ROW_DIVISOR == 0 && currentAppender.length() >= targetFileSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM3MzAzNg==", "bodyText": "Extract this class for NonPartitionWriter too?", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449373036", "createdAt": "2020-07-03T04:45:19Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/writer/PartitionWriter.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.PartitionKey;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The PartitionFanoutWriter will open a writing data file for each partition and route the given record to the\n+ * corresponding data file in the correct partition.\n+ *\n+ * @param <T> defines the data type of record to write.\n+ */\n+class PartitionWriter<T> extends BaseTaskWriter<T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(PartitionWriter.class);\n+\n+  private final PartitionSpec spec;\n+  private final FileAppenderFactory<T> factory;\n+  private final Function<PartitionKey, EncryptedOutputFile> outputFileGetter;\n+  private final Function<T, PartitionKey> keyGetter;\n+  private final Map<PartitionKey, WrappedFileAppender<T>> writers;\n+  private final long targetFileSize;\n+  private final FileFormat fileFormat;\n+  private final List<DataFile> completeDataFiles;\n+\n+  PartitionWriter(PartitionSpec spec,\n+                  FileAppenderFactory<T> factory,\n+                  Function<PartitionKey, EncryptedOutputFile> outputFileGetter,\n+                  Function<T, PartitionKey> keyGetter,\n+                  long targetFileSize,\n+                  FileFormat fileFormat) {\n+    this.spec = spec;\n+    this.factory = factory;\n+    this.outputFileGetter = outputFileGetter;\n+    this.keyGetter = keyGetter;\n+    this.writers = Maps.newHashMap();\n+    this.targetFileSize = targetFileSize;\n+    this.fileFormat = fileFormat;\n+    this.completeDataFiles = Lists.newArrayList();\n+  }\n+\n+  @Override\n+  public void append(T record) throws IOException {\n+    PartitionKey partitionKey = keyGetter.apply(record);\n+    Preconditions.checkArgument(partitionKey != null, \"Partition key shouldn't be null\");\n+\n+    WrappedFileAppender<T> writer = writers.get(partitionKey);\n+    if (writer == null) {\n+      writer = createWrappedFileAppender(partitionKey);\n+      writers.put(partitionKey, writer);\n+    }\n+    writer.fileAppender.add(record);\n+\n+    // Roll the writer if reach the target file size.\n+    writer.currentRows++;\n+    if (writer.currentRows % ROW_DIVISOR == 0 && writer.fileAppender.length() >= targetFileSize) {\n+      closeCurrentWriter(writer);\n+      writers.remove(partitionKey);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    for (WrappedFileAppender<T> wrap : writers.values()) {\n+      closeCurrentWriter(wrap);\n+      LOG.debug(\"Close file appender: {}, completeDataFiles: {}\",\n+          wrap.encryptedOutputFile.encryptingOutputFile().location(),\n+          completeDataFiles.size());\n+    }\n+    this.writers.clear();\n+  }\n+\n+  @Override\n+  public List<DataFile> getCompleteFiles() {\n+    if (completeDataFiles.size() > 0) {\n+      return ImmutableList.copyOf(this.completeDataFiles);\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  @Override\n+  public void reset() {\n+    this.completeDataFiles.clear();\n+  }\n+\n+  private WrappedFileAppender<T> createWrappedFileAppender(PartitionKey partitionKey) {\n+    EncryptedOutputFile outputFile = outputFileGetter.apply(partitionKey);\n+    FileAppender<T> appender = factory.newAppender(outputFile.encryptingOutputFile(), fileFormat);\n+    return new WrappedFileAppender<>(partitionKey, outputFile, appender);\n+  }\n+\n+  private void closeCurrentWriter(WrappedFileAppender<T> wrap) throws IOException {\n+    DataFile dataFile = closeFileAppender(wrap.fileAppender, wrap.encryptedOutputFile, spec, wrap.partitionKey);\n+    completeDataFiles.add(dataFile);\n+  }\n+\n+  private static class WrappedFileAppender<T> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 131}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyMTU5NzEx", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442159711", "createdAt": "2020-07-03T05:48:32Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QwNTo0ODozMlrOGskbpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QwNTo0ODozMlrOGskbpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM4NzQzMA==", "bodyText": "You may need to check the metrics.recordCount(), we don't need construct the DataFile without records.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449387430", "createdAt": "2020-07-03T05:48:32Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/writer/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+\n+  protected DataFile closeFileAppender(FileAppender<T> fileAppender, EncryptedOutputFile currentFile,\n+                                       PartitionSpec spec, StructLike partitionKey) throws IOException {\n+    // Close the file appender firstly.\n+    fileAppender.close();\n+\n+    // metrics are only valid after the appender is closed.\n+    Metrics metrics = fileAppender.metrics();\n+    long fileSizeInBytes = fileAppender.length();\n+    List<Long> splitOffsets = fileAppender.splitOffsets();\n+\n+    // Construct the DataFile and add it into the completeDataFiles.\n+    return DataFiles.builder(spec)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 45}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyMTYwOTk1", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442160995", "createdAt": "2020-07-03T05:52:34Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QwNTo1MjozNFrOGskfxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QwNjowMzo0M1rOGskrMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM4ODQ4Ng==", "bodyText": "NIT: add (name = \"format = {0}, partitioned= {1}\")", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449388486", "createdAt": "2020-07-03T05:52:34Z", "author": {"login": "JingsongLi"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestIcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestIcebergStreamWriter {\n+  private static final Configuration CONF = new Configuration();\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+\n+  private String tablePath;\n+  private Table table;\n+\n+  private final FileFormat format;\n+  private final boolean partitioned;\n+\n+  // TODO add AVRO, ORC unit test once the readers and writers are ready.\n+  @Parameterized.Parameters", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM5MTQwOQ==", "bodyText": "Maybe you can not just apply from objects in row.\nIIUC, In Flink Row, the structure of bytes is byte[], and in Iceberg, the structure of bytes is ByteBuffer.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449391409", "createdAt": "2020-07-03T06:03:43Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/PartitionKey.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.transforms.Transform;\n+import org.apache.iceberg.types.Types;\n+\n+public class PartitionKey implements StructLike {\n+\n+  private final Object[] partitionTuple;\n+\n+  private PartitionKey(Object[] partitionTuple) {\n+    this.partitionTuple = partitionTuple;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (!(o instanceof PartitionKey)) {\n+      return false;\n+    }\n+\n+    PartitionKey that = (PartitionKey) o;\n+    return Arrays.equals(partitionTuple, that.partitionTuple);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return Arrays.hashCode(partitionTuple);\n+  }\n+\n+  @Override\n+  public int size() {\n+    return partitionTuple.length;\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    return javaClass.cast(partitionTuple[pos]);\n+  }\n+\n+  public Object[] getPartitionTuple() {\n+    return partitionTuple;\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    partitionTuple[pos] = value;\n+  }\n+\n+  private static Map<Integer, Integer> buildFieldId2PosMap(Schema schema) {\n+    Map<Integer, Integer> fieldId2Position = Maps.newHashMap();\n+    List<Types.NestedField> nestedFields = schema.asStruct().fields();\n+    for (int i = 0; i < nestedFields.size(); i++) {\n+      fieldId2Position.put(nestedFields.get(i).fieldId(), i);\n+    }\n+    return fieldId2Position;\n+  }\n+\n+  public static Builder builder(PartitionSpec spec) {\n+    return new Builder(spec);\n+  }\n+\n+  public static class Builder {\n+    private final int size;\n+\n+    private final int[] pos;\n+    private final Transform[] transforms;\n+\n+    private Builder(PartitionSpec spec) {\n+      List<PartitionField> fields = spec.fields();\n+      this.size = fields.size();\n+      this.pos = new int[size];\n+      this.transforms = new Transform[size];\n+\n+      Map<Integer, Integer> fieldId2Pos = buildFieldId2PosMap(spec.schema());\n+\n+      for (int i = 0; i < size; i += 1) {\n+        PartitionField field = fields.get(i);\n+        Integer position = fieldId2Pos.get(field.sourceId());\n+        Preconditions.checkArgument(position != null,\n+            \"Field source id from PartitionSpec MUST exist in the original schema\");\n+        this.pos[i] = position;\n+        this.transforms[i] = field.transform();\n+      }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public PartitionKey build(Row row) {\n+      Object[] partitionTuple = new Object[size];\n+      for (int i = 0; i < partitionTuple.length; i += 1) {\n+        partitionTuple[i] = transforms[i].apply(row.getField(pos[i]));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 121}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTYzMTM1", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442563135", "createdAt": "2020-07-03T21:30:28Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTozMDoyOFrOGs38kA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTozMDoyOFrOGs38kA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwNzE1Mg==", "bodyText": "Is there a way to tell which methods are intended to be called before serialization and which ones will be called after serialization? This one is clearly before serialization because it uses table, which is set in the constructor and is transient. I think it would help readability if we had an annotation or some way to highlight the methods that will be used after serialization, so we can check that they don't use transient objects without initializing them.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449707152", "createdAt": "2020-07-03T21:30:28Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.java.ClosureCleaner;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.OutputFileFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<SerializableDataFile>\n+    implements OneInputStreamOperator<Row, SerializableDataFile> {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final SerializableConfiguration conf;\n+  private Schema readSchema;\n+\n+  private transient Table table;\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+\n+  /**\n+   * Be careful to do the initialization in this constructor, because in {@link DataStream#addSink(SinkFunction)}\n+   * it will call {@link ClosureCleaner#clean(Object, ExecutionConfig.ClosureCleanerLevel, boolean)} to set all the\n+   * non-serializable members to be null.\n+   *\n+   * @param tablePath  The base path of the iceberg table.\n+   * @param readSchema The schema of source data.\n+   * @param conf       The hadoop's configuration.\n+   */\n+  private IcebergStreamWriter(String tablePath, Schema readSchema, Configuration conf) {\n+    this.tablePath = tablePath;\n+    this.conf = new SerializableConfiguration(conf);\n+    this.readSchema = readSchema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.table = TableUtil.findTable(tablePath, conf.get());\n+    if (this.readSchema != null) {\n+      // reassign ids to match the existing table schema\n+      readSchema = TypeUtil.reassignIds(readSchema, table.schema());\n+      TypeUtil.validateWriteSchema(readSchema, table.schema(), true, true);\n+    }\n+\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(table);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(table, fileFormat, subTaskId);\n+    this.writer = TaskWriterFactory.createTaskWriter(table.spec(),\n+        appenderFactory,\n+        outputFileFactory,\n+        getTargetFileSizeBytes(),\n+        fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer {} subtask {} begin preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+    for (DataFile dataFile : writer.getCompleteFiles()) {\n+      emit(dataFile);\n+    }\n+    // Remember to clear the writer's cached complete files.\n+    writer.reset();\n+    LOG.info(\"Iceberg writer {} subtask {} completed preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<Row> element) throws Exception {\n+    Row value = element.getValue();\n+    writer.append(value);\n+\n+    // Emit the data file entries to downstream committer operator if there exist any complete files.\n+    List<DataFile> completeFiles = writer.getCompleteFiles();\n+    if (!completeFiles.isEmpty()) {\n+      completeFiles.forEach(this::emit);\n+      // Remember to clear the writer's cached complete files.\n+      writer.reset();\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws Exception {\n+    if (writer != null) {\n+      writer.close();\n+      writer = null;\n+    }\n+  }\n+\n+  private void emit(DataFile dataFile) {\n+    output.collect(new StreamRecord<>(new SerializableDataFile(dataFile)));\n+  }\n+\n+  private FileFormat getFileFormat() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 152}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTY0MTYx", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442564161", "createdAt": "2020-07-03T21:38:33Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTozODozM1rOGs4AXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTozODozM1rOGs4AXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwODEyNw==", "bodyText": "I don't think it is a good idea to mimic the behavior of Spark 2 here. That's really limited. What about using job configuration to instantiate a catalog and load a table by name, like the newer Spark 3 integration does? Here's the javadoc: https://github.com/apache/iceberg/blob/master/spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java#L64-L77\nThat uses this logic to determine the catalog and load a table:\n  /**\n   * Build an Iceberg {@link Catalog} to be used by this Spark catalog adapter.\n   *\n   * @param name Spark's catalog name\n   * @param options Spark's catalog options\n   * @return an Iceberg catalog\n   */\n  protected Catalog buildIcebergCatalog(String name, CaseInsensitiveStringMap options) {\n    Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n    String catalogType = options.getOrDefault(\"type\", \"hive\");\n    switch (catalogType) {\n      case \"hive\":\n        int clientPoolSize = options.getInt(\"clients\", 2);\n        String uri = options.get(\"uri\");\n        return new HiveCatalog(name, uri, clientPoolSize, conf);\n\n      case \"hadoop\":\n        String warehouseLocation = options.get(\"warehouse\");\n        return new HadoopCatalog(name, conf, warehouseLocation);\n\n      default:\n        throw new UnsupportedOperationException(\"Unknown catalog type: \" + catalogType);\n    }\n  }", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449708127", "createdAt": "2020-07-03T21:38:33Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/TableUtil.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+\n+class TableUtil {\n+\n+  private TableUtil() {\n+  }\n+\n+  static Table findTable(String path, Configuration conf) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 34}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTY0Mjk5", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442564299", "createdAt": "2020-07-03T21:39:36Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTozOTozNlrOGs4A4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTozOTozNlrOGs4A4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwODI1OA==", "bodyText": "Minor: in other modules, we don't use this. when reading a field, only when assigning to it so it is clear whether the assignment is to a local variable or a field.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449708258", "createdAt": "2020-07-03T21:39:36Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.java.ClosureCleaner;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.OutputFileFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<SerializableDataFile>\n+    implements OneInputStreamOperator<Row, SerializableDataFile> {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final SerializableConfiguration conf;\n+  private Schema readSchema;\n+\n+  private transient Table table;\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+\n+  /**\n+   * Be careful to do the initialization in this constructor, because in {@link DataStream#addSink(SinkFunction)}\n+   * it will call {@link ClosureCleaner#clean(Object, ExecutionConfig.ClosureCleanerLevel, boolean)} to set all the\n+   * non-serializable members to be null.\n+   *\n+   * @param tablePath  The base path of the iceberg table.\n+   * @param readSchema The schema of source data.\n+   * @param conf       The hadoop's configuration.\n+   */\n+  private IcebergStreamWriter(String tablePath, Schema readSchema, Configuration conf) {\n+    this.tablePath = tablePath;\n+    this.conf = new SerializableConfiguration(conf);\n+    this.readSchema = readSchema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.table = TableUtil.findTable(tablePath, conf.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 93}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTY0MzYw", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442564360", "createdAt": "2020-07-03T21:40:07Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTo0MDowN1rOGs4BKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTo0MDowN1rOGs4BKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwODMyOA==", "bodyText": "Why is this reassigning the read schema's IDs?", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449708328", "createdAt": "2020-07-03T21:40:07Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.java.ClosureCleaner;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.OutputFileFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<SerializableDataFile>\n+    implements OneInputStreamOperator<Row, SerializableDataFile> {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final SerializableConfiguration conf;\n+  private Schema readSchema;\n+\n+  private transient Table table;\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+\n+  /**\n+   * Be careful to do the initialization in this constructor, because in {@link DataStream#addSink(SinkFunction)}\n+   * it will call {@link ClosureCleaner#clean(Object, ExecutionConfig.ClosureCleanerLevel, boolean)} to set all the\n+   * non-serializable members to be null.\n+   *\n+   * @param tablePath  The base path of the iceberg table.\n+   * @param readSchema The schema of source data.\n+   * @param conf       The hadoop's configuration.\n+   */\n+  private IcebergStreamWriter(String tablePath, Schema readSchema, Configuration conf) {\n+    this.tablePath = tablePath;\n+    this.conf = new SerializableConfiguration(conf);\n+    this.readSchema = readSchema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.table = TableUtil.findTable(tablePath, conf.get());\n+    if (this.readSchema != null) {\n+      // reassign ids to match the existing table schema\n+      readSchema = TypeUtil.reassignIds(readSchema, table.schema());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 96}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTY1MTIw", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442565120", "createdAt": "2020-07-03T21:46:28Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTo0NjoyOFrOGs4EMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTo0NjoyOFrOGs4EMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwOTEwNw==", "bodyText": "This won't handle nested data. I think it would be better to use a wrapper for Row and then use the standard accessors that are provided by schema.accessorForField(fieldId). That Row wrapper would implement StructLike, which is what the accessors use. It would also be responsible for converting to the internal representation of data values, like the wrapper for Iceberg generic rows.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449709107", "createdAt": "2020-07-03T21:46:28Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/PartitionKey.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.transforms.Transform;\n+import org.apache.iceberg.types.Types;\n+\n+public class PartitionKey implements StructLike {\n+\n+  private final Object[] partitionTuple;\n+\n+  private PartitionKey(Object[] partitionTuple) {\n+    this.partitionTuple = partitionTuple;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (!(o instanceof PartitionKey)) {\n+      return false;\n+    }\n+\n+    PartitionKey that = (PartitionKey) o;\n+    return Arrays.equals(partitionTuple, that.partitionTuple);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return Arrays.hashCode(partitionTuple);\n+  }\n+\n+  @Override\n+  public int size() {\n+    return partitionTuple.length;\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    return javaClass.cast(partitionTuple[pos]);\n+  }\n+\n+  public Object[] getPartitionTuple() {\n+    return partitionTuple;\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    partitionTuple[pos] = value;\n+  }\n+\n+  private static Map<Integer, Integer> buildFieldId2PosMap(Schema schema) {\n+    Map<Integer, Integer> fieldId2Position = Maps.newHashMap();\n+    List<Types.NestedField> nestedFields = schema.asStruct().fields();\n+    for (int i = 0; i < nestedFields.size(); i++) {\n+      fieldId2Position.put(nestedFields.get(i).fieldId(), i);\n+    }\n+    return fieldId2Position;\n+  }\n+\n+  public static Builder builder(PartitionSpec spec) {\n+    return new Builder(spec);\n+  }\n+\n+  public static class Builder {\n+    private final int size;\n+\n+    private final int[] pos;\n+    private final Transform[] transforms;\n+\n+    private Builder(PartitionSpec spec) {\n+      List<PartitionField> fields = spec.fields();\n+      this.size = fields.size();\n+      this.pos = new int[size];\n+      this.transforms = new Transform[size];\n+\n+      Map<Integer, Integer> fieldId2Pos = buildFieldId2PosMap(spec.schema());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 105}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTY1NDc5", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442565479", "createdAt": "2020-07-03T21:49:49Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTo0OTo1MFrOGs4Fqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTo0OTo1MFrOGs4Fqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwOTQ4Mw==", "bodyText": "In other cases, we avoid allocating a new key for the partition tuple every row, and we defensively copy partition keys. It might be a good idea to generalize the PartitionKey from Spark and use it here.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449709483", "createdAt": "2020-07-03T21:49:50Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/PartitionKey.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.transforms.Transform;\n+import org.apache.iceberg.types.Types;\n+\n+public class PartitionKey implements StructLike {\n+\n+  private final Object[] partitionTuple;\n+\n+  private PartitionKey(Object[] partitionTuple) {\n+    this.partitionTuple = partitionTuple;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (!(o instanceof PartitionKey)) {\n+      return false;\n+    }\n+\n+    PartitionKey that = (PartitionKey) o;\n+    return Arrays.equals(partitionTuple, that.partitionTuple);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return Arrays.hashCode(partitionTuple);\n+  }\n+\n+  @Override\n+  public int size() {\n+    return partitionTuple.length;\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    return javaClass.cast(partitionTuple[pos]);\n+  }\n+\n+  public Object[] getPartitionTuple() {\n+    return partitionTuple;\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    partitionTuple[pos] = value;\n+  }\n+\n+  private static Map<Integer, Integer> buildFieldId2PosMap(Schema schema) {\n+    Map<Integer, Integer> fieldId2Position = Maps.newHashMap();\n+    List<Types.NestedField> nestedFields = schema.asStruct().fields();\n+    for (int i = 0; i < nestedFields.size(); i++) {\n+      fieldId2Position.put(nestedFields.get(i).fieldId(), i);\n+    }\n+    return fieldId2Position;\n+  }\n+\n+  public static Builder builder(PartitionSpec spec) {\n+    return new Builder(spec);\n+  }\n+\n+  public static class Builder {\n+    private final int size;\n+\n+    private final int[] pos;\n+    private final Transform[] transforms;\n+\n+    private Builder(PartitionSpec spec) {\n+      List<PartitionField> fields = spec.fields();\n+      this.size = fields.size();\n+      this.pos = new int[size];\n+      this.transforms = new Transform[size];\n+\n+      Map<Integer, Integer> fieldId2Pos = buildFieldId2PosMap(spec.schema());\n+\n+      for (int i = 0; i < size; i += 1) {\n+        PartitionField field = fields.get(i);\n+        Integer position = fieldId2Pos.get(field.sourceId());\n+        Preconditions.checkArgument(position != null,\n+            \"Field source id from PartitionSpec MUST exist in the original schema\");\n+        this.pos[i] = position;\n+        this.transforms[i] = field.transform();\n+      }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public PartitionKey build(Row row) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 118}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTY1Njgx", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442565681", "createdAt": "2020-07-03T21:51:32Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTo1MTozMlrOGs4Gcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTo1MTozMlrOGs4Gcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwOTY4Mg==", "bodyText": "Why is this needed? The DataFile implementations used internally are Serializable. Although we don't implement Serializable in the DataFile API, as long as you're using the internal implementation of DataFile, you should be able to serialize it.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449709682", "createdAt": "2020-07-03T21:51:32Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/SerializableDataFile.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.Serializable;\n+import org.apache.iceberg.DataFile;\n+\n+public class SerializableDataFile implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 25}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTY2MDk5", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442566099", "createdAt": "2020-07-03T21:55:23Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTo1NToyM1rOGs4IPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTo1NToyM1rOGs4IPw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMDE0Mw==", "bodyText": "Table is not serializable. That's why the Spark output file factory has fields for LocationProvider, FileIO, EncryptionManager, and PartitionSpec that are serializable. Those are held by the task and this is created on each task after serialization to workers.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449710143", "createdAt": "2020-07-03T21:55:23Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/writer/OutputFileFactory.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.util.UUID;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.PartitionKey;\n+import org.apache.iceberg.io.OutputFile;\n+\n+public class OutputFileFactory {\n+  private final String uuid = UUID.randomUUID().toString();\n+\n+  private final Table table;\n+  private final FileFormat format;\n+  private final long taskId;\n+  private int fileCount;\n+\n+  public OutputFileFactory(Table table, FileFormat format, long taskId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 37}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTY2NTQ0", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442566544", "createdAt": "2020-07-03T21:59:31Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTo1OTozMVrOGs4KAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMTo1OTozMVrOGs4KAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMDU5NA==", "bodyText": "If you're following the Spark file naming convention, then the the first number is equivalent to getRuntimeContext().getIndexOfThisSubtask(). The second number is the unique ID for that task in Spark, so that task reattempts don't collide.\nIs there an attempt ID or something in Flink? I don't see much value in using a hash code, unless it is System.identityHashCode and can take on more values than this.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449710594", "createdAt": "2020-07-03T21:59:31Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/writer/OutputFileFactory.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.util.UUID;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.PartitionKey;\n+import org.apache.iceberg.io.OutputFile;\n+\n+public class OutputFileFactory {\n+  private final String uuid = UUID.randomUUID().toString();\n+\n+  private final Table table;\n+  private final FileFormat format;\n+  private final long taskId;\n+  private int fileCount;\n+\n+  public OutputFileFactory(Table table, FileFormat format, long taskId) {\n+    this.table = table;\n+    this.format = format;\n+    this.taskId = taskId;\n+    this.fileCount = 0;\n+  }\n+\n+  /**\n+   * All the data files inside the same task will share the same uuid identifier but could be distinguished by the\n+   * increasing file count.\n+   *\n+   * @return the data file name to be written.\n+   */\n+  private String generateFilename() {\n+    int hashCode = Math.abs(this.hashCode() % 10 ^ 5);\n+    return format.addExtension(String.format(\"%05d-%d-%s-%05d\", hashCode, taskId, uuid, fileCount++));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 52}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTY3NTI0", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442567524", "createdAt": "2020-07-03T22:08:14Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMjowODoxNFrOGs4Nsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMjowODoxNFrOGs4Nsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMTUzOQ==", "bodyText": "withEncryptedOutputFile sets all of the information from the file, so you don't need to set the path separately.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449711539", "createdAt": "2020-07-03T22:08:14Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/writer/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+\n+  protected DataFile closeFileAppender(FileAppender<T> fileAppender, EncryptedOutputFile currentFile,\n+                                       PartitionSpec spec, StructLike partitionKey) throws IOException {\n+    // Close the file appender firstly.\n+    fileAppender.close();\n+\n+    // metrics are only valid after the appender is closed.\n+    Metrics metrics = fileAppender.metrics();\n+    long fileSizeInBytes = fileAppender.length();\n+    List<Long> splitOffsets = fileAppender.splitOffsets();\n+\n+    // Construct the DataFile and add it into the completeDataFiles.\n+    return DataFiles.builder(spec)\n+        .withEncryptedOutputFile(currentFile)\n+        .withPath(currentFile.encryptingOutputFile().location())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 47}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTY3NTc1", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442567575", "createdAt": "2020-07-03T22:08:44Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMjowODo0NFrOGs4N1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMjowODo0NFrOGs4N1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMTU3NQ==", "bodyText": "Looks like this comment is no longer correct because completeDataFiles is private in the subclasses.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449711575", "createdAt": "2020-07-03T22:08:44Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/writer/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+\n+  protected DataFile closeFileAppender(FileAppender<T> fileAppender, EncryptedOutputFile currentFile,\n+                                       PartitionSpec spec, StructLike partitionKey) throws IOException {\n+    // Close the file appender firstly.\n+    fileAppender.close();\n+\n+    // metrics are only valid after the appender is closed.\n+    Metrics metrics = fileAppender.metrics();\n+    long fileSizeInBytes = fileAppender.length();\n+    List<Long> splitOffsets = fileAppender.splitOffsets();\n+\n+    // Construct the DataFile and add it into the completeDataFiles.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 44}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTY3ODcz", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442567873", "createdAt": "2020-07-03T22:11:39Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMjoxMTozOVrOGs4PEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMjoxMTozOVrOGs4PEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMTg4OQ==", "bodyText": "Shouldn't this also ensure that all of the appenders are closed?", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449711889", "createdAt": "2020-07-03T22:11:39Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/writer/PartitionWriter.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.PartitionKey;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The PartitionFanoutWriter will open a writing data file for each partition and route the given record to the\n+ * corresponding data file in the correct partition.\n+ *\n+ * @param <T> defines the data type of record to write.\n+ */\n+class PartitionWriter<T> extends BaseTaskWriter<T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(PartitionWriter.class);\n+\n+  private final PartitionSpec spec;\n+  private final FileAppenderFactory<T> factory;\n+  private final Function<PartitionKey, EncryptedOutputFile> outputFileGetter;\n+  private final Function<T, PartitionKey> keyGetter;\n+  private final Map<PartitionKey, WrappedFileAppender<T>> writers;\n+  private final long targetFileSize;\n+  private final FileFormat fileFormat;\n+  private final List<DataFile> completeDataFiles;\n+\n+  PartitionWriter(PartitionSpec spec,\n+                  FileAppenderFactory<T> factory,\n+                  Function<PartitionKey, EncryptedOutputFile> outputFileGetter,\n+                  Function<T, PartitionKey> keyGetter,\n+                  long targetFileSize,\n+                  FileFormat fileFormat) {\n+    this.spec = spec;\n+    this.factory = factory;\n+    this.outputFileGetter = outputFileGetter;\n+    this.keyGetter = keyGetter;\n+    this.writers = Maps.newHashMap();\n+    this.targetFileSize = targetFileSize;\n+    this.fileFormat = fileFormat;\n+    this.completeDataFiles = Lists.newArrayList();\n+  }\n+\n+  @Override\n+  public void append(T record) throws IOException {\n+    PartitionKey partitionKey = keyGetter.apply(record);\n+    Preconditions.checkArgument(partitionKey != null, \"Partition key shouldn't be null\");\n+\n+    WrappedFileAppender<T> writer = writers.get(partitionKey);\n+    if (writer == null) {\n+      writer = createWrappedFileAppender(partitionKey);\n+      writers.put(partitionKey, writer);\n+    }\n+    writer.fileAppender.add(record);\n+\n+    // Roll the writer if reach the target file size.\n+    writer.currentRows++;\n+    if (writer.currentRows % ROW_DIVISOR == 0 && writer.fileAppender.length() >= targetFileSize) {\n+      closeCurrentWriter(writer);\n+      writers.remove(partitionKey);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    for (WrappedFileAppender<T> wrap : writers.values()) {\n+      closeCurrentWriter(wrap);\n+      LOG.debug(\"Close file appender: {}, completeDataFiles: {}\",\n+          wrap.encryptedOutputFile.encryptingOutputFile().location(),\n+          completeDataFiles.size());\n+    }\n+    this.writers.clear();\n+  }\n+\n+  @Override\n+  public List<DataFile> getCompleteFiles() {\n+    if (completeDataFiles.size() > 0) {\n+      return ImmutableList.copyOf(this.completeDataFiles);\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  @Override\n+  public void reset() {\n+    this.completeDataFiles.clear();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 117}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTY4NTkx", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442568591", "createdAt": "2020-07-03T22:18:24Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMjoxODoyNVrOGs4R1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMjoxODoyNVrOGs4R1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMjU5Ng==", "bodyText": "I would normally expect reset to restore the writer to a new state for reuse. Looks like in this case it is clearing the files that have already been consumed. I think that this could have a better name for the operation.\nAlso, I would rather improve this API. It seems brittle to get the list and then reset it through the writer. I think it would be cleaner to have a poll call that retrieves one data file and removes it from the internal tracking, if one is ready. Then this could call poll until there are no more files.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449712596", "createdAt": "2020-07-03T22:18:25Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.java.ClosureCleaner;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.OutputFileFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<SerializableDataFile>\n+    implements OneInputStreamOperator<Row, SerializableDataFile> {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final SerializableConfiguration conf;\n+  private Schema readSchema;\n+\n+  private transient Table table;\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+\n+  /**\n+   * Be careful to do the initialization in this constructor, because in {@link DataStream#addSink(SinkFunction)}\n+   * it will call {@link ClosureCleaner#clean(Object, ExecutionConfig.ClosureCleanerLevel, boolean)} to set all the\n+   * non-serializable members to be null.\n+   *\n+   * @param tablePath  The base path of the iceberg table.\n+   * @param readSchema The schema of source data.\n+   * @param conf       The hadoop's configuration.\n+   */\n+  private IcebergStreamWriter(String tablePath, Schema readSchema, Configuration conf) {\n+    this.tablePath = tablePath;\n+    this.conf = new SerializableConfiguration(conf);\n+    this.readSchema = readSchema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.table = TableUtil.findTable(tablePath, conf.get());\n+    if (this.readSchema != null) {\n+      // reassign ids to match the existing table schema\n+      readSchema = TypeUtil.reassignIds(readSchema, table.schema());\n+      TypeUtil.validateWriteSchema(readSchema, table.schema(), true, true);\n+    }\n+\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(table);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(table, fileFormat, subTaskId);\n+    this.writer = TaskWriterFactory.createTaskWriter(table.spec(),\n+        appenderFactory,\n+        outputFileFactory,\n+        getTargetFileSizeBytes(),\n+        fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer {} subtask {} begin preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+    for (DataFile dataFile : writer.getCompleteFiles()) {\n+      emit(dataFile);\n+    }\n+    // Remember to clear the writer's cached complete files.\n+    writer.reset();\n+    LOG.info(\"Iceberg writer {} subtask {} completed preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<Row> element) throws Exception {\n+    Row value = element.getValue();\n+    writer.append(value);\n+\n+    // Emit the data file entries to downstream committer operator if there exist any complete files.\n+    List<DataFile> completeFiles = writer.getCompleteFiles();\n+    if (!completeFiles.isEmpty()) {\n+      completeFiles.forEach(this::emit);\n+      // Remember to clear the writer's cached complete files.\n+      writer.reset();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 136}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQyNTY4NjY2", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-442568666", "createdAt": "2020-07-03T22:19:04Z", "commit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMjoxOTowNFrOGs4R_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QyMjoxOTowNFrOGs4R_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMjYzOQ==", "bodyText": "When this close happens, what emits the data files?", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449712639", "createdAt": "2020-07-03T22:19:04Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.java.ClosureCleaner;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.OutputFileFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<SerializableDataFile>\n+    implements OneInputStreamOperator<Row, SerializableDataFile> {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final SerializableConfiguration conf;\n+  private Schema readSchema;\n+\n+  private transient Table table;\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+\n+  /**\n+   * Be careful to do the initialization in this constructor, because in {@link DataStream#addSink(SinkFunction)}\n+   * it will call {@link ClosureCleaner#clean(Object, ExecutionConfig.ClosureCleanerLevel, boolean)} to set all the\n+   * non-serializable members to be null.\n+   *\n+   * @param tablePath  The base path of the iceberg table.\n+   * @param readSchema The schema of source data.\n+   * @param conf       The hadoop's configuration.\n+   */\n+  private IcebergStreamWriter(String tablePath, Schema readSchema, Configuration conf) {\n+    this.tablePath = tablePath;\n+    this.conf = new SerializableConfiguration(conf);\n+    this.readSchema = readSchema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.table = TableUtil.findTable(tablePath, conf.get());\n+    if (this.readSchema != null) {\n+      // reassign ids to match the existing table schema\n+      readSchema = TypeUtil.reassignIds(readSchema, table.schema());\n+      TypeUtil.validateWriteSchema(readSchema, table.schema(), true, true);\n+    }\n+\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(table);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(table, fileFormat, subTaskId);\n+    this.writer = TaskWriterFactory.createTaskWriter(table.spec(),\n+        appenderFactory,\n+        outputFileFactory,\n+        getTargetFileSizeBytes(),\n+        fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer {} subtask {} begin preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+    for (DataFile dataFile : writer.getCompleteFiles()) {\n+      emit(dataFile);\n+    }\n+    // Remember to clear the writer's cached complete files.\n+    writer.reset();\n+    LOG.info(\"Iceberg writer {} subtask {} completed preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<Row> element) throws Exception {\n+    Row value = element.getValue();\n+    writer.append(value);\n+\n+    // Emit the data file entries to downstream committer operator if there exist any complete files.\n+    List<DataFile> completeFiles = writer.getCompleteFiles();\n+    if (!completeFiles.isEmpty()) {\n+      completeFiles.forEach(this::emit);\n+      // Remember to clear the writer's cached complete files.\n+      writer.reset();\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws Exception {\n+    if (writer != null) {\n+      writer.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "60b566fbe23e118823020827ba79cea178f2089e"}, "originalPosition": 143}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2fea93a361278d875cf7d9a1ff25112e458516a3", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/2fea93a361278d875cf7d9a1ff25112e458516a3", "committedDate": "2020-07-06T08:39:29Z", "message": "Addressing the reviewing comment."}, "afterCommit": {"oid": "3e297442b9d4c98f5fdc20ee318856b43d575b0a", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/3e297442b9d4c98f5fdc20ee318856b43d575b0a", "committedDate": "2020-07-06T10:10:35Z", "message": "Rebase the origin/master and add Avro writers and unit tests."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ0NDg1OTA0", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-444485904", "createdAt": "2020-07-08T07:38:25Z", "commit": {"oid": "c91d89a0ab414e3cb021de5164d5dbe40030ec87"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQwNzozODoyNVrOGub0pw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOVQwMjo1MjoyM1rOGu_5AA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTM0MzUyNw==", "bodyText": "isEmpty redundant?", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r451343527", "createdAt": "2020-07-08T07:38:25Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.OutputFileFactory;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<DataFile>\n+    implements OneInputStreamOperator<Row, DataFile>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final PartitionSpec spec;\n+  private final LocationProvider locations;\n+  private final Map<String, String> properties;\n+  private final FileIO io;\n+  private final EncryptionManager encryptionManager;\n+  private final Schema schema;\n+\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+  private transient int attemptId;\n+\n+  private IcebergStreamWriter(String tablePath, PartitionSpec spec, LocationProvider locations,\n+                              Map<String, String> properties, FileIO io, EncryptionManager encryptionManager,\n+                              Schema schema) {\n+    this.tablePath = tablePath;\n+    this.spec = spec;\n+    this.locations = locations;\n+    this.properties = properties;\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.schema = schema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+    this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    long targetFileSize = getTargetFileSizeBytes();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(schema, properties);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(spec, fileFormat, locations, io,\n+        encryptionManager, subTaskId, attemptId);\n+    this.writer = TaskWriterFactory.createTaskWriter(spec,\n+        appenderFactory,\n+        outputFileFactory,\n+        targetFileSize,\n+        fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer({}) begin preparing for checkpoint {}\", toString(), checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+    for (DataFile dataFile : writer.pollCompleteFiles()) {\n+      emit(dataFile);\n+    }\n+    LOG.info(\"Iceberg writer({}) completed preparing for checkpoint {}\", toString(), checkpointId);\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<Row> element) throws Exception {\n+    Row value = element.getValue();\n+    writer.append(value);\n+\n+    // Emit the data file entries to downstream committer operator if there exist any complete files.\n+    List<DataFile> completeFiles = writer.pollCompleteFiles();\n+    if (!completeFiles.isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c91d89a0ab414e3cb021de5164d5dbe40030ec87"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTM0Mzg0Mw==", "bodyText": "NIT: writer.pollCompleteFiles().forEach(this::emit);", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r451343843", "createdAt": "2020-07-08T07:39:00Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.OutputFileFactory;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<DataFile>\n+    implements OneInputStreamOperator<Row, DataFile>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final PartitionSpec spec;\n+  private final LocationProvider locations;\n+  private final Map<String, String> properties;\n+  private final FileIO io;\n+  private final EncryptionManager encryptionManager;\n+  private final Schema schema;\n+\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+  private transient int attemptId;\n+\n+  private IcebergStreamWriter(String tablePath, PartitionSpec spec, LocationProvider locations,\n+                              Map<String, String> properties, FileIO io, EncryptionManager encryptionManager,\n+                              Schema schema) {\n+    this.tablePath = tablePath;\n+    this.spec = spec;\n+    this.locations = locations;\n+    this.properties = properties;\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.schema = schema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+    this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    long targetFileSize = getTargetFileSizeBytes();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(schema, properties);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(spec, fileFormat, locations, io,\n+        encryptionManager, subTaskId, attemptId);\n+    this.writer = TaskWriterFactory.createTaskWriter(spec,\n+        appenderFactory,\n+        outputFileFactory,\n+        targetFileSize,\n+        fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer({}) begin preparing for checkpoint {}\", toString(), checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+    for (DataFile dataFile : writer.pollCompleteFiles()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c91d89a0ab414e3cb021de5164d5dbe40030ec87"}, "originalPosition": 118}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzNDQ2NA==", "bodyText": "Revert this?", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r451934464", "createdAt": "2020-07-09T02:52:23Z", "author": {"login": "JingsongLi"}, "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -360,7 +360,7 @@ public ByteBuffer keyMetadata() {\n     if (list != null) {\n       List<E> copy = Lists.newArrayListWithExpectedSize(list.size());\n       copy.addAll(list);\n-      return Collections.unmodifiableList(copy);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c91d89a0ab414e3cb021de5164d5dbe40030ec87"}, "originalPosition": 4}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fae8b217d9576fc6dcb036442e0cfe945404deba", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/fae8b217d9576fc6dcb036442e0cfe945404deba", "committedDate": "2020-07-09T03:11:09Z", "message": "Address serveral minor issues"}, "afterCommit": {"oid": "2355608252b282dc00357654d9dcd53bd3e46e30", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/2355608252b282dc00357654d9dcd53bd3e46e30", "committedDate": "2020-07-09T04:27:16Z", "message": "Address serveral minor issues"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2355608252b282dc00357654d9dcd53bd3e46e30", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/2355608252b282dc00357654d9dcd53bd3e46e30", "committedDate": "2020-07-09T04:27:16Z", "message": "Address serveral minor issues"}, "afterCommit": {"oid": "c62f38af5db893d15e6221baa32ae6adce8ef024", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/c62f38af5db893d15e6221baa32ae6adce8ef024", "committedDate": "2020-07-16T03:57:44Z", "message": "Rebase and use the abstracted PartitionKey in iceberg-api"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUyMTA2NjYx", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-452106661", "createdAt": "2020-07-21T03:49:23Z", "commit": {"oid": "95d5affa6a772198c9bd162636b87d336f463f05"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMzo0OToyNFrOG0nEzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMzo0OToyNFrOG0nEzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgxOTM0Mw==", "bodyText": "Can we just pass a Iceberg Table instead of path here? We can get ride of TableUtil.findTable (Or just move it to test).", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r457819343", "createdAt": "2020-07-21T03:49:24Z", "author": {"login": "JingsongLi"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.OutputFileFactory;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<DataFile>\n+    implements OneInputStreamOperator<Row, DataFile>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final PartitionSpec spec;\n+  private final LocationProvider locations;\n+  private final Map<String, String> properties;\n+  private final FileIO io;\n+  private final EncryptionManager encryptionManager;\n+  private final Schema schema;\n+\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+  private transient int attemptId;\n+\n+  private IcebergStreamWriter(String tablePath, PartitionSpec spec, LocationProvider locations,\n+                              Map<String, String> properties, FileIO io, EncryptionManager encryptionManager,\n+                              Schema schema) {\n+    this.tablePath = tablePath;\n+    this.spec = spec;\n+    this.locations = locations;\n+    this.properties = properties;\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.schema = schema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+    this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    long targetFileSize = getTargetFileSizeBytes();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(schema, properties);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(spec, fileFormat, locations, io,\n+        encryptionManager, subTaskId, attemptId);\n+    this.writer = TaskWriterFactory.createTaskWriter(schema, spec, appenderFactory, outputFileFactory,\n+        targetFileSize, fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer({}) begin preparing for checkpoint {}\", toString(), checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+\n+    writer.pollCompleteFiles().forEach(this::emit);\n+    LOG.info(\"Iceberg writer({}) completed preparing for checkpoint {}\", toString(), checkpointId);\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<Row> element) throws Exception {\n+    Row value = element.getValue();\n+    writer.append(value);\n+\n+    // Emit the data file entries to downstream committer operator if there exist any complete files.\n+    writer.pollCompleteFiles().forEach(this::emit);\n+  }\n+\n+  @Override\n+  public void dispose() throws Exception {\n+    super.dispose();\n+    if (writer != null) {\n+      writer.close();\n+      writer = null;\n+    }\n+  }\n+\n+  @Override\n+  public void endInput() throws IOException {\n+    // For bounded stream, it may don't enable the checkpoint mechanism so we'd better to emit the remaining\n+    // data files to downstream before closing the writer so that we won't miss any of them.\n+    writer.close();\n+    for (DataFile dataFile : writer.pollCompleteFiles()) {\n+      emit(dataFile);\n+    }\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return MoreObjects.toStringHelper(this)\n+        .add(\"table_path\", tablePath)\n+        .add(\"subtask_id\", subTaskId)\n+        .add(\"attempt_id\", attemptId)\n+        .toString();\n+  }\n+\n+  private void emit(DataFile dataFile) {\n+    output.collect(new StreamRecord<>(dataFile));\n+  }\n+\n+  private FileFormat getFileFormat() {\n+    String formatString = properties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT);\n+    return FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  private long getTargetFileSizeBytes() {\n+    return PropertyUtil.propertyAsLong(properties,\n+        WRITE_TARGET_FILE_SIZE_BYTES,\n+        WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+  }\n+\n+  private static class FlinkFileAppenderFactory implements FileAppenderFactory<Row> {\n+    private final Schema schema;\n+    private final Map<String, String> props;\n+\n+    private FlinkFileAppenderFactory(Schema schema, Map<String, String> props) {\n+      this.schema = schema;\n+      this.props = props;\n+    }\n+\n+    @Override\n+    public FileAppender<Row> newAppender(OutputFile outputFile, FileFormat format) {\n+      MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+      try {\n+        switch (format) {\n+          case PARQUET:\n+            return Parquet.write(outputFile)\n+                .createWriterFunc(FlinkParquetWriters::buildWriter)\n+                .setAll(props)\n+                .metricsConfig(metricsConfig)\n+                .schema(schema)\n+                .overwrite()\n+                .build();\n+\n+          case AVRO:\n+            return Avro.write(outputFile)\n+                .createWriterFunc(FlinkAvroWriter::new)\n+                .setAll(props)\n+                .schema(schema)\n+                .overwrite()\n+                .build();\n+\n+          case ORC:\n+          default:\n+            throw new UnsupportedOperationException(\"Cannot write unknown format: \" + format);\n+        }\n+      } catch (IOException e) {\n+        throw new UncheckedIOException(e);\n+      }\n+    }\n+  }\n+\n+  static IcebergStreamWriter createStreamWriter(String path, TableSchema tableSchema, Configuration conf) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95d5affa6a772198c9bd162636b87d336f463f05"}, "originalPosition": 212}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU2MDQ3Mjg0", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-456047284", "createdAt": "2020-07-27T18:51:25Z", "commit": {"oid": "95d5affa6a772198c9bd162636b87d336f463f05"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxODo1MToyNVrOG3vO9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxODo1MToyNVrOG3vO9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTA5ODc0Mg==", "bodyText": "Is it allowed to call pollCompleteFiles() after close(). IIUC, the writer is still needed when emitting the remaining datafiles as well as when calling pollCompleteFiles(). But possibly the contract around close is different than I expect it to be.\nAdditionally, the comment here is somewhat confusing. Based on the discussion above, and based on the task lifecycle of Flink operators outlined here, it seems like it might be more appropriate to say something along the lines of \"Once the close method of this IcebergStreamWriter is invoked, we'll no longer be able to emit any remaining data files downstream. To get around this, we implement the BoundedOneInput interface in order to finish processing and emit any remaining data before graceful shutdown\".\nMy third and final concern is that if we're closing the TaskWriter here during a graceful shutdown, if we only poll for complete files, what will happen to any remaining data that's being buffered in files that are not marked as complete? IIUC, the TaskWriter does not keep its opened / in-process files in flink's state such that it can be recovered after restore or replayed from the last checkpoint after a fatal. If I have that correct, it seems to me that we would want to poll for simply any remaining open files and emit them, otherwise we risk data loss. Can somebody help me better understand how we avoid losing data from in-process but incomplete files during a graceful shutdown (say a user takes a savepoint in order to restart their application and deploy a new version of their code)?", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r461098742", "createdAt": "2020-07-27T18:51:25Z", "author": {"login": "kbendick"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.OutputFileFactory;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<DataFile>\n+    implements OneInputStreamOperator<Row, DataFile>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final PartitionSpec spec;\n+  private final LocationProvider locations;\n+  private final Map<String, String> properties;\n+  private final FileIO io;\n+  private final EncryptionManager encryptionManager;\n+  private final Schema schema;\n+\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+  private transient int attemptId;\n+\n+  private IcebergStreamWriter(String tablePath, PartitionSpec spec, LocationProvider locations,\n+                              Map<String, String> properties, FileIO io, EncryptionManager encryptionManager,\n+                              Schema schema) {\n+    this.tablePath = tablePath;\n+    this.spec = spec;\n+    this.locations = locations;\n+    this.properties = properties;\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.schema = schema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+    this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    long targetFileSize = getTargetFileSizeBytes();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(schema, properties);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(spec, fileFormat, locations, io,\n+        encryptionManager, subTaskId, attemptId);\n+    this.writer = TaskWriterFactory.createTaskWriter(schema, spec, appenderFactory, outputFileFactory,\n+        targetFileSize, fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer({}) begin preparing for checkpoint {}\", toString(), checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+\n+    writer.pollCompleteFiles().forEach(this::emit);\n+    LOG.info(\"Iceberg writer({}) completed preparing for checkpoint {}\", toString(), checkpointId);\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<Row> element) throws Exception {\n+    Row value = element.getValue();\n+    writer.append(value);\n+\n+    // Emit the data file entries to downstream committer operator if there exist any complete files.\n+    writer.pollCompleteFiles().forEach(this::emit);\n+  }\n+\n+  @Override\n+  public void dispose() throws Exception {\n+    super.dispose();\n+    if (writer != null) {\n+      writer.close();\n+      writer = null;\n+    }\n+  }\n+\n+  @Override\n+  public void endInput() throws IOException {\n+    // For bounded stream, it may don't enable the checkpoint mechanism so we'd better to emit the remaining\n+    // data files to downstream before closing the writer so that we won't miss any of them.\n+    writer.close();\n+    for (DataFile dataFile : writer.pollCompleteFiles()) {\n+      emit(dataFile);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "95d5affa6a772198c9bd162636b87d336f463f05"}, "originalPosition": 144}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "95d5affa6a772198c9bd162636b87d336f463f05", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/95d5affa6a772198c9bd162636b87d336f463f05", "committedDate": "2020-07-16T06:32:26Z", "message": "Refactor to abstract the BaseTaskWriter."}, "afterCommit": {"oid": "25d8f8404500db432c7015c3a4e83f9ffa9d9af3", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/25d8f8404500db432c7015c3a4e83f9ffa9d9af3", "committedDate": "2020-08-03T07:55:06Z", "message": "Rebase to use the abstracted task writers."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f64a21972d9e4597dc66df41a51a2455aed6c64e", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/f64a21972d9e4597dc66df41a51a2455aed6c64e", "committedDate": "2020-08-03T12:33:20Z", "message": "Add customized data file serializer."}, "afterCommit": {"oid": "b54d02430bf9813849c79ff93e7a5360b9dd5ff6", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/b54d02430bf9813849c79ff93e7a5360b9dd5ff6", "committedDate": "2020-08-03T12:55:55Z", "message": "Add customized data file serializer."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMDIwMDMy", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-461020032", "createdAt": "2020-08-04T17:25:33Z", "commit": {"oid": "7989956bff8f3770afe9d0a853bde181fb2918c7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxNzoyNTozM1rOG7qO7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxNzoyNTozM1rOG7qO7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIxMTExOQ==", "bodyText": "Could you revert this change? We try to avoid changes that are non-functional and can cause commit conflicts between pull requests and while cherry-picking commits. In addition, keeping these aligned just leads to more changes. I think it's fine to format these aligned in new code, but I don't think it is a good idea to go back and make extra changes just to align.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r465211119", "createdAt": "2020-08-04T17:25:33Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -134,14 +134,14 @@ public PartitionData copy() {\n     this.nullValueCounts = nullValueCounts;\n     this.lowerBounds = SerializableByteBufferMap.wrap(lowerBounds);\n     this.upperBounds = SerializableByteBufferMap.wrap(upperBounds);\n-    this.splitOffsets = copy(splitOffsets);\n+    this.splitOffsets = splitOffsets == null ? null : splitOffsets.toArray(new Long[0]);\n     this.keyMetadata = ByteBuffers.toByteArray(keyMetadata);\n   }\n \n   /**\n    * Copy constructor.\n    *\n-   * @param toCopy a generic data file to copy.\n+   * @param toCopy   a generic data file to copy.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7989956bff8f3770afe9d0a853bde181fb2918c7"}, "originalPosition": 22}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMDIxNTA4", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-461021508", "createdAt": "2020-08-04T17:27:33Z", "commit": {"oid": "7989956bff8f3770afe9d0a853bde181fb2918c7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxNzoyNzozNFrOG7qTlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQxNzoyNzozNFrOG7qTlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIxMjMxMA==", "bodyText": "The corresponding get implementation also needs to be updated. That's what is causing test failures.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r465212310", "createdAt": "2020-08-04T17:27:34Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -234,7 +235,7 @@ public void put(int i, Object value) {\n         this.keyMetadata = ByteBuffers.toByteArray((ByteBuffer) value);\n         return;\n       case 12:\n-        this.splitOffsets = (List<Long>) value;\n+        this.splitOffsets = value != null ? ((List<Long>) value).toArray(new Long[0]) : null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7989956bff8f3770afe9d0a853bde181fb2918c7"}, "originalPosition": 41}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMjY5Njgy", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-461269682", "createdAt": "2020-08-05T00:55:19Z", "commit": {"oid": "7989956bff8f3770afe9d0a853bde181fb2918c7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMDo1NToyMFrOG72YDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMDo1NToyMFrOG72YDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMDA2MA==", "bodyText": "Minor: I recommend using table.name() instead of table.location(). The name is set by the catalog, so it should be how the table was identified. For Hive, it is catalog.db.table and for Hadoop it is the location. So it is a more natural identifier to use.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r465410060", "createdAt": "2020-08-05T00:55:20Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+\n+class IcebergStreamWriter<T> extends AbstractStreamOperator<DataFile>\n+    implements OneInputStreamOperator<T, DataFile>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private final String tablePath;\n+\n+  private transient TaskWriterFactory<T> taskWriterFactory;\n+  private transient TaskWriter<T> writer;\n+  private transient int subTaskId;\n+  private transient int attemptId;\n+\n+  IcebergStreamWriter(String tablePath, TaskWriterFactory<T> taskWriterFactory) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7989956bff8f3770afe9d0a853bde181fb2918c7"}, "originalPosition": 43}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYxMjcwMjA0", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-461270204", "createdAt": "2020-08-05T00:57:05Z", "commit": {"oid": "7989956bff8f3770afe9d0a853bde181fb2918c7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMDo1NzowNVrOG72aBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMDo1NzowNVrOG72aBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMDU2NA==", "bodyText": "Is this guaranteed to be called from the same thread as processElement?", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r465410564", "createdAt": "2020-08-05T00:57:05Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+\n+class IcebergStreamWriter<T> extends AbstractStreamOperator<DataFile>\n+    implements OneInputStreamOperator<T, DataFile>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private final String tablePath;\n+\n+  private transient TaskWriterFactory<T> taskWriterFactory;\n+  private transient TaskWriter<T> writer;\n+  private transient int subTaskId;\n+  private transient int attemptId;\n+\n+  IcebergStreamWriter(String tablePath, TaskWriterFactory<T> taskWriterFactory) {\n+    this.tablePath = tablePath;\n+    this.taskWriterFactory = taskWriterFactory;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+    this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+    // Initialize the task writer factory.\n+    this.taskWriterFactory.initialize(subTaskId, attemptId);\n+\n+    // Initialize the task writer.\n+    this.writer = taskWriterFactory.create();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7989956bff8f3770afe9d0a853bde181fb2918c7"}, "originalPosition": 61}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d271f6ffde40f912d276c91083ed119b05d3b2db", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/d271f6ffde40f912d276c91083ed119b05d3b2db", "committedDate": "2020-08-06T02:01:28Z", "message": "Rebase to use the abstracted task writers."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "550345ba07b1428ff52687fecb97fb3e4a604071", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/550345ba07b1428ff52687fecb97fb3e4a604071", "committedDate": "2020-08-06T02:01:28Z", "message": "Add customized data file serializer."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9e5aa9dcbc9222ba177eeaec28443cbfc818d829", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/9e5aa9dcbc9222ba177eeaec28443cbfc818d829", "committedDate": "2020-08-06T02:01:28Z", "message": "Address the kyro serialize issues"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "09f5f4edc37a685674daf70a14711685bae05b55", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/09f5f4edc37a685674daf70a14711685bae05b55", "committedDate": "2020-08-06T02:06:58Z", "message": "Address comments"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c0d7045e504b81d5765b9acd378e76a6995af397", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/c0d7045e504b81d5765b9acd378e76a6995af397", "committedDate": "2020-08-05T04:38:05Z", "message": "Address comments"}, "afterCommit": {"oid": "09f5f4edc37a685674daf70a14711685bae05b55", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/09f5f4edc37a685674daf70a14711685bae05b55", "committedDate": "2020-08-06T02:06:58Z", "message": "Address comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b5790f37a3cf369af2e942b4a5a2e3615caa7e38", "author": {"user": {"login": "openinx", "name": "openinx"}}, "url": "https://github.com/apache/iceberg/commit/b5790f37a3cf369af2e942b4a5a2e3615caa7e38", "committedDate": "2020-08-06T02:15:31Z", "message": "Remove the avro building in FlinkFileAppenderFactory."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYzNDUzNTYw", "url": "https://github.com/apache/iceberg/pull/1145#pullrequestreview-463453560", "createdAt": "2020-08-07T16:41:10Z", "commit": {"oid": "b5790f37a3cf369af2e942b4a5a2e3615caa7e38"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjo0MToxMFrOG9gnKw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNjo0MToxMFrOG9gnKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE1MDYzNQ==", "bodyText": "Should this return an ImmutableList so that it cannot be modified? That's what was happening before, although I think it matter less if this is creating a new list each time it is returned.", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r467150635", "createdAt": "2020-08-07T16:41:10Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -357,7 +358,7 @@ public ByteBuffer keyMetadata() {\n \n   @Override\n   public List<Long> splitOffsets() {\n-    return splitOffsets;\n+    return splitOffsets != null ? Lists.newArrayList(splitOffsets) : null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b5790f37a3cf369af2e942b4a5a2e3615caa7e38"}, "originalPosition": 51}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4574, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}