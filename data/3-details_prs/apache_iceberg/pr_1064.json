{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDIyOTA3OTA0", "number": 1064, "title": "Add DeleteFile and manifest reader and writer for deletes", "bodyText": "This adds a new interface, DeleteFile, and implementations of ManfiestReader and ManifestWriter for deletes.\nDeleteFile and DataFile now inherit from a common interface, ContentFile, with all of the metadata. The purpose of separate interfaces is to keep data and delete files separate in the APIs. DataFile can't be written to a delete manifest, for example. This also uses a common implementation, BaseFile for both GenericDataFile and GenericDeleteFile.\nBecause ManifestEntry stores a DataFile or a DeleteFile, this adds a type parameter to it. Adding this type parameter is why so many files changed, but most of the changes are in the last commit and are just parameter additions.\nSome classes that may be used to return DeleteFiles (or ManifestEntry) currently return only DataFile, like ManifestGroup. This is currently safe because there is no code to read a DeleteManifest in those classes. We can update the implementations as we add support for delete manifests.", "createdAt": "2020-05-25T21:57:51Z", "url": "https://github.com/apache/iceberg/pull/1064", "merged": true, "mergeCommit": {"oid": "527240b445b23cef1a655eccbb3b2c0eb7d178c1"}, "closed": true, "closedAt": "2020-05-29T19:26:56Z", "author": {"login": "rdblue"}, "timelineItems": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABck4U4jgBqjMzNzE0MTA3MTg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcmHmEJgFqTQyMTIyNTQ3MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9fcc894ce1e037ee278f31509394dfc1bcd7e362", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/9fcc894ce1e037ee278f31509394dfc1bcd7e362", "committedDate": "2020-05-25T23:00:40Z", "message": "Extract ContentFile from DataFile and add type param to ManifestEntry."}, "afterCommit": {"oid": "1152bc110d7b69741f4545ac04c2d9fe40584a22", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/1152bc110d7b69741f4545ac04c2d9fe40584a22", "committedDate": "2020-05-25T23:04:38Z", "message": "Extract ContentFile from DataFile and add type param to ManifestEntry."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1152bc110d7b69741f4545ac04c2d9fe40584a22", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/1152bc110d7b69741f4545ac04c2d9fe40584a22", "committedDate": "2020-05-25T23:04:38Z", "message": "Extract ContentFile from DataFile and add type param to ManifestEntry."}, "afterCommit": {"oid": "3dd65213bea1513bae7753c9873df7f109d1ca82", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/3dd65213bea1513bae7753c9873df7f109d1ca82", "committedDate": "2020-05-25T23:06:44Z", "message": "Extract ContentFile from DataFile and add type param to ManifestEntry."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIwNzQyMjgw", "url": "https://github.com/apache/iceberg/pull/1064#pullrequestreview-420742280", "createdAt": "2020-05-29T08:13:15Z", "commit": {"oid": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwODoxMzoxNlrOGcTE_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQwODozNzoyMlrOGcT2zQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMyNTg4NA==", "bodyText": "I see the comment says that the BaseFile is the base class for DataFile and DeleteFile,  is it suitable to make the FileContent use FileContent.DATA by default ?  Just curious.", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432325884", "createdAt": "2020-05-29T08:13:16Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.MoreObjects;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.avro.specific.SpecificData;\n+import org.apache.iceberg.avro.AvroSchemaUtil;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+\n+/**\n+ * Base class for both {@link DataFile} and {@link DeleteFile}.\n+ */\n+abstract class BaseFile<F>\n+    implements ContentFile<F>, IndexedRecord, StructLike, SpecificData.SchemaConstructable, Serializable {\n+  static final Types.StructType EMPTY_STRUCT_TYPE = Types.StructType.of();\n+  static final PartitionData EMPTY_PARTITION_DATA = new PartitionData(EMPTY_STRUCT_TYPE) {\n+    @Override\n+    public PartitionData copy() {\n+      return this; // this does not change\n+    }\n+  };\n+\n+  private int[] fromProjectionPos;\n+  private Types.StructType partitionType;\n+\n+  private FileContent content = FileContent.DATA;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMzMDIyOQ==", "bodyText": "I guess we should return this.content.id() here ?", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432330229", "createdAt": "2020-05-29T08:21:45Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.MoreObjects;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.avro.specific.SpecificData;\n+import org.apache.iceberg.avro.AvroSchemaUtil;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+\n+/**\n+ * Base class for both {@link DataFile} and {@link DeleteFile}.\n+ */\n+abstract class BaseFile<F>\n+    implements ContentFile<F>, IndexedRecord, StructLike, SpecificData.SchemaConstructable, Serializable {\n+  static final Types.StructType EMPTY_STRUCT_TYPE = Types.StructType.of();\n+  static final PartitionData EMPTY_PARTITION_DATA = new PartitionData(EMPTY_STRUCT_TYPE) {\n+    @Override\n+    public PartitionData copy() {\n+      return this; // this does not change\n+    }\n+  };\n+\n+  private int[] fromProjectionPos;\n+  private Types.StructType partitionType;\n+\n+  private FileContent content = FileContent.DATA;\n+  private String filePath = null;\n+  private FileFormat format = null;\n+  private PartitionData partitionData = null;\n+  private Long recordCount = null;\n+  private long fileSizeInBytes = -1L;\n+\n+  // optional fields\n+  private Map<Integer, Long> columnSizes = null;\n+  private Map<Integer, Long> valueCounts = null;\n+  private Map<Integer, Long> nullValueCounts = null;\n+  private Map<Integer, ByteBuffer> lowerBounds = null;\n+  private Map<Integer, ByteBuffer> upperBounds = null;\n+  private List<Long> splitOffsets = null;\n+  private byte[] keyMetadata = null;\n+\n+  // cached schema\n+  private transient org.apache.avro.Schema avroSchema = null;\n+\n+  /**\n+   * Used by Avro reflection to instantiate this class when reading manifest files.\n+   */\n+  BaseFile(org.apache.avro.Schema avroSchema) {\n+    this.avroSchema = avroSchema;\n+\n+    Types.StructType schema = AvroSchemaUtil.convert(avroSchema).asNestedType().asStructType();\n+\n+    // partition type may be null if the field was not projected\n+    Type partType = schema.fieldType(\"partition\");\n+    if (partType != null) {\n+      this.partitionType = partType.asNestedType().asStructType();\n+    } else {\n+      this.partitionType = EMPTY_STRUCT_TYPE;\n+    }\n+\n+    List<Types.NestedField> fields = schema.fields();\n+    List<Types.NestedField> allFields = DataFile.getType(partitionType).fields();\n+    this.fromProjectionPos = new int[fields.size()];\n+    for (int i = 0; i < fromProjectionPos.length; i += 1) {\n+      boolean found = false;\n+      for (int j = 0; j < allFields.size(); j += 1) {\n+        if (fields.get(i).fieldId() == allFields.get(j).fieldId()) {\n+          found = true;\n+          fromProjectionPos[i] = j;\n+        }\n+      }\n+\n+      if (!found) {\n+        throw new IllegalArgumentException(\"Cannot find projected field: \" + fields.get(i));\n+      }\n+    }\n+\n+    this.partitionData = new PartitionData(partitionType);\n+  }\n+\n+  BaseFile(FileContent content, String filePath, FileFormat format,\n+           PartitionData partition, long fileSizeInBytes, long recordCount,\n+           Map<Integer, Long> columnSizes, Map<Integer, Long> valueCounts, Map<Integer, Long> nullValueCounts,\n+           Map<Integer, ByteBuffer> lowerBounds, Map<Integer, ByteBuffer> upperBounds, List<Long> splitOffsets,\n+           ByteBuffer keyMetadata) {\n+    this.content = content;\n+    this.filePath = filePath;\n+    this.format = format;\n+\n+    // this constructor is used by DataFiles.Builder, which passes null for unpartitioned data\n+    if (partition == null) {\n+      this.partitionData = EMPTY_PARTITION_DATA;\n+      this.partitionType = EMPTY_PARTITION_DATA.getPartitionType();\n+    } else {\n+      this.partitionData = partition;\n+      this.partitionType = partition.getPartitionType();\n+    }\n+\n+    // this will throw NPE if metrics.recordCount is null\n+    this.recordCount = recordCount;\n+    this.fileSizeInBytes = fileSizeInBytes;\n+    this.columnSizes = columnSizes;\n+    this.valueCounts = valueCounts;\n+    this.nullValueCounts = nullValueCounts;\n+    this.lowerBounds = SerializableByteBufferMap.wrap(lowerBounds);\n+    this.upperBounds = SerializableByteBufferMap.wrap(upperBounds);\n+    this.splitOffsets = copy(splitOffsets);\n+    this.keyMetadata = ByteBuffers.toByteArray(keyMetadata);\n+  }\n+\n+  /**\n+   * Copy constructor.\n+   *\n+   * @param toCopy a generic data file to copy.\n+   * @param fullCopy whether to copy all fields or to drop column-level stats\n+   */\n+  BaseFile(BaseFile<F> toCopy, boolean fullCopy) {\n+    this.content = toCopy.content;\n+    this.filePath = toCopy.filePath;\n+    this.format = toCopy.format;\n+    this.partitionData = toCopy.partitionData.copy();\n+    this.partitionType = toCopy.partitionType;\n+    this.recordCount = toCopy.recordCount;\n+    this.fileSizeInBytes = toCopy.fileSizeInBytes;\n+    if (fullCopy) {\n+      // TODO: support lazy conversion to/from map\n+      this.columnSizes = copy(toCopy.columnSizes);\n+      this.valueCounts = copy(toCopy.valueCounts);\n+      this.nullValueCounts = copy(toCopy.nullValueCounts);\n+      this.lowerBounds = SerializableByteBufferMap.wrap(copy(toCopy.lowerBounds));\n+      this.upperBounds = SerializableByteBufferMap.wrap(copy(toCopy.upperBounds));\n+    } else {\n+      this.columnSizes = null;\n+      this.valueCounts = null;\n+      this.nullValueCounts = null;\n+      this.lowerBounds = null;\n+      this.upperBounds = null;\n+    }\n+    this.fromProjectionPos = toCopy.fromProjectionPos;\n+    this.keyMetadata = toCopy.keyMetadata == null ? null : Arrays.copyOf(toCopy.keyMetadata, toCopy.keyMetadata.length);\n+    this.splitOffsets = copy(toCopy.splitOffsets);\n+  }\n+\n+  /**\n+   * Constructor for Java serialization.\n+   */\n+  BaseFile() {\n+  }\n+\n+  @Override\n+  public org.apache.avro.Schema getSchema() {\n+    if (avroSchema == null) {\n+      this.avroSchema = getAvroSchema(partitionType);\n+    }\n+    return avroSchema;\n+  }\n+\n+  @Override\n+  @SuppressWarnings(\"unchecked\")\n+  public void put(int i, Object value) {\n+    int pos = i;\n+    // if the schema was projected, map the incoming ordinal to the expected one\n+    if (fromProjectionPos != null) {\n+      pos = fromProjectionPos[i];\n+    }\n+    switch (pos) {\n+      case 0:\n+        this.content = value != null ? FileContent.values()[(Integer) value] : FileContent.DATA;\n+        return;\n+      case 1:\n+        // always coerce to String for Serializable\n+        this.filePath = value.toString();\n+        return;\n+      case 2:\n+        this.format = FileFormat.valueOf(value.toString());\n+        return;\n+      case 3:\n+        this.partitionData = (PartitionData) value;\n+        return;\n+      case 4:\n+        this.recordCount = (Long) value;\n+        return;\n+      case 5:\n+        this.fileSizeInBytes = (Long) value;\n+        return;\n+      case 6:\n+        this.columnSizes = (Map<Integer, Long>) value;\n+        return;\n+      case 7:\n+        this.valueCounts = (Map<Integer, Long>) value;\n+        return;\n+      case 8:\n+        this.nullValueCounts = (Map<Integer, Long>) value;\n+        return;\n+      case 9:\n+        this.lowerBounds = SerializableByteBufferMap.wrap((Map<Integer, ByteBuffer>) value);\n+        return;\n+      case 10:\n+        this.upperBounds = SerializableByteBufferMap.wrap((Map<Integer, ByteBuffer>) value);\n+        return;\n+      case 11:\n+        this.keyMetadata = ByteBuffers.toByteArray((ByteBuffer) value);\n+        return;\n+      case 12:\n+        this.splitOffsets = (List<Long>) value;\n+        return;\n+      default:\n+        // ignore the object, it must be from a newer version of the format\n+    }\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    put(pos, value);\n+  }\n+\n+  @Override\n+  public Object get(int i) {\n+    int pos = i;\n+    // if the schema was projected, map the incoming ordinal to the expected one\n+    if (fromProjectionPos != null) {\n+      pos = fromProjectionPos[i];\n+    }\n+    switch (pos) {\n+      case 0:\n+        return FileContent.DATA.id();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856"}, "originalPosition": 255}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMzMTUyNA==", "bodyText": "Missing a FileContent field here I guess.", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432331524", "createdAt": "2020-05-29T08:24:06Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.MoreObjects;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.avro.specific.SpecificData;\n+import org.apache.iceberg.avro.AvroSchemaUtil;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+\n+/**\n+ * Base class for both {@link DataFile} and {@link DeleteFile}.\n+ */\n+abstract class BaseFile<F>\n+    implements ContentFile<F>, IndexedRecord, StructLike, SpecificData.SchemaConstructable, Serializable {\n+  static final Types.StructType EMPTY_STRUCT_TYPE = Types.StructType.of();\n+  static final PartitionData EMPTY_PARTITION_DATA = new PartitionData(EMPTY_STRUCT_TYPE) {\n+    @Override\n+    public PartitionData copy() {\n+      return this; // this does not change\n+    }\n+  };\n+\n+  private int[] fromProjectionPos;\n+  private Types.StructType partitionType;\n+\n+  private FileContent content = FileContent.DATA;\n+  private String filePath = null;\n+  private FileFormat format = null;\n+  private PartitionData partitionData = null;\n+  private Long recordCount = null;\n+  private long fileSizeInBytes = -1L;\n+\n+  // optional fields\n+  private Map<Integer, Long> columnSizes = null;\n+  private Map<Integer, Long> valueCounts = null;\n+  private Map<Integer, Long> nullValueCounts = null;\n+  private Map<Integer, ByteBuffer> lowerBounds = null;\n+  private Map<Integer, ByteBuffer> upperBounds = null;\n+  private List<Long> splitOffsets = null;\n+  private byte[] keyMetadata = null;\n+\n+  // cached schema\n+  private transient org.apache.avro.Schema avroSchema = null;\n+\n+  /**\n+   * Used by Avro reflection to instantiate this class when reading manifest files.\n+   */\n+  BaseFile(org.apache.avro.Schema avroSchema) {\n+    this.avroSchema = avroSchema;\n+\n+    Types.StructType schema = AvroSchemaUtil.convert(avroSchema).asNestedType().asStructType();\n+\n+    // partition type may be null if the field was not projected\n+    Type partType = schema.fieldType(\"partition\");\n+    if (partType != null) {\n+      this.partitionType = partType.asNestedType().asStructType();\n+    } else {\n+      this.partitionType = EMPTY_STRUCT_TYPE;\n+    }\n+\n+    List<Types.NestedField> fields = schema.fields();\n+    List<Types.NestedField> allFields = DataFile.getType(partitionType).fields();\n+    this.fromProjectionPos = new int[fields.size()];\n+    for (int i = 0; i < fromProjectionPos.length; i += 1) {\n+      boolean found = false;\n+      for (int j = 0; j < allFields.size(); j += 1) {\n+        if (fields.get(i).fieldId() == allFields.get(j).fieldId()) {\n+          found = true;\n+          fromProjectionPos[i] = j;\n+        }\n+      }\n+\n+      if (!found) {\n+        throw new IllegalArgumentException(\"Cannot find projected field: \" + fields.get(i));\n+      }\n+    }\n+\n+    this.partitionData = new PartitionData(partitionType);\n+  }\n+\n+  BaseFile(FileContent content, String filePath, FileFormat format,\n+           PartitionData partition, long fileSizeInBytes, long recordCount,\n+           Map<Integer, Long> columnSizes, Map<Integer, Long> valueCounts, Map<Integer, Long> nullValueCounts,\n+           Map<Integer, ByteBuffer> lowerBounds, Map<Integer, ByteBuffer> upperBounds, List<Long> splitOffsets,\n+           ByteBuffer keyMetadata) {\n+    this.content = content;\n+    this.filePath = filePath;\n+    this.format = format;\n+\n+    // this constructor is used by DataFiles.Builder, which passes null for unpartitioned data\n+    if (partition == null) {\n+      this.partitionData = EMPTY_PARTITION_DATA;\n+      this.partitionType = EMPTY_PARTITION_DATA.getPartitionType();\n+    } else {\n+      this.partitionData = partition;\n+      this.partitionType = partition.getPartitionType();\n+    }\n+\n+    // this will throw NPE if metrics.recordCount is null\n+    this.recordCount = recordCount;\n+    this.fileSizeInBytes = fileSizeInBytes;\n+    this.columnSizes = columnSizes;\n+    this.valueCounts = valueCounts;\n+    this.nullValueCounts = nullValueCounts;\n+    this.lowerBounds = SerializableByteBufferMap.wrap(lowerBounds);\n+    this.upperBounds = SerializableByteBufferMap.wrap(upperBounds);\n+    this.splitOffsets = copy(splitOffsets);\n+    this.keyMetadata = ByteBuffers.toByteArray(keyMetadata);\n+  }\n+\n+  /**\n+   * Copy constructor.\n+   *\n+   * @param toCopy a generic data file to copy.\n+   * @param fullCopy whether to copy all fields or to drop column-level stats\n+   */\n+  BaseFile(BaseFile<F> toCopy, boolean fullCopy) {\n+    this.content = toCopy.content;\n+    this.filePath = toCopy.filePath;\n+    this.format = toCopy.format;\n+    this.partitionData = toCopy.partitionData.copy();\n+    this.partitionType = toCopy.partitionType;\n+    this.recordCount = toCopy.recordCount;\n+    this.fileSizeInBytes = toCopy.fileSizeInBytes;\n+    if (fullCopy) {\n+      // TODO: support lazy conversion to/from map\n+      this.columnSizes = copy(toCopy.columnSizes);\n+      this.valueCounts = copy(toCopy.valueCounts);\n+      this.nullValueCounts = copy(toCopy.nullValueCounts);\n+      this.lowerBounds = SerializableByteBufferMap.wrap(copy(toCopy.lowerBounds));\n+      this.upperBounds = SerializableByteBufferMap.wrap(copy(toCopy.upperBounds));\n+    } else {\n+      this.columnSizes = null;\n+      this.valueCounts = null;\n+      this.nullValueCounts = null;\n+      this.lowerBounds = null;\n+      this.upperBounds = null;\n+    }\n+    this.fromProjectionPos = toCopy.fromProjectionPos;\n+    this.keyMetadata = toCopy.keyMetadata == null ? null : Arrays.copyOf(toCopy.keyMetadata, toCopy.keyMetadata.length);\n+    this.splitOffsets = copy(toCopy.splitOffsets);\n+  }\n+\n+  /**\n+   * Constructor for Java serialization.\n+   */\n+  BaseFile() {\n+  }\n+\n+  @Override\n+  public org.apache.avro.Schema getSchema() {\n+    if (avroSchema == null) {\n+      this.avroSchema = getAvroSchema(partitionType);\n+    }\n+    return avroSchema;\n+  }\n+\n+  @Override\n+  @SuppressWarnings(\"unchecked\")\n+  public void put(int i, Object value) {\n+    int pos = i;\n+    // if the schema was projected, map the incoming ordinal to the expected one\n+    if (fromProjectionPos != null) {\n+      pos = fromProjectionPos[i];\n+    }\n+    switch (pos) {\n+      case 0:\n+        this.content = value != null ? FileContent.values()[(Integer) value] : FileContent.DATA;\n+        return;\n+      case 1:\n+        // always coerce to String for Serializable\n+        this.filePath = value.toString();\n+        return;\n+      case 2:\n+        this.format = FileFormat.valueOf(value.toString());\n+        return;\n+      case 3:\n+        this.partitionData = (PartitionData) value;\n+        return;\n+      case 4:\n+        this.recordCount = (Long) value;\n+        return;\n+      case 5:\n+        this.fileSizeInBytes = (Long) value;\n+        return;\n+      case 6:\n+        this.columnSizes = (Map<Integer, Long>) value;\n+        return;\n+      case 7:\n+        this.valueCounts = (Map<Integer, Long>) value;\n+        return;\n+      case 8:\n+        this.nullValueCounts = (Map<Integer, Long>) value;\n+        return;\n+      case 9:\n+        this.lowerBounds = SerializableByteBufferMap.wrap((Map<Integer, ByteBuffer>) value);\n+        return;\n+      case 10:\n+        this.upperBounds = SerializableByteBufferMap.wrap((Map<Integer, ByteBuffer>) value);\n+        return;\n+      case 11:\n+        this.keyMetadata = ByteBuffers.toByteArray((ByteBuffer) value);\n+        return;\n+      case 12:\n+        this.splitOffsets = (List<Long>) value;\n+        return;\n+      default:\n+        // ignore the object, it must be from a newer version of the format\n+    }\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    put(pos, value);\n+  }\n+\n+  @Override\n+  public Object get(int i) {\n+    int pos = i;\n+    // if the schema was projected, map the incoming ordinal to the expected one\n+    if (fromProjectionPos != null) {\n+      pos = fromProjectionPos[i];\n+    }\n+    switch (pos) {\n+      case 0:\n+        return FileContent.DATA.id();\n+      case 1:\n+        return filePath;\n+      case 2:\n+        return format != null ? format.toString() : null;\n+      case 3:\n+        return partitionData;\n+      case 4:\n+        return recordCount;\n+      case 5:\n+        return fileSizeInBytes;\n+      case 6:\n+        return columnSizes;\n+      case 7:\n+        return valueCounts;\n+      case 8:\n+        return nullValueCounts;\n+      case 9:\n+        return lowerBounds;\n+      case 10:\n+        return upperBounds;\n+      case 11:\n+        return keyMetadata != null ? ByteBuffer.wrap(keyMetadata) : null;\n+      case 12:\n+        return splitOffsets;\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown field ordinal: \" + pos);\n+    }\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    return javaClass.cast(get(pos));\n+  }\n+\n+  @Override\n+  public int size() {\n+    return DataFile.getType(EMPTY_STRUCT_TYPE).fields().size();\n+  }\n+\n+  public FileContent content() {\n+    return content;\n+  }\n+\n+  public CharSequence path() {\n+    return filePath;\n+  }\n+\n+  public FileFormat format() {\n+    return format;\n+  }\n+\n+  public StructLike partition() {\n+    return partitionData;\n+  }\n+\n+  public long recordCount() {\n+    return recordCount;\n+  }\n+\n+  public long fileSizeInBytes() {\n+    return fileSizeInBytes;\n+  }\n+\n+  public Map<Integer, Long> columnSizes() {\n+    return columnSizes;\n+  }\n+\n+  public Map<Integer, Long> valueCounts() {\n+    return valueCounts;\n+  }\n+\n+  public Map<Integer, Long> nullValueCounts() {\n+    return nullValueCounts;\n+  }\n+\n+  public Map<Integer, ByteBuffer> lowerBounds() {\n+    return lowerBounds;\n+  }\n+\n+  public Map<Integer, ByteBuffer> upperBounds() {\n+    return upperBounds;\n+  }\n+\n+  public ByteBuffer keyMetadata() {\n+    return keyMetadata != null ? ByteBuffer.wrap(keyMetadata) : null;\n+  }\n+\n+  public List<Long> splitOffsets() {\n+    return splitOffsets;\n+  }\n+\n+  private static <K, V> Map<K, V> copy(Map<K, V> map) {\n+    if (map != null) {\n+      Map<K, V> copy = Maps.newHashMapWithExpectedSize(map.size());\n+      copy.putAll(map);\n+      return Collections.unmodifiableMap(copy);\n+    }\n+    return null;\n+  }\n+\n+  private static <E> List<E> copy(List<E> list) {\n+    if (list != null) {\n+      List<E> copy = Lists.newArrayListWithExpectedSize(list.size());\n+      copy.addAll(list);\n+      return Collections.unmodifiableList(copy);\n+    }\n+    return null;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return MoreObjects.toStringHelper(this)\n+        .add(\"file_path\", filePath)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856"}, "originalPosition": 368}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMzMjM5OA==", "bodyText": "Nice ..", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432332398", "createdAt": "2020-05-29T08:25:43Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/BaseManifestReader.java", "diffHunk": "@@ -47,17 +47,18 @@\n /**\n  * Base reader for data and delete manifest files.\n  *\n- * @param <T> The Java class of files returned by this reader.\n+ * @param <F> The Java class of files returned by this reader.\n  * @param <ThisT> The Java class of this reader, returned by configuration methods.\n  */\n-abstract class BaseManifestReader<T, ThisT> extends CloseableGroup implements CloseableIterable<T> {\n+abstract class BaseManifestReader<F extends ContentFile<F>, ThisT>\n+    extends CloseableGroup implements CloseableIterable<F> {\n   static final ImmutableList<String> ALL_COLUMNS = ImmutableList.of(\"*\");\n   private static final Set<String> STATS_COLUMNS = Sets.newHashSet(\n       \"value_counts\", \"null_value_counts\", \"lower_bounds\", \"upper_bounds\");\n \n   protected enum FileType {\n     DATA_FILES(GenericDataFile.class.getName()),\n-    DELETE_FILES(\"...\");\n+    DELETE_FILES(GenericDeleteFile.class.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjMzODYzNw==", "bodyText": "For my understanding,  the DATA manifest & DELETE manifest could share the same read / write path so I think we could use the common reader+writer.  Is there any other reason that we need to make them separate paths ?", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432338637", "createdAt": "2020-05-29T08:37:22Z", "author": {"login": "openinx"}, "path": "core/src/main/java/org/apache/iceberg/ManifestFiles.java", "diffHunk": "@@ -55,6 +55,8 @@ public static ManifestReader read(ManifestFile manifest, FileIO io) {\n    * @return a {@link ManifestReader}\n    */\n   public static ManifestReader read(ManifestFile manifest, FileIO io, Map<Integer, PartitionSpec> specsById) {\n+    Preconditions.checkArgument(manifest.content() == ManifestContent.DATA,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIxMDg1ODUz", "url": "https://github.com/apache/iceberg/pull/1064#pullrequestreview-421085853", "createdAt": "2020-05-29T16:00:12Z", "commit": {"oid": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNjowMDoxM1rOGcizcw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNjowMDoxM1rOGcizcw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4MzUzOQ==", "bodyText": "Do we keep GenericDataFile on purpose here?", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432583539", "createdAt": "2020-05-29T16:00:13Z", "author": {"login": "aokolnychyi"}, "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -0,0 +1,389 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.MoreObjects;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.nio.ByteBuffer;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.avro.generic.IndexedRecord;\n+import org.apache.avro.specific.SpecificData;\n+import org.apache.iceberg.avro.AvroSchemaUtil;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+\n+/**\n+ * Base class for both {@link DataFile} and {@link DeleteFile}.\n+ */\n+abstract class BaseFile<F>\n+    implements ContentFile<F>, IndexedRecord, StructLike, SpecificData.SchemaConstructable, Serializable {\n+  static final Types.StructType EMPTY_STRUCT_TYPE = Types.StructType.of();\n+  static final PartitionData EMPTY_PARTITION_DATA = new PartitionData(EMPTY_STRUCT_TYPE) {\n+    @Override\n+    public PartitionData copy() {\n+      return this; // this does not change\n+    }\n+  };\n+\n+  private int[] fromProjectionPos;\n+  private Types.StructType partitionType;\n+\n+  private FileContent content = FileContent.DATA;\n+  private String filePath = null;\n+  private FileFormat format = null;\n+  private PartitionData partitionData = null;\n+  private Long recordCount = null;\n+  private long fileSizeInBytes = -1L;\n+\n+  // optional fields\n+  private Map<Integer, Long> columnSizes = null;\n+  private Map<Integer, Long> valueCounts = null;\n+  private Map<Integer, Long> nullValueCounts = null;\n+  private Map<Integer, ByteBuffer> lowerBounds = null;\n+  private Map<Integer, ByteBuffer> upperBounds = null;\n+  private List<Long> splitOffsets = null;\n+  private byte[] keyMetadata = null;\n+\n+  // cached schema\n+  private transient org.apache.avro.Schema avroSchema = null;\n+\n+  /**\n+   * Used by Avro reflection to instantiate this class when reading manifest files.\n+   */\n+  BaseFile(org.apache.avro.Schema avroSchema) {\n+    this.avroSchema = avroSchema;\n+\n+    Types.StructType schema = AvroSchemaUtil.convert(avroSchema).asNestedType().asStructType();\n+\n+    // partition type may be null if the field was not projected\n+    Type partType = schema.fieldType(\"partition\");\n+    if (partType != null) {\n+      this.partitionType = partType.asNestedType().asStructType();\n+    } else {\n+      this.partitionType = EMPTY_STRUCT_TYPE;\n+    }\n+\n+    List<Types.NestedField> fields = schema.fields();\n+    List<Types.NestedField> allFields = DataFile.getType(partitionType).fields();\n+    this.fromProjectionPos = new int[fields.size()];\n+    for (int i = 0; i < fromProjectionPos.length; i += 1) {\n+      boolean found = false;\n+      for (int j = 0; j < allFields.size(); j += 1) {\n+        if (fields.get(i).fieldId() == allFields.get(j).fieldId()) {\n+          found = true;\n+          fromProjectionPos[i] = j;\n+        }\n+      }\n+\n+      if (!found) {\n+        throw new IllegalArgumentException(\"Cannot find projected field: \" + fields.get(i));\n+      }\n+    }\n+\n+    this.partitionData = new PartitionData(partitionType);\n+  }\n+\n+  BaseFile(FileContent content, String filePath, FileFormat format,\n+           PartitionData partition, long fileSizeInBytes, long recordCount,\n+           Map<Integer, Long> columnSizes, Map<Integer, Long> valueCounts, Map<Integer, Long> nullValueCounts,\n+           Map<Integer, ByteBuffer> lowerBounds, Map<Integer, ByteBuffer> upperBounds, List<Long> splitOffsets,\n+           ByteBuffer keyMetadata) {\n+    this.content = content;\n+    this.filePath = filePath;\n+    this.format = format;\n+\n+    // this constructor is used by DataFiles.Builder, which passes null for unpartitioned data\n+    if (partition == null) {\n+      this.partitionData = EMPTY_PARTITION_DATA;\n+      this.partitionType = EMPTY_PARTITION_DATA.getPartitionType();\n+    } else {\n+      this.partitionData = partition;\n+      this.partitionType = partition.getPartitionType();\n+    }\n+\n+    // this will throw NPE if metrics.recordCount is null\n+    this.recordCount = recordCount;\n+    this.fileSizeInBytes = fileSizeInBytes;\n+    this.columnSizes = columnSizes;\n+    this.valueCounts = valueCounts;\n+    this.nullValueCounts = nullValueCounts;\n+    this.lowerBounds = SerializableByteBufferMap.wrap(lowerBounds);\n+    this.upperBounds = SerializableByteBufferMap.wrap(upperBounds);\n+    this.splitOffsets = copy(splitOffsets);\n+    this.keyMetadata = ByteBuffers.toByteArray(keyMetadata);\n+  }\n+\n+  /**\n+   * Copy constructor.\n+   *\n+   * @param toCopy a generic data file to copy.\n+   * @param fullCopy whether to copy all fields or to drop column-level stats\n+   */\n+  BaseFile(BaseFile<F> toCopy, boolean fullCopy) {\n+    this.content = toCopy.content;\n+    this.filePath = toCopy.filePath;\n+    this.format = toCopy.format;\n+    this.partitionData = toCopy.partitionData.copy();\n+    this.partitionType = toCopy.partitionType;\n+    this.recordCount = toCopy.recordCount;\n+    this.fileSizeInBytes = toCopy.fileSizeInBytes;\n+    if (fullCopy) {\n+      // TODO: support lazy conversion to/from map\n+      this.columnSizes = copy(toCopy.columnSizes);\n+      this.valueCounts = copy(toCopy.valueCounts);\n+      this.nullValueCounts = copy(toCopy.nullValueCounts);\n+      this.lowerBounds = SerializableByteBufferMap.wrap(copy(toCopy.lowerBounds));\n+      this.upperBounds = SerializableByteBufferMap.wrap(copy(toCopy.upperBounds));\n+    } else {\n+      this.columnSizes = null;\n+      this.valueCounts = null;\n+      this.nullValueCounts = null;\n+      this.lowerBounds = null;\n+      this.upperBounds = null;\n+    }\n+    this.fromProjectionPos = toCopy.fromProjectionPos;\n+    this.keyMetadata = toCopy.keyMetadata == null ? null : Arrays.copyOf(toCopy.keyMetadata, toCopy.keyMetadata.length);\n+    this.splitOffsets = copy(toCopy.splitOffsets);\n+  }\n+\n+  /**\n+   * Constructor for Java serialization.\n+   */\n+  BaseFile() {\n+  }\n+\n+  @Override\n+  public org.apache.avro.Schema getSchema() {\n+    if (avroSchema == null) {\n+      this.avroSchema = getAvroSchema(partitionType);\n+    }\n+    return avroSchema;\n+  }\n+\n+  @Override\n+  @SuppressWarnings(\"unchecked\")\n+  public void put(int i, Object value) {\n+    int pos = i;\n+    // if the schema was projected, map the incoming ordinal to the expected one\n+    if (fromProjectionPos != null) {\n+      pos = fromProjectionPos[i];\n+    }\n+    switch (pos) {\n+      case 0:\n+        this.content = value != null ? FileContent.values()[(Integer) value] : FileContent.DATA;\n+        return;\n+      case 1:\n+        // always coerce to String for Serializable\n+        this.filePath = value.toString();\n+        return;\n+      case 2:\n+        this.format = FileFormat.valueOf(value.toString());\n+        return;\n+      case 3:\n+        this.partitionData = (PartitionData) value;\n+        return;\n+      case 4:\n+        this.recordCount = (Long) value;\n+        return;\n+      case 5:\n+        this.fileSizeInBytes = (Long) value;\n+        return;\n+      case 6:\n+        this.columnSizes = (Map<Integer, Long>) value;\n+        return;\n+      case 7:\n+        this.valueCounts = (Map<Integer, Long>) value;\n+        return;\n+      case 8:\n+        this.nullValueCounts = (Map<Integer, Long>) value;\n+        return;\n+      case 9:\n+        this.lowerBounds = SerializableByteBufferMap.wrap((Map<Integer, ByteBuffer>) value);\n+        return;\n+      case 10:\n+        this.upperBounds = SerializableByteBufferMap.wrap((Map<Integer, ByteBuffer>) value);\n+        return;\n+      case 11:\n+        this.keyMetadata = ByteBuffers.toByteArray((ByteBuffer) value);\n+        return;\n+      case 12:\n+        this.splitOffsets = (List<Long>) value;\n+        return;\n+      default:\n+        // ignore the object, it must be from a newer version of the format\n+    }\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    put(pos, value);\n+  }\n+\n+  @Override\n+  public Object get(int i) {\n+    int pos = i;\n+    // if the schema was projected, map the incoming ordinal to the expected one\n+    if (fromProjectionPos != null) {\n+      pos = fromProjectionPos[i];\n+    }\n+    switch (pos) {\n+      case 0:\n+        return FileContent.DATA.id();\n+      case 1:\n+        return filePath;\n+      case 2:\n+        return format != null ? format.toString() : null;\n+      case 3:\n+        return partitionData;\n+      case 4:\n+        return recordCount;\n+      case 5:\n+        return fileSizeInBytes;\n+      case 6:\n+        return columnSizes;\n+      case 7:\n+        return valueCounts;\n+      case 8:\n+        return nullValueCounts;\n+      case 9:\n+        return lowerBounds;\n+      case 10:\n+        return upperBounds;\n+      case 11:\n+        return keyMetadata != null ? ByteBuffer.wrap(keyMetadata) : null;\n+      case 12:\n+        return splitOffsets;\n+      default:\n+        throw new UnsupportedOperationException(\"Unknown field ordinal: \" + pos);\n+    }\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    return javaClass.cast(get(pos));\n+  }\n+\n+  @Override\n+  public int size() {\n+    return DataFile.getType(EMPTY_STRUCT_TYPE).fields().size();\n+  }\n+\n+  public FileContent content() {\n+    return content;\n+  }\n+\n+  public CharSequence path() {\n+    return filePath;\n+  }\n+\n+  public FileFormat format() {\n+    return format;\n+  }\n+\n+  public StructLike partition() {\n+    return partitionData;\n+  }\n+\n+  public long recordCount() {\n+    return recordCount;\n+  }\n+\n+  public long fileSizeInBytes() {\n+    return fileSizeInBytes;\n+  }\n+\n+  public Map<Integer, Long> columnSizes() {\n+    return columnSizes;\n+  }\n+\n+  public Map<Integer, Long> valueCounts() {\n+    return valueCounts;\n+  }\n+\n+  public Map<Integer, Long> nullValueCounts() {\n+    return nullValueCounts;\n+  }\n+\n+  public Map<Integer, ByteBuffer> lowerBounds() {\n+    return lowerBounds;\n+  }\n+\n+  public Map<Integer, ByteBuffer> upperBounds() {\n+    return upperBounds;\n+  }\n+\n+  public ByteBuffer keyMetadata() {\n+    return keyMetadata != null ? ByteBuffer.wrap(keyMetadata) : null;\n+  }\n+\n+  public List<Long> splitOffsets() {\n+    return splitOffsets;\n+  }\n+\n+  private static <K, V> Map<K, V> copy(Map<K, V> map) {\n+    if (map != null) {\n+      Map<K, V> copy = Maps.newHashMapWithExpectedSize(map.size());\n+      copy.putAll(map);\n+      return Collections.unmodifiableMap(copy);\n+    }\n+    return null;\n+  }\n+\n+  private static <E> List<E> copy(List<E> list) {\n+    if (list != null) {\n+      List<E> copy = Lists.newArrayListWithExpectedSize(list.size());\n+      copy.addAll(list);\n+      return Collections.unmodifiableList(copy);\n+    }\n+    return null;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return MoreObjects.toStringHelper(this)\n+        .add(\"file_path\", filePath)\n+        .add(\"file_format\", format)\n+        .add(\"partition\", partitionData)\n+        .add(\"record_count\", recordCount)\n+        .add(\"file_size_in_bytes\", fileSizeInBytes)\n+        .add(\"column_sizes\", columnSizes)\n+        .add(\"value_counts\", valueCounts)\n+        .add(\"null_value_counts\", nullValueCounts)\n+        .add(\"lower_bounds\", lowerBounds)\n+        .add(\"upper_bounds\", upperBounds)\n+        .add(\"key_metadata\", keyMetadata == null ? \"null\" : \"(redacted)\")\n+        .add(\"split_offsets\", splitOffsets == null ? \"null\" : splitOffsets)\n+        .toString();\n+  }\n+\n+  private static org.apache.avro.Schema getAvroSchema(Types.StructType partitionType) {\n+    Types.StructType type = DataFile.getType(partitionType);\n+    return AvroSchemaUtil.convert(type, ImmutableMap.of(\n+        type, GenericDataFile.class.getName(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856"}, "originalPosition": 386}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIxMDg5ODEy", "url": "https://github.com/apache/iceberg/pull/1064#pullrequestreview-421089812", "createdAt": "2020-05-29T16:05:38Z", "commit": {"oid": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNjowNTozOFrOGci_rQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNjowNTozOFrOGci_rQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4NjY2OQ==", "bodyText": "nit: I think this can be on a single line now (i.e. iterator())", "url": "https://github.com/apache/iceberg/pull/1064#discussion_r432586669", "createdAt": "2020-05-29T16:05:38Z", "author": {"login": "aokolnychyi"}, "path": "core/src/main/java/org/apache/iceberg/BaseManifestReader.java", "diffHunk": "@@ -218,13 +219,12 @@ public ThisT caseSensitive(boolean isCaseSensitive) {\n    * @return an Iterator of DataFile. Makes defensive copies of files before returning\n    */\n   @Override\n-  @SuppressWarnings(\"unchecked\")\n-  public CloseableIterator<T> iterator() {\n+  public CloseableIterator<F> iterator() {\n     if (dropStats(rowFilter, columns)) {\n-      return (CloseableIterator<T>) CloseableIterable.transform(liveEntries(), e -> e.file().copyWithoutStats())\n+      return CloseableIterable.transform(liveEntries(), e -> e.file().copyWithoutStats())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856"}, "originalPosition": 74}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5ed51eaec32559019754bb880b3094c71181e440", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/5ed51eaec32559019754bb880b3094c71181e440", "committedDate": "2020-05-29T18:44:55Z", "message": "Split DataFile into DataFile and DeleteFile."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6edd6022b227e71fb37b3a54ad087e3d74002e5c", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/6edd6022b227e71fb37b3a54ad087e3d74002e5c", "committedDate": "2020-05-29T18:44:55Z", "message": "Fix FileType in BaseManifestReader."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "66535cd0ee97798aacced1f51ff7e0138ab25d33", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/66535cd0ee97798aacced1f51ff7e0138ab25d33", "committedDate": "2020-05-29T18:44:55Z", "message": "Add DeleteManifestReader."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ed2a15636b53c0064f2d623480729d999c0575b4", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/ed2a15636b53c0064f2d623480729d999c0575b4", "committedDate": "2020-05-29T18:44:55Z", "message": "Add delete manifest writer."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3d1ee652568a7e26dd31c9d9b46da694ef510970", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/3d1ee652568a7e26dd31c9d9b46da694ef510970", "committedDate": "2020-05-29T18:44:55Z", "message": "Remove wrapper from ManifestEntry."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0f4e0634d3b4dbba80892a4af66dfa7322c755f8", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/0f4e0634d3b4dbba80892a4af66dfa7322c755f8", "committedDate": "2020-05-29T18:44:55Z", "message": "Extract ContentFile from DataFile and add type param to ManifestEntry."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e45a0d781b19e0460d6f859097fa047061b8ce98", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/e45a0d781b19e0460d6f859097fa047061b8ce98", "committedDate": "2020-05-29T18:44:55Z", "message": "Minor fixes."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6a796c5bc02528d38084ee9cfbfcf44e567d28cb", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/6a796c5bc02528d38084ee9cfbfcf44e567d28cb", "committedDate": "2020-05-29T18:44:55Z", "message": "Update for review comments."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9183fa21cf05db9a8f0bc78a94d0ec23358a8856", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/9183fa21cf05db9a8f0bc78a94d0ec23358a8856", "committedDate": "2020-05-27T00:20:40Z", "message": "Merge branch 'master' into v2-delete-files"}, "afterCommit": {"oid": "6a796c5bc02528d38084ee9cfbfcf44e567d28cb", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/6a796c5bc02528d38084ee9cfbfcf44e567d28cb", "committedDate": "2020-05-29T18:44:55Z", "message": "Update for review comments."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDIxMjI1NDcw", "url": "https://github.com/apache/iceberg/pull/1064#pullrequestreview-421225470", "createdAt": "2020-05-29T19:26:07Z", "commit": {"oid": "6a796c5bc02528d38084ee9cfbfcf44e567d28cb"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4448, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}