{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDUzODY0ODgy", "number": 1222, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMDo1MjoyNVrOEQbCBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDozNjo1M1rOERm18w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1NjU1NTU3OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/data/avro/DataReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwMDo1MjoyNVrOG0kMpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxNzoyODoxMFrOG1sxvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzc3MjE5Ng==", "bodyText": "Should remove this before committing.", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r457772196", "createdAt": "2020-07-21T00:52:25Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/data/avro/DataReader.java", "diffHunk": "@@ -117,6 +126,7 @@ private ReadBuilder(Map<Integer, ?> idToConstant) {\n     @Override\n     public ValueReader<?> record(Types.StructType struct, Schema record,\n                                  List<String> names, List<ValueReader<?>> fields) {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2b15b522e039429da9e7fed5cd67da3e9125a463"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODk2MTM0MQ==", "bodyText": "Removing the empty line? I'm OK", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r458961341", "createdAt": "2020-07-22T17:28:10Z", "author": {"login": "rdsr"}, "path": "core/src/main/java/org/apache/iceberg/data/avro/DataReader.java", "diffHunk": "@@ -117,6 +126,7 @@ private ReadBuilder(Map<Integer, ?> idToConstant) {\n     @Override\n     public ValueReader<?> record(Types.StructType struct, Schema record,\n                                  List<String> names, List<ValueReader<?>> fields) {\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzc3MjE5Ng=="}, "originalCommit": {"oid": "2b15b522e039429da9e7fed5cd67da3e9125a463"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NTk2Nzk2OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/avro/ValueReaders.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwNjozMjoxMlrOG19vfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNzo1NTowN1rOG2Va6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIzOTI5Mg==", "bodyText": "I don't understand this comment line\n\nonly if the position reader is set\n\nreader is set where?\n\nand this is a top-level field\n\nI don't see where the top-level field restriction comes from", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459239292", "createdAt": "2020-07-23T06:32:12Z", "author": {"login": "shardulm94"}, "path": "core/src/main/java/org/apache/iceberg/avro/ValueReaders.java", "diffHunk": "@@ -582,13 +585,37 @@ protected StructReader(List<ValueReader<?>> readers, Types.StructType struct, Ma\n         if (idToConstant.containsKey(field.fieldId())) {\n           positionList.add(pos);\n           constantList.add(idToConstant.get(field.fieldId()));\n+        } else if (field.fieldId() == MetadataColumns.ROW_POSITION.fieldId()) {\n+          // replace the _pos field reader with a position reader\n+          // only if the position reader is set and this is a top-level field", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYyNzI0Mw==", "bodyText": "This is out of date. I had to move the check because the position reader is set after this constructor. I'll update the comment.", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459627243", "createdAt": "2020-07-23T17:55:07Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/avro/ValueReaders.java", "diffHunk": "@@ -582,13 +585,37 @@ protected StructReader(List<ValueReader<?>> readers, Types.StructType struct, Ma\n         if (idToConstant.containsKey(field.fieldId())) {\n           positionList.add(pos);\n           constantList.add(idToConstant.get(field.fieldId()));\n+        } else if (field.fieldId() == MetadataColumns.ROW_POSITION.fieldId()) {\n+          // replace the _pos field reader with a position reader\n+          // only if the position reader is set and this is a top-level field", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTIzOTI5Mg=="}, "originalCommit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2NjAyNTg5OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/avro/AvroIO.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QwNjo1NTowMFrOG1-QXg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDozNDo1M1rOG2aneA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0NzcxMA==", "bodyText": "Not sure if this valid start state, but I think there can be an edge case here\nrow-count|compressed-size-in-bytes|block-bytes|sync|EOF\n                                                 ^\n                                                 |\n                                               start\n\nIn this case it will seek and read the sync and then continue to read next block row count even after EOF\nSo should probably be while (nextSyncPos + 16 < start)?", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459247710", "createdAt": "2020-07-23T06:55:00Z", "author": {"login": "shardulm94"}, "path": "core/src/main/java/org/apache/iceberg/avro/AvroIO.java", "diffHunk": "@@ -131,4 +144,55 @@ public boolean markSupported() {\n       return stream.markSupported();\n     }\n   }\n+\n+  static long findStartingRowPos(Supplier<SeekableInputStream> open, long start) {\n+    try (SeekableInputStream in = open.get()) {\n+      // use a direct decoder that will not buffer so the position of the input stream is accurate\n+      BinaryDecoder decoder = DecoderFactory.get().directBinaryDecoder(in, null);\n+\n+      // an Avro file's layout looks like this:\n+      //   header|block|block|...\n+      // the header contains:\n+      //   magic|string-map|sync\n+      // each block consists of:\n+      //   row-count|compressed-size-in-bytes|block-bytes|sync\n+\n+      // it is necessary to read the header here because this is the only way to get the expected file sync bytes\n+      byte[] magic = MAGIC_READER.read(decoder, null);\n+      if (!Arrays.equals(AVRO_MAGIC, magic)) {\n+        throw new InvalidAvroMagicException(\"Not an Avro file\");\n+      }\n+\n+      META_READER.read(decoder, null); // ignore the file metadata, it isn't needed\n+      byte[] fileSync = SYNC_READER.read(decoder, null);\n+\n+      // the while loop reads row counts and seeks past the block bytes until the next sync pos is >= start, which\n+      // indicates that the next sync is the start of the split.\n+      byte[] blockSync = new byte[16];\n+      long totalRows = 0;\n+      long nextSyncPos = in.getPos();\n+\n+      while (nextSyncPos < start) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYzMjUyMQ==", "bodyText": "The loop condition is correct with respect to start because a block starts at the beginning of a sync. You're right about the EOF behavior, though. I'll take a look at that.", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459632521", "createdAt": "2020-07-23T18:04:24Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/avro/AvroIO.java", "diffHunk": "@@ -131,4 +144,55 @@ public boolean markSupported() {\n       return stream.markSupported();\n     }\n   }\n+\n+  static long findStartingRowPos(Supplier<SeekableInputStream> open, long start) {\n+    try (SeekableInputStream in = open.get()) {\n+      // use a direct decoder that will not buffer so the position of the input stream is accurate\n+      BinaryDecoder decoder = DecoderFactory.get().directBinaryDecoder(in, null);\n+\n+      // an Avro file's layout looks like this:\n+      //   header|block|block|...\n+      // the header contains:\n+      //   magic|string-map|sync\n+      // each block consists of:\n+      //   row-count|compressed-size-in-bytes|block-bytes|sync\n+\n+      // it is necessary to read the header here because this is the only way to get the expected file sync bytes\n+      byte[] magic = MAGIC_READER.read(decoder, null);\n+      if (!Arrays.equals(AVRO_MAGIC, magic)) {\n+        throw new InvalidAvroMagicException(\"Not an Avro file\");\n+      }\n+\n+      META_READER.read(decoder, null); // ignore the file metadata, it isn't needed\n+      byte[] fileSync = SYNC_READER.read(decoder, null);\n+\n+      // the while loop reads row counts and seeks past the block bytes until the next sync pos is >= start, which\n+      // indicates that the next sync is the start of the split.\n+      byte[] blockSync = new byte[16];\n+      long totalRows = 0;\n+      long nextSyncPos = in.getPos();\n+\n+      while (nextSyncPos < start) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0NzcxMA=="}, "originalCommit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2NzE1Mw==", "bodyText": "Thanks for the insightful comment! (before it went away \ud83d\ude03 ) I was not familiar Avro's split reading behavior.", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459667153", "createdAt": "2020-07-23T19:06:36Z", "author": {"login": "shardulm94"}, "path": "core/src/main/java/org/apache/iceberg/avro/AvroIO.java", "diffHunk": "@@ -131,4 +144,55 @@ public boolean markSupported() {\n       return stream.markSupported();\n     }\n   }\n+\n+  static long findStartingRowPos(Supplier<SeekableInputStream> open, long start) {\n+    try (SeekableInputStream in = open.get()) {\n+      // use a direct decoder that will not buffer so the position of the input stream is accurate\n+      BinaryDecoder decoder = DecoderFactory.get().directBinaryDecoder(in, null);\n+\n+      // an Avro file's layout looks like this:\n+      //   header|block|block|...\n+      // the header contains:\n+      //   magic|string-map|sync\n+      // each block consists of:\n+      //   row-count|compressed-size-in-bytes|block-bytes|sync\n+\n+      // it is necessary to read the header here because this is the only way to get the expected file sync bytes\n+      byte[] magic = MAGIC_READER.read(decoder, null);\n+      if (!Arrays.equals(AVRO_MAGIC, magic)) {\n+        throw new InvalidAvroMagicException(\"Not an Avro file\");\n+      }\n+\n+      META_READER.read(decoder, null); // ignore the file metadata, it isn't needed\n+      byte[] fileSync = SYNC_READER.read(decoder, null);\n+\n+      // the while loop reads row counts and seeks past the block bytes until the next sync pos is >= start, which\n+      // indicates that the next sync is the start of the split.\n+      byte[] blockSync = new byte[16];\n+      long totalRows = 0;\n+      long nextSyncPos = in.getPos();\n+\n+      while (nextSyncPos < start) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0NzcxMA=="}, "originalCommit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY4MTUzNQ==", "bodyText": "Sorry, I should have edited, not deleted. I missed the part about it being just before the EOF.", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459681535", "createdAt": "2020-07-23T19:34:09Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/avro/AvroIO.java", "diffHunk": "@@ -131,4 +144,55 @@ public boolean markSupported() {\n       return stream.markSupported();\n     }\n   }\n+\n+  static long findStartingRowPos(Supplier<SeekableInputStream> open, long start) {\n+    try (SeekableInputStream in = open.get()) {\n+      // use a direct decoder that will not buffer so the position of the input stream is accurate\n+      BinaryDecoder decoder = DecoderFactory.get().directBinaryDecoder(in, null);\n+\n+      // an Avro file's layout looks like this:\n+      //   header|block|block|...\n+      // the header contains:\n+      //   magic|string-map|sync\n+      // each block consists of:\n+      //   row-count|compressed-size-in-bytes|block-bytes|sync\n+\n+      // it is necessary to read the header here because this is the only way to get the expected file sync bytes\n+      byte[] magic = MAGIC_READER.read(decoder, null);\n+      if (!Arrays.equals(AVRO_MAGIC, magic)) {\n+        throw new InvalidAvroMagicException(\"Not an Avro file\");\n+      }\n+\n+      META_READER.read(decoder, null); // ignore the file metadata, it isn't needed\n+      byte[] fileSync = SYNC_READER.read(decoder, null);\n+\n+      // the while loop reads row counts and seeks past the block bytes until the next sync pos is >= start, which\n+      // indicates that the next sync is the start of the split.\n+      byte[] blockSync = new byte[16];\n+      long totalRows = 0;\n+      long nextSyncPos = in.getPos();\n+\n+      while (nextSyncPos < start) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0NzcxMA=="}, "originalCommit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxMjM3Ng==", "bodyText": "The fix is to catch EOFException and return the number of rows.", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459712376", "createdAt": "2020-07-23T20:34:53Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/avro/AvroIO.java", "diffHunk": "@@ -131,4 +144,55 @@ public boolean markSupported() {\n       return stream.markSupported();\n     }\n   }\n+\n+  static long findStartingRowPos(Supplier<SeekableInputStream> open, long start) {\n+    try (SeekableInputStream in = open.get()) {\n+      // use a direct decoder that will not buffer so the position of the input stream is accurate\n+      BinaryDecoder decoder = DecoderFactory.get().directBinaryDecoder(in, null);\n+\n+      // an Avro file's layout looks like this:\n+      //   header|block|block|...\n+      // the header contains:\n+      //   magic|string-map|sync\n+      // each block consists of:\n+      //   row-count|compressed-size-in-bytes|block-bytes|sync\n+\n+      // it is necessary to read the header here because this is the only way to get the expected file sync bytes\n+      byte[] magic = MAGIC_READER.read(decoder, null);\n+      if (!Arrays.equals(AVRO_MAGIC, magic)) {\n+        throw new InvalidAvroMagicException(\"Not an Avro file\");\n+      }\n+\n+      META_READER.read(decoder, null); // ignore the file metadata, it isn't needed\n+      byte[] fileSync = SYNC_READER.read(decoder, null);\n+\n+      // the while loop reads row counts and seeks past the block bytes until the next sync pos is >= start, which\n+      // indicates that the next sync is the start of the split.\n+      byte[] blockSync = new byte[16];\n+      long totalRows = 0;\n+      long nextSyncPos = in.getPos();\n+\n+      while (nextSyncPos < start) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0NzcxMA=="}, "originalCommit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODQwMjU1OnYy", "diffSide": "RIGHT", "path": "core/src/main/java/org/apache/iceberg/avro/AvroIO.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxNzo0NToyNFrOG2VFCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODo1MjozMFrOG2XYyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYyMTY0MA==", "bodyText": "nit: This is deprecated in favor of UncheckedIOException", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459621640", "createdAt": "2020-07-23T17:45:24Z", "author": {"login": "rdsr"}, "path": "core/src/main/java/org/apache/iceberg/avro/AvroIO.java", "diffHunk": "@@ -131,4 +144,55 @@ public boolean markSupported() {\n       return stream.markSupported();\n     }\n   }\n+\n+  static long findStartingRowPos(Supplier<SeekableInputStream> open, long start) {\n+    try (SeekableInputStream in = open.get()) {\n+      // use a direct decoder that will not buffer so the position of the input stream is accurate\n+      BinaryDecoder decoder = DecoderFactory.get().directBinaryDecoder(in, null);\n+\n+      // an Avro file's layout looks like this:\n+      //   header|block|block|...\n+      // the header contains:\n+      //   magic|string-map|sync\n+      // each block consists of:\n+      //   row-count|compressed-size-in-bytes|block-bytes|sync\n+\n+      // it is necessary to read the header here because this is the only way to get the expected file sync bytes\n+      byte[] magic = MAGIC_READER.read(decoder, null);\n+      if (!Arrays.equals(AVRO_MAGIC, magic)) {\n+        throw new InvalidAvroMagicException(\"Not an Avro file\");\n+      }\n+\n+      META_READER.read(decoder, null); // ignore the file metadata, it isn't needed\n+      byte[] fileSync = SYNC_READER.read(decoder, null);\n+\n+      // the while loop reads row counts and seeks past the block bytes until the next sync pos is >= start, which\n+      // indicates that the next sync is the start of the split.\n+      byte[] blockSync = new byte[16];\n+      long totalRows = 0;\n+      long nextSyncPos = in.getPos();\n+\n+      while (nextSyncPos < start) {\n+        if (nextSyncPos != in.getPos()) {\n+          in.seek(nextSyncPos);\n+          SYNC_READER.read(decoder, blockSync);\n+\n+          if (!Arrays.equals(fileSync, blockSync)) {\n+            throw new RuntimeIOException(\"Invalid sync at %s\", nextSyncPos);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYyODE0Mg==", "bodyText": "Yes, but readers still expect it from existing code paths. Since this code is called in paths that may still depend on RuntimeIOException, we should still throw it here. I think we should continue to throw it until the 0.11 release, when we can move all of the instances to UncheckedIOException.", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459628142", "createdAt": "2020-07-23T17:56:37Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/avro/AvroIO.java", "diffHunk": "@@ -131,4 +144,55 @@ public boolean markSupported() {\n       return stream.markSupported();\n     }\n   }\n+\n+  static long findStartingRowPos(Supplier<SeekableInputStream> open, long start) {\n+    try (SeekableInputStream in = open.get()) {\n+      // use a direct decoder that will not buffer so the position of the input stream is accurate\n+      BinaryDecoder decoder = DecoderFactory.get().directBinaryDecoder(in, null);\n+\n+      // an Avro file's layout looks like this:\n+      //   header|block|block|...\n+      // the header contains:\n+      //   magic|string-map|sync\n+      // each block consists of:\n+      //   row-count|compressed-size-in-bytes|block-bytes|sync\n+\n+      // it is necessary to read the header here because this is the only way to get the expected file sync bytes\n+      byte[] magic = MAGIC_READER.read(decoder, null);\n+      if (!Arrays.equals(AVRO_MAGIC, magic)) {\n+        throw new InvalidAvroMagicException(\"Not an Avro file\");\n+      }\n+\n+      META_READER.read(decoder, null); // ignore the file metadata, it isn't needed\n+      byte[] fileSync = SYNC_READER.read(decoder, null);\n+\n+      // the while loop reads row counts and seeks past the block bytes until the next sync pos is >= start, which\n+      // indicates that the next sync is the start of the split.\n+      byte[] blockSync = new byte[16];\n+      long totalRows = 0;\n+      long nextSyncPos = in.getPos();\n+\n+      while (nextSyncPos < start) {\n+        if (nextSyncPos != in.getPos()) {\n+          in.seek(nextSyncPos);\n+          SYNC_READER.read(decoder, blockSync);\n+\n+          if (!Arrays.equals(fileSync, blockSync)) {\n+            throw new RuntimeIOException(\"Invalid sync at %s\", nextSyncPos);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYyMTY0MA=="}, "originalCommit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1OTQ2Ng==", "bodyText": "Makes sense", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459659466", "createdAt": "2020-07-23T18:52:30Z", "author": {"login": "rdsr"}, "path": "core/src/main/java/org/apache/iceberg/avro/AvroIO.java", "diffHunk": "@@ -131,4 +144,55 @@ public boolean markSupported() {\n       return stream.markSupported();\n     }\n   }\n+\n+  static long findStartingRowPos(Supplier<SeekableInputStream> open, long start) {\n+    try (SeekableInputStream in = open.get()) {\n+      // use a direct decoder that will not buffer so the position of the input stream is accurate\n+      BinaryDecoder decoder = DecoderFactory.get().directBinaryDecoder(in, null);\n+\n+      // an Avro file's layout looks like this:\n+      //   header|block|block|...\n+      // the header contains:\n+      //   magic|string-map|sync\n+      // each block consists of:\n+      //   row-count|compressed-size-in-bytes|block-bytes|sync\n+\n+      // it is necessary to read the header here because this is the only way to get the expected file sync bytes\n+      byte[] magic = MAGIC_READER.read(decoder, null);\n+      if (!Arrays.equals(AVRO_MAGIC, magic)) {\n+        throw new InvalidAvroMagicException(\"Not an Avro file\");\n+      }\n+\n+      META_READER.read(decoder, null); // ignore the file metadata, it isn't needed\n+      byte[] fileSync = SYNC_READER.read(decoder, null);\n+\n+      // the while loop reads row counts and seeks past the block bytes until the next sync pos is >= start, which\n+      // indicates that the next sync is the start of the split.\n+      byte[] blockSync = new byte[16];\n+      long totalRows = 0;\n+      long nextSyncPos = in.getPos();\n+\n+      while (nextSyncPos < start) {\n+        if (nextSyncPos != in.getPos()) {\n+          in.seek(nextSyncPos);\n+          SYNC_READER.read(decoder, blockSync);\n+\n+          if (!Arrays.equals(fileSync, blockSync)) {\n+            throw new RuntimeIOException(\"Invalid sync at %s\", nextSyncPos);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYyMTY0MA=="}, "originalCommit": {"oid": "d107f778da2ea0997d9eda087982c20ed1e1f95c"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODk3NjUxOnYy", "diffSide": "RIGHT", "path": "core/src/test/java/org/apache/iceberg/avro/TestAvroFileSplit.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDozNjo1M1rOG2argA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMDozNjo1M1rOG2argA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTcxMzQwOA==", "bodyText": "@shardulm94, here's a test for the EOF case.", "url": "https://github.com/apache/iceberg/pull/1222#discussion_r459713408", "createdAt": "2020-07-23T20:36:53Z", "author": {"login": "rdblue"}, "path": "core/src/test/java/org/apache/iceberg/avro/TestAvroFileSplit.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.avro;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.UUID;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.avro.DataWriter;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.types.Types.NestedField;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+public class TestAvroFileSplit {\n+  private static final Schema SCHEMA = new Schema(\n+      NestedField.required(1, \"id\", Types.LongType.get()),\n+      NestedField.required(2, \"data\", Types.StringType.get()));\n+\n+  private static final int NUM_RECORDS = 100_000;\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  public List<Record> expected = null;\n+  public InputFile file = null;\n+\n+  @Before\n+  public void writeDataFile() throws IOException {\n+    this.expected = Lists.newArrayList();\n+\n+    OutputFile out = Files.localOutput(temp.newFile());\n+\n+    try (FileAppender<Object> writer = Avro.write(out)\n+        .set(TableProperties.AVRO_COMPRESSION, \"uncompressed\")\n+        .createWriterFunc(DataWriter::create)\n+        .schema(SCHEMA)\n+        .overwrite()\n+        .build()) {\n+\n+      Record record = GenericRecord.create(SCHEMA);\n+      for (long i = 0; i < NUM_RECORDS; i += 1) {\n+        Record next = record.copy(ImmutableMap.of(\n+            \"id\", i,\n+            \"data\", UUID.randomUUID().toString()));\n+        expected.add(next);\n+        writer.add(next);\n+      }\n+    }\n+\n+    this.file = out.toInputFile();\n+  }\n+\n+  @Test\n+  public void testSplitDataSkipping() throws IOException {\n+    long end = file.getLength();\n+    long splitLocation = end / 2;\n+\n+    List<Record> firstHalf = readAvro(file, SCHEMA, 0, splitLocation);\n+    Assert.assertNotEquals(\"First split should not be empty\", 0, firstHalf.size());\n+\n+    List<Record> secondHalf = readAvro(file, SCHEMA, splitLocation + 1, end - splitLocation - 1);\n+    Assert.assertNotEquals(\"Second split should not be empty\", 0, secondHalf.size());\n+\n+    Assert.assertEquals(\"Total records should match expected\",\n+        expected.size(), firstHalf.size() + secondHalf.size());\n+\n+    for (int i = 0; i < firstHalf.size(); i += 1) {\n+      Assert.assertEquals(expected.get(i), firstHalf.get(i));\n+    }\n+\n+    for (int i = 0; i < secondHalf.size(); i += 1) {\n+      Assert.assertEquals(expected.get(firstHalf.size() + i), secondHalf.get(i));\n+    }\n+  }\n+\n+  @Test\n+  public void testPosField() throws IOException {\n+    Schema projection = new Schema(\n+        SCHEMA.columns().get(0),\n+        MetadataColumns.ROW_POSITION,\n+        SCHEMA.columns().get(1));\n+\n+    List<Record> records = readAvro(file, projection, 0, file.getLength());\n+\n+    for (int i = 0; i < expected.size(); i += 1) {\n+      Assert.assertEquals(\"Field _pos should match\",\n+          (long) i, records.get(i).getField(MetadataColumns.ROW_POSITION.name()));\n+      Assert.assertEquals(\"Field id should match\",\n+          expected.get(i).getField(\"id\"), records.get(i).getField(\"id\"));\n+      Assert.assertEquals(\"Field data should match\",\n+          expected.get(i).getField(\"data\"), records.get(i).getField(\"data\"));\n+    }\n+  }\n+\n+  @Test\n+  public void testPosFieldWithSplits() throws IOException {\n+    Schema projection = new Schema(\n+        SCHEMA.columns().get(0),\n+        MetadataColumns.ROW_POSITION,\n+        SCHEMA.columns().get(1));\n+\n+    long end = file.getLength();\n+    long splitLocation = end / 2;\n+\n+    List<Record> secondHalf = readAvro(file, projection, splitLocation + 1, end - splitLocation - 1);\n+    Assert.assertNotEquals(\"Second split should not be empty\", 0, secondHalf.size());\n+\n+    List<Record> firstHalf = readAvro(file, projection, 0, splitLocation);\n+    Assert.assertNotEquals(\"First split should not be empty\", 0, firstHalf.size());\n+\n+    Assert.assertEquals(\"Total records should match expected\",\n+        expected.size(), firstHalf.size() + secondHalf.size());\n+\n+    for (int i = 0; i < firstHalf.size(); i += 1) {\n+      Assert.assertEquals(\"Field _pos should match\",\n+          (long) i, firstHalf.get(i).getField(MetadataColumns.ROW_POSITION.name()));\n+      Assert.assertEquals(\"Field id should match\",\n+          expected.get(i).getField(\"id\"), firstHalf.get(i).getField(\"id\"));\n+      Assert.assertEquals(\"Field data should match\",\n+          expected.get(i).getField(\"data\"), firstHalf.get(i).getField(\"data\"));\n+    }\n+\n+    for (int i = 0; i < secondHalf.size(); i += 1) {\n+      Assert.assertEquals(\"Field _pos should match\",\n+          (long) (firstHalf.size() + i), secondHalf.get(i).getField(MetadataColumns.ROW_POSITION.name()));\n+      Assert.assertEquals(\"Field id should match\",\n+          expected.get(firstHalf.size() + i).getField(\"id\"), secondHalf.get(i).getField(\"id\"));\n+      Assert.assertEquals(\"Field data should match\",\n+          expected.get(firstHalf.size() + i).getField(\"data\"), secondHalf.get(i).getField(\"data\"));\n+    }\n+  }\n+\n+  @Test\n+  public void testPosWithEOFSplit() throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "df1cae9a872567e9efaa012aaa3d13234fda73b5"}, "originalPosition": 166}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3781, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}