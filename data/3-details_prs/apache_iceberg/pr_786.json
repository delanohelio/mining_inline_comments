{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzcyNDE4MzE5", "number": 786, "title": "replace SparkDataFile with DataFile", "bodyText": "This fixes #763", "createdAt": "2020-02-07T13:43:18Z", "url": "https://github.com/apache/iceberg/pull/786", "merged": true, "mergeCommit": {"oid": "4d96944679a8ab9aa028299477bbf085621d55b8"}, "closed": true, "closedAt": "2020-02-19T17:17:28Z", "author": {"login": "chenjunjiedada"}, "timelineItems": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcB_i7zgH2gAyMzcyNDE4MzE5OmQwNmMwNzI0ZDVmMDJjNTVmNDc5ZWFmZThkZmUyMWE0NzEyNzI2MjY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcFbJXNAH2gAyMzcyNDE4MzE5OmRhY2E3ZjE3MjViODFiYzEzYzBiYWNmYWU1NDE5ZTJiYTMyM2VjYTY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "d06c0724d5f02c55f479eafe8dfe21a471272626", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/d06c0724d5f02c55f479eafe8dfe21a471272626", "committedDate": "2020-02-07T13:42:11Z", "message": "replace SparkDataFile with DataFile"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU2MzYzMTQ3", "url": "https://github.com/apache/iceberg/pull/786#pullrequestreview-356363147", "createdAt": "2020-02-11T00:37:23Z", "commit": {"oid": "d06c0724d5f02c55f479eafe8dfe21a471272626"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMDozNzoyM1rOFn6kbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xMVQwMTowMTo0MFrOFn6-PQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM5ODM4MA==", "bodyText": "I think it will be a good idea to have the spec argument before metricsConfig and conf as the last two are optional. I would make it consistent in all touched files (i.e. first spec then config).\n  def listPartition(\n      partition: Map[String, String],\n      uri: String,\n      format: String,\n      spec: PartitionSpec,\n      conf: Configuration = new Configuration(),\n      metricsConfig: MetricsConfig = MetricsConfig.getDefault): Seq[DataFile] = {", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377398380", "createdAt": "2020-02-11T00:37:23Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -174,22 +175,23 @@ object SparkTableUtil {\n    * @param format partition format, avro or parquet\n    * @param conf a Hadoop conf\n    * @param metricsConfig a metrics conf\n-   * @return a seq of [[SparkDataFile]]\n+   * @return a seq of [[DataFile]]\n    */\n   def listPartition(\n       partition: Map[String, String],\n       uri: String,\n       format: String,\n       conf: Configuration = new Configuration(),\n-      metricsConfig: MetricsConfig = MetricsConfig.getDefault): Seq[SparkDataFile] = {\n+      metricsConfig: MetricsConfig = MetricsConfig.getDefault,\n+      spec: PartitionSpec): Seq[DataFile] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d06c0724d5f02c55f479eafe8dfe21a471272626"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMjQ2NA==", "bodyText": "Do we still need these methods?", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377402464", "createdAt": "2020-02-11T00:52:38Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -200,50 +202,6 @@ object SparkTableUtil {\n    */\n   case class SparkPartition(values: Map[String, String], uri: String, format: String)\n \n-  /**\n-   * Case class representing a data file.\n-   */\n-  case class SparkDataFile(\n-      path: String,\n-      partition: collection.Map[String, String],\n-      format: String,\n-      fileSize: Long,\n-      rowGroupSize: Long,\n-      rowCount: Long,\n-      columnSizes: Array[Long],\n-      valueCounts: Array[Long],\n-      nullValueCounts: Array[Long],\n-      lowerBounds: Seq[Array[Byte]],\n-      upperBounds: Seq[Array[Byte]]\n-    ) {\n-\n-    /**\n-     * Convert this to a [[DataFile]] that can be added to a [[org.apache.iceberg.Table]].\n-     *\n-     * @param spec a [[PartitionSpec]] that will be used to parse the partition key\n-     * @return a [[DataFile]] that can be passed to [[org.apache.iceberg.AppendFiles]]\n-     */\n-    def toDataFile(spec: PartitionSpec): DataFile = {\n-      // values are strings, so pass a path to let the builder coerce to the right types\n-      val partitionKey = spec.fields.asScala.map(_.name).map { name =>\n-        s\"$name=${partition(name)}\"\n-      }.mkString(\"/\")\n-\n-      DataFiles.builder(spec)\n-        .withPath(path)\n-        .withFormat(format)\n-        .withFileSizeInBytes(fileSize)\n-        .withMetrics(new Metrics(rowCount,\n-          arrayToMap(columnSizes),\n-          arrayToMap(valueCounts),\n-          arrayToMap(nullValueCounts),\n-          arrayToMap(lowerBounds),\n-          arrayToMap(upperBounds)))\n-        .withPartitionPath(partitionKey)\n-        .build()\n-    }\n-  }\n-\n   private def bytesMapToArray(map: java.util.Map[Integer, ByteBuffer]): Seq[Array[Byte]] = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d06c0724d5f02c55f479eafe8dfe21a471272626"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMzQwMw==", "bodyText": "What about initializing metrics before as a separate variable?\n    fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n      val metrics = new Metrics(-1L, arrayToMap(null), arrayToMap(null), arrayToMap(null))\n      val partitionKey = spec.fields.asScala.map(_.name).map { name =>\n        s\"$name=${partitionPath(name)}\"\n      }.mkString(\"/\")\n\n      DataFiles.builder(spec)\n        .withPath(stat.getPath.toString)\n        .withFormat(\"avro\")\n        .withFileSizeInBytes(stat.getLen)\n        .withMetrics(metrics)\n        .withPartitionPath(partitionKey)\n        .build()\n    }", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377403403", "createdAt": "2020-02-11T00:56:04Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -329,21 +287,26 @@ object SparkTableUtil {\n   private def listAvroPartition(\n       partitionPath: Map[String, String],\n       partitionUri: String,\n-      conf: Configuration): Seq[SparkDataFile] = {\n+      conf: Configuration,\n+      spec: PartitionSpec): Seq[DataFile] = {\n     val partition = new Path(partitionUri)\n     val fs = partition.getFileSystem(conf)\n \n     fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n-      SparkDataFile(\n-        stat.getPath.toString,\n-        partitionPath, \"avro\", stat.getLen,\n-        stat.getBlockSize,\n-        -1,\n-        null,\n-        null,\n-        null,\n-        null,\n-        null)\n+      val partitionKey = spec.fields.asScala.map(_.name).map { name =>\n+        s\"$name=${partitionPath(name)}\"\n+      }.mkString(\"/\")\n+\n+      DataFiles.builder(spec)\n+        .withPath(stat.getPath.toString)\n+        .withFormat(\"avro\")\n+        .withFileSizeInBytes(stat.getLen)\n+        .withMetrics(new Metrics(-1L,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d06c0724d5f02c55f479eafe8dfe21a471272626"}, "originalPosition": 137}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwNDk4OQ==", "bodyText": "What about this?\nval partition = Map.empty[String, String]\nval spec = PartitionSpec.unpartitioned()\nval conf = spark.sessionState.newHadoopConf()\nval metricsConfig = MetricsConfig.fromProperties(targetTable.properties)\n\nval files = listPartition(partition, sourceTable.location.toString, format.get, spec, conf, metricsConfig)", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377404989", "createdAt": "2020-02-11T01:01:40Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -492,10 +454,11 @@ object SparkTableUtil {\n     val conf = spark.sessionState.newHadoopConf()\n     val metricsConfig = MetricsConfig.fromProperties(targetTable.properties)\n \n-    val files = listPartition(Map.empty, sourceTable.location.toString, format.get, conf, metricsConfig)\n+    val files = listPartition(Map.empty, sourceTable.location.toString, format.get, conf, metricsConfig,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d06c0724d5f02c55f479eafe8dfe21a471272626"}, "originalPosition": 243}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "db85445e56496734bf596a9f49d229424df549a7", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/db85445e56496734bf596a9f49d229424df549a7", "committedDate": "2020-02-11T01:43:37Z", "message": "minor changes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU5MTgwMzA4", "url": "https://github.com/apache/iceberg/pull/786#pullrequestreview-359180308", "createdAt": "2020-02-14T19:48:36Z", "commit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxOTo0ODozNlrOFqBz9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNFQxOTo0ODozNlrOFqBz9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYxNDE5Nw==", "bodyText": "Why do we want to remove the sort? I think it is needed to collocate files for the same partition next to each other so that partition skipping is quick.", "url": "https://github.com/apache/iceberg/pull/786#discussion_r379614197", "createdAt": "2020-02-14T19:48:36Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -527,9 +425,8 @@ object SparkTableUtil {\n     val metricsConfig = MetricsConfig.fromProperties(targetTable.properties)\n \n     val manifests = partitionDS\n-      .flatMap(partition => listPartition(partition, serializableConf, metricsConfig))\n+      .flatMap(partition => listPartition(partition, spec, serializableConf, metricsConfig))\n       .repartition(numShufflePartitions)\n-      .orderBy($\"path\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 333}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzU5NDIyMzYw", "url": "https://github.com/apache/iceberg/pull/786#pullrequestreview-359422360", "createdAt": "2020-02-16T19:02:14Z", "commit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNlQxOTowMjoxNFrOFqUzBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xNlQxOTowMjoxNFrOFqUzBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ==", "bodyText": "Shouldn't rowCount be a positive number?", "url": "https://github.com/apache/iceberg/pull/786#discussion_r379925255", "createdAt": "2020-02-16T19:02:14Z", "author": {"login": "rdsr"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -329,70 +222,74 @@ object SparkTableUtil {\n   private def listAvroPartition(\n       partitionPath: Map[String, String],\n       partitionUri: String,\n-      conf: Configuration): Seq[SparkDataFile] = {\n+      spec: PartitionSpec,\n+      conf: Configuration): Seq[DataFile] = {\n     val partition = new Path(partitionUri)\n     val fs = partition.getFileSystem(conf)\n \n     fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n-      SparkDataFile(\n-        stat.getPath.toString,\n-        partitionPath, \"avro\", stat.getLen,\n-        stat.getBlockSize,\n-        -1,\n-        null,\n-        null,\n-        null,\n-        null,\n-        null)\n+      val metrics = new Metrics(-1L, arrayToMap(null), arrayToMap(null), arrayToMap(null))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db85445e56496734bf596a9f49d229424df549a7"}, "originalPosition": 192}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2e0f4102aa11b6a2be56da23949f9ca81076923b", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/2e0f4102aa11b6a2be56da23949f9ca81076923b", "committedDate": "2020-02-18T00:44:47Z", "message": "sort data file by path"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDcyNTUw", "url": "https://github.com/apache/iceberg/pull/786#pullrequestreview-360072550", "createdAt": "2020-02-18T05:03:03Z", "commit": {"oid": "2e0f4102aa11b6a2be56da23949f9ca81076923b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwNTowMzowM1rOFq1Npw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0xOFQwNTowMzowM1rOFq1Npw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQ1NjM1OQ==", "bodyText": "nit: this can fit into 1 line if we reuse dataFileEncoder:\nimplicit val pathDataFileEncoder: Encoder[(String, DataFile)] = Encoders.tuple(Encoders.STRING, dataFileEncoder)", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380456359", "createdAt": "2020-02-18T05:03:03Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -516,6 +413,9 @@ object SparkTableUtil {\n       stagingDir: String): Unit = {\n \n     implicit val manifestFileEncoder: Encoder[ManifestFile] = Encoders.javaSerialization[ManifestFile]\n+    implicit val dataFileEncoder: Encoder[DataFile] = Encoders.javaSerialization[DataFile]\n+    implicit val pathDataFileEncoder: Encoder[(String, DataFile)] = Encoders.tuple(Encoders.STRING,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2e0f4102aa11b6a2be56da23949f9ca81076923b"}, "originalPosition": 323}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzYwMDczMDM1", "url": "https://github.com/apache/iceberg/pull/786#pullrequestreview-360073035", "createdAt": "2020-02-18T05:05:09Z", "commit": {"oid": "2e0f4102aa11b6a2be56da23949f9ca81076923b"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "daca7f1725b81bc13c0bacfae5419e2ba323eca6", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/daca7f1725b81bc13c0bacfae5419e2ba323eca6", "committedDate": "2020-02-18T05:33:22Z", "message": "fix nit"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4926, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}