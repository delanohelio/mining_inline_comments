{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAxNjMxNjAx", "number": 909, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMDozNTo0MFrODwjpdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQwNDozNzoyM1rODw6RXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyMjQyMjk1OnYy", "diffSide": "LEFT", "path": "core/src/main/java/org/apache/iceberg/avro/ValueReaders.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMDozNTo0MFrOGDodsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMDozNTo0MFrOGDodsw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ2MTg3NQ==", "bodyText": "I'm moving this out of Avro and adding a callback to convert the constants to PartitionUtil.constantsMap. That way, Spark can supply a conversion function and use it in both places, instead of duplicating the conversion in Avro and Parquet readers.", "url": "https://github.com/apache/iceberg/pull/909#discussion_r406461875", "createdAt": "2020-04-09T20:35:40Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/avro/ValueReaders.java", "diffHunk": "@@ -597,10 +595,6 @@ protected StructReader(List<ValueReader<?>> readers, Types.StructType struct, Ma\n \n     protected abstract void set(S struct, int pos, Object value);\n \n-    protected Object prepareConstant(Type type, Object value) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6965f4049ea7a9ae013ad0d586bff038b136983b"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyMjQzNTEyOnYy", "diffSide": "LEFT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkValueReaders.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMDozOToyMFrOGDok_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMDozOToyMFrOGDok_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ2Mzc0Mw==", "bodyText": "Moved into Spark.", "url": "https://github.com/apache/iceberg/pull/909#discussion_r406463743", "createdAt": "2020-04-09T20:39:20Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkValueReaders.java", "diffHunk": "@@ -287,30 +284,5 @@ protected void set(InternalRow struct, int pos, Object value) {\n         struct.setNullAt(pos);\n       }\n     }\n-\n-    @Override\n-    protected Object prepareConstant(Type type, Object value) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6c5db2d4dbf6f7cfd40a1785abbb8cb74539a879"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUyNjEyOTU5OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQwNDozNzoyM1rOGEKVHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xMVQxNTo1MTowNlrOGEOIpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzAxNjczNA==", "bodyText": "Why not write the data for the parameterized format for which the test is running?", "url": "https://github.com/apache/iceberg/pull/909#discussion_r407016734", "createdAt": "2020-04-11T04:37:23Z", "author": {"login": "rdsr"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java", "diffHunk": "@@ -307,4 +308,72 @@ public void testPartitionValueTypes() throws Exception {\n       TestTables.clearTables();\n     }\n   }\n+\n+  @Test\n+  public void testNestedPartitionValues() throws Exception {\n+    Assume.assumeTrue(\"ORC can't project nested partition values\", !format.equalsIgnoreCase(\"orc\"));\n+\n+    String[] columnNames = new String[] {\n+        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n+    };\n+\n+    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n+    Schema nestedSchema = new Schema(optional(1, \"nested\", SUPPORTED_PRIMITIVES.asStruct()));\n+\n+    // create a table around the source data\n+    String sourceLocation = temp.newFolder(\"source_table\").toString();\n+    Table source = tables.create(nestedSchema, sourceLocation);\n+\n+    // write out an Avro data file with all of the data types for source data\n+    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n+    File avroData = temp.newFile(\"data.avro\");\n+    Assert.assertTrue(avroData.delete());\n+    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n+        .schema(source.schema())\n+        .build()) {\n+      appender.addAll(expected);\n+    }\n+\n+    // add the Avro data file to the source table", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "db7a4c724a2eaa2b2f6ea8841a3e5fdac4911b62"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA3OTA3Ng==", "bodyText": "This is just source data for the write from Spark with the target format. Since it isn't part of the test, we don't want it to change at all in ways that might affect the test.", "url": "https://github.com/apache/iceberg/pull/909#discussion_r407079076", "createdAt": "2020-04-11T15:51:06Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java", "diffHunk": "@@ -307,4 +308,72 @@ public void testPartitionValueTypes() throws Exception {\n       TestTables.clearTables();\n     }\n   }\n+\n+  @Test\n+  public void testNestedPartitionValues() throws Exception {\n+    Assume.assumeTrue(\"ORC can't project nested partition values\", !format.equalsIgnoreCase(\"orc\"));\n+\n+    String[] columnNames = new String[] {\n+        \"b\", \"i\", \"l\", \"f\", \"d\", \"date\", \"ts\", \"s\", \"bytes\", \"dec_9_0\", \"dec_11_2\", \"dec_38_10\"\n+    };\n+\n+    HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n+    Schema nestedSchema = new Schema(optional(1, \"nested\", SUPPORTED_PRIMITIVES.asStruct()));\n+\n+    // create a table around the source data\n+    String sourceLocation = temp.newFolder(\"source_table\").toString();\n+    Table source = tables.create(nestedSchema, sourceLocation);\n+\n+    // write out an Avro data file with all of the data types for source data\n+    List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n+    File avroData = temp.newFile(\"data.avro\");\n+    Assert.assertTrue(avroData.delete());\n+    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(avroData))\n+        .schema(source.schema())\n+        .build()) {\n+      appender.addAll(expected);\n+    }\n+\n+    // add the Avro data file to the source table", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzAxNjczNA=="}, "originalCommit": {"oid": "db7a4c724a2eaa2b2f6ea8841a3e5fdac4911b62"}, "originalPosition": 38}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2903, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}