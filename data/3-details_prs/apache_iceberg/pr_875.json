{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk0Mzc3ODM4", "number": 875, "title": "Spark: Implement an action to rewrite manifests", "bodyText": "This PR adds a Spark action that rewrites manifests and optimizes the layout of metadata for faster job planning.\nIceberg users have two ways to optimize metadata: rely on the automatic merging of manifests (e.g. MergeAppend) or rewrite manifests on demand using RewriteManifests. The automatic merge of manifests is a great feature and works really well when writes to a table are aligned with partitions. If each incoming batch writes to many partitions, RewriteManifests can be used to rewrite certain manifests on demand. RewriteManifests is handy for rewriting a reasonable number of manifests but it is slow for rewriting all metadata in huge tables. In addition, RewriteManifests requires us to define clustering values manually and there is no automatic bin-packing of small clusters.\nWith snapshot id inheritance, we have a new way to approach this problem. Specifically, we can prepare manifests in a distributed manner on executors and cheaply commit them on the driver. This serves as a great basis for the Spark action that rewrites manifests.", "createdAt": "2020-03-26T19:35:05Z", "url": "https://github.com/apache/iceberg/pull/875", "merged": true, "mergeCommit": {"oid": "0c8ab3d856998fee0b03f403630ccb58fcbf9fdd"}, "closed": true, "closedAt": "2020-04-23T17:43:06Z", "author": {"login": "aokolnychyi"}, "timelineItems": {"totalCount": 24, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcVCIXDABqjMyMDY0NDE4NTg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcafoEQAFqTM5OTI4MDk4OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "39a7b908170696d02c93915e1f2f0b022dcab6ee", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/39a7b908170696d02c93915e1f2f0b022dcab6ee", "committedDate": "2020-03-25T19:28:23Z", "message": "Spark: Implement an action to rewrite manifests"}, "afterCommit": {"oid": "1ae293ed593542597a95cc44cc734f7f6be11efb", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/1ae293ed593542597a95cc44cc734f7f6be11efb", "committedDate": "2020-04-06T14:47:57Z", "message": "Spark: Implement an action to rewrite manifests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg4NzEwNzE4", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-388710718", "createdAt": "2020-04-07T00:36:50Z", "commit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwMDozNjo1MFrOGBu3HQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwMDozNjo1MFrOGBu3HQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2OTUzMw==", "bodyText": "Minor: I usually like to add a constant for the check interval so it can be configured easily.", "url": "https://github.com/apache/iceberg/pull/875#discussion_r404469533", "createdAt": "2020-04-07T00:36:50Z", "author": {"login": "rdblue"}, "path": "core/src/main/java/org/apache/iceberg/ManifestsWriter.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.io.OutputFile;\n+\n+/**\n+ * A manifest writer that splits entries into multiple files in order to produce files of the configured size.\n+ */\n+public class ManifestsWriter implements Closeable {\n+  private final PartitionSpec spec;\n+  private final Long snapshotId;\n+  private final Supplier<OutputFile> outputFileSupplier;\n+  private final long targetManifestSizeBytes;\n+  private final List<ManifestFile> manifests;\n+  private ManifestWriter writer;\n+  private long currentNumEntries;\n+\n+  public ManifestsWriter(PartitionSpec spec, Long snapshotId,\n+                         Supplier<OutputFile> outputFileSupplier,\n+                         long targetManifestSizeBytes) {\n+    this.spec = spec;\n+    this.snapshotId = snapshotId;\n+    this.outputFileSupplier = outputFileSupplier;\n+    this.targetManifestSizeBytes = targetManifestSizeBytes;\n+    this.manifests = Lists.newArrayList();\n+    this.currentNumEntries = 0;\n+  }\n+\n+  public ManifestsWriter(PartitionSpec spec,\n+                         Supplier<OutputFile> outputFileSupplier,\n+                         long targetManifestSizeBytes) {\n+    this(spec, null, outputFileSupplier, targetManifestSizeBytes);\n+  }\n+\n+  public void add(DataFile addedFile) {\n+    lazyWriter().add(addedFile);\n+    currentNumEntries++;\n+  }\n+\n+  void add(ManifestEntry entry) {\n+    lazyWriter().add(entry);\n+    currentNumEntries++;\n+  }\n+\n+  public void existing(DataFile existingFile, long fileSnapshotId) {\n+    lazyWriter().existing(existingFile, fileSnapshotId);\n+    currentNumEntries++;\n+  }\n+\n+  void existing(ManifestEntry entry) {\n+    lazyWriter().existing(entry);\n+    currentNumEntries++;\n+  }\n+\n+  public void delete(DataFile deletedFile) {\n+    lazyWriter().delete(deletedFile);\n+    currentNumEntries++;\n+  }\n+\n+  void delete(ManifestEntry entry) {\n+    lazyWriter().delete(entry);\n+    currentNumEntries++;\n+  }\n+\n+  public void close() {\n+    if (writer != null) {\n+      try {\n+        writer.close();\n+        manifests.add(writer.toManifestFile());\n+        currentNumEntries = 0;\n+      } catch (IOException e) {\n+        throw new RuntimeIOException(e);\n+      }\n+    }\n+  }\n+\n+  public List<ManifestFile> manifests() {\n+    return manifests;\n+  }\n+\n+  private ManifestWriter lazyWriter() {\n+    // verify the size of the current manifest every 25 entries to avoid calling writer.length() every time\n+    if (writer == null) {\n+      writer = new ManifestWriter(spec, outputFileSupplier.get(), snapshotId);\n+    } else if (currentNumEntries % 25 == 0 && writer.length() >= targetManifestSizeBytes) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531"}, "originalPosition": 109}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg4NzExNzg2", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-388711786", "createdAt": "2020-04-07T00:40:10Z", "commit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwMDo0MDoxMFrOGBu68A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwMDo0MDoxMFrOGBu68A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ3MDUxMg==", "bodyText": "This bit can be tricky if snapshot id inheritance is enabled concurrently.", "url": "https://github.com/apache/iceberg/pull/875#discussion_r404470512", "createdAt": "2020-04-07T00:40:10Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.util.BinPacking;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.FlatMapGroupsFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.TypedColumn;\n+import org.apache.spark.sql.expressions.Aggregator;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+// TODO: concurrent modification of snapshotIdInheritanceEnabled or specs?\n+public class RewriteManifestsAction\n+    implements SnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final Map<Integer, PartitionSpec> specs;\n+  private final Map<String, String> summary;\n+  private final int defaultParallelism;\n+  private final boolean snapshotIdInheritanceEnabled;\n+  private final long targetManifestSizeBytes;\n+\n+  private final Encoder<ManifestFile> manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+  private final Encoder<Entry> entryEncoder = Encoders.javaSerialization(Entry.class);\n+  private final Encoder<Bin> binEncoder = Encoders.bean(Bin.class);\n+\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.specs = table.specs();\n+    this.summary = Maps.newHashMap();\n+    this.defaultParallelism = Integer.parseInt(\n+        spark.conf().get(\"spark.default.parallelism\", \"200\"));\n+    this.snapshotIdInheritanceEnabled = PropertyUtil.propertyAsBoolean(\n+        table.properties(),\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED,\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED_DEFAULT);\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      fileIO = table.io();\n+    }\n+  }\n+\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsAction set(String property, String value) {\n+    summary.put(property, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {\n+    Preconditions.checkArgument(stagingLocation != null, \"Staging location must be set\");\n+\n+    List<ManifestFile> matchingManifests = findMatchingManifests();\n+    if (matchingManifests.isEmpty()) {\n+      return null;\n+    }\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+\n+    int parallelism = Math.min(matchingManifests.size(), defaultParallelism);\n+    JavaRDD<ManifestFile> manifestRDD = sparkContext.parallelize(matchingManifests, parallelism);\n+    Dataset<ManifestFile> manifestDS = spark.createDataset(manifestRDD.rdd(), manifestEncoder);\n+    Dataset<Entry> manifestEntryDS = manifestDS.flatMap(toEntries(io, specs), entryEncoder);\n+\n+    try {\n+      manifestEntryDS.cache();\n+\n+      long manifestEntrySizeBytes = computeManifestEntrySizeBytes(matchingManifests);\n+      Map<Integer, List<PartitionMetadata>> metadataSizeSummary = computeMetadataSizeSummary(\n+          manifestEntryDS,\n+          manifestEntrySizeBytes);\n+\n+      Map<Integer, Map<StructLike, Integer>> bins = computeBins(metadataSizeSummary);\n+\n+      // the size of bins is estimated to be roughly targetManifestSizeBytes\n+      // we allow the actual size of manifests to be 5% higher to avoid closing\n+      // manifests near the end of bins if the estimation is not precise enough\n+      // it is better to have slightly bigger manifests rather than manifests with a couple of entries\n+      long manifestSizeBytes = (long) (1.05 * targetManifestSizeBytes);\n+      List<ManifestFile> newManifests = manifestEntryDS\n+          .groupByKey(toBin(bins), binEncoder)\n+          .flatMapGroups(toManifest(io, manifestSizeBytes, stagingLocation, specs), manifestEncoder)\n+          .collectAsList();\n+\n+      replaceManifests(matchingManifests, newManifests);\n+\n+      return new RewriteManifestsActionResult(matchingManifests, newManifests);\n+    } finally {\n+      manifestEntryDS.unpersist(false);\n+    }\n+  }\n+\n+  private List<ManifestFile> findMatchingManifests() {\n+    Snapshot snapshot = table.currentSnapshot();\n+    if (snapshot == null) {\n+      return ImmutableList.of();\n+    }\n+    return snapshot.manifests().stream().filter(predicate).collect(Collectors.toList());\n+  }\n+\n+  // computes the average manifest entry size based on available stats for manifests\n+  private Long computeManifestEntrySizeBytes(List<ManifestFile> manifests) {\n+    long totalSize = 0L;\n+    int numEntries = 0;\n+\n+    for (ManifestFile m : manifests) {\n+      ValidationException.check(\n+          m.addedFilesCount() != null && m.existingFilesCount() != null && m.deletedFilesCount() != null,\n+          \"No file counts in manifest: \" + m.path());\n+\n+      totalSize += m.length();\n+      numEntries += m.addedFilesCount() + m.existingFilesCount() + m.deletedFilesCount();\n+    }\n+\n+    ValidationException.check(totalSize > 0L, \"Total size of manifests must be greater than 0\");\n+    ValidationException.check(numEntries > 0, \"Number of manifest entries must be greater than 0\");\n+\n+    return totalSize / numEntries;\n+  }\n+\n+  // computes the estimated metadata size per spec and partition\n+  private Map<Integer, List<PartitionMetadata>> computeMetadataSizeSummary(\n+      Dataset<Entry> manifestEntryDS,\n+      long manifestEntrySizeBytes) {\n+\n+    MetadataSizeAggregator agg = new MetadataSizeAggregator(specs, manifestEntrySizeBytes);\n+    TypedColumn<Entry, Map<Integer, List<PartitionMetadata>>> column = agg.toColumn().name(\"result\");\n+    return manifestEntryDS.select(column).collectAsList().get(0);\n+  }\n+\n+  // groups smaller partitions into bins of the target size\n+  private Map<Integer, Map<StructLike, Integer>> computeBins(Map<Integer, List<PartitionMetadata>> sizeSummary) {\n+    Map<Integer, Map<StructLike, Integer>> binMap = Maps.newHashMap();\n+\n+    sizeSummary.forEach((specId, sizes) -> {\n+      BinPacking.ListPacker<PartitionMetadata> packer = new BinPacking.ListPacker<>(targetManifestSizeBytes, 1, false);\n+      List<List<PartitionMetadata>> bins = packer.pack(sizes, PartitionMetadata::getMetadataSizeBytes);\n+\n+      for (int binIndex = 0; binIndex < bins.size(); binIndex++) {\n+        List<PartitionMetadata> bin = bins.get(binIndex);\n+\n+        for (PartitionMetadata binEntry : bin) {\n+          Map<StructLike, Integer> map = binMap.computeIfAbsent(binEntry.specId, key -> Maps.newHashMap());\n+          map.put(binEntry.partition, binIndex);\n+        }\n+      }\n+    });\n+\n+    return binMap;\n+  }\n+\n+  private void replaceManifests(Iterable<ManifestFile> deletedManifests, Iterable<ManifestFile> addedManifests) {\n+    try {\n+      RewriteManifests rewriteManifests = table.rewriteManifests();\n+      deletedManifests.forEach(rewriteManifests::deleteManifest);\n+      addedManifests.forEach(rewriteManifests::addManifest);\n+      summary.forEach(rewriteManifests::set);\n+      rewriteManifests.commit();\n+\n+      if (!snapshotIdInheritanceEnabled) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531"}, "originalPosition": 235}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg4NzEyODg4", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-388712888", "createdAt": "2020-04-07T00:43:47Z", "commit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwMDo0Mzo0N1rOGBu-2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwMDo0Mzo0N1rOGBu-2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ3MTUxMw==", "bodyText": "If snapshot id inheritance is not enabled, newManifests will be rewritten during the commit. However, the action will still return the original staged manifests.", "url": "https://github.com/apache/iceberg/pull/875#discussion_r404471513", "createdAt": "2020-04-07T00:43:47Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.util.BinPacking;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.FlatMapGroupsFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.TypedColumn;\n+import org.apache.spark.sql.expressions.Aggregator;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+// TODO: concurrent modification of snapshotIdInheritanceEnabled or specs?\n+public class RewriteManifestsAction\n+    implements SnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final Map<Integer, PartitionSpec> specs;\n+  private final Map<String, String> summary;\n+  private final int defaultParallelism;\n+  private final boolean snapshotIdInheritanceEnabled;\n+  private final long targetManifestSizeBytes;\n+\n+  private final Encoder<ManifestFile> manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+  private final Encoder<Entry> entryEncoder = Encoders.javaSerialization(Entry.class);\n+  private final Encoder<Bin> binEncoder = Encoders.bean(Bin.class);\n+\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.specs = table.specs();\n+    this.summary = Maps.newHashMap();\n+    this.defaultParallelism = Integer.parseInt(\n+        spark.conf().get(\"spark.default.parallelism\", \"200\"));\n+    this.snapshotIdInheritanceEnabled = PropertyUtil.propertyAsBoolean(\n+        table.properties(),\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED,\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED_DEFAULT);\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      fileIO = table.io();\n+    }\n+  }\n+\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsAction set(String property, String value) {\n+    summary.put(property, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {\n+    Preconditions.checkArgument(stagingLocation != null, \"Staging location must be set\");\n+\n+    List<ManifestFile> matchingManifests = findMatchingManifests();\n+    if (matchingManifests.isEmpty()) {\n+      return null;\n+    }\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+\n+    int parallelism = Math.min(matchingManifests.size(), defaultParallelism);\n+    JavaRDD<ManifestFile> manifestRDD = sparkContext.parallelize(matchingManifests, parallelism);\n+    Dataset<ManifestFile> manifestDS = spark.createDataset(manifestRDD.rdd(), manifestEncoder);\n+    Dataset<Entry> manifestEntryDS = manifestDS.flatMap(toEntries(io, specs), entryEncoder);\n+\n+    try {\n+      manifestEntryDS.cache();\n+\n+      long manifestEntrySizeBytes = computeManifestEntrySizeBytes(matchingManifests);\n+      Map<Integer, List<PartitionMetadata>> metadataSizeSummary = computeMetadataSizeSummary(\n+          manifestEntryDS,\n+          manifestEntrySizeBytes);\n+\n+      Map<Integer, Map<StructLike, Integer>> bins = computeBins(metadataSizeSummary);\n+\n+      // the size of bins is estimated to be roughly targetManifestSizeBytes\n+      // we allow the actual size of manifests to be 5% higher to avoid closing\n+      // manifests near the end of bins if the estimation is not precise enough\n+      // it is better to have slightly bigger manifests rather than manifests with a couple of entries\n+      long manifestSizeBytes = (long) (1.05 * targetManifestSizeBytes);\n+      List<ManifestFile> newManifests = manifestEntryDS\n+          .groupByKey(toBin(bins), binEncoder)\n+          .flatMapGroups(toManifest(io, manifestSizeBytes, stagingLocation, specs), manifestEncoder)\n+          .collectAsList();\n+\n+      replaceManifests(matchingManifests, newManifests);\n+\n+      return new RewriteManifestsActionResult(matchingManifests, newManifests);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531"}, "originalPosition": 162}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5Mzg3OTEx", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-389387911", "createdAt": "2020-04-07T18:36:57Z", "commit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODozNjo1N1rOGCQ-qg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODozNjo1N1rOGCQ-qg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyODUyMg==", "bodyText": "I think it would simplify the implementation if you added a way to configure the spec (or spec id) you want to rewrite and default to the current spec.\nI think it's a reasonable trade-off to process just one at a time. I can imagine a few cases where a table has multiple specs:\n\nTransitioning from an old spec to a new one: we probably only need to process the old spec's files once since new data will be written with the new one.\nWhen data is written using one spec and then transitioned to another for long-term storage. Like the first case, I think you'd only want to process one spec, the one that data is written with.\n\nFocusing on a single spec allows you to simplify this.", "url": "https://github.com/apache/iceberg/pull/875#discussion_r405028522", "createdAt": "2020-04-07T18:36:57Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.util.BinPacking;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.FlatMapGroupsFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.TypedColumn;\n+import org.apache.spark.sql.expressions.Aggregator;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+// TODO: concurrent modification of snapshotIdInheritanceEnabled or specs?\n+public class RewriteManifestsAction\n+    implements SnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final Map<Integer, PartitionSpec> specs;\n+  private final Map<String, String> summary;\n+  private final int defaultParallelism;\n+  private final boolean snapshotIdInheritanceEnabled;\n+  private final long targetManifestSizeBytes;\n+\n+  private final Encoder<ManifestFile> manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+  private final Encoder<Entry> entryEncoder = Encoders.javaSerialization(Entry.class);\n+  private final Encoder<Bin> binEncoder = Encoders.bean(Bin.class);\n+\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.specs = table.specs();\n+    this.summary = Maps.newHashMap();\n+    this.defaultParallelism = Integer.parseInt(\n+        spark.conf().get(\"spark.default.parallelism\", \"200\"));\n+    this.snapshotIdInheritanceEnabled = PropertyUtil.propertyAsBoolean(\n+        table.properties(),\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED,\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED_DEFAULT);\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      fileIO = table.io();\n+    }\n+  }\n+\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsAction set(String property, String value) {\n+    summary.put(property, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531"}, "originalPosition": 125}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5Mzk2MDYz", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-389396063", "createdAt": "2020-04-07T18:48:04Z", "commit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODo0ODowNVrOGCRY9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxODo0ODowNVrOGCRY9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzNTI1Mg==", "bodyText": "I think I can produce basically the same result as this step with this SQL query:\nwith\n  manifests as (select * from db.table.manifests where partition_spec_id = 0),\n  entries as (select input_file_name() as manifest, * from db.table.entries where status < 2),\n  matching_entry_counts as (\n      select count(1) as entry_count, e.data_file.partition\n      from entries e join manifests m on m.path = e.manifest\n      group by e.data_file.partition)\nselect * from matching_entry_counts\nNote that this assumes we're only processing one spec at a time.", "url": "https://github.com/apache/iceberg/pull/875#discussion_r405035252", "createdAt": "2020-04-07T18:48:05Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.util.BinPacking;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.FlatMapGroupsFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.TypedColumn;\n+import org.apache.spark.sql.expressions.Aggregator;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+// TODO: concurrent modification of snapshotIdInheritanceEnabled or specs?\n+public class RewriteManifestsAction\n+    implements SnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final Map<Integer, PartitionSpec> specs;\n+  private final Map<String, String> summary;\n+  private final int defaultParallelism;\n+  private final boolean snapshotIdInheritanceEnabled;\n+  private final long targetManifestSizeBytes;\n+\n+  private final Encoder<ManifestFile> manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+  private final Encoder<Entry> entryEncoder = Encoders.javaSerialization(Entry.class);\n+  private final Encoder<Bin> binEncoder = Encoders.bean(Bin.class);\n+\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.specs = table.specs();\n+    this.summary = Maps.newHashMap();\n+    this.defaultParallelism = Integer.parseInt(\n+        spark.conf().get(\"spark.default.parallelism\", \"200\"));\n+    this.snapshotIdInheritanceEnabled = PropertyUtil.propertyAsBoolean(\n+        table.properties(),\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED,\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED_DEFAULT);\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      fileIO = table.io();\n+    }\n+  }\n+\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsAction set(String property, String value) {\n+    summary.put(property, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {\n+    Preconditions.checkArgument(stagingLocation != null, \"Staging location must be set\");\n+\n+    List<ManifestFile> matchingManifests = findMatchingManifests();\n+    if (matchingManifests.isEmpty()) {\n+      return null;\n+    }\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+\n+    int parallelism = Math.min(matchingManifests.size(), defaultParallelism);\n+    JavaRDD<ManifestFile> manifestRDD = sparkContext.parallelize(matchingManifests, parallelism);\n+    Dataset<ManifestFile> manifestDS = spark.createDataset(manifestRDD.rdd(), manifestEncoder);\n+    Dataset<Entry> manifestEntryDS = manifestDS.flatMap(toEntries(io, specs), entryEncoder);\n+\n+    try {\n+      manifestEntryDS.cache();\n+\n+      long manifestEntrySizeBytes = computeManifestEntrySizeBytes(matchingManifests);\n+      Map<Integer, List<PartitionMetadata>> metadataSizeSummary = computeMetadataSizeSummary(\n+          manifestEntryDS,\n+          manifestEntrySizeBytes);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531"}, "originalPosition": 146}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDA2MTkw", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-389406190", "createdAt": "2020-04-07T19:01:37Z", "commit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxOTowMTozOFrOGCR5YQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxOTowMTozOFrOGCR5YQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA0MzU1Mw==", "bodyText": "I'm not sure that computing bins is necessary. This handles skew so that the stage runtime is reasonable, but that doesn't matter for the outputs because ManifestsWriter will keep manifests to a reasonable size. So this is a runtime optimization only.\nAlso, because we're using a constant times the number of entries, manifestEntrySizeBytes, this is basically identical to using a count of entries and dividing the target size by the constant: targetEntries = targetManifestSizeBytes / manifestEntrySizeBytes. That means we're aiming for a some number of entries in each bin, which could be thought of as aiming for some number of bins given a total number of entries.\nSince we're trying to divide the input data into some number of bins, cluster by partition, and handle skew, that's something that Spark can do for us automatically. By adding a global sort and a target number of tasks, we can get the same thing without doing our own bin packing calculation.", "url": "https://github.com/apache/iceberg/pull/875#discussion_r405043553", "createdAt": "2020-04-07T19:01:38Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.util.BinPacking;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.FlatMapGroupsFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.TypedColumn;\n+import org.apache.spark.sql.expressions.Aggregator;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+// TODO: concurrent modification of snapshotIdInheritanceEnabled or specs?\n+public class RewriteManifestsAction\n+    implements SnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final Map<Integer, PartitionSpec> specs;\n+  private final Map<String, String> summary;\n+  private final int defaultParallelism;\n+  private final boolean snapshotIdInheritanceEnabled;\n+  private final long targetManifestSizeBytes;\n+\n+  private final Encoder<ManifestFile> manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+  private final Encoder<Entry> entryEncoder = Encoders.javaSerialization(Entry.class);\n+  private final Encoder<Bin> binEncoder = Encoders.bean(Bin.class);\n+\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.specs = table.specs();\n+    this.summary = Maps.newHashMap();\n+    this.defaultParallelism = Integer.parseInt(\n+        spark.conf().get(\"spark.default.parallelism\", \"200\"));\n+    this.snapshotIdInheritanceEnabled = PropertyUtil.propertyAsBoolean(\n+        table.properties(),\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED,\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED_DEFAULT);\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      fileIO = table.io();\n+    }\n+  }\n+\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsAction set(String property, String value) {\n+    summary.put(property, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {\n+    Preconditions.checkArgument(stagingLocation != null, \"Staging location must be set\");\n+\n+    List<ManifestFile> matchingManifests = findMatchingManifests();\n+    if (matchingManifests.isEmpty()) {\n+      return null;\n+    }\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+\n+    int parallelism = Math.min(matchingManifests.size(), defaultParallelism);\n+    JavaRDD<ManifestFile> manifestRDD = sparkContext.parallelize(matchingManifests, parallelism);\n+    Dataset<ManifestFile> manifestDS = spark.createDataset(manifestRDD.rdd(), manifestEncoder);\n+    Dataset<Entry> manifestEntryDS = manifestDS.flatMap(toEntries(io, specs), entryEncoder);\n+\n+    try {\n+      manifestEntryDS.cache();\n+\n+      long manifestEntrySizeBytes = computeManifestEntrySizeBytes(matchingManifests);\n+      Map<Integer, List<PartitionMetadata>> metadataSizeSummary = computeMetadataSizeSummary(\n+          manifestEntryDS,\n+          manifestEntrySizeBytes);\n+\n+      Map<Integer, Map<StructLike, Integer>> bins = computeBins(metadataSizeSummary);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531"}, "originalPosition": 148}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzg5NDc5MTk4", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-389479198", "createdAt": "2020-04-07T20:49:13Z", "commit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo0OToxM1rOGCVjzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QyMDo0OToxM1rOGCVjzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwMzU2NQ==", "bodyText": "We never recommend caching and instead recommend using the shuffle system. Basically all you need to do is add a repartition(parallelism) and then run a map that does nothing. Then future executions will pick up the repartition's shuffle data.\nThat's much better than caching for clusters that use dynamic allocation:\n\nIt doesn't depend on the application being scaled up to keep cache balanced\nIt doesn't keep executors around for longer than needed\nIt has a smaller memory requirement (which we can't control)", "url": "https://github.com/apache/iceberg/pull/875#discussion_r405103565", "createdAt": "2020-04-07T20:49:13Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.util.BinPacking;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.FlatMapGroupsFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.TypedColumn;\n+import org.apache.spark.sql.expressions.Aggregator;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+// TODO: concurrent modification of snapshotIdInheritanceEnabled or specs?\n+public class RewriteManifestsAction\n+    implements SnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final Map<Integer, PartitionSpec> specs;\n+  private final Map<String, String> summary;\n+  private final int defaultParallelism;\n+  private final boolean snapshotIdInheritanceEnabled;\n+  private final long targetManifestSizeBytes;\n+\n+  private final Encoder<ManifestFile> manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+  private final Encoder<Entry> entryEncoder = Encoders.javaSerialization(Entry.class);\n+  private final Encoder<Bin> binEncoder = Encoders.bean(Bin.class);\n+\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.specs = table.specs();\n+    this.summary = Maps.newHashMap();\n+    this.defaultParallelism = Integer.parseInt(\n+        spark.conf().get(\"spark.default.parallelism\", \"200\"));\n+    this.snapshotIdInheritanceEnabled = PropertyUtil.propertyAsBoolean(\n+        table.properties(),\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED,\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED_DEFAULT);\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      fileIO = table.io();\n+    }\n+  }\n+\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsAction set(String property, String value) {\n+    summary.put(property, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {\n+    Preconditions.checkArgument(stagingLocation != null, \"Staging location must be set\");\n+\n+    List<ManifestFile> matchingManifests = findMatchingManifests();\n+    if (matchingManifests.isEmpty()) {\n+      return null;\n+    }\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+\n+    int parallelism = Math.min(matchingManifests.size(), defaultParallelism);\n+    JavaRDD<ManifestFile> manifestRDD = sparkContext.parallelize(matchingManifests, parallelism);\n+    Dataset<ManifestFile> manifestDS = spark.createDataset(manifestRDD.rdd(), manifestEncoder);\n+    Dataset<Entry> manifestEntryDS = manifestDS.flatMap(toEntries(io, specs), entryEncoder);\n+\n+    try {\n+      manifestEntryDS.cache();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531"}, "originalPosition": 141}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkyMzU3NTk5", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-392357599", "createdAt": "2020-04-13T18:57:54Z", "commit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxODo1Nzo1NFrOGExMCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xM1QxODo1Nzo1NFrOGExMCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY1MzM4NA==", "bodyText": "This API is useful and extendable for other usecases. Would it be a good idea to add this to the Iceberg Spec? (In a separate followup issue maybe)", "url": "https://github.com/apache/iceberg/pull/875#discussion_r407653384", "createdAt": "2020-04-13T18:57:54Z", "author": {"login": "prodeezy"}, "path": "spark/src/main/java/org/apache/iceberg/Actions.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import org.apache.spark.sql.SparkSession;\n+\n+public class Actions {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531"}, "originalPosition": 24}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/aa8ec21506acfa0f69e34024b795f953f91a0531", "committedDate": "2020-04-06T19:39:04Z", "message": "Add action result validation"}, "afterCommit": {"oid": "bb797f1421a681311704e634e4552dfa847a59dd", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/bb797f1421a681311704e634e4552dfa847a59dd", "committedDate": "2020-04-14T19:41:52Z", "message": "Switch to metadata tables"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bb797f1421a681311704e634e4552dfa847a59dd", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/bb797f1421a681311704e634e4552dfa847a59dd", "committedDate": "2020-04-14T19:41:52Z", "message": "Switch to metadata tables"}, "afterCommit": {"oid": "05d7d2399557691686b07c561e8d2907d27f4c81", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/05d7d2399557691686b07c561e8d2907d27f4c81", "committedDate": "2020-04-16T17:51:50Z", "message": "Spark: Implement an action to rewrite manifests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk0ODc5MDUy", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-394879052", "createdAt": "2020-04-16T18:07:24Z", "commit": {"oid": "05d7d2399557691686b07c561e8d2907d27f4c81"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxODowNzoyNVrOGGxPpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxODowNzoyNVrOGGxPpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc1MTQ2Mw==", "bodyText": "This part requires further discussion. Right now, we accept a Java Predicate that we run on manifests locally and then we have a broadcast join between the matching manifest names and all manifest entries. The problem is that we read all manifest entries no matter what. However, this does not seem to be a huge problem as the metadata size is small. In addition, the current snapshot might change between the time we iterate through manifests locally and the time when we query the metadata table. It should work correctly, though, as the commit would fail if a manifest we are trying to replace is missing.\nAs an alternative, we could accept a Spark filter instead and run it on top of the metadata table. In that case, we would need to run one more job to collect matching manifest file names and construct GenericManifestFiles from those names so that we can pass them to the commit operation.", "url": "https://github.com/apache/iceberg/pull/875#discussion_r409751463", "createdAt": "2020-04-16T18:07:25Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,342 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteManifests;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.spark.SparkDataFile;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.internal.SQLConf;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteManifestsAction\n+    extends BaseSnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Encoder<ManifestFile> manifestEncoder;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final long targetManifestSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+  private boolean useCaching = true;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+  }\n+\n+  @Override\n+  protected RewriteManifestsAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteManifestsAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction useCaching(boolean newUseCaching) {\n+    this.useCaching = newUseCaching;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {\n+    Preconditions.checkArgument(stagingLocation != null, \"Staging location must be set\");\n+\n+    List<ManifestFile> matchingManifests = findMatchingManifests();\n+    if (matchingManifests.isEmpty()) {\n+      return RewriteManifestsActionResult.empty();\n+    }\n+\n+    long totalSizeBytes = 0L;\n+    int numEntries = 0;\n+\n+    for (ManifestFile manifest : matchingManifests) {\n+      ValidationException.check(hasFileCounts(manifest), \"No file counts in manifest: \" + manifest.path());\n+\n+      totalSizeBytes += manifest.length();\n+      numEntries += manifest.addedFilesCount() + manifest.existingFilesCount() + manifest.deletedFilesCount();\n+    }\n+\n+    int targetNumManifests = targetNumManifests(totalSizeBytes);\n+    int targetNumManifestEntries = targetNumManifestEntries(numEntries, targetNumManifests);\n+\n+    Dataset<Row> manifestEntryDF = buildManifestEntryDF(matchingManifests);\n+\n+    List<ManifestFile> newManifests;\n+    if (spec.fields().size() < 1) {\n+      newManifests = writeManifestsForUnpartitionedTable(manifestEntryDF, targetNumManifests);\n+    } else {\n+      newManifests = writeManifestsForPartitionedTable(manifestEntryDF, targetNumManifests, targetNumManifestEntries);\n+    }\n+\n+    replaceManifests(matchingManifests, newManifests);\n+\n+    return new RewriteManifestsActionResult(matchingManifests, newManifests);\n+  }\n+\n+  private Dataset<Row> buildManifestEntryDF(List<ManifestFile> manifests) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05d7d2399557691686b07c561e8d2907d27f4c81"}, "originalPosition": 171}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk0ODc5ODAz", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-394879803", "createdAt": "2020-04-16T18:08:25Z", "commit": {"oid": "05d7d2399557691686b07c561e8d2907d27f4c81"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxODowODoyNVrOGGxSFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxODowODoyNVrOGGxSFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc1MjA4NA==", "bodyText": "I am still not sure about this. Any ideas are welcome.", "url": "https://github.com/apache/iceberg/pull/875#discussion_r409752084", "createdAt": "2020-04-16T18:08:25Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsActionResult.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import com.google.common.collect.ImmutableList;\n+import java.util.List;\n+import org.apache.iceberg.ManifestFile;\n+\n+public class RewriteManifestsActionResult {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05d7d2399557691686b07c561e8d2907d27f4c81"}, "originalPosition": 26}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk0OTEzOTQy", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-394913942", "createdAt": "2020-04-16T18:56:08Z", "commit": {"oid": "05d7d2399557691686b07c561e8d2907d27f4c81"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxODo1NjowOVrOGGy-dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxODo1NjowOVrOGGy-dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc3OTgzMQ==", "bodyText": "You might consider using a bean that implements DataFile instead. If you did that, then Spark would construct the record for you by calling setX methods instead of you needing to keep track of the offsets. I think it would be more reliable.", "url": "https://github.com/apache/iceberg/pull/875#discussion_r409779831", "createdAt": "2020-04-16T18:56:09Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java", "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Row;\n+\n+public class SparkDataFile implements DataFile {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05d7d2399557691686b07c561e8d2907d27f4c81"}, "originalPosition": 33}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk0OTE1NDQw", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-394915440", "createdAt": "2020-04-16T18:58:15Z", "commit": {"oid": "05d7d2399557691686b07c561e8d2907d27f4c81"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxODo1ODoxNVrOGGzC4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNlQxODo1ODoxNVrOGGzC4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc4MDk2Mw==", "bodyText": "Is this needed?", "url": "https://github.com/apache/iceberg/pull/875#discussion_r409780963", "createdAt": "2020-04-16T18:58:15Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java", "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Row;\n+\n+public class SparkDataFile implements DataFile {\n+\n+  private final Type lowerBoundsType;\n+  private final Type upperBoundsType;\n+  private final int fieldShift;\n+  private final SparkStructLike wrappedPartition;\n+  private Row wrapped;\n+\n+  public SparkDataFile(Types.StructType type) {\n+    this.lowerBoundsType = type.fieldType(\"lower_bounds\");\n+    this.upperBoundsType = type.fieldType(\"upper_bounds\");\n+    this.wrappedPartition = new SparkStructLike(type.fieldType(\"partition\").asStructType());\n+    // the partition field is absent for unpartitioned tables\n+    this.fieldShift = wrappedPartition.size() != 0 ? 1 : 0;\n+  }\n+\n+  public SparkDataFile wrap(Row row) {\n+    this.wrapped = row;\n+    if (wrappedPartition.size() > 0) {\n+      this.wrappedPartition.wrap(row.getAs(2));\n+    }\n+    return this;\n+  }\n+\n+  @Override\n+  public CharSequence path() {\n+    return wrapped.getAs(0);\n+  }\n+\n+  @Override\n+  public FileFormat format() {\n+    String formatAsString = wrapped.<String>getAs(1).toUpperCase(Locale.ROOT);\n+    return FileFormat.valueOf(formatAsString);\n+  }\n+\n+  @Override\n+  public StructLike partition() {\n+    return wrappedPartition;\n+  }\n+\n+  @Override\n+  public long recordCount() {\n+    return wrapped.getAs(fieldShift + 2);\n+  }\n+\n+  @Override\n+  public long fileSizeInBytes() {\n+    return wrapped.getAs(fieldShift + 3);\n+  }\n+\n+  @Override\n+  public Map<Integer, Long> columnSizes() {\n+    return wrapped.getJavaMap(fieldShift + 5);\n+  }\n+\n+  @Override\n+  public Map<Integer, Long> valueCounts() {\n+    return wrapped.getJavaMap(fieldShift + 6);\n+  }\n+\n+  @Override\n+  public Map<Integer, Long> nullValueCounts() {\n+    return wrapped.getJavaMap(fieldShift + 7);\n+  }\n+\n+  @Override\n+  public Map<Integer, ByteBuffer> lowerBounds() {\n+    return convert(lowerBoundsType, wrapped.getJavaMap(fieldShift + 8));\n+  }\n+\n+  @Override\n+  public Map<Integer, ByteBuffer> upperBounds() {\n+    return convert(upperBoundsType, wrapped.getJavaMap(fieldShift + 9));\n+  }\n+\n+  @Override\n+  public ByteBuffer keyMetadata() {\n+    byte[] bytes = (byte[]) wrapped.get(fieldShift + 10);\n+    return bytes != null ? ByteBuffer.wrap(bytes) : null;\n+  }\n+\n+  @Override\n+  public DataFile copy() {\n+    throw new UnsupportedOperationException(\"Not implemented: copy\");\n+  }\n+\n+  @Override\n+  public DataFile copyWithoutStats() {\n+    throw new UnsupportedOperationException(\"Not implemented: copyWithoutStats\");\n+  }\n+\n+  @Override\n+  public List<Long> splitOffsets() {\n+    return wrapped.getList(fieldShift + 11);\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private <T> T convert(Type valueType, Object value) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "05d7d2399557691686b07c561e8d2907d27f4c81"}, "originalPosition": 130}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "96ce82db28057f719a25415a5475fb3374a336aa", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/96ce82db28057f719a25415a5475fb3374a336aa", "committedDate": "2020-04-23T15:46:30Z", "message": "Spark: Implement an action to rewrite manifests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "05d7d2399557691686b07c561e8d2907d27f4c81", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/05d7d2399557691686b07c561e8d2907d27f4c81", "committedDate": "2020-04-16T17:51:50Z", "message": "Spark: Implement an action to rewrite manifests"}, "afterCommit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/aa8ec21506acfa0f69e34024b795f953f91a0531", "committedDate": "2020-04-06T19:39:04Z", "message": "Add action result validation"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/aa8ec21506acfa0f69e34024b795f953f91a0531", "committedDate": "2020-04-06T19:39:04Z", "message": "Add action result validation"}, "afterCommit": {"oid": "96ce82db28057f719a25415a5475fb3374a336aa", "author": {"user": {"login": "aokolnychyi", "name": "Anton Okolnychyi"}}, "url": "https://github.com/apache/iceberg/commit/96ce82db28057f719a25415a5475fb3374a336aa", "committedDate": "2020-04-23T15:46:30Z", "message": "Spark: Implement an action to rewrite manifests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk5MjM4NTc1", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-399238575", "createdAt": "2020-04-23T15:51:29Z", "commit": {"oid": "96ce82db28057f719a25415a5475fb3374a336aa"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxNTo1MToyOVrOGKvXpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxNTo1MToyOVrOGKvXpA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzkxNTA0NA==", "bodyText": "We apply the user predicate after the predicate on the partition spec.", "url": "https://github.com/apache/iceberg/pull/875#discussion_r413915044", "createdAt": "2020-04-23T15:51:29Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteManifests;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.spark.SparkDataFile;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.internal.SQLConf;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action that rewrites manifests in a distributed manner and co-locates metadata for partitions.\n+ * <p>\n+ * By default, this action rewrites all manifests for the current partition spec. The behavior can\n+ * be modified by passing a custom predicate to {@link #rewriteIf(Predicate)} and a custom spec id\n+ * to {@link #specId(int)}. In addition, this action requires a staging location for new manifests\n+ * that should be configured via {@link #stagingLocation}.\n+ */\n+public class RewriteManifestsAction\n+    extends BaseSnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Encoder<ManifestFile> manifestEncoder;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final long targetManifestSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+  private boolean useCaching = true;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+  }\n+\n+  @Override\n+  protected RewriteManifestsAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteManifestsAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Rewrites only manifests that match the given predicate.\n+   *\n+   * @param newPredicate a predicate\n+   * @return this for method chaining\n+   */\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  /**\n+   * Passes a location where the manifests should be written.\n+   *\n+   * @param newStagingLocation a staging location\n+   * @return this for method chaining\n+   */\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  /**\n+   * Configures whether the action should cache manifest entries used in multiple jobs.\n+   *\n+   * @param newUseCaching a flag whether to use caching\n+   * @return this for method chaining\n+   */\n+  public RewriteManifestsAction useCaching(boolean newUseCaching) {\n+    this.useCaching = newUseCaching;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {\n+    Preconditions.checkArgument(stagingLocation != null, \"Staging location must be set\");\n+\n+    List<ManifestFile> matchingManifests = findMatchingManifests();\n+    if (matchingManifests.isEmpty()) {\n+      return RewriteManifestsActionResult.empty();\n+    }\n+\n+    long totalSizeBytes = 0L;\n+    int numEntries = 0;\n+\n+    for (ManifestFile manifest : matchingManifests) {\n+      ValidationException.check(hasFileCounts(manifest), \"No file counts in manifest: \" + manifest.path());\n+\n+      totalSizeBytes += manifest.length();\n+      numEntries += manifest.addedFilesCount() + manifest.existingFilesCount() + manifest.deletedFilesCount();\n+    }\n+\n+    int targetNumManifests = targetNumManifests(totalSizeBytes);\n+    int targetNumManifestEntries = targetNumManifestEntries(numEntries, targetNumManifests);\n+\n+    Dataset<Row> manifestEntryDF = buildManifestEntryDF(matchingManifests);\n+\n+    List<ManifestFile> newManifests;\n+    if (spec.fields().size() < 1) {\n+      newManifests = writeManifestsForUnpartitionedTable(manifestEntryDF, targetNumManifests);\n+    } else {\n+      newManifests = writeManifestsForPartitionedTable(manifestEntryDF, targetNumManifests, targetNumManifestEntries);\n+    }\n+\n+    replaceManifests(matchingManifests, newManifests);\n+\n+    return new RewriteManifestsActionResult(matchingManifests, newManifests);\n+  }\n+\n+  private Dataset<Row> buildManifestEntryDF(List<ManifestFile> manifests) {\n+    Dataset<Row> manifestDF = spark\n+        .createDataset(Lists.transform(manifests, ManifestFile::path), Encoders.STRING())\n+        .toDF(\"manifest\");\n+\n+    String entriesMetadataTable = metadataTableName(MetadataTableType.ENTRIES);\n+    Dataset<Row> manifestEntryDF = spark.read().format(\"iceberg\")\n+        .load(entriesMetadataTable)\n+        .filter(\"status < 2\") // select only live entries\n+        .selectExpr(\"input_file_name() as manifest\", \"snapshot_id\", \"sequence_number\", \"data_file\");\n+\n+    Column joinCond = manifestDF.col(\"manifest\").equalTo(manifestEntryDF.col(\"manifest\"));\n+    return manifestEntryDF\n+        .join(manifestDF, joinCond, \"left_semi\")\n+        .select(\"snapshot_id\", \"sequence_number\", \"data_file\");\n+  }\n+\n+  private List<ManifestFile> writeManifestsForUnpartitionedTable(Dataset<Row> manifestEntryDF, int numManifests) {\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    StructType sparkType = (StructType) manifestEntryDF.schema().apply(\"data_file\").dataType();\n+\n+    // we rely only on the target number of manifests for unpartitioned tables\n+    // as we should not worry about having too much metadata per partition\n+    long maxNumManifestEntries = Long.MAX_VALUE;\n+\n+    return manifestEntryDF\n+        .repartition(numManifests)\n+        .mapPartitions(toManifests(io, maxNumManifestEntries, stagingLocation, spec, sparkType), manifestEncoder)\n+        .collectAsList();\n+  }\n+\n+  private List<ManifestFile> writeManifestsForPartitionedTable(\n+      Dataset<Row> manifestEntryDF, int numManifests,\n+      int targetNumManifestEntries) {\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    StructType sparkType = (StructType) manifestEntryDF.schema().apply(\"data_file\").dataType();\n+\n+    // we allow the actual size of manifests to be 10% higher if the estimation is not precise enough\n+    long maxNumManifestEntries = (long) (1.1 * targetNumManifestEntries);\n+\n+    return withReusableDS(manifestEntryDF, df -> {\n+      Column partitionColumn = df.col(\"data_file.partition\");\n+      return df.repartitionByRange(numManifests, partitionColumn)\n+          .sortWithinPartitions(partitionColumn)\n+          .mapPartitions(toManifests(io, maxNumManifestEntries, stagingLocation, spec, sparkType), manifestEncoder)\n+          .collectAsList();\n+    });\n+  }\n+\n+  private <T, U> U withReusableDS(Dataset<T> ds, Function<Dataset<T>, U> func) {\n+    Dataset<T> reusableDS;\n+    if (useCaching) {\n+      reusableDS = ds.cache();\n+    } else {\n+      int parallelism = SQLConf.get().numShufflePartitions();\n+      reusableDS = ds.repartition(parallelism).map((MapFunction<T, T>) value -> value, ds.exprEnc());\n+    }\n+\n+    try {\n+      return func.apply(reusableDS);\n+    } finally {\n+      if (useCaching) {\n+        reusableDS.unpersist(false);\n+      }\n+    }\n+  }\n+\n+  private List<ManifestFile> findMatchingManifests() {\n+    Snapshot currentSnapshot = table.currentSnapshot();\n+\n+    if (currentSnapshot == null) {\n+      return ImmutableList.of();\n+    }\n+\n+    return currentSnapshot.manifests().stream()\n+        .filter(manifest -> manifest.partitionSpecId() == spec.specId() && predicate.test(manifest))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96ce82db28057f719a25415a5475fb3374a336aa"}, "originalPosition": 275}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk5MjQxMDM5", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-399241039", "createdAt": "2020-04-23T15:54:06Z", "commit": {"oid": "96ce82db28057f719a25415a5475fb3374a336aa"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxNTo1NDowNlrOGKvfwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxNTo1NDowNlrOGKvfwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzkxNzEyMQ==", "bodyText": "This is a debatable decision. It felt safer not to rely on the order of the fields in DataFile in case if something changes.", "url": "https://github.com/apache/iceberg/pull/875#discussion_r413917121", "createdAt": "2020-04-23T15:54:06Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java", "diffHunk": "@@ -33,37 +34,67 @@\n \n public class SparkDataFile implements DataFile {\n \n+  private final int filePathPosition;\n+  private final int fileFormatPosition;\n+  private final int partitionPosition;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96ce82db28057f719a25415a5475fb3374a336aa"}, "originalPosition": 14}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk5MjQyNzg2", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-399242786", "createdAt": "2020-04-23T15:56:00Z", "commit": {"oid": "96ce82db28057f719a25415a5475fb3374a336aa"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxNTo1NjowMFrOGKvlcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxNTo1NjowMFrOGKvlcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzkxODU3Nw==", "bodyText": "Unfortunately, Spark does not handle nulls in getJavaList and getJavaMap correctly. Internally, it calls .asJava on null that causes a NPE. I hit that for cases when offsets are not present.", "url": "https://github.com/apache/iceberg/pull/875#discussion_r413918577", "createdAt": "2020-04-23T15:56:00Z", "author": {"login": "aokolnychyi"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java", "diffHunk": "@@ -74,42 +105,44 @@ public StructLike partition() {\n \n   @Override\n   public long recordCount() {\n-    return wrapped.getAs(fieldPositions[3]);\n+    return wrapped.getAs(recordCountPosition);\n   }\n \n   @Override\n   public long fileSizeInBytes() {\n-    return wrapped.getAs(fieldPositions[4]);\n+    return wrapped.getAs(fileSizeInBytesPosition);\n   }\n \n   @Override\n   public Map<Integer, Long> columnSizes() {\n-    return wrapped.getJavaMap(fieldPositions[6]);\n+    return !wrapped.isNullAt(columnSizesPosition) ? wrapped.getJavaMap(columnSizesPosition) : null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96ce82db28057f719a25415a5475fb3374a336aa"}, "originalPosition": 98}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk5MjY1ODk3", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-399265897", "createdAt": "2020-04-23T16:21:18Z", "commit": {"oid": "96ce82db28057f719a25415a5475fb3374a336aa"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxNjoyMToxOVrOGKwzKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxNjoyMToxOVrOGKwzKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzkzODQ3Mg==", "bodyText": "Nit: I generally prefer reversing the order instead of using !:\n\nIt's usually easier to understand the positive: if isNullAt ? null : ...\nChanging these later with a second condition is easier if the condition is positive", "url": "https://github.com/apache/iceberg/pull/875#discussion_r413938472", "createdAt": "2020-04-23T16:21:19Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java", "diffHunk": "@@ -74,42 +105,44 @@ public StructLike partition() {\n \n   @Override\n   public long recordCount() {\n-    return wrapped.getAs(fieldPositions[3]);\n+    return wrapped.getAs(recordCountPosition);\n   }\n \n   @Override\n   public long fileSizeInBytes() {\n-    return wrapped.getAs(fieldPositions[4]);\n+    return wrapped.getAs(fileSizeInBytesPosition);\n   }\n \n   @Override\n   public Map<Integer, Long> columnSizes() {\n-    return wrapped.getJavaMap(fieldPositions[6]);\n+    return !wrapped.isNullAt(columnSizesPosition) ? wrapped.getJavaMap(columnSizesPosition) : null;\n   }\n \n   @Override\n   public Map<Integer, Long> valueCounts() {\n-    return wrapped.getJavaMap(fieldPositions[7]);\n+    return !wrapped.isNullAt(valueCountsPosition) ? wrapped.getJavaMap(valueCountsPosition) : null;\n   }\n \n   @Override\n   public Map<Integer, Long> nullValueCounts() {\n-    return wrapped.getJavaMap(fieldPositions[8]);\n+    return !wrapped.isNullAt(nullValueCountsPosition) ? wrapped.getJavaMap(nullValueCountsPosition) : null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96ce82db28057f719a25415a5475fb3374a336aa"}, "originalPosition": 110}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk5Mjc1MTE5", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-399275119", "createdAt": "2020-04-23T16:32:00Z", "commit": {"oid": "96ce82db28057f719a25415a5475fb3374a336aa"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxNjozMjowMFrOGKxTuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yM1QxNjozMjowMFrOGKxTuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzk0NjgwOA==", "bodyText": "Could we default to the equivalent of TableOperations.metadataFileLocation(...).parent()?", "url": "https://github.com/apache/iceberg/pull/875#discussion_r413946808", "createdAt": "2020-04-23T16:32:00Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteManifests;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.spark.SparkDataFile;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.internal.SQLConf;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action that rewrites manifests in a distributed manner and co-locates metadata for partitions.\n+ * <p>\n+ * By default, this action rewrites all manifests for the current partition spec. The behavior can\n+ * be modified by passing a custom predicate to {@link #rewriteIf(Predicate)} and a custom spec id\n+ * to {@link #specId(int)}. In addition, this action requires a staging location for new manifests\n+ * that should be configured via {@link #stagingLocation}.\n+ */\n+public class RewriteManifestsAction\n+    extends BaseSnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Encoder<ManifestFile> manifestEncoder;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final long targetManifestSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+  private boolean useCaching = true;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+  }\n+\n+  @Override\n+  protected RewriteManifestsAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteManifestsAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Rewrites only manifests that match the given predicate.\n+   *\n+   * @param newPredicate a predicate\n+   * @return this for method chaining\n+   */\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  /**\n+   * Passes a location where the manifests should be written.\n+   *\n+   * @param newStagingLocation a staging location\n+   * @return this for method chaining\n+   */\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  /**\n+   * Configures whether the action should cache manifest entries used in multiple jobs.\n+   *\n+   * @param newUseCaching a flag whether to use caching\n+   * @return this for method chaining\n+   */\n+  public RewriteManifestsAction useCaching(boolean newUseCaching) {\n+    this.useCaching = newUseCaching;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {\n+    Preconditions.checkArgument(stagingLocation != null, \"Staging location must be set\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "96ce82db28057f719a25415a5475fb3374a336aa"}, "originalPosition": 165}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzk5MjgwOTg5", "url": "https://github.com/apache/iceberg/pull/875#pullrequestreview-399280989", "createdAt": "2020-04-23T16:38:56Z", "commit": {"oid": "96ce82db28057f719a25415a5475fb3374a336aa"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4742, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}