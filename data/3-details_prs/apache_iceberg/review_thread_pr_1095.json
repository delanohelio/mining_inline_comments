{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI4MjAwNDQ4", "number": 1095, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoyNToxNlrOEEaZ2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNzo0ODoxOFrOEGc90Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDYyMzYwOnYy", "diffSide": "RIGHT", "path": "site/docs/api-quickstart.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoyNToxNlrOGiBQRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwMToxMDoxMFrOGiLOkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMyNTMxOA==", "bodyText": "This needs to note the disadvantages of using HadoopCatalog so that users are informed about the problems. Specifically, this can only be used with HDFS. Can you add that to the first sentence, like this?\n\nA Hadoop catalog doesn't need to connect to a Hive MetaStore, but can only be used with HDFS or similar file systems that support atomic rename.", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r438325318", "createdAt": "2020-06-10T18:25:16Z", "author": {"login": "rdblue"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+The Hadoop catalog doesn't need to connects to a Hive MetaStore. To get a Hadoop catalog see:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ4ODcyMw==", "bodyText": "I think so.", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r438488723", "createdAt": "2020-06-11T01:10:10Z", "author": {"login": "hzfanxinxin"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+The Hadoop catalog doesn't need to connects to a Hive MetaStore. To get a Hadoop catalog see:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMyNTMxOA=="}, "originalCommit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDYyNzk4OnYy", "diffSide": "RIGHT", "path": "site/docs/api-quickstart.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoyNjozMFrOGiBTAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQyMjo0NjoyM1rOGkFQWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMyNjAxNw==", "bodyText": "This doesn't use the right URI format, schema://authority/path. This URI would use warehouse_path for the HDFS host in the authority section.", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r438326017", "createdAt": "2020-06-10T18:26:30Z", "author": {"login": "rdblue"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+The Hadoop catalog doesn't need to connects to a Hive MetaStore. To get a Hadoop catalog see:\n+\n+```scala\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+\n+val conf = new Configuration();\n+val warehousePath = \"hdfs://warehouse_path\";", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ5MDk3OA==", "bodyText": "hdfs://warehouse/path, is it ok?", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r438490978", "createdAt": "2020-06-11T01:18:58Z", "author": {"login": "hzfanxinxin"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+The Hadoop catalog doesn't need to connects to a Hive MetaStore. To get a Hadoop catalog see:\n+\n+```scala\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+\n+val conf = new Configuration();\n+val warehousePath = \"hdfs://warehouse_path\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMyNjAxNw=="}, "originalCommit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ4ODAyNA==", "bodyText": "No, then warehouse is the authority section. You should change it to something like hdfs://host:8020/warehouse/path/", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r440488024", "createdAt": "2020-06-15T22:46:23Z", "author": {"login": "rdblue"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+The Hadoop catalog doesn't need to connects to a Hive MetaStore. To get a Hadoop catalog see:\n+\n+```scala\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+\n+val conf = new Configuration();\n+val warehousePath = \"hdfs://warehouse_path\";", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMyNjAxNw=="}, "originalCommit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDYyODk2OnYy", "diffSide": "RIGHT", "path": "site/docs/api-quickstart.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoyNjo0OVrOGiBTuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQyMjo0NjozNFrOGkFQjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMyNjIwMA==", "bodyText": "I don't think rename is implemented, is it?", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r438326200", "createdAt": "2020-06-10T18:26:49Z", "author": {"login": "rdblue"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+The Hadoop catalog doesn't need to connects to a Hive MetaStore. To get a Hadoop catalog see:\n+\n+```scala\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+\n+val conf = new Configuration();\n+val warehousePath = \"hdfs://warehouse_path\";\n+val catalog = new HadoopCatalog(conf, warehousePath);\n+```\n+\n+Like Hive catalog, Hadoop catalog implements the interface `Catalog`. So it also contains methods for working with tables, like createTable, loadTable, renameTable, and dropTable.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ4ODc4Mg==", "bodyText": "Yes, you are right. So should i remove renameTable?", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r438488782", "createdAt": "2020-06-11T01:10:20Z", "author": {"login": "hzfanxinxin"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+The Hadoop catalog doesn't need to connects to a Hive MetaStore. To get a Hadoop catalog see:\n+\n+```scala\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+\n+val conf = new Configuration();\n+val warehousePath = \"hdfs://warehouse_path\";\n+val catalog = new HadoopCatalog(conf, warehousePath);\n+```\n+\n+Like Hive catalog, Hadoop catalog implements the interface `Catalog`. So it also contains methods for working with tables, like createTable, loadTable, renameTable, and dropTable.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMyNjIwMA=="}, "originalCommit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ4ODA3OA==", "bodyText": "Yes, please.", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r440488078", "createdAt": "2020-06-15T22:46:34Z", "author": {"login": "rdblue"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+The Hadoop catalog doesn't need to connects to a Hive MetaStore. To get a Hadoop catalog see:\n+\n+```scala\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+\n+val conf = new Configuration();\n+val warehousePath = \"hdfs://warehouse_path\";\n+val catalog = new HadoopCatalog(conf, warehousePath);\n+```\n+\n+Like Hive catalog, Hadoop catalog implements the interface `Catalog`. So it also contains methods for working with tables, like createTable, loadTable, renameTable, and dropTable.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMyNjIwMA=="}, "originalCommit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMDYzMTU0OnYy", "diffSide": "RIGHT", "path": "site/docs/api-quickstart.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMFQxODoyNzozN1rOGiBVgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQyMjo0ODoxMVrOGkFShA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMyNjY1Ng==", "bodyText": "This is strange because it actually loads the table using HadoopTables and not the HadoopCatalog. The URI passed here must match the one created by the catalog.", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r438326656", "createdAt": "2020-06-10T18:27:37Z", "author": {"login": "rdblue"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+The Hadoop catalog doesn't need to connects to a Hive MetaStore. To get a Hadoop catalog see:\n+\n+```scala\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+\n+val conf = new Configuration();\n+val warehousePath = \"hdfs://warehouse_path\";\n+val catalog = new HadoopCatalog(conf, warehousePath);\n+```\n+\n+Like Hive catalog, Hadoop catalog implements the interface `Catalog`. So it also contains methods for working with tables, like createTable, loadTable, renameTable, and dropTable.\n+                                                                                       \n+This example create a table with Hadoop catalog:\n+\n+```scala\n+val name = TableIdentifier.of(\"logging\", \"logs\")\n+val table = catalog.createTable(name, schema, spec)\n+\n+// write into the new logs table with Spark 2.4\n+logsDF.write\n+    .format(\"iceberg\")\n+    .mode(\"append\")\n+    .save(\"hdfs://warehouse_path/logging/logs\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ5Mzg0NQ==", "bodyText": "Yes, this is also a problem that bothers me. I look the method findTable in class IcebergSource.java, it just support HadoopTables and HiveCatalog. So i think if we can add a option like .option(\"catalog\",\"HadoopCatalog\") to the DataSourceOptions, and the default value of catalog option is HiveCatalog.  what's your opinion on this?", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r438493845", "createdAt": "2020-06-11T01:30:25Z", "author": {"login": "hzfanxinxin"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+The Hadoop catalog doesn't need to connects to a Hive MetaStore. To get a Hadoop catalog see:\n+\n+```scala\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+\n+val conf = new Configuration();\n+val warehousePath = \"hdfs://warehouse_path\";\n+val catalog = new HadoopCatalog(conf, warehousePath);\n+```\n+\n+Like Hive catalog, Hadoop catalog implements the interface `Catalog`. So it also contains methods for working with tables, like createTable, loadTable, renameTable, and dropTable.\n+                                                                                       \n+This example create a table with Hadoop catalog:\n+\n+```scala\n+val name = TableIdentifier.of(\"logging\", \"logs\")\n+val table = catalog.createTable(name, schema, spec)\n+\n+// write into the new logs table with Spark 2.4\n+logsDF.write\n+    .format(\"iceberg\")\n+    .mode(\"append\")\n+    .save(\"hdfs://warehouse_path/logging/logs\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMyNjY1Ng=="}, "originalCommit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MDQ4ODU4MA==", "bodyText": "This seems like something we might want to solve in configuration instead of hard-coding support for HiveCatalog. This problem might go away when we get a better API in Spark 3.", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r440488580", "createdAt": "2020-06-15T22:48:11Z", "author": {"login": "rdblue"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+The Hadoop catalog doesn't need to connects to a Hive MetaStore. To get a Hadoop catalog see:\n+\n+```scala\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+\n+val conf = new Configuration();\n+val warehousePath = \"hdfs://warehouse_path\";\n+val catalog = new HadoopCatalog(conf, warehousePath);\n+```\n+\n+Like Hive catalog, Hadoop catalog implements the interface `Catalog`. So it also contains methods for working with tables, like createTable, loadTable, renameTable, and dropTable.\n+                                                                                       \n+This example create a table with Hadoop catalog:\n+\n+```scala\n+val name = TableIdentifier.of(\"logging\", \"logs\")\n+val table = catalog.createTable(name, schema, spec)\n+\n+// write into the new logs table with Spark 2.4\n+logsDF.write\n+    .format(\"iceberg\")\n+    .mode(\"append\")\n+    .save(\"hdfs://warehouse_path/logging/logs\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODMyNjY1Ng=="}, "originalCommit": {"oid": "14730fdb80eccde245b80b50b6175609921ad492"}, "originalPosition": 29}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1MjAxNDg5OnYy", "diffSide": "RIGHT", "path": "site/docs/api-quickstart.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNzo0ODoxOFrOGlQi-Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xN1QxNzo0ODoxOFrOGlQi-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTcyMTU5Mw==", "bodyText": "This location also needs to be fixed to avoid using warehouse_path as the authority section of the URI.", "url": "https://github.com/apache/iceberg/pull/1095#discussion_r441721593", "createdAt": "2020-06-17T17:48:18Z", "author": {"login": "rdblue"}, "path": "site/docs/api-quickstart.md", "diffHunk": "@@ -48,6 +48,36 @@ logsDF.write\n \n The logs [schema](#create-a-schema) and [partition spec](#create-a-partition-spec) are created below.\n \n+### Using a Hadoop catalog\n+\n+A Hadoop catalog doesn't need to connect to a Hive MetaStore, but can only be used with HDFS or similar file systems that support atomic rename. To get a Hadoop catalog see:\n+\n+```scala\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+\n+val conf = new Configuration();\n+val warehousePath = \"hdfs://host:8020/warehouse_path\";\n+val catalog = new HadoopCatalog(conf, warehousePath);\n+```\n+\n+Like Hive catalog, Hadoop catalog implements the interface `Catalog`. So it also contains methods for working with tables, like createTable, loadTable, and dropTable.\n+                                                                                       \n+This example create a table with Hadoop catalog:\n+\n+```scala\n+val name = TableIdentifier.of(\"logging\", \"logs\")\n+val table = catalog.createTable(name, schema, spec)\n+\n+// write into the new logs table with Spark 2.4\n+logsDF.write\n+    .format(\"iceberg\")\n+    .mode(\"append\")\n+    .save(\"hdfs://warehouse_path/logging/logs\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4f8130778fd4e0eec7e35d45d4549e17a2040417"}, "originalPosition": 29}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3874, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}