{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTE3OTI0MzM2", "number": 1743, "reviewThreads": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxNzozNDoxMVrOE24ESQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxNjoyODozMVrOE3x0kg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1OTc3MTYxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxNzozNDoxMlrOHv55uA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDowODo1M1rOHw0gBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTk5Mzc4NA==", "bodyText": "This is ugly. I'd appreciate any ideas.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r519993784", "createdAt": "2020-11-09T17:34:12Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+\n+      validateParams(procedure)\n+      validateMethodHandle(procedure)\n+\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def validateMethodHandle(procedure: Procedure): Unit = {\n+    val params = procedure.parameters\n+    val outputType = procedure.outputType\n+\n+    val methodHandle = procedure.methodHandle\n+    val methodType = methodHandle.`type`\n+    val methodReturnType = methodType.returnType\n+\n+    // method cannot accept var ags\n+    if (methodHandle.isVarargsCollector) {\n+      throw new AnalysisException(\"Method must have fixed arity\")\n+    }\n+\n+    // verify the number of params in the procedure match the number of params in the method\n+    if (params.length != methodType.parameterCount) {\n+      throw new AnalysisException(\"Method parameter count must match the number of procedure parameters\")\n+    }\n+\n+    // the MethodHandle API does not allow us to check the generic type\n+    // so we only verify the return type is either void or iterable\n+\n+    if (outputType.nonEmpty && methodReturnType != classOf[java.lang.Iterable[_]]) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTk5NDYyMw==", "bodyText": "In the current approach, methodHandle is expected to be either void or return java.lang.Iterable of Spark Rows.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r519994623", "createdAt": "2020-11-09T17:35:29Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+\n+      validateParams(procedure)\n+      validateMethodHandle(procedure)\n+\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def validateMethodHandle(procedure: Procedure): Unit = {\n+    val params = procedure.parameters\n+    val outputType = procedure.outputType\n+\n+    val methodHandle = procedure.methodHandle\n+    val methodType = methodHandle.`type`\n+    val methodReturnType = methodType.returnType\n+\n+    // method cannot accept var ags\n+    if (methodHandle.isVarargsCollector) {\n+      throw new AnalysisException(\"Method must have fixed arity\")\n+    }\n+\n+    // verify the number of params in the procedure match the number of params in the method\n+    if (params.length != methodType.parameterCount) {\n+      throw new AnalysisException(\"Method parameter count must match the number of procedure parameters\")\n+    }\n+\n+    // the MethodHandle API does not allow us to check the generic type\n+    // so we only verify the return type is either void or iterable\n+\n+    if (outputType.nonEmpty && methodReturnType != classOf[java.lang.Iterable[_]]) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTk5Mzc4NA=="}, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1Mzg2Mg==", "bodyText": "Got rid of this.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520953862", "createdAt": "2020-11-11T00:08:53Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+\n+      validateParams(procedure)\n+      validateMethodHandle(procedure)\n+\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def validateMethodHandle(procedure: Procedure): Unit = {\n+    val params = procedure.parameters\n+    val outputType = procedure.outputType\n+\n+    val methodHandle = procedure.methodHandle\n+    val methodType = methodHandle.`type`\n+    val methodReturnType = methodType.returnType\n+\n+    // method cannot accept var ags\n+    if (methodHandle.isVarargsCollector) {\n+      throw new AnalysisException(\"Method must have fixed arity\")\n+    }\n+\n+    // verify the number of params in the procedure match the number of params in the method\n+    if (params.length != methodType.parameterCount) {\n+      throw new AnalysisException(\"Method parameter count must match the number of procedure parameters\")\n+    }\n+\n+    // the MethodHandle API does not allow us to check the generic type\n+    // so we only verify the return type is either void or iterable\n+\n+    if (outputType.nonEmpty && methodReturnType != classOf[java.lang.Iterable[_]]) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTk5Mzc4NA=="}, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI1OTgwMDkyOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/Procedure.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxNzo0MToyN1rOHv6Lqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxNzo0MToyN1rOHv6Lqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTk5ODM3OA==", "bodyText": "Presto uses method handles for stored procedures too.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r519998378", "createdAt": "2020-11-09T17:41:27Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/Procedure.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.catalog;\n+\n+import java.lang.invoke.MethodHandle;\n+import org.apache.spark.sql.types.StructType;\n+\n+public interface Procedure {\n+  ProcedureParameter[] parameters();\n+  StructType outputType();\n+  MethodHandle methodHandle();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDIyNzQzOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/iceberg/spark/extensions/IcebergSparkSessionExtensions.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToyNjo1M1rOHv-RXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDowODozN1rOHw0frg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NTM3Mg==", "bodyText": "The ignored argument is a SparkSession, which I think should be passed into ResolveProcedures so that it doesn't need to reference SparkSession.active.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520065372", "createdAt": "2020-11-09T19:26:53Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/iceberg/spark/extensions/IcebergSparkSessionExtensions.scala", "diffHunk": "@@ -20,13 +20,15 @@\n package org.apache.iceberg.spark.extensions\n \n import org.apache.spark.sql.SparkSessionExtensions\n+import org.apache.spark.sql.catalyst.analysis.ResolveProcedures\n import org.apache.spark.sql.catalyst.parser.extensions.IcebergSparkSqlExtensionsParser\n import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Strategy\n \n class IcebergSparkSessionExtensions extends (SparkSessionExtensions => Unit) {\n \n   override def apply(extensions: SparkSessionExtensions): Unit = {\n     extensions.injectParser { case (_, parser) => new IcebergSparkSqlExtensionsParser(parser) }\n+    extensions.injectResolutionRule { _ => ResolveProcedures }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1Mzc3NA==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520953774", "createdAt": "2020-11-11T00:08:37Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/iceberg/spark/extensions/IcebergSparkSessionExtensions.scala", "diffHunk": "@@ -20,13 +20,15 @@\n package org.apache.iceberg.spark.extensions\n \n import org.apache.spark.sql.SparkSessionExtensions\n+import org.apache.spark.sql.catalyst.analysis.ResolveProcedures\n import org.apache.spark.sql.catalyst.parser.extensions.IcebergSparkSqlExtensionsParser\n import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Strategy\n \n class IcebergSparkSessionExtensions extends (SparkSessionExtensions => Unit) {\n \n   override def apply(extensions: SparkSessionExtensions): Unit = {\n     extensions.injectParser { case (_, parser) => new IcebergSparkSqlExtensionsParser(parser) }\n+    extensions.injectResolutionRule { _ => ResolveProcedures }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NTM3Mg=="}, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDIyODc4OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToyNzoxM1rOHv-SIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDowOTowM1rOHw0gMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NTU3MQ==", "bodyText": "The session should be passed in from rule injection.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520065571", "createdAt": "2020-11-09T19:27:13Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1MzkwNg==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520953906", "createdAt": "2020-11-11T00:09:03Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NTU3MQ=="}, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDIzNDA0OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedDataSourceV2Strategy.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQxOToyODo0NVrOHv-Vdg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDowOTozOVrOHw0hKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NjQyMg==", "bodyText": "Should this be in ExtendedDataSourceV2Strategy? It doesn't seem related to DSv2.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520066422", "createdAt": "2020-11-09T19:28:45Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedDataSourceV2Strategy.scala", "diffHunk": "@@ -20,14 +20,21 @@\n package org.apache.spark.sql.execution.datasources.v2\n \n import org.apache.spark.sql.{AnalysisException, Strategy}\n-import org.apache.spark.sql.catalyst.plans.logical.{CallStatement, LogicalPlan}\n+import org.apache.spark.sql.catalyst.CatalystTypeConverters\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, LogicalPlan}\n import org.apache.spark.sql.execution.SparkPlan\n \n object ExtendedDataSourceV2Strategy extends Strategy {\n \n   override def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {\n-    case _: CallStatement =>\n-      throw new AnalysisException(\"CALL statements are not currently supported\")\n+    case c @ Call(procedure, args) =>\n+      CallExec(c.output, procedure.methodHandle, args.map(toScalaValue)) :: Nil", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1NDE1NQ==", "bodyText": "@rdblue, I think this is debatable. It does depend on the catalog API part of DS V2 so I've put it here. Any suggestions?", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520954155", "createdAt": "2020-11-11T00:09:39Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedDataSourceV2Strategy.scala", "diffHunk": "@@ -20,14 +20,21 @@\n package org.apache.spark.sql.execution.datasources.v2\n \n import org.apache.spark.sql.{AnalysisException, Strategy}\n-import org.apache.spark.sql.catalyst.plans.logical.{CallStatement, LogicalPlan}\n+import org.apache.spark.sql.catalyst.CatalystTypeConverters\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, LogicalPlan}\n import org.apache.spark.sql.execution.SparkPlan\n \n object ExtendedDataSourceV2Strategy extends Strategy {\n \n   override def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {\n-    case _: CallStatement =>\n-      throw new AnalysisException(\"CALL statements are not currently supported\")\n+    case c @ Call(procedure, args) =>\n+      CallExec(c.output, procedure.methodHandle, args.map(toScalaValue)) :: Nil", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NjQyMg=="}, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDY1NTc1OnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/Procedure.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMToyOToxMVrOHwCVHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDowODowM1rOHw0e-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDEzMTg2OA==", "bodyText": "Does this need to be a struct?", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520131868", "createdAt": "2020-11-09T21:29:11Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/Procedure.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.catalog;\n+\n+import java.lang.invoke.MethodHandle;\n+import org.apache.spark.sql.types.StructType;\n+\n+public interface Procedure {\n+  ProcedureParameter[] parameters();\n+  StructType outputType();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1MzU5NQ==", "bodyText": "Procedures are supposed to return Spark rows that will be reported as output. So it is the type of those rows.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520953595", "createdAt": "2020-11-11T00:08:03Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/Procedure.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.catalog;\n+\n+import java.lang.invoke.MethodHandle;\n+import org.apache.spark.sql.types.StructType;\n+\n+public interface Procedure {\n+  ProcedureParameter[] parameters();\n+  StructType outputType();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDEzMTg2OA=="}, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDg1NjM2OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjozMTo0OFrOHwEPBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDowNDo1NFrOHw0bBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MzA3Ng==", "bodyText": "This is going to force people to use value literals, like 12L. Can we insert a cast and execute it if the type is compatible and the cast is safe? I'm thinking it would be fine for int -> long, but not for string -> int.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520163076", "createdAt": "2020-11-09T22:31:48Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+\n+      validateParams(procedure)\n+      validateMethodHandle(procedure)\n+\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def validateMethodHandle(procedure: Procedure): Unit = {\n+    val params = procedure.parameters\n+    val outputType = procedure.outputType\n+\n+    val methodHandle = procedure.methodHandle\n+    val methodType = methodHandle.`type`\n+    val methodReturnType = methodType.returnType\n+\n+    // method cannot accept var ags\n+    if (methodHandle.isVarargsCollector) {\n+      throw new AnalysisException(\"Method must have fixed arity\")\n+    }\n+\n+    // verify the number of params in the procedure match the number of params in the method\n+    if (params.length != methodType.parameterCount) {\n+      throw new AnalysisException(\"Method parameter count must match the number of procedure parameters\")\n+    }\n+\n+    // the MethodHandle API does not allow us to check the generic type\n+    // so we only verify the return type is either void or iterable\n+\n+    if (outputType.nonEmpty && methodReturnType != classOf[java.lang.Iterable[_]]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines $outputType \" +\n+        \"as its output so must return java.lang.Iterable of Spark Rows\")\n+    }\n+\n+    if (outputType.isEmpty && methodReturnType != classOf[Void]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines no output columns \" +\n+        \"so must be void\")\n+    }\n+  }\n+\n+  private def buildArgExprs(procedure: Procedure, args: Seq[CallArgument]): Seq[Expression] = {\n+    val params = procedure.parameters\n+\n+    // build a map of declared parameter names to their positions\n+    val nameToPositionMap = params.map(_.name).zipWithIndex.toMap\n+\n+    // build a map of parameter names to args\n+    val nameToArgMap = buildNameToArgMap(params, args, nameToPositionMap)\n+\n+    // verify all required parameters are provided\n+    val missingParamNames = params.filter(_.required).collect {\n+      case param if !nameToArgMap.contains(param.name) => param.name\n+    }\n+\n+    if (missingParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Missing required parameters: ${missingParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    val argExprs = new Array[Expression](params.size)\n+\n+    nameToArgMap.foreach { case (name, arg) =>\n+      val position = nameToPositionMap(name)\n+      val param = params(position)\n+      val paramType = param.dataType\n+      val argType = arg.expr.dataType\n+      if (paramType != argType) {\n+        throw new AnalysisException(s\"Wrong arg type for ${param.name}: expected $paramType but got $argType\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkxODM0Mw==", "bodyText": "I think we can try to use Cast$canUpCast for this.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520918343", "createdAt": "2020-11-10T22:36:38Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+\n+      validateParams(procedure)\n+      validateMethodHandle(procedure)\n+\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def validateMethodHandle(procedure: Procedure): Unit = {\n+    val params = procedure.parameters\n+    val outputType = procedure.outputType\n+\n+    val methodHandle = procedure.methodHandle\n+    val methodType = methodHandle.`type`\n+    val methodReturnType = methodType.returnType\n+\n+    // method cannot accept var ags\n+    if (methodHandle.isVarargsCollector) {\n+      throw new AnalysisException(\"Method must have fixed arity\")\n+    }\n+\n+    // verify the number of params in the procedure match the number of params in the method\n+    if (params.length != methodType.parameterCount) {\n+      throw new AnalysisException(\"Method parameter count must match the number of procedure parameters\")\n+    }\n+\n+    // the MethodHandle API does not allow us to check the generic type\n+    // so we only verify the return type is either void or iterable\n+\n+    if (outputType.nonEmpty && methodReturnType != classOf[java.lang.Iterable[_]]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines $outputType \" +\n+        \"as its output so must return java.lang.Iterable of Spark Rows\")\n+    }\n+\n+    if (outputType.isEmpty && methodReturnType != classOf[Void]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines no output columns \" +\n+        \"so must be void\")\n+    }\n+  }\n+\n+  private def buildArgExprs(procedure: Procedure, args: Seq[CallArgument]): Seq[Expression] = {\n+    val params = procedure.parameters\n+\n+    // build a map of declared parameter names to their positions\n+    val nameToPositionMap = params.map(_.name).zipWithIndex.toMap\n+\n+    // build a map of parameter names to args\n+    val nameToArgMap = buildNameToArgMap(params, args, nameToPositionMap)\n+\n+    // verify all required parameters are provided\n+    val missingParamNames = params.filter(_.required).collect {\n+      case param if !nameToArgMap.contains(param.name) => param.name\n+    }\n+\n+    if (missingParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Missing required parameters: ${missingParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    val argExprs = new Array[Expression](params.size)\n+\n+    nameToArgMap.foreach { case (name, arg) =>\n+      val position = nameToPositionMap(name)\n+      val param = params(position)\n+      val paramType = param.dataType\n+      val argType = arg.expr.dataType\n+      if (paramType != argType) {\n+        throw new AnalysisException(s\"Wrong arg type for ${param.name}: expected $paramType but got $argType\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MzA3Ng=="}, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1MjI5OA==", "bodyText": "I added a cast if it is safe.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520952298", "createdAt": "2020-11-11T00:04:09Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+\n+      validateParams(procedure)\n+      validateMethodHandle(procedure)\n+\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def validateMethodHandle(procedure: Procedure): Unit = {\n+    val params = procedure.parameters\n+    val outputType = procedure.outputType\n+\n+    val methodHandle = procedure.methodHandle\n+    val methodType = methodHandle.`type`\n+    val methodReturnType = methodType.returnType\n+\n+    // method cannot accept var ags\n+    if (methodHandle.isVarargsCollector) {\n+      throw new AnalysisException(\"Method must have fixed arity\")\n+    }\n+\n+    // verify the number of params in the procedure match the number of params in the method\n+    if (params.length != methodType.parameterCount) {\n+      throw new AnalysisException(\"Method parameter count must match the number of procedure parameters\")\n+    }\n+\n+    // the MethodHandle API does not allow us to check the generic type\n+    // so we only verify the return type is either void or iterable\n+\n+    if (outputType.nonEmpty && methodReturnType != classOf[java.lang.Iterable[_]]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines $outputType \" +\n+        \"as its output so must return java.lang.Iterable of Spark Rows\")\n+    }\n+\n+    if (outputType.isEmpty && methodReturnType != classOf[Void]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines no output columns \" +\n+        \"so must be void\")\n+    }\n+  }\n+\n+  private def buildArgExprs(procedure: Procedure, args: Seq[CallArgument]): Seq[Expression] = {\n+    val params = procedure.parameters\n+\n+    // build a map of declared parameter names to their positions\n+    val nameToPositionMap = params.map(_.name).zipWithIndex.toMap\n+\n+    // build a map of parameter names to args\n+    val nameToArgMap = buildNameToArgMap(params, args, nameToPositionMap)\n+\n+    // verify all required parameters are provided\n+    val missingParamNames = params.filter(_.required).collect {\n+      case param if !nameToArgMap.contains(param.name) => param.name\n+    }\n+\n+    if (missingParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Missing required parameters: ${missingParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    val argExprs = new Array[Expression](params.size)\n+\n+    nameToArgMap.foreach { case (name, arg) =>\n+      val position = nameToPositionMap(name)\n+      val param = params(position)\n+      val paramType = param.dataType\n+      val argType = arg.expr.dataType\n+      if (paramType != argType) {\n+        throw new AnalysisException(s\"Wrong arg type for ${param.name}: expected $paramType but got $argType\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MzA3Ng=="}, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1MjU4MQ==", "bodyText": "All expressions will undergo optimizations so we will get rid of casts before creating an exec node.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520952581", "createdAt": "2020-11-11T00:04:54Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+\n+      validateParams(procedure)\n+      validateMethodHandle(procedure)\n+\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def validateMethodHandle(procedure: Procedure): Unit = {\n+    val params = procedure.parameters\n+    val outputType = procedure.outputType\n+\n+    val methodHandle = procedure.methodHandle\n+    val methodType = methodHandle.`type`\n+    val methodReturnType = methodType.returnType\n+\n+    // method cannot accept var ags\n+    if (methodHandle.isVarargsCollector) {\n+      throw new AnalysisException(\"Method must have fixed arity\")\n+    }\n+\n+    // verify the number of params in the procedure match the number of params in the method\n+    if (params.length != methodType.parameterCount) {\n+      throw new AnalysisException(\"Method parameter count must match the number of procedure parameters\")\n+    }\n+\n+    // the MethodHandle API does not allow us to check the generic type\n+    // so we only verify the return type is either void or iterable\n+\n+    if (outputType.nonEmpty && methodReturnType != classOf[java.lang.Iterable[_]]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines $outputType \" +\n+        \"as its output so must return java.lang.Iterable of Spark Rows\")\n+    }\n+\n+    if (outputType.isEmpty && methodReturnType != classOf[Void]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines no output columns \" +\n+        \"so must be void\")\n+    }\n+  }\n+\n+  private def buildArgExprs(procedure: Procedure, args: Seq[CallArgument]): Seq[Expression] = {\n+    val params = procedure.parameters\n+\n+    // build a map of declared parameter names to their positions\n+    val nameToPositionMap = params.map(_.name).zipWithIndex.toMap\n+\n+    // build a map of parameter names to args\n+    val nameToArgMap = buildNameToArgMap(params, args, nameToPositionMap)\n+\n+    // verify all required parameters are provided\n+    val missingParamNames = params.filter(_.required).collect {\n+      case param if !nameToArgMap.contains(param.name) => param.name\n+    }\n+\n+    if (missingParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Missing required parameters: ${missingParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    val argExprs = new Array[Expression](params.size)\n+\n+    nameToArgMap.foreach { case (name, arg) =>\n+      val position = nameToPositionMap(name)\n+      val param = params(position)\n+      val paramType = param.dataType\n+      val argType = arg.expr.dataType\n+      if (paramType != argType) {\n+        throw new AnalysisException(s\"Wrong arg type for ${param.name}: expected $paramType but got $argType\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MzA3Ng=="}, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2MDg2MTEzOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjozMzozM1rOHwER6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMjo0Mjo1OVrOHwygZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MzgxOQ==", "bodyText": "This is Presto's current behavior?", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520163819", "createdAt": "2020-11-09T22:33:33Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+\n+      validateParams(procedure)\n+      validateMethodHandle(procedure)\n+\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def validateMethodHandle(procedure: Procedure): Unit = {\n+    val params = procedure.parameters\n+    val outputType = procedure.outputType\n+\n+    val methodHandle = procedure.methodHandle\n+    val methodType = methodHandle.`type`\n+    val methodReturnType = methodType.returnType\n+\n+    // method cannot accept var ags\n+    if (methodHandle.isVarargsCollector) {\n+      throw new AnalysisException(\"Method must have fixed arity\")\n+    }\n+\n+    // verify the number of params in the procedure match the number of params in the method\n+    if (params.length != methodType.parameterCount) {\n+      throw new AnalysisException(\"Method parameter count must match the number of procedure parameters\")\n+    }\n+\n+    // the MethodHandle API does not allow us to check the generic type\n+    // so we only verify the return type is either void or iterable\n+\n+    if (outputType.nonEmpty && methodReturnType != classOf[java.lang.Iterable[_]]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines $outputType \" +\n+        \"as its output so must return java.lang.Iterable of Spark Rows\")\n+    }\n+\n+    if (outputType.isEmpty && methodReturnType != classOf[Void]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines no output columns \" +\n+        \"so must be void\")\n+    }\n+  }\n+\n+  private def buildArgExprs(procedure: Procedure, args: Seq[CallArgument]): Seq[Expression] = {\n+    val params = procedure.parameters\n+\n+    // build a map of declared parameter names to their positions\n+    val nameToPositionMap = params.map(_.name).zipWithIndex.toMap\n+\n+    // build a map of parameter names to args\n+    val nameToArgMap = buildNameToArgMap(params, args, nameToPositionMap)\n+\n+    // verify all required parameters are provided\n+    val missingParamNames = params.filter(_.required).collect {\n+      case param if !nameToArgMap.contains(param.name) => param.name\n+    }\n+\n+    if (missingParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Missing required parameters: ${missingParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    val argExprs = new Array[Expression](params.size)\n+\n+    nameToArgMap.foreach { case (name, arg) =>\n+      val position = nameToPositionMap(name)\n+      val param = params(position)\n+      val paramType = param.dataType\n+      val argType = arg.expr.dataType\n+      if (paramType != argType) {\n+        throw new AnalysisException(s\"Wrong arg type for ${param.name}: expected $paramType but got $argType\")\n+      }\n+      argExprs(position) = arg.expr\n+    }\n+\n+    // assign nulls to optional params that were not set\n+    params.foreach {\n+      case p if !p.required && !nameToArgMap.contains(p.name) =>\n+        val position = nameToPositionMap(p.name)\n+        argExprs(position) = Literal.create(null, p.dataType)\n+      case _ =>\n+    }\n+\n+    argExprs\n+  }\n+\n+  private def buildNameToArgMap(\n+      params: Seq[ProcedureParameter],\n+      args: Seq[CallArgument],\n+      nameToPositionMap: Map[String, Int]): Map[String, CallArgument] = {\n+\n+    val containsNamedArg = args.exists(_.isInstanceOf[NamedArgument])\n+    val containsPositionalArg = args.exists(_.isInstanceOf[PositionalArgument])\n+\n+    if (containsNamedArg && containsPositionalArg) {\n+      throw new AnalysisException(\"Named and positional arguments cannot be mixed\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMTE5MA==", "bodyText": "Yes, Presto does not allow mixing named and positional args.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520921190", "createdAt": "2020-11-10T22:42:59Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+\n+      validateParams(procedure)\n+      validateMethodHandle(procedure)\n+\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def validateMethodHandle(procedure: Procedure): Unit = {\n+    val params = procedure.parameters\n+    val outputType = procedure.outputType\n+\n+    val methodHandle = procedure.methodHandle\n+    val methodType = methodHandle.`type`\n+    val methodReturnType = methodType.returnType\n+\n+    // method cannot accept var ags\n+    if (methodHandle.isVarargsCollector) {\n+      throw new AnalysisException(\"Method must have fixed arity\")\n+    }\n+\n+    // verify the number of params in the procedure match the number of params in the method\n+    if (params.length != methodType.parameterCount) {\n+      throw new AnalysisException(\"Method parameter count must match the number of procedure parameters\")\n+    }\n+\n+    // the MethodHandle API does not allow us to check the generic type\n+    // so we only verify the return type is either void or iterable\n+\n+    if (outputType.nonEmpty && methodReturnType != classOf[java.lang.Iterable[_]]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines $outputType \" +\n+        \"as its output so must return java.lang.Iterable of Spark Rows\")\n+    }\n+\n+    if (outputType.isEmpty && methodReturnType != classOf[Void]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines no output columns \" +\n+        \"so must be void\")\n+    }\n+  }\n+\n+  private def buildArgExprs(procedure: Procedure, args: Seq[CallArgument]): Seq[Expression] = {\n+    val params = procedure.parameters\n+\n+    // build a map of declared parameter names to their positions\n+    val nameToPositionMap = params.map(_.name).zipWithIndex.toMap\n+\n+    // build a map of parameter names to args\n+    val nameToArgMap = buildNameToArgMap(params, args, nameToPositionMap)\n+\n+    // verify all required parameters are provided\n+    val missingParamNames = params.filter(_.required).collect {\n+      case param if !nameToArgMap.contains(param.name) => param.name\n+    }\n+\n+    if (missingParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Missing required parameters: ${missingParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    val argExprs = new Array[Expression](params.size)\n+\n+    nameToArgMap.foreach { case (name, arg) =>\n+      val position = nameToPositionMap(name)\n+      val param = params(position)\n+      val paramType = param.dataType\n+      val argType = arg.expr.dataType\n+      if (paramType != argType) {\n+        throw new AnalysisException(s\"Wrong arg type for ${param.name}: expected $paramType but got $argType\")\n+      }\n+      argExprs(position) = arg.expr\n+    }\n+\n+    // assign nulls to optional params that were not set\n+    params.foreach {\n+      case p if !p.required && !nameToArgMap.contains(p.name) =>\n+        val position = nameToPositionMap(p.name)\n+        argExprs(position) = Literal.create(null, p.dataType)\n+      case _ =>\n+    }\n+\n+    argExprs\n+  }\n+\n+  private def buildNameToArgMap(\n+      params: Seq[ProcedureParameter],\n+      args: Seq[CallArgument],\n+      nameToPositionMap: Map[String, Int]): Map[String, CallArgument] = {\n+\n+    val containsNamedArg = args.exists(_.isInstanceOf[NamedArgument])\n+    val containsPositionalArg = args.exists(_.isInstanceOf[PositionalArgument])\n+\n+    if (containsNamedArg && containsPositionalArg) {\n+      throw new AnalysisException(\"Named and positional arguments cannot be mixed\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MzgxOQ=="}, "originalCommit": {"oid": "5d23e76c259f8d500347863e1b72938c794e03b4"}, "originalPosition": 146}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NTkwOTIzOnYy", "diffSide": "LEFT", "path": "project/scalastyle_config.xml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDowMzozMFrOHw0ZNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDoxMjoxNlrOHw0kjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1MjExNg==", "bodyText": "We've discussed how handy this rule is multiple times. I'd remove it if there are no objections.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520952116", "createdAt": "2020-11-11T00:03:30Z", "author": {"login": "aokolnychyi"}, "path": "project/scalastyle_config.xml", "diffHunk": "@@ -85,11 +85,6 @@\n             <parameter name=\"maxTypes\"><![CDATA[30]]></parameter>\n         </parameters>\n     </check>\n-    <check level=\"error\" class=\"org.scalastyle.scalariform.CyclomaticComplexityChecker\" enabled=\"true\">", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1NTAyMQ==", "bodyText": "I can move this into a separate PR and disable in Java too.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520955021", "createdAt": "2020-11-11T00:12:16Z", "author": {"login": "aokolnychyi"}, "path": "project/scalastyle_config.xml", "diffHunk": "@@ -85,11 +85,6 @@\n             <parameter name=\"maxTypes\"><![CDATA[30]]></parameter>\n         </parameters>\n     </check>\n-    <check level=\"error\" class=\"org.scalastyle.scalariform.CyclomaticComplexityChecker\" enabled=\"true\">", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1MjExNg=="}, "originalCommit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NTkxNjg1OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/CallExec.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDowNzowM1rOHw0dww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDowNzowM1rOHw0dww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1MzI4Mw==", "bodyText": "@rdblue, I do not pass a type alongside the row since the rules rely on parameters reported by the procedure to align input expressions.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520953283", "createdAt": "2020-11-11T00:07:03Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/CallExec.scala", "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.connector.catalog.Procedure\n+\n+case class CallExec(\n+    output: Seq[Attribute],\n+    procedure: Procedure,\n+    input: InternalRow) extends V2CommandExec {\n+\n+  override protected def run(): Seq[InternalRow] = {\n+    procedure.call(input)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NjAwMzUxOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo0NjowNlrOHw1QLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo0NjowNlrOHw1QLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2NjE4OQ==", "bodyText": "We know from validation above that if the map doesn't contain an argument, that it must be required. Not a big deal to have the additional check, though.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520966189", "createdAt": "2020-11-11T00:46:06Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Cast, Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+case class ResolveProcedures(spark: SparkSession) extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = spark.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+      validateParams(procedure)\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def buildArgExprs(procedure: Procedure, args: Seq[CallArgument]): Seq[Expression] = {\n+    val params = procedure.parameters\n+\n+    // build a map of declared parameter names to their positions\n+    val nameToPositionMap = params.map(_.name).zipWithIndex.toMap\n+\n+    // build a map of parameter names to args\n+    val nameToArgMap = buildNameToArgMap(params, args, nameToPositionMap)\n+\n+    // verify all required parameters are provided\n+    val missingParamNames = params.filter(_.required).collect {\n+      case param if !nameToArgMap.contains(param.name) => param.name\n+    }\n+\n+    if (missingParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Missing required parameters: ${missingParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    val argExprs = new Array[Expression](params.size)\n+\n+    nameToArgMap.foreach { case (name, arg) =>\n+      val position = nameToPositionMap(name)\n+      val param = params(position)\n+      val paramType = param.dataType\n+      val argType = arg.expr.dataType\n+\n+      if (paramType != argType && !Cast.canUpCast(argType, paramType)) {\n+        throw new AnalysisException(s\"Wrong arg type for ${param.name}: expected $paramType but got $argType\")\n+      }\n+\n+      if (paramType != argType) {\n+        argExprs(position) = Cast(arg.expr, paramType)\n+      } else {\n+        argExprs(position) = arg.expr\n+      }\n+    }\n+\n+    // assign nulls to optional params that were not set\n+    params.foreach {\n+      case p if !p.required && !nameToArgMap.contains(p.name) =>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NjAwOTM4OnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/Call.scala", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo0Nzo1MFrOHw1UHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxNjozMTowN1rOHxUt2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2NzE5Nw==", "bodyText": "I think this needs to be a lazy val, or else the attributes will have different IDs every time it is called.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520967197", "createdAt": "2020-11-11T00:47:50Z", "author": {"login": "rdblue"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/Call.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Expression}\n+import org.apache.spark.sql.connector.catalog.Procedure\n+import scala.collection.Seq\n+\n+case class Call(procedure: Procedure, args: Seq[Expression]) extends Command {\n+  override def output: Seq[Attribute] = procedure.outputType.toAttributes", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQ4MTY5MA==", "bodyText": "Fixed.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r521481690", "createdAt": "2020-11-11T16:31:07Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/Call.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Expression}\n+import org.apache.spark.sql.connector.catalog.Procedure\n+import scala.collection.Seq\n+\n+case class Call(procedure: Procedure, args: Seq[Expression]) extends Command {\n+  override def output: Seq[Attribute] = procedure.outputType.toAttributes", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2NzE5Nw=="}, "originalCommit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NjAxNjMwOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/ProcedureParameter.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo0OToxOVrOHw1YQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxNjozMDozMVrOHxUrxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2ODI1Nw==", "bodyText": "I think for all of the connector interfaces, we should add clear Javadoc.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520968257", "createdAt": "2020-11-11T00:49:19Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/ProcedureParameter.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.catalog;\n+\n+import org.apache.spark.sql.types.DataType;\n+\n+public interface ProcedureParameter {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQ4MTE1Ng==", "bodyText": "I'll add Javadoc in a follow-up PR. I need to document other places like IcebergSparkSessionExtensions. I want to have at least one procedure to be merged using these APIs.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r521481156", "createdAt": "2020-11-11T16:30:31Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/ProcedureParameter.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.catalog;\n+\n+import org.apache.spark.sql.types.DataType;\n+\n+public interface ProcedureParameter {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2ODI1Nw=="}, "originalCommit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NjAyNzMxOnYy", "diffSide": "RIGHT", "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/Procedure.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo1MzoxMVrOHw1ekw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxNjoyOTowNlrOHxUmcQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2OTg3NQ==", "bodyText": "One more thing: we should also add a describe method so that we can show these correctly in plans.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520969875", "createdAt": "2020-11-11T00:53:11Z", "author": {"login": "rdblue"}, "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/Procedure.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.catalog;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.StructType;\n+\n+public interface Procedure {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQ3OTc5Mw==", "bodyText": "Done.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r521479793", "createdAt": "2020-11-11T16:29:06Z", "author": {"login": "aokolnychyi"}, "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/Procedure.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.catalog;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.StructType;\n+\n+public interface Procedure {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2OTg3NQ=="}, "originalCommit": {"oid": "588a116401751f3d59ed46aae74a38aa54947367"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2OTIzNDEwOnYy", "diffSide": "RIGHT", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/Call.scala", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxNjoyODozMVrOHxUk9w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQxNjoyODozMVrOHxUk9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQ3OTQxNQ==", "bodyText": "@rdblue, I implemented simpleString in Call and CallExec that use description in Procedure.", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r521479415", "createdAt": "2020-11-11T16:28:31Z", "author": {"login": "aokolnychyi"}, "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/Call.scala", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Expression}\n+import org.apache.spark.sql.catalyst.util.truncatedString\n+import org.apache.spark.sql.connector.catalog.Procedure\n+import scala.collection.Seq\n+\n+case class Call(procedure: Procedure, args: Seq[Expression]) extends Command {\n+  override lazy val output: Seq[Attribute] = procedure.outputType.toAttributes\n+\n+  override def simpleString(maxFields: Int): String = {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b6ade274936b6fb810d3999a8c95efb83cba6520"}, "originalPosition": 30}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3419, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}