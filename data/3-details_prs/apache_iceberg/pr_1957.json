{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQyMjg1OTM4", "number": 1957, "title": "Spark: Print file location in case of error during reads", "bodyText": "Make it easier to find out exactly which file was being read when we encountered an error either opening the task or when reading rows from the task\nHeres an example of how it looks now\nException: Error reading file: hdfs://namenode/table/path/part-r-00001.orc\n\tat org.apache.iceberg.spark.source.BaseDataReader.next(BaseDataReader.java:97)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:49)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$12$$anon$1.hasNext(WholeStageCodegenExec.scala:634)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:429)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: Can not promote LONG type to INTEGER\n\tat org.apache.iceberg.relocated.com.google.common.base.Preconditions.checkArgument(Preconditions.java:441)\n\tat org.apache.iceberg.orc.ORCSchemaUtil.buildOrcProjection(ORCSchemaUtil.java:301)\n\tat org.apache.iceberg.orc.ORCSchemaUtil.buildOrcProjection(ORCSchemaUtil.java:275)\n\tat org.apache.iceberg.orc.ORCSchemaUtil.buildOrcProjection(ORCSchemaUtil.java:275)\n\tat org.apache.iceberg.orc.ORCSchemaUtil.buildOrcProjection(ORCSchemaUtil.java:258)\n\tat org.apache.iceberg.orc.OrcIterable.iterator(OrcIterable.java:97)\n\tat org.apache.iceberg.spark.source.RowDataReader.open(RowDataReader.java:100)\n\tat org.apache.iceberg.spark.source.BaseDataReader.next(BaseDataReader.java:84)", "createdAt": "2020-12-18T04:52:16Z", "url": "https://github.com/apache/iceberg/pull/1957", "merged": true, "mergeCommit": {"oid": "a0cc290dd365bd945802204ec24aa88c68faddf6"}, "closed": true, "closedAt": "2020-12-19T22:24:53Z", "author": {"login": "shardulm94"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdnQqRUAH2gAyNTQyMjg1OTM4OjVmMzdhYmYxYWMxN2UwMDA2OTI2YzczNzU3YjAxZjJkNTMxMTcxOWI=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdn0ajggFqTU1NTk3MjE4Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "5f37abf1ac17e0006926c73757b01f2d5311719b", "author": {"user": {"login": "shardulm94", "name": "Shardul Mahadik"}}, "url": "https://github.com/apache/iceberg/commit/5f37abf1ac17e0006926c73757b01f2d5311719b", "committedDate": "2020-12-18T04:44:56Z", "message": "Spark: Print file location in case of error during reads"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1NzA0MjMy", "url": "https://github.com/apache/iceberg/pull/1957#pullrequestreview-555704232", "createdAt": "2020-12-18T18:08:50Z", "commit": {"oid": "5f37abf1ac17e0006926c73757b01f2d5311719b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODowODo1MFrOIItSwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODowODo1MFrOIItSwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAwMTYwMg==", "bodyText": "Did you intend to set this in the constructor?", "url": "https://github.com/apache/iceberg/pull/1957#discussion_r546001602", "createdAt": "2020-12-18T18:08:50Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseDataReader.java", "diffHunk": "@@ -56,6 +56,7 @@\n \n   private CloseableIterator<T> currentIterator;\n   private T current = null;\n+  private FileScanTask currentTask = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f37abf1ac17e0006926c73757b01f2d5311719b"}, "originalPosition": 4}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1NzA3MjEx", "url": "https://github.com/apache/iceberg/pull/1957#pullrequestreview-555707211", "createdAt": "2020-12-18T18:13:23Z", "commit": {"oid": "5f37abf1ac17e0006926c73757b01f2d5311719b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODoxMzoyM1rOIItbqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xOFQxODoxMzoyM1rOIItbqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAwMzg4MA==", "bodyText": "The main problem I have with this is that it alters the exception by discarding its type. Callers can no longer catch exceptions and handle errors. For example, a caller might catch a SocketException and retry, but let an EOFException propagate.\nIs there another way to attach the data? What about logging the file path and exception instead of modifying the exception?\nAnother option is to create a new exception and attach it as suppressed, then throw the original unmodified.", "url": "https://github.com/apache/iceberg/pull/1957#discussion_r546003880", "createdAt": "2020-12-18T18:13:23Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseDataReader.java", "diffHunk": "@@ -77,16 +78,30 @@\n   }\n \n   public boolean next() throws IOException {\n-    while (true) {\n-      if (currentIterator.hasNext()) {\n-        this.current = currentIterator.next();\n-        return true;\n-      } else if (tasks.hasNext()) {\n-        this.currentIterator.close();\n-        this.currentIterator = open(tasks.next());\n+    try {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          this.current = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          this.currentIterator.close();\n+          this.currentTask = tasks.next();\n+          this.currentIterator = open(currentTask);\n+        } else {\n+          this.currentIterator.close();\n+          return false;\n+        }\n+      }\n+    } catch (IOException | RuntimeException e) {\n+      if (currentTask == null || currentTask.isDataTask()) {\n+        throw e;\n       } else {\n-        this.currentIterator.close();\n-        return false;\n+        String message = String.format(\"Error reading file: %s\", getInputFile(currentTask).location());\n+        if (e instanceof IOException) {\n+          throw new IOException(message, e);\n+        } else {\n+          throw new RuntimeException(message, e);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5f37abf1ac17e0006926c73757b01f2d5311719b"}, "originalPosition": 44}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8114b9f25832061e442b3ea71cc1a90cd05c839b", "author": {"user": {"login": "shardulm94", "name": "Shardul Mahadik"}}, "url": "https://github.com/apache/iceberg/commit/8114b9f25832061e442b3ea71cc1a90cd05c839b", "committedDate": "2020-12-19T02:37:39Z", "message": "Address PR comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU1OTcyMTgz", "url": "https://github.com/apache/iceberg/pull/1957#pullrequestreview-555972183", "createdAt": "2020-12-19T22:24:21Z", "commit": {"oid": "8114b9f25832061e442b3ea71cc1a90cd05c839b"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3297, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}