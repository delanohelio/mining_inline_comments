{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkzNzIzMDc0", "number": 1517, "title": "Flink: apply row-level delete when reading", "bodyText": "This includes #1497, I will rebase when #1497 get merged.", "createdAt": "2020-09-27T13:39:09Z", "url": "https://github.com/apache/iceberg/pull/1517", "merged": true, "mergeCommit": {"oid": "a238a90eb987bfbf5d14b5cab8d109e53a75e861"}, "closed": true, "closedAt": "2020-10-14T23:11:41Z", "author": {"login": "chenjunjiedada"}, "timelineItems": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdM_oqzAFqTQ5NzA4NDQyOQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdSTzb5gH2gAyNDkzNzIzMDc0OjlmYjdjOTQxOTUxYzA2YzQ5OGUzNTg1Yjc3ODRjODE0NDcyNDBlYTc=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3MDg0NDI5", "url": "https://github.com/apache/iceberg/pull/1517#pullrequestreview-497084429", "createdAt": "2020-09-27T14:12:46Z", "commit": {"oid": "bb46c7f7bd8ba27af2094635a6ca4714df8d7418"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yN1QxNDoxMjo0NlrOHYnpBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yN1QxNDoxMjo0NlrOHYnpBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTU3NzM0OA==", "bodyText": "@JingsongLi , This is used to fix the UT.  How can we copy the RowData?", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r495577348", "createdAt": "2020-09-27T14:12:46Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataIterator.java", "diffHunk": "@@ -92,12 +102,12 @@\n     return iter;\n   }\n \n-  private CloseableIterable<RowData> newAvroIterable(FileScanTask task, Map<Integer, ?> idToConstant) {\n+  private CloseableIterable<RowData> newAvroIterable(FileScanTask task, Schema schema, Map<Integer, ?> idToConstant) {\n     Avro.ReadBuilder builder = Avro.read(getInputFile(task))\n-        .reuseContainers()\n-        .project(projectedSchema)\n+        .reuseContainers(false)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bb46c7f7bd8ba27af2094635a6ca4714df8d7418"}, "originalPosition": 82}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3NzY4ODM5", "url": "https://github.com/apache/iceberg/pull/1517#pullrequestreview-497768839", "createdAt": "2020-09-28T17:56:57Z", "commit": {"oid": "8526b6deedb747200fb21507f5716cf5e8396470"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNzo1Njo1OFrOHZJlxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNzo1Njo1OFrOHZJlxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEzMzU3Mw==", "bodyText": "Why rename this variable?", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r496133573", "createdAt": "2020-09-28T17:56:58Z", "author": {"login": "rdblue"}, "path": "data/src/test/java/org/apache/iceberg/data/DeletesReadTest.java", "diffHunk": "@@ -240,17 +252,11 @@ public void testEqualityDeleteByNull() throws IOException {\n     Assert.assertEquals(\"Table should contain expected rows\", expected, actual);\n   }\n \n-  private static StructLikeSet rowSet(Table table) throws IOException {\n-    return rowSet(table, \"*\");\n+  private StructLikeSet rowSet(Table tbl) throws IOException {\n+    return rowSet(tbl, \"*\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8526b6deedb747200fb21507f5716cf5e8396470"}, "originalPosition": 102}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8526b6deedb747200fb21507f5716cf5e8396470", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/8526b6deedb747200fb21507f5716cf5e8396470", "committedDate": "2020-09-28T03:22:34Z", "message": "revert change for Avro iterator"}, "afterCommit": {"oid": "dc2b9dbb61bad6eb4bb366817108ea63375d4803", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/dc2b9dbb61bad6eb4bb366817108ea63375d4803", "committedDate": "2020-10-10T08:00:20Z", "message": "Flink: apply row-level delete when reading\n\nConflicts:\n\tflink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java\n\tflink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n\tflink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormat.java"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2MTI3Mjg0", "url": "https://github.com/apache/iceberg/pull/1517#pullrequestreview-506127284", "createdAt": "2020-10-10T21:24:34Z", "commit": {"oid": "dc2b9dbb61bad6eb4bb366817108ea63375d4803"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQyMToyNDozNFrOHfiifw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQyMToyNDozNFrOHfiifw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzMzc5MQ==", "bodyText": "distinct compares files using equals, which is not overridden for data or delete files. This should instead use the approach that Spark uses:\n    Map<String, ByteBuffer> keyMetadata = Maps.newHashMap();\n    task.files().stream()\n        .flatMap(fileScanTask -> Stream.concat(Stream.of(fileScanTask.file()), fileScanTask.deletes().stream()))\n        .forEach(file -> keyMetadata.put(file.path().toString(), file.keyMetadata()));\n    Stream<EncryptedInputFile> encrypted = keyMetadata.entrySet().stream()\n        .map(entry -> EncryptedFiles.encryptedInput(io.newInputFile(entry.getKey()), entry.getValue()));\n\n    // decrypt with the batch call to avoid multiple RPCs to a key server, if possible\n    Iterable<InputFile> decryptedFiles = encryptionManager.decrypt(encrypted::iterator);", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r502833791", "createdAt": "2020-10-10T21:24:34Z", "author": {"login": "rdblue"}, "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -50,23 +54,36 @@\n abstract class DataIterator<T> implements CloseableIterator<T> {\n \n   private Iterator<FileScanTask> tasks;\n-  private final FileIO io;\n-  private final EncryptionManager encryption;\n+  private final Map<String, InputFile> inputFiles;\n \n   private CloseableIterator<T> currentIterator;\n \n   DataIterator(CombinedScanTask task, FileIO io, EncryptionManager encryption) {\n     this.tasks = task.files().iterator();\n-    this.io = io;\n-    this.encryption = encryption;\n+\n+    Stream<EncryptedInputFile> encrypted = task.files().stream()\n+        .flatMap(fileScanTask -> Stream.concat(Stream.of(fileScanTask.file()), fileScanTask.deletes().stream()))\n+        .distinct()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2b9dbb61bad6eb4bb366817108ea63375d4803"}, "originalPosition": 40}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2MTI3NjM0", "url": "https://github.com/apache/iceberg/pull/1517#pullrequestreview-506127634", "createdAt": "2020-10-10T21:30:51Z", "commit": {"oid": "dc2b9dbb61bad6eb4bb366817108ea63375d4803"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQyMTozMDo1MVrOHfiksQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQyMTozMDo1MVrOHfiksQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzNDM1Mw==", "bodyText": "If this used a Hive table instead, then it wouldn't be necessary to keep state that isn't passed as method arguments. I think that would be less brittle.", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r502834353", "createdAt": "2020-10-10T21:30:51Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormatReaderDeletes.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.data.DeleteReadTests;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.RowDataWrapper;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkInputFormatReaderDeletes extends DeleteReadTests {\n+  private final Configuration conf = new Configuration();\n+  private final HadoopTables tables = new HadoopTables(conf);\n+  private final FileFormat format;\n+\n+  private String tableLocation;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2b9dbb61bad6eb4bb366817108ea63375d4803"}, "originalPosition": 52}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2MTI4MjQ3", "url": "https://github.com/apache/iceberg/pull/1517#pullrequestreview-506128247", "createdAt": "2020-10-10T21:43:01Z", "commit": {"oid": "dc2b9dbb61bad6eb4bb366817108ea63375d4803"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQyMTo0MzowMVrOHfio0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQyMTo0MzowMVrOHfio0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzNTQwOA==", "bodyText": "Why half-configure the builder above and then finish it here? I think it would be simpler to use this:\nSchema projected = testTable.schema().select(columns);\nRowType rowType = FlinkSchemaUtil.convert(projected);\nFlinkInputFormat inputFormat = FlinkSource.forRowData()\n    .tableLoader(TableLoader.fromHadoopTable(tableLocation))\n    .project(FlinkSchemaUtil.toSchema(rowType))\n    .buildFormat();", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r502835408", "createdAt": "2020-10-10T21:43:01Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormatReaderDeletes.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.data.DeleteReadTests;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.RowDataWrapper;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkInputFormatReaderDeletes extends DeleteReadTests {\n+  private final Configuration conf = new Configuration();\n+  private final HadoopTables tables = new HadoopTables(conf);\n+  private final FileFormat format;\n+\n+  private String tableLocation;\n+\n+  @Parameterized.Parameters(name = \"fileFormat={0}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { FileFormat.PARQUET },\n+        new Object[] { FileFormat.AVRO },\n+        new Object[] { FileFormat.ORC }\n+    };\n+  }\n+\n+  public TestFlinkInputFormatReaderDeletes(FileFormat inputFormat) {\n+    this.format = inputFormat;\n+  }\n+\n+  @Override\n+  protected Table createTable(String name, Schema schema, PartitionSpec spec) throws IOException {\n+    File location = temp.newFolder(format.name(), name);\n+    Assert.assertTrue(location.delete());\n+    this.tableLocation = location.toURI().toString();\n+\n+    Map<String, String> props = Maps.newHashMap();\n+    props.put(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+\n+    Table table = tables.create(schema, spec, props, tableLocation);\n+    TableOperations ops = ((BaseTable) table).operations();\n+    TableMetadata meta = ops.current();\n+    ops.commit(meta, meta.upgradeToFormatVersion(2));\n+\n+    return table;\n+  }\n+\n+  @Override\n+  protected void dropTable(String name) {\n+    tables.dropTable(tableLocation, true);\n+  }\n+\n+  @Override\n+  protected StructLikeSet rowSet(String name, Table testTable, String... columns) throws IOException {\n+    FlinkSource.Builder builder = FlinkSource.forRowData().tableLoader(TableLoader.fromHadoopTable(tableLocation));\n+    Schema projected = testTable.schema().select(columns);\n+    RowType rowType = FlinkSchemaUtil.convert(projected);\n+    FlinkInputFormat inputFormat = builder.project(FlinkSchemaUtil.toSchema(rowType)).buildFormat();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2b9dbb61bad6eb4bb366817108ea63375d4803"}, "originalPosition": 94}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2MTI4Mzc1", "url": "https://github.com/apache/iceberg/pull/1517#pullrequestreview-506128375", "createdAt": "2020-10-10T21:46:03Z", "commit": {"oid": "dc2b9dbb61bad6eb4bb366817108ea63375d4803"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQyMTo0NjowM1rOHfip4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQyMTo0NjowM1rOHfip4w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzNTY4Mw==", "bodyText": "I think it is a bad practice to make helper methods in one test suite public and use them in another suite. Instead, helper methods should be moved to an appropriate test utility class. That way, we don't have test utility code that is hard to find because it lives in whatever test was written first.", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r502835683", "createdAt": "2020-10-10T21:46:03Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormat.java", "diffHunk": "@@ -104,6 +105,22 @@ public void testNestedProjection() throws Exception {\n   }\n \n   private List<Row> runFormat(FlinkInputFormat inputFormat) throws IOException {\n+    return getRows(inputFormat);\n+  }\n+\n+  public static List<RowData> getRowData(FlinkInputFormat inputFormat) throws IOException {\n+    RowType rowType = FlinkSchemaUtil.convert(inputFormat.projectedSchema());\n+\n+    DataStructureConverter<Object, Object> converter = DataStructureConverters.getConverter(\n+        TypeConversions.fromLogicalToDataType(rowType));\n+\n+    return getRows(inputFormat).stream()\n+        .map(converter::toInternal)\n+        .map(RowData.class::cast)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static List<Row> getRows(FlinkInputFormat inputFormat) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dc2b9dbb61bad6eb4bb366817108ea63375d4803"}, "originalPosition": 27}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ab23ed6907e13d52d1d153cc7f5a9ad674fbfc87", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/ab23ed6907e13d52d1d153cc7f5a9ad674fbfc87", "committedDate": "2020-10-11T13:39:23Z", "message": "use hive catalog to create table"}, "afterCommit": {"oid": "5656be83274c4e64b754e27cddac2957380e8039", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/5656be83274c4e64b754e27cddac2957380e8039", "committedDate": "2020-10-11T13:47:51Z", "message": "use hive catalog to create table"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2MzIyNDc5", "url": "https://github.com/apache/iceberg/pull/1517#pullrequestreview-506322479", "createdAt": "2020-10-12T06:26:48Z", "commit": {"oid": "5656be83274c4e64b754e27cddac2957380e8039"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNjoyNjo0OFrOHfwjsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQwNjoyNjo0OFrOHfwjsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzA2MzQ3Mw==", "bodyText": "@JingsongLi , is this the right way to create the CatalogLoader?", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r503063473", "createdAt": "2020-10-12T06:26:48Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormatReaderDeletes.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.DeleteReadTests;\n+import org.apache.iceberg.flink.CatalogLoader;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.RowDataWrapper;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.flink.TestHelpers;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.hive.TestHiveMetastore;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkInputFormatReaderDeletes extends DeleteReadTests {\n+  private static HiveConf hiveConf = null;\n+  private static HiveCatalog catalog = null;\n+  private static TestHiveMetastore metastore = null;\n+\n+  private final FileFormat format;\n+\n+  @Parameterized.Parameters(name = \"fileFormat={0}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { FileFormat.PARQUET },\n+        new Object[] { FileFormat.AVRO },\n+        new Object[] { FileFormat.ORC }\n+    };\n+  }\n+\n+  public TestFlinkInputFormatReaderDeletes(FileFormat inputFormat) {\n+    this.format = inputFormat;\n+  }\n+\n+  @BeforeClass\n+  public static void startMetastore() {\n+    TestFlinkInputFormatReaderDeletes.metastore = new TestHiveMetastore();\n+    metastore.start();\n+    TestFlinkInputFormatReaderDeletes.hiveConf = metastore.hiveConf();\n+    TestFlinkInputFormatReaderDeletes.catalog = new HiveCatalog(hiveConf);\n+  }\n+\n+  @AfterClass\n+  public static void stopMetastore() {\n+    metastore.stop();\n+    catalog.close();\n+    TestFlinkInputFormatReaderDeletes.catalog = null;\n+  }\n+\n+  @Override\n+  protected Table createTable(String name, Schema schema, PartitionSpec spec) {\n+    Map<String, String> props = Maps.newHashMap();\n+    props.put(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", name), schema, spec, props);\n+    TableOperations ops = ((BaseTable) table).operations();\n+    TableMetadata meta = ops.current();\n+    ops.commit(meta, meta.upgradeToFormatVersion(2));\n+\n+    return table;\n+  }\n+\n+  @Override\n+  protected void dropTable(String name) {\n+    catalog.dropTable(TableIdentifier.of(\"default\", name));\n+  }\n+\n+  @Override\n+  protected StructLikeSet rowSet(String name, Table testTable, String... columns) throws IOException {\n+    Schema projected = testTable.schema().select(columns);\n+    RowType rowType = FlinkSchemaUtil.convert(projected);\n+    CatalogLoader hiveCatalogLoader = CatalogLoader.hive(catalog.name(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5656be83274c4e64b754e27cddac2957380e8039"}, "originalPosition": 108}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2ODExMDM4", "url": "https://github.com/apache/iceberg/pull/1517#pullrequestreview-506811038", "createdAt": "2020-10-12T17:50:23Z", "commit": {"oid": "5656be83274c4e64b754e27cddac2957380e8039"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxNzo1MDoyNFrOHgH78w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxNzo1MDoyNFrOHgH78w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ0NjUxNQ==", "bodyText": "This seems strange to me. Why convert rows to external and convert them back to internal in the getRowData method? Why not move this implementation into getRowData and convert to external in this one?\nAlso, is there a better name for these methods? What about readRows or scan? Those would be a bit more clear about what is going on in these. The original was called runFormat, which is also a good name.", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r503446515", "createdAt": "2020-10-12T17:50:24Z", "author": {"login": "rdblue"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestHelpers.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.conversion.DataStructureConverter;\n+import org.apache.flink.table.data.conversion.DataStructureConverters;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.flink.source.FlinkInputFormat;\n+import org.apache.iceberg.flink.source.FlinkInputSplit;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class TestHelpers {\n+\n+  private TestHelpers() {\n+  }\n+\n+  public static List<Row> getRows(FlinkInputFormat inputFormat, RowType rowType) throws IOException {\n+    FlinkInputSplit[] splits = inputFormat.createInputSplits(0);\n+    List<Row> results = Lists.newArrayList();\n+\n+    DataStructureConverter<Object, Object> converter = DataStructureConverters.getConverter(\n+        TypeConversions.fromLogicalToDataType(rowType));\n+\n+    for (FlinkInputSplit s : splits) {\n+      inputFormat.open(s);\n+      while (!inputFormat.reachedEnd()) {\n+        RowData row = inputFormat.nextRecord(null);\n+        results.add((Row) converter.toExternal(row));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5656be83274c4e64b754e27cddac2957380e8039"}, "originalPosition": 51}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ab3cad9d5496cbe21d3843aa19ba32a5e5a20527", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/ab3cad9d5496cbe21d3843aa19ba32a5e5a20527", "committedDate": "2020-10-13T06:53:55Z", "message": "Flink: apply row-level delete when reading\n\nConflicts:\n\tflink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java\n\tflink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n\tflink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormat.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3115544e11e808756a06b39b2ba1479d00143698", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/3115544e11e808756a06b39b2ba1479d00143698", "committedDate": "2020-10-13T06:53:55Z", "message": "use hive catalog to create table"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5656be83274c4e64b754e27cddac2957380e8039", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/5656be83274c4e64b754e27cddac2957380e8039", "committedDate": "2020-10-11T13:47:51Z", "message": "use hive catalog to create table"}, "afterCommit": {"oid": "75dce7f98c0dc26fd6a27cf931130e7b69c8dceb", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/75dce7f98c0dc26fd6a27cf931130e7b69c8dceb", "committedDate": "2020-10-13T06:53:56Z", "message": "rename the helper function and merge test helpers"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "265524fb0c1eba181a2c3376a94600358b5e0bba", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/265524fb0c1eba181a2c3376a94600358b5e0bba", "committedDate": "2020-10-13T07:07:28Z", "message": "rename the helper function and merge test helpers"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "75dce7f98c0dc26fd6a27cf931130e7b69c8dceb", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/75dce7f98c0dc26fd6a27cf931130e7b69c8dceb", "committedDate": "2020-10-13T06:53:56Z", "message": "rename the helper function and merge test helpers"}, "afterCommit": {"oid": "265524fb0c1eba181a2c3376a94600358b5e0bba", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/265524fb0c1eba181a2c3376a94600358b5e0bba", "committedDate": "2020-10-13T07:07:28Z", "message": "rename the helper function and merge test helpers"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3a61448aa453806183a7c9aed5d906974bdc0cf4", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/3a61448aa453806183a7c9aed5d906974bdc0cf4", "committedDate": "2020-10-13T09:36:19Z", "message": "copy RowData instead of converting back and forth"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c8eecd3318514fdb3e4a7cbe13636cbc9d3de76c", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/c8eecd3318514fdb3e4a7cbe13636cbc9d3de76c", "committedDate": "2020-10-13T09:30:10Z", "message": "copy RowData instead of converting back and forth"}, "afterCommit": {"oid": "3a61448aa453806183a7c9aed5d906974bdc0cf4", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/3a61448aa453806183a7c9aed5d906974bdc0cf4", "committedDate": "2020-10-13T09:36:19Z", "message": "copy RowData instead of converting back and forth"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA3MjUwOTcy", "url": "https://github.com/apache/iceberg/pull/1517#pullrequestreview-507250972", "createdAt": "2020-10-13T09:38:15Z", "commit": {"oid": "3a61448aa453806183a7c9aed5d906974bdc0cf4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwOTozODoxNVrOHgeKhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwOTozODoxNVrOHgeKhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgxMDY5Mw==", "bodyText": "@JingsongLi , I can not use RowDataSerializer directly since the returned RowData may contain metadata column after merging with position deletes. So I created this function to do the copy job.", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r503810693", "createdAt": "2020-10-13T09:38:15Z", "author": {"login": "chenjunjiedada"}, "path": "flink/src/test/java/org/apache/iceberg/flink/TestHelpers.java", "diffHunk": "@@ -49,6 +61,49 @@\n   private TestHelpers() {\n   }\n \n+  public static RowData copyRowData(RowData from, RowType rowType) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a61448aa453806183a7c9aed5d906974bdc0cf4"}, "originalPosition": 43}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9fb7c941951c06c498e3585b7784c81447240ea7", "author": {"user": {"login": "chenjunjiedada", "name": "Chen, Junjie"}}, "url": "https://github.com/apache/iceberg/commit/9fb7c941951c06c498e3585b7784c81447240ea7", "committedDate": "2020-10-14T02:32:15Z", "message": "fix failed unit tests"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3856, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}