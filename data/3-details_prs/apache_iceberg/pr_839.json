{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg3NDgzNDcx", "number": 839, "title": "Spark: Add tests for identity partition data", "bodyText": "This adds missing tests that that identity-partition data values are joined correctly to rows when reading data in Spark. This doesn't test cases where physical columns are missing or partitions based on nested fields.", "createdAt": "2020-03-12T21:22:13Z", "url": "https://github.com/apache/iceberg/pull/839", "merged": true, "mergeCommit": {"oid": "6cafdab76864685d5489ba03cd041865b5b57f01"}, "closed": true, "closedAt": "2020-03-19T19:50:57Z", "author": {"login": "rdblue"}, "timelineItems": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcNCb1xgH2gAyMzg3NDgzNDcxOjRhZDVlOTE1YmQ5NjNiYWE1NDYzOGY5YjIyNzlmZmIwMzdjMjM2NDg=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcPRY7xAFqTM3ODA0NDI3OQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "4ad5e915bd963baa54638f9b2279ffb037c23648", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/4ad5e915bd963baa54638f9b2279ffb037c23648", "committedDate": "2020-03-12T21:17:19Z", "message": "Add tests for identity partition data."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc0MTEwNTgw", "url": "https://github.com/apache/iceberg/pull/839#pullrequestreview-374110580", "createdAt": "2020-03-13T08:10:41Z", "commit": {"oid": "4ad5e915bd963baa54638f9b2279ffb037c23648"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QwODoxMDo0MlrOF16u-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xM1QwODoyNTowN1rOF17EAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA4MTE0Nw==", "bodyText": "nit: looks like the var is assigned but never used ?", "url": "https://github.com/apache/iceberg/pull/839#discussion_r392081147", "createdAt": "2020-03-13T08:10:42Z", "author": {"login": "openinx"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestIdentityPartitionData  {\n+  private static final Configuration CONF = new Configuration();\n+  private static final HadoopTables TABLES = new HadoopTables(CONF);\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { \"parquet\" },\n+        new Object[] { \"avro\" }\n+    };\n+  }\n+\n+  private final String format;\n+\n+  public TestIdentityPartitionData(String format) {\n+    this.format = format;\n+  }\n+\n+  private static SparkSession spark = null;\n+  private static JavaSparkContext sc = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ad5e915bd963baa54638f9b2279ffb037c23648"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA4NjUzMQ==", "bodyText": "nit: I think the testFullProjection is a special case of testProjections , would merge them together ?", "url": "https://github.com/apache/iceberg/pull/839#discussion_r392086531", "createdAt": "2020-03-13T08:25:07Z", "author": {"login": "openinx"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestIdentityPartitionData  {\n+  private static final Configuration CONF = new Configuration();\n+  private static final HadoopTables TABLES = new HadoopTables(CONF);\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { \"parquet\" },\n+        new Object[] { \"avro\" }\n+    };\n+  }\n+\n+  private final String format;\n+\n+  public TestIdentityPartitionData(String format) {\n+    this.format = format;\n+  }\n+\n+  private static SparkSession spark = null;\n+  private static JavaSparkContext sc = null;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestIdentityPartitionData.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n+    TestIdentityPartitionData.sc = new JavaSparkContext(spark.sparkContext());\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestIdentityPartitionData.spark;\n+    TestIdentityPartitionData.spark = null;\n+    TestIdentityPartitionData.sc = null;\n+    currentSpark.stop();\n+  }\n+\n+  private static final Schema LOG_SCHEMA = new Schema(\n+      Types.NestedField.optional(1, \"id\", Types.IntegerType.get()),\n+      Types.NestedField.optional(2, \"date\", Types.StringType.get()),\n+      Types.NestedField.optional(3, \"level\", Types.StringType.get()),\n+      Types.NestedField.optional(4, \"message\", Types.StringType.get())\n+  );\n+\n+  private static final List<LogMessage> LOGS = ImmutableList.of(\n+      LogMessage.debug(\"2020-02-02\", \"debug event 1\"),\n+      LogMessage.info(\"2020-02-02\", \"info event 1\"),\n+      LogMessage.debug(\"2020-02-02\", \"debug event 2\"),\n+      LogMessage.info(\"2020-02-03\", \"info event 2\"),\n+      LogMessage.debug(\"2020-02-03\", \"debug event 3\"),\n+      LogMessage.info(\"2020-02-03\", \"info event 3\"),\n+      LogMessage.error(\"2020-02-03\", \"error event 1\"),\n+      LogMessage.debug(\"2020-02-04\", \"debug event 4\"),\n+      LogMessage.warn(\"2020-02-04\", \"warn event 1\"),\n+      LogMessage.debug(\"2020-02-04\", \"debug event 5\")\n+  );\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private PartitionSpec spec = PartitionSpec.builderFor(LOG_SCHEMA).identity(\"date\").identity(\"level\").build();\n+  private Table table = null;\n+  private Dataset<Row> logs = null;\n+\n+  @Before\n+  public void setupTable() throws Exception {\n+    File location = temp.newFolder(\"logs\");\n+    Assert.assertTrue(\"Temp folder should exist\", location.exists());\n+\n+    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);\n+    this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n+    this.logs = spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n+\n+    logs.write().format(\"iceberg\").mode(\"append\").save(location.toString());\n+  }\n+\n+  @Test\n+  public void testFullProjection() {\n+    List<Row> expected = logs.orderBy(\"id\").collectAsList();\n+    List<Row> actual = spark.read().format(\"iceberg\").load(table.location()).orderBy(\"id\").collectAsList();\n+    Assert.assertEquals(\"Rows should match\", expected, actual);\n+  }\n+\n+  @Test\n+  public void testProjections() {\n+    String[][] cases = new String[][] {\n+        // individual fields\n+        new String[] { \"date\" },\n+        new String[] { \"level\" },\n+        new String[] { \"message\" },\n+        // field pairs\n+        new String[] { \"date\", \"message\" },\n+        new String[] { \"level\", \"message\" },\n+        new String[] { \"date\", \"level\" },\n+        // out-of-order pairs\n+        new String[] { \"message\", \"date\" },\n+        new String[] { \"message\", \"level\" },\n+        new String[] { \"level\", \"date\" },\n+        // full projection, different orderings\n+        new String[] { \"date\", \"level\", \"message\" },\n+        new String[] { \"level\", \"date\", \"message\" },\n+        new String[] { \"date\", \"message\", \"level\" },\n+        new String[] { \"level\", \"message\", \"date\" },\n+        new String[] { \"message\", \"date\", \"level\" },\n+        new String[] { \"message\", \"level\", \"date\" }\n+    };\n+\n+    for (String[] ordering : cases) {\n+      List<Row> expected = logs.select(\"id\", ordering).orderBy(\"id\").collectAsList();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ad5e915bd963baa54638f9b2279ffb037c23648"}, "originalPosition": 156}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc3ODA3MTAz", "url": "https://github.com/apache/iceberg/pull/839#pullrequestreview-377807103", "createdAt": "2020-03-19T15:03:05Z", "commit": {"oid": "4ad5e915bd963baa54638f9b2279ffb037c23648"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNTowMzowNVrOF4yp5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQxNTowMzowNVrOF4yp5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5NDUwMQ==", "bodyText": "Do you want to include ORC as well?", "url": "https://github.com/apache/iceberg/pull/839#discussion_r395094501", "createdAt": "2020-03-19T15:03:05Z", "author": {"login": "rdsr"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestIdentityPartitionData  {\n+  private static final Configuration CONF = new Configuration();\n+  private static final HadoopTables TABLES = new HadoopTables(CONF);\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { \"parquet\" },\n+        new Object[] { \"avro\" }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4ad5e915bd963baa54638f9b2279ffb037c23648"}, "originalPosition": 58}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "159a87921fd41e01de6dc90a5ee1c251e1118a31", "author": {"user": {"login": "rdblue", "name": "Ryan Blue"}}, "url": "https://github.com/apache/iceberg/commit/159a87921fd41e01de6dc90a5ee1c251e1118a31", "committedDate": "2020-03-19T17:53:17Z", "message": "Fix tests."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3Mzc4MDQ0Mjc5", "url": "https://github.com/apache/iceberg/pull/839#pullrequestreview-378044279", "createdAt": "2020-03-19T19:50:34Z", "commit": {"oid": "159a87921fd41e01de6dc90a5ee1c251e1118a31"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4711, "cost": 1, "resetAt": "2021-10-29T19:57:52Z"}}}