{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDkzNDU1NTIw", "number": 1514, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMTo0NDozOVrOEoRRaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMTo0NDozOVrOEoRRaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwNjYxNDgwOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseDataReader.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMTo0NDozOVrOHZQwfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMTo1ODo1NFrOHZRTGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI1MTAwNg==", "bodyText": "There is no equals and hashCode implementation for CharSequence, so it can't be used reliably as a map key to deduplicate.\nAlso, the only thing you need from the file instance is the key metadata, so you could keep a map of string path to key metadata, like this:\n    Map<String, ByteBuffer> keyMetadata = Maps.newHashMap();\n    task.files().stream()\n        .flatMap(fileScanTask -> Stream.concat(Stream.of(fileScanTask.file()), fileScanTask.deletes().stream()))\n        .forEach(file -> keyMetadata.put(file.path().toString(), file.keyMetadata()));\n    Stream<EncryptedInputFile> encrypted = keyMetadata.entrySet().stream()\n        .map(entry -> EncryptedFiles.encryptedInput(io.newInputFile(entry.getKey()), entry.getValue()));", "url": "https://github.com/apache/iceberg/pull/1514#discussion_r496251006", "createdAt": "2020-09-28T21:44:39Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseDataReader.java", "diffHunk": "@@ -58,9 +60,12 @@\n \n   BaseDataReader(CombinedScanTask task, FileIO io, EncryptionManager encryptionManager) {\n     this.tasks = task.files().iterator();\n-    Stream<EncryptedInputFile> encrypted = task.files().stream()\n-        .flatMap(fileScanTask -> Stream.concat(Stream.of(fileScanTask.file()), fileScanTask.deletes().stream()))\n-        .map(file -> EncryptedFiles.encryptedInput(io.newInputFile(file.path().toString()), file.keyMetadata()));\n+    Map<CharSequence, DeleteFile> uniqueDeleteFileMap = Maps.newHashMap();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2fa9ac689d8776526a0d754a09878f5a0b668bd"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI1OTg2NA==", "bodyText": "I actually started with deduping both DataFile and DeleteFile, but thought of keeping it just for DeleteFile, to not touch other workflows. will follow your recommendation. Thanks!", "url": "https://github.com/apache/iceberg/pull/1514#discussion_r496259864", "createdAt": "2020-09-28T21:58:54Z", "author": {"login": "mehtaashish23"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseDataReader.java", "diffHunk": "@@ -58,9 +60,12 @@\n \n   BaseDataReader(CombinedScanTask task, FileIO io, EncryptionManager encryptionManager) {\n     this.tasks = task.files().iterator();\n-    Stream<EncryptedInputFile> encrypted = task.files().stream()\n-        .flatMap(fileScanTask -> Stream.concat(Stream.of(fileScanTask.file()), fileScanTask.deletes().stream()))\n-        .map(file -> EncryptedFiles.encryptedInput(io.newInputFile(file.path().toString()), file.keyMetadata()));\n+    Map<CharSequence, DeleteFile> uniqueDeleteFileMap = Maps.newHashMap();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI1MTAwNg=="}, "originalCommit": {"oid": "e2fa9ac689d8776526a0d754a09878f5a0b668bd"}, "originalPosition": 23}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3539, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}