{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzg0NDUzNjk1", "number": 828, "reviewThreads": {"totalCount": 83, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxOTo1NDo0N1rODl7IqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQyMzozOTo0NVrOEFNo3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMDkyNzc3OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxOTo1NDo0N1rOFzFmEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxOTo1NDo0N1rOFzFmEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTExMzM2Mw==", "bodyText": "Why convert directly from Arrow to Spark? Shouldn't this make guarantees about Arrow storage fields that are used for a given Iceberg type instead?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389113363", "createdAt": "2020-03-06T19:54:47Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.arrow;\n+\n+import org.apache.arrow.memory.RootAllocator;\n+import org.apache.arrow.vector.types.DateUnit;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.TimeUnit;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.DecimalType;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class ArrowUtils {\n+\n+  private static ArrowUtils instance;\n+  private RootAllocator rootAllocator;\n+\n+  private ArrowUtils() {\n+    rootAllocator = new RootAllocator(Long.MAX_VALUE);\n+  }\n+\n+  public static ArrowUtils instance() {\n+    if (instance == null) {\n+      instance = new ArrowUtils();\n+    }\n+    return instance;\n+  }\n+\n+  public RootAllocator rootAllocator() {\n+    return rootAllocator;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  public DataType fromArrowType(ArrowType data) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMDkzMDQ3OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxOTo1NTo0N1rOFzFnuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQxOTo1NTo0N1rOFzFnuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTExMzc4Nw==", "bodyText": "bSize is batch size? If so, batchSize is more clear.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389113787", "createdAt": "2020-03-06T19:55:47Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+  private final int batchSize;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers, int bSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMDk0NDg2OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDowMDo1NVrOFzFwuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDowMDo1NVrOFzFwuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTExNjA5MQ==", "bodyText": "Should there be more checking that numRows is valid? It should be the same for all vectors, right?\nWhat happens when all of the result vectors are null? Looks like this will set the batch length to 0, but I think we want to set it to something valid.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389116091", "createdAt": "2020-03-06T20:00:55Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+  private final int batchSize;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers, int bSize) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());\n+    int idx = 0;\n+    for (VectorizedReader reader : readers) {\n+      this.readers[idx] = (VectorizedArrowReader) reader;\n+      idx++;\n+    }\n+    this.batchSize = bSize;\n+  }\n+\n+  @Override\n+  public final void setRowGroupInfo(PageReadStore pageStore, Map<ColumnPath, ColumnChunkMetaData> metaData) {\n+    for (int i = 0; i < readers.length; i += 1) {\n+      if (readers[i] != null) {\n+        readers[i].setRowGroupInfo(pageStore, metaData);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void reuseContainers(boolean reuse) {\n+    for (VectorizedReader reader : readers) {\n+      reader.reuseContainers(reuse);\n+    }\n+  }\n+\n+  @Override\n+  public final ColumnarBatch read(int numValsToRead) {\n+    ColumnVector[] arrowColumnVectors = new ColumnVector[readers.length];\n+    int numRows = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMDk0OTg0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDowMjo0NlrOFzFz0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDowMjo0NlrOFzFz0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTExNjg4Mw==", "bodyText": "You might want to move this into a factory method for IcebergArrowColumnVector instead of embedding it here. This logic looks more related to how column vectors work than to the batch. Here, you could just call IcebergArrowColumnVector.forHolder(holder) and that would return either the null vector or a real one.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389116883", "createdAt": "2020-03-06T20:02:46Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+  private final int batchSize;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers, int bSize) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());\n+    int idx = 0;\n+    for (VectorizedReader reader : readers) {\n+      this.readers[idx] = (VectorizedArrowReader) reader;\n+      idx++;\n+    }\n+    this.batchSize = bSize;\n+  }\n+\n+  @Override\n+  public final void setRowGroupInfo(PageReadStore pageStore, Map<ColumnPath, ColumnChunkMetaData> metaData) {\n+    for (int i = 0; i < readers.length; i += 1) {\n+      if (readers[i] != null) {\n+        readers[i].setRowGroupInfo(pageStore, metaData);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void reuseContainers(boolean reuse) {\n+    for (VectorizedReader reader : readers) {\n+      reader.reuseContainers(reuse);\n+    }\n+  }\n+\n+  @Override\n+  public final ColumnarBatch read(int numValsToRead) {\n+    ColumnVector[] arrowColumnVectors = new ColumnVector[readers.length];\n+    int numRows = 0;\n+    for (int i = 0; i < readers.length; i += 1) {\n+      VectorHolder holder = readers[i].read(numValsToRead);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMDk1MDU3OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDowMzowMlrOFzF0Sw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDowMzowMlrOFzF0Sw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTExNzAwMw==", "bodyText": "Nit: no need for this empty line.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389117003", "createdAt": "2020-03-06T20:03:02Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMDk1NjM3OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDowNToxNFrOFzF3-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDowNToxNFrOFzF3-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTExNzk0NA==", "bodyText": "The convention for singletons used elsewhere is to create a static field named INSTANCE and name this method get:\npublic class SomeSingleton {\n  private static final SomeSingleton INSTANCE = new SomeSingleton();\n  public static SomeSingleton get() {\n    return INSTANCE;\n  }\n}", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389117944", "createdAt": "2020-03-06T20:05:14Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.arrow;\n+\n+import org.apache.arrow.memory.RootAllocator;\n+import org.apache.arrow.vector.types.DateUnit;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.TimeUnit;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.DecimalType;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class ArrowUtils {\n+\n+  private static ArrowUtils instance;\n+  private RootAllocator rootAllocator;\n+\n+  private ArrowUtils() {\n+    rootAllocator = new RootAllocator(Long.MAX_VALUE);\n+  }\n+\n+  public static ArrowUtils instance() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMDk2NDAyOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDowNzo1MFrOFzF8dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDowNzo1MFrOFzF8dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTExOTA5NQ==", "bodyText": "The data type methods could be static instead. Could you break this class into SparkArrowTypeUtil and something like ArrowAllocation?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389119095", "createdAt": "2020-03-06T20:07:50Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowUtils.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.arrow;\n+\n+import org.apache.arrow.memory.RootAllocator;\n+import org.apache.arrow.vector.types.DateUnit;\n+import org.apache.arrow.vector.types.FloatingPointPrecision;\n+import org.apache.arrow.vector.types.TimeUnit;\n+import org.apache.arrow.vector.types.pojo.ArrowType;\n+import org.apache.arrow.vector.types.pojo.Field;\n+import org.apache.spark.sql.types.ArrayType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.DecimalType;\n+import org.apache.spark.sql.types.Metadata;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+\n+public class ArrowUtils {\n+\n+  private static ArrowUtils instance;\n+  private RootAllocator rootAllocator;\n+\n+  private ArrowUtils() {\n+    rootAllocator = new RootAllocator(Long.MAX_VALUE);\n+  }\n+\n+  public static ArrowUtils instance() {\n+    if (instance == null) {\n+      instance = new ArrowUtils();\n+    }\n+    return instance;\n+  }\n+\n+  public RootAllocator rootAllocator() {\n+    return rootAllocator;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  public DataType fromArrowType(ArrowType data) {\n+\n+    if (data instanceof ArrowType.Bool) {\n+      return DataTypes.BooleanType;\n+    } else if (data instanceof ArrowType.Int) {\n+      ArrowType.Int intData = (ArrowType.Int) data;\n+      if (intData.getIsSigned() && intData.getBitWidth() == 8) {\n+        return DataTypes.ByteType;\n+      } else if (intData.getIsSigned() && intData.getBitWidth() == 8 * 2) {\n+        return DataTypes.ShortType;\n+      } else if (intData.getIsSigned() && intData.getBitWidth() == 8 * 4) {\n+        return DataTypes.IntegerType;\n+      } else if (intData.getIsSigned() && intData.getBitWidth() == 8 * 8) {\n+        return DataTypes.LongType;\n+      }\n+    } else if (data instanceof ArrowType.FloatingPoint) {\n+      ArrowType.FloatingPoint floatData = (ArrowType.FloatingPoint) data;\n+      if (floatData.getPrecision() == FloatingPointPrecision.SINGLE) {\n+        return DataTypes.FloatType;\n+      } else if (floatData.getPrecision() == FloatingPointPrecision.DOUBLE) {\n+        return DataTypes.DoubleType;\n+      }\n+    } else if (data instanceof ArrowType.Utf8) {\n+      return DataTypes.StringType;\n+    } else if (data instanceof ArrowType.Binary) {\n+      return DataTypes.BinaryType;\n+    } else if (data instanceof ArrowType.Decimal) {\n+      ArrowType.Decimal decimalData = (ArrowType.Decimal) data;\n+      return new DecimalType(decimalData.getPrecision(), decimalData.getScale());\n+    } else if (data instanceof ArrowType.Date && ((ArrowType.Date) data).getUnit() == DateUnit.DAY) {\n+      return DataTypes.DateType;\n+    } else if (data instanceof ArrowType.Timestamp && ((ArrowType.Timestamp) data).getUnit() == TimeUnit.MICROSECOND) {\n+      return DataTypes.TimestampType;\n+    }\n+\n+    throw new UnsupportedOperationException(\"Unsupported data type: \" + data);\n+  }\n+\n+  public DataType fromArrowField(Field field) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 95}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMDk3MTE4OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoxMDoyOVrOFzGA9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQyMzoxOTozN1rOF0kXCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyMDI0NA==", "bodyText": "I thought we supported maps?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389120244", "createdAt": "2020-03-06T20:10:29Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY2NTk5Mg==", "bodyText": "Not currently, no.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r390665992", "createdAt": "2020-03-10T23:19:37Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyMDI0NA=="}, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMDk3NTE0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoxMTo0MlrOFzGDTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoxMTo0MlrOFzGDTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyMDg0Ng==", "bodyText": "Doesn't Spark check nullability and only call these methods if isNullAt is false? I thought that was why it isn't necessary to check this in the primitive methods, like getLong.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389120846", "createdAt": "2020-03-06T20:11:42Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 155}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMDk4NzYzOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoxNjozMVrOFzGLKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoxNjozMVrOFzGLKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyMjg1OA==", "bodyText": "I think this should be moved out of this method. There's no need for this method to have side-effects, and this makes it hard to understand how this class works because childColumns appears to be uninitialized when reading the constructor. I'd much rather add childColumns() to the ArrowVectorAccessor interface and initialize like this:\n  this.accessor = VectorAccessors.get(vector);\n  this.childColumns = accessor.childColumns();\n\nYou could also remove the childColumns field and always call accessor.childColumns().", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389122858", "createdAt": "2020-03-06T20:16:31Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private ArrowVectorAccessor getVectorAccessor(ColumnDescriptor desc, ValueVector vector) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector);\n+          case INT32:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof TinyIntVector) {\n+        return new ByteAccessor((TinyIntVector) vector);\n+      } else if (vector instanceof SmallIntVector) {\n+        return new ShortAccessor((SmallIntVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        ArrowVectorAccessor structAccessor = new StructAccessor(structVector);\n+        childColumns = new ArrowColumnVector[structVector.size()];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 339}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMDk5NzM5OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoyMDoyNlrOFzGRXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoyMDoyNlrOFzGRXw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyNDQ0Nw==", "bodyText": "It would be nice to have an error message with the type that was accessed.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389124447", "createdAt": "2020-03-06T20:20:26Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 199}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTAwODgxOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoyNDozNlrOFzGYgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoyNDozNlrOFzGYgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyNjI3NQ==", "bodyText": "Can we move these to a separate file? The convention used elsewhere is to put ArrowVectorAccessor in a file and the implementations in a ArrowVectorAccessors class with a private constructor.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389126275", "createdAt": "2020-03-06T20:24:36Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 182}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTAxMTA3OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoyNToyN1rOFzGZ5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoyNToyN1rOFzGZ5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyNjYzMQ==", "bodyText": "Isn't the use of final in Arrow causing us trouble? Why use final here? Does it have a performance benefit?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389126631", "createdAt": "2020-03-06T20:25:27Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 190}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTAxNDIxOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoyNjozM1rOFzGb2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoyNjozM1rOFzGb2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyNzEzMQ==", "bodyText": "ArrowVectorAccessor doesn't define getStruct. Do you mean getChild?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389127131", "createdAt": "2020-03-06T20:26:33Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private ArrowVectorAccessor getVectorAccessor(ColumnDescriptor desc, ValueVector vector) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector);\n+          case INT32:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof TinyIntVector) {\n+        return new ByteAccessor((TinyIntVector) vector);\n+      } else if (vector instanceof SmallIntVector) {\n+        return new ShortAccessor((SmallIntVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        ArrowVectorAccessor structAccessor = new StructAccessor(structVector);\n+        childColumns = new ArrowColumnVector[structVector.size()];\n+        for (int i = 0; i < childColumns.length; ++i) {\n+          childColumns[i] = new ArrowColumnVector(structVector.getVectorById(i));\n+        }\n+        return structAccessor;\n+      }\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  private class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private class ByteAccessor extends ArrowVectorAccessor {\n+\n+    private final TinyIntVector vector;\n+\n+    ByteAccessor(TinyIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte getByte(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class ShortAccessor extends ArrowVectorAccessor {\n+\n+    private final SmallIntVector vector;\n+\n+    ShortAccessor(SmallIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final short getShort(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryIntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryIntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return dictionary.decodeToInt(vector.get(rowId));\n+    }\n+  }\n+\n+  private class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryLongAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return dictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryFloatAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return dictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryDoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryDoubleAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return dictionary.decodeToDouble(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DecimalAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.DecimalArrowVector vector;\n+\n+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      return Decimal.apply(vector.getObject(rowId), precision, scale);\n+    }\n+  }\n+\n+  private class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.VarcharArrowVector vector;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      vector.get(rowId, stringResult);\n+      if (stringResult.isSet == 0) {\n+        return null;\n+      } else {\n+        return UTF8String.fromAddress(\n+            null,\n+            stringResult.buffer.memoryAddress() + stringResult.start,\n+            stringResult.end - stringResult.start);\n+      }\n+    }\n+  }\n+\n+  private class DictionaryStringAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryStringAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      Binary binary = dictionary.decodeToBinary(vector.get(rowId));\n+      return UTF8String.fromBytes(binary.getBytesUnsafe());\n+    }\n+  }\n+\n+  private class FixedSizeBinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final FixedSizeBinaryVector vector;\n+\n+    FixedSizeBinaryAccessor(FixedSizeBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private class BinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final VarBinaryVector vector;\n+\n+    BinaryAccessor(VarBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private class DictionaryBinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryBinaryAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      Binary binary = dictionary.decodeToBinary(vector.get(rowId));\n+      return binary.getBytesUnsafe();\n+    }\n+  }\n+\n+  private class DateAccessor extends ArrowVectorAccessor {\n+\n+    private final DateDayVector vector;\n+\n+    DateAccessor(DateDayVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryDateAccessor extends DictionaryIntAccessor {\n+    DictionaryDateAccessor(IntVector vector) {\n+      super(vector);\n+    }\n+  }\n+\n+  private class TimestampAccessor extends ArrowVectorAccessor {\n+\n+    private final TimeStampMicroTZVector vector;\n+\n+    TimestampAccessor(TimeStampMicroTZVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryTimestampAccessor extends DictionaryLongAccessor {\n+    DictionaryTimestampAccessor(IntVector vector) {\n+      super(vector);\n+    }\n+  }\n+\n+  private class ArrayAccessor extends ArrowVectorAccessor {\n+\n+    private final ListVector vector;\n+    private final ArrowColumnVector arrayData;\n+\n+    ArrayAccessor(ListVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+      this.arrayData = new ArrowColumnVector(vector.getDataVector());\n+    }\n+\n+    @Override\n+    final ColumnarArray getArray(int rowId) {\n+      ArrowBuf offsets = vector.getOffsetBuffer();\n+      int index = rowId * ListVector.OFFSET_WIDTH;\n+      int start = offsets.getInt(index);\n+      int end = offsets.getInt(index + ListVector.OFFSET_WIDTH);\n+      return new ColumnarArray(arrayData, start, end - start);\n+    }\n+  }\n+\n+  /**\n+   * Any call to \"get\" method will throw UnsupportedOperationException.\n+   * <p>\n+   * Access struct values in a ArrowColumnVector doesn't use this vector. Instead, it uses getStruct() method defined in\n+   * the parent class. Any call to \"get\" method in this class is a bug in the code.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 688}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTAxNzUxOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoyNzo0NlrOFzGd8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoyNzo0NlrOFzGd8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyNzY2NA==", "bodyText": "I think these should process the dictionary first to transform each value to a Decimal and then use that table instead of the dictionary at read time.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389127664", "createdAt": "2020-03-06T20:27:46Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private ArrowVectorAccessor getVectorAccessor(ColumnDescriptor desc, ValueVector vector) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector);\n+          case INT32:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof TinyIntVector) {\n+        return new ByteAccessor((TinyIntVector) vector);\n+      } else if (vector instanceof SmallIntVector) {\n+        return new ShortAccessor((SmallIntVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        ArrowVectorAccessor structAccessor = new StructAccessor(structVector);\n+        childColumns = new ArrowColumnVector[structVector.size()];\n+        for (int i = 0; i < childColumns.length; ++i) {\n+          childColumns[i] = new ArrowColumnVector(structVector.getVectorById(i));\n+        }\n+        return structAccessor;\n+      }\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  private class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private class ByteAccessor extends ArrowVectorAccessor {\n+\n+    private final TinyIntVector vector;\n+\n+    ByteAccessor(TinyIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte getByte(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class ShortAccessor extends ArrowVectorAccessor {\n+\n+    private final SmallIntVector vector;\n+\n+    ShortAccessor(SmallIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final short getShort(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryIntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryIntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return dictionary.decodeToInt(vector.get(rowId));\n+    }\n+  }\n+\n+  private class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryLongAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return dictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryFloatAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return dictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryDoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryDoubleAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return dictionary.decodeToDouble(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DecimalAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.DecimalArrowVector vector;\n+\n+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      return Decimal.apply(vector.getObject(rowId), precision, scale);\n+    }\n+  }\n+\n+  private class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.VarcharArrowVector vector;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      vector.get(rowId, stringResult);\n+      if (stringResult.isSet == 0) {\n+        return null;\n+      } else {\n+        return UTF8String.fromAddress(\n+            null,\n+            stringResult.buffer.memoryAddress() + stringResult.start,\n+            stringResult.end - stringResult.start);\n+      }\n+    }\n+  }\n+\n+  private class DictionaryStringAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryStringAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      Binary binary = dictionary.decodeToBinary(vector.get(rowId));\n+      return UTF8String.fromBytes(binary.getBytesUnsafe());\n+    }\n+  }\n+\n+  private class FixedSizeBinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final FixedSizeBinaryVector vector;\n+\n+    FixedSizeBinaryAccessor(FixedSizeBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private class BinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final VarBinaryVector vector;\n+\n+    BinaryAccessor(VarBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private class DictionaryBinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryBinaryAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      Binary binary = dictionary.decodeToBinary(vector.get(rowId));\n+      return binary.getBytesUnsafe();\n+    }\n+  }\n+\n+  private class DateAccessor extends ArrowVectorAccessor {\n+\n+    private final DateDayVector vector;\n+\n+    DateAccessor(DateDayVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryDateAccessor extends DictionaryIntAccessor {\n+    DictionaryDateAccessor(IntVector vector) {\n+      super(vector);\n+    }\n+  }\n+\n+  private class TimestampAccessor extends ArrowVectorAccessor {\n+\n+    private final TimeStampMicroTZVector vector;\n+\n+    TimestampAccessor(TimeStampMicroTZVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryTimestampAccessor extends DictionaryLongAccessor {\n+    DictionaryTimestampAccessor(IntVector vector) {\n+      super(vector);\n+    }\n+  }\n+\n+  private class ArrayAccessor extends ArrowVectorAccessor {\n+\n+    private final ListVector vector;\n+    private final ArrowColumnVector arrayData;\n+\n+    ArrayAccessor(ListVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+      this.arrayData = new ArrowColumnVector(vector.getDataVector());\n+    }\n+\n+    @Override\n+    final ColumnarArray getArray(int rowId) {\n+      ArrowBuf offsets = vector.getOffsetBuffer();\n+      int index = rowId * ListVector.OFFSET_WIDTH;\n+      int start = offsets.getInt(index);\n+      int end = offsets.getInt(index + ListVector.OFFSET_WIDTH);\n+      return new ColumnarArray(arrayData, start, end - start);\n+    }\n+  }\n+\n+  /**\n+   * Any call to \"get\" method will throw UnsupportedOperationException.\n+   * <p>\n+   * Access struct values in a ArrowColumnVector doesn't use this vector. Instead, it uses getStruct() method defined in\n+   * the parent class. Any call to \"get\" method in this class is a bug in the code.\n+   */\n+  private class StructAccessor extends ArrowVectorAccessor {\n+\n+    StructAccessor(StructVector vector) {\n+      super(vector);\n+    }\n+  }\n+\n+  private class DictionaryDecimalBinaryAccessor extends ArrowVectorAccessor {\n+    private final IntVector vector;\n+\n+    DictionaryDecimalBinaryAccessor(IntVector vector, int precision, int scale) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    //TODO: still need to evaluate if this is the most efficient way\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      Binary value = dictionary.decodeToBinary(vector.get(rowId));\n+      BigInteger unscaledValue = new BigInteger(value.getBytesUnsafe());\n+      return Decimal.apply(unscaledValue.longValue(), precision, scale);\n+    }\n+  }\n+\n+  private class DictionaryDecimalLongAccessor extends ArrowVectorAccessor {\n+    private final IntVector vector;\n+\n+    DictionaryDecimalLongAccessor(IntVector vector, int precision, int scale) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    //TODO: still need to evaluate if this is the most efficient way\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      long unscaledValue = dictionary.decodeToLong(vector.get(rowId));\n+      return Decimal.apply(unscaledValue, precision, scale);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 732}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTAyMTgxOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoyOToyNlrOFzGg0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoyOToyNlrOFzGg0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyODQwMA==", "bodyText": "Nit: the indentation here was correct before.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389128400", "createdAt": "2020-03-06T20:29:26Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -91,20 +64,16 @@\n import org.apache.spark.sql.sources.v2.reader.SupportsPushDownFilters;\n import org.apache.spark.sql.sources.v2.reader.SupportsPushDownRequiredColumns;\n import org.apache.spark.sql.sources.v2.reader.SupportsReportStatistics;\n-import org.apache.spark.sql.types.BinaryType;\n+import org.apache.spark.sql.sources.v2.reader.SupportsScanColumnarBatch;\n import org.apache.spark.sql.types.DataType;\n-import org.apache.spark.sql.types.Decimal;\n-import org.apache.spark.sql.types.DecimalType;\n-import org.apache.spark.sql.types.StringType;\n import org.apache.spark.sql.types.StructField;\n import org.apache.spark.sql.types.StructType;\n-import org.apache.spark.unsafe.types.UTF8String;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n-import scala.collection.JavaConverters;\n \n-class Reader implements DataSourceReader, SupportsPushDownFilters, SupportsPushDownRequiredColumns,\n-    SupportsReportStatistics {\n+class Reader implements DataSourceReader, SupportsScanColumnarBatch, SupportsPushDownFilters,\n+        SupportsPushDownRequiredColumns, SupportsReportStatistics {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTAyMjE3OnYy", "diffSide": "LEFT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoyOTozNVrOFzGhCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoyOTozNVrOFzGhCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyODQ1OQ==", "bodyText": "Nit: no need to remove this line.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389128459", "createdAt": "2020-03-06T20:29:35Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -158,11 +129,19 @@\n     } else {\n       this.localityPreferred = false;\n     }\n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTAyNTk0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozMToyMFrOFzGjjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozMToyMFrOFzGjjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyOTEwMQ==", "bodyText": "Options that enable/disable should use \"enabled\". That avoids users needing to think about whether options enable or disable - true always enables and false always disables.\nI recommend using iceberg.read.parquet-vectorization.enabled.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389129101", "createdAt": "2020-03-06T20:31:20Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -158,11 +129,19 @@\n     } else {\n       this.localityPreferred = false;\n     }\n-\n     this.schema = table.schema();\n     this.io = io;\n     this.encryptionManager = encryptionManager;\n     this.caseSensitive = caseSensitive;\n+    // override logic to check when batched reads is enabled by turning off batched reads\n+    boolean disableBatchedReads =\n+            options.get(\"iceberg.read.disablevectorizedreads\").map(Boolean::parseBoolean).orElse(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 122}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTAyODAwOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozMjoxN1rOFzGk4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozMjoxN1rOFzGk4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyOTQ0Mg==", "bodyText": "How about iceberg.read.parquet-vectorization.batch-size?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389129442", "createdAt": "2020-03-06T20:32:17Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -158,11 +129,19 @@\n     } else {\n       this.localityPreferred = false;\n     }\n-\n     this.schema = table.schema();\n     this.io = io;\n     this.encryptionManager = encryptionManager;\n     this.caseSensitive = caseSensitive;\n+    // override logic to check when batched reads is enabled by turning off batched reads\n+    boolean disableBatchedReads =\n+            options.get(\"iceberg.read.disablevectorizedreads\").map(Boolean::parseBoolean).orElse(false);\n+    if (disableBatchedReads) {\n+      enableBatchRead = Boolean.FALSE;\n+    }\n+    Optional<String> numRecordsPerBatchOpt = options.get(\"iceberg.read.numrecordsperbatch\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 126}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTAzMzA0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozNDoxN1rOFzGoHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozNDoxN1rOFzGoHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMDI2OQ==", "bodyText": "Javadoc: Instead of [...], did you mean to use either {@link ...} to link to the class, or {@code ...} to use fixed-width font?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389130269", "createdAt": "2020-03-06T20:34:17Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -188,6 +167,27 @@ public StructType readSchema() {\n     return lazyType();\n   }\n \n+  /**\n+   * This is called in the Spark Driver when data is to be materialized into [ColumnarBatch]s", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 137}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTAzNTU5OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/InternalRowTaskDataReader.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozNToyNlrOFzGpxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOFQyMTo1NToyMFrOF4YQMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMDY5Mg==", "bodyText": "Can we move this refactor into a separate PR?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389130692", "createdAt": "2020-03-06T20:35:26Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/InternalRowTaskDataReader.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.SparkAvroReader;\n+import org.apache.iceberg.spark.data.SparkOrcReader;\n+import org.apache.iceberg.spark.data.SparkParquetReaders;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.catalyst.expressions.JoinedRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n+import org.apache.spark.sql.types.BinaryType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.types.DecimalType;\n+import org.apache.spark.sql.types.StringType;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.unsafe.types.UTF8String;\n+import scala.collection.JavaConverters;\n+\n+class InternalRowTaskDataReader extends BaseTaskDataReader<InternalRow> implements InputPartitionReader<InternalRow> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDU3NjExMg==", "bodyText": "I am going to leave these changes in this PR and start a new PR for the refactor. Once the refactor PR is approved, I will merge those changes into this PR.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r394576112", "createdAt": "2020-03-18T19:03:47Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/InternalRowTaskDataReader.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.SparkAvroReader;\n+import org.apache.iceberg.spark.data.SparkOrcReader;\n+import org.apache.iceberg.spark.data.SparkParquetReaders;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.catalyst.expressions.JoinedRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n+import org.apache.spark.sql.types.BinaryType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.types.DecimalType;\n+import org.apache.spark.sql.types.StringType;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.unsafe.types.UTF8String;\n+import scala.collection.JavaConverters;\n+\n+class InternalRowTaskDataReader extends BaseTaskDataReader<InternalRow> implements InputPartitionReader<InternalRow> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMDY5Mg=="}, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDY2MTkzOQ==", "bodyText": "Opened #853", "url": "https://github.com/apache/iceberg/pull/828#discussion_r394661939", "createdAt": "2020-03-18T21:55:20Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/InternalRowTaskDataReader.java", "diffHunk": "@@ -0,0 +1,296 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.SparkAvroReader;\n+import org.apache.iceberg.spark.data.SparkOrcReader;\n+import org.apache.iceberg.spark.data.SparkParquetReaders;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.catalyst.expressions.JoinedRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n+import org.apache.spark.sql.types.BinaryType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.types.DecimalType;\n+import org.apache.spark.sql.types.StringType;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.unsafe.types.UTF8String;\n+import scala.collection.JavaConverters;\n+\n+class InternalRowTaskDataReader extends BaseTaskDataReader<InternalRow> implements InputPartitionReader<InternalRow> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMDY5Mg=="}, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTAzODU2OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/ColumnarBatchTaskDataReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozNjoyNlrOFzGriw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozNjoyNlrOFzGriw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTE0Nw==", "bodyText": "It would be nice to make the names shorter. What about BaseTaskReader, BatchTaskReader, and RowTaskReader?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389131147", "createdAt": "2020-03-06T20:36:26Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/ColumnarBatchTaskDataReader.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Iterator;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+class ColumnarBatchTaskDataReader extends BaseTaskDataReader<ColumnarBatch>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTA0MDY1OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozNzowOFrOFzGszw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozNzowOFrOFzGszw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTQ3MQ==", "bodyText": "Nit: Can you remove =>?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389131471", "createdAt": "2020-03-06T20:37:08Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -188,6 +167,27 @@ public StructType readSchema() {\n     return lazyType();\n   }\n \n+  /**\n+   * This is called in the Spark Driver when data is to be materialized into [ColumnarBatch]s\n+   */\n+  @Override\n+  public List<InputPartition<ColumnarBatch>> planBatchInputPartitions() {\n+    Preconditions.checkState(enableBatchRead != null && enableBatchRead, \"Batched reads not enabled\");\n+    Preconditions.checkState(batchSize > 0, \"Invalid batch size\");\n+    String tableSchemaString = SchemaParser.toJson(table.schema());\n+    String expectedSchemaString = SchemaParser.toJson(lazySchema());\n+\n+    List<InputPartition<ColumnarBatch>> readTasks = Lists.newArrayList();\n+    for (CombinedScanTask task : tasks()) {\n+      readTasks.add(\n+              new ColumnarBatchReadTask(task, tableSchemaString, expectedSchemaString,\n+                      io, encryptionManager, caseSensitive, batchSize));\n+    }\n+    LOG.info(\"=> Batching input partitions with {} tasks.\", readTasks.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTA0Mjc5OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozNzo1M1rOFzGuJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDo1MjozMFrOGHgoKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTgxNQ==", "bodyText": "Why would enableBatchRead be null? Why not use a boolean instead?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389131815", "createdAt": "2020-03-06T20:37:53Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -188,6 +167,27 @@ public StructType readSchema() {\n     return lazyType();\n   }\n \n+  /**\n+   * This is called in the Spark Driver when data is to be materialized into [ColumnarBatch]s\n+   */\n+  @Override\n+  public List<InputPartition<ColumnarBatch>> planBatchInputPartitions() {\n+    Preconditions.checkState(enableBatchRead != null && enableBatchRead, \"Batched reads not enabled\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYzODE2OQ==", "bodyText": "enableBatchRead is of type Boolean. It is lazily initialized in enableBatchRead().", "url": "https://github.com/apache/iceberg/pull/828#discussion_r390638169", "createdAt": "2020-03-10T22:05:22Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -188,6 +167,27 @@ public StructType readSchema() {\n     return lazyType();\n   }\n \n+  /**\n+   * This is called in the Spark Driver when data is to be materialized into [ColumnarBatch]s\n+   */\n+  @Override\n+  public List<InputPartition<ColumnarBatch>> planBatchInputPartitions() {\n+    Preconditions.checkState(enableBatchRead != null && enableBatchRead, \"Batched reads not enabled\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTgxNQ=="}, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNzc4Nw==", "bodyText": "Accessing the enableBatchRead that is lazily computed creates an order dependency between this and enableBatchRead(). I'd rather simplify this and use the method call directly.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410527787", "createdAt": "2020-04-18T00:52:30Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -188,6 +167,27 @@ public StructType readSchema() {\n     return lazyType();\n   }\n \n+  /**\n+   * This is called in the Spark Driver when data is to be materialized into [ColumnarBatch]s\n+   */\n+  @Override\n+  public List<InputPartition<ColumnarBatch>> planBatchInputPartitions() {\n+    Preconditions.checkState(enableBatchRead != null && enableBatchRead, \"Batched reads not enabled\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTgxNQ=="}, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 141}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTA0NDc2OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozODozNVrOFzGvZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozODozNVrOFzGvZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMjEzNQ==", "bodyText": "Style: control flow statements should be followed by empty lines.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389132135", "createdAt": "2020-03-06T20:38:35Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -259,6 +259,43 @@ public Statistics estimateStatistics() {\n     return new Stats(sizeInBytes, numRows);\n   }\n \n+  @Override\n+  public boolean enableBatchRead() {\n+    return lazyCheckEnableBatchRead();\n+  }\n+\n+  private boolean lazyCheckEnableBatchRead() {\n+    boolean allParquetFiles =\n+            tasks().stream()\n+                    .allMatch(combinedScanTask -> combinedScanTask.files()\n+                            .stream()\n+                            .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n+                                    FileFormat.PARQUET)));\n+    if (!allParquetFiles) {\n+      this.enableBatchRead = false;\n+      return false;\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 188}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTA0OTc4OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDo0MDozMlrOFzGyiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDo0MDozMlrOFzGyiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMjkzNw==", "bodyText": "Style: We typically indent method args from the same location. That would be either like it was before, or moving all of the method args to use the 4-space continuation indent.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389132937", "createdAt": "2020-03-06T20:40:32Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -305,61 +342,74 @@ public Statistics estimateStatistics() {\n   @Override\n   public String toString() {\n     return String.format(\n-        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive);\n+        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead);\n   }\n \n-  private static class ReadTask implements InputPartition<InternalRow>, Serializable {\n-    private final CombinedScanTask task;\n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+  private static class BaseReadTask implements Serializable {\n+    final CombinedScanTask task;\n     private final String tableSchemaString;\n     private final String expectedSchemaString;\n-    private final Broadcast<FileIO> io;\n-    private final Broadcast<EncryptionManager> encryptionManager;\n-    private final boolean caseSensitive;\n-    private final boolean localityPreferred;\n+    final Broadcast<FileIO> io;\n+    final Broadcast<EncryptionManager> encryptionManager;\n+    final boolean caseSensitive;\n \n     private transient Schema tableSchema = null;\n     private transient Schema expectedSchema = null;\n-    private transient String[] preferredLocations;\n \n-    private ReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-                     Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-                     boolean caseSensitive, boolean localityPreferred) {\n+    BaseReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n+        Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 246}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTA1MTczOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDo0MToxNlrOFzGzsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDo0MToxNlrOFzGzsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMzIzNA==", "bodyText": "Nit: unnecessary newline.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389133234", "createdAt": "2020-03-06T20:41:16Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -305,61 +342,74 @@ public Statistics estimateStatistics() {\n   @Override\n   public String toString() {\n     return String.format(\n-        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive);\n+        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead);\n   }\n \n-  private static class ReadTask implements InputPartition<InternalRow>, Serializable {\n-    private final CombinedScanTask task;\n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+  private static class BaseReadTask implements Serializable {\n+    final CombinedScanTask task;\n     private final String tableSchemaString;\n     private final String expectedSchemaString;\n-    private final Broadcast<FileIO> io;\n-    private final Broadcast<EncryptionManager> encryptionManager;\n-    private final boolean caseSensitive;\n-    private final boolean localityPreferred;\n+    final Broadcast<FileIO> io;\n+    final Broadcast<EncryptionManager> encryptionManager;\n+    final boolean caseSensitive;\n \n     private transient Schema tableSchema = null;\n     private transient Schema expectedSchema = null;\n-    private transient String[] preferredLocations;\n \n-    private ReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-                     Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-                     boolean caseSensitive, boolean localityPreferred) {\n+    BaseReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n+        Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n+        boolean caseSensitive) {\n       this.task = task;\n       this.tableSchemaString = tableSchemaString;\n       this.expectedSchemaString = expectedSchemaString;\n       this.io = io;\n       this.encryptionManager = encryptionManager;\n       this.caseSensitive = caseSensitive;\n-      this.localityPreferred = localityPreferred;\n-      this.preferredLocations = getPreferredLocations();\n     }\n \n-    @Override\n-    public InputPartitionReader<InternalRow> createPartitionReader() {\n-      return new TaskDataReader(task, lazyTableSchema(), lazyExpectedSchema(), io.value(),\n-        encryptionManager.value(), caseSensitive);\n-    }\n-\n-    @Override\n-    public String[] preferredLocations() {\n-      return preferredLocations;\n-    }\n-\n-    private Schema lazyTableSchema() {\n+    Schema lazyTableSchema() {\n       if (tableSchema == null) {\n         this.tableSchema = SchemaParser.fromJson(tableSchemaString);\n       }\n       return tableSchema;\n     }\n \n-    private Schema lazyExpectedSchema() {\n+    Schema lazyExpectedSchema() {\n       if (expectedSchema == null) {\n         this.expectedSchema = SchemaParser.fromJson(expectedSchemaString);\n       }\n       return expectedSchema;\n     }\n \n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 285}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTA1NTQxOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDo0MjozM1rOFzG17A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDo0MjozM1rOFzG17A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMzgwNA==", "bodyText": "Nit: extra newline.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389133804", "createdAt": "2020-03-06T20:42:33Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -699,4 +486,5 @@ public int size() {\n       throw new UnsupportedOperationException(\"Not implemented: set\");\n     }\n   }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 619}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTA1NzExOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDo0MzowOVrOFzG24w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDo0MzowOVrOFzG24w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzNDA1MQ==", "bodyText": "Why doesn't this support preferred locations?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389134051", "createdAt": "2020-03-06T20:43:09Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -383,286 +433,23 @@ private Schema lazyExpectedSchema() {\n     }\n   }\n \n-  private static class TaskDataReader implements InputPartitionReader<InternalRow> {\n-    // for some reason, the apply method can't be called from Java without reflection\n-    private static final DynMethods.UnboundMethod APPLY_PROJECTION = DynMethods.builder(\"apply\")\n-        .impl(UnsafeProjection.class, InternalRow.class)\n-        .build();\n-\n-    private final Iterator<FileScanTask> tasks;\n-    private final Schema tableSchema;\n-    private final Schema expectedSchema;\n-    private final FileIO fileIo;\n-    private final Map<String, InputFile> inputFiles;\n-    private final boolean caseSensitive;\n-\n-    private Iterator<InternalRow> currentIterator = null;\n-    private Closeable currentCloseable = null;\n-    private InternalRow current = null;\n-\n-    TaskDataReader(CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n-                   EncryptionManager encryptionManager, boolean caseSensitive) {\n-      this.fileIo = fileIo;\n-      this.tasks = task.files().iterator();\n-      this.tableSchema = tableSchema;\n-      this.expectedSchema = expectedSchema;\n-      Iterable<InputFile> decryptedFiles = encryptionManager.decrypt(Iterables.transform(task.files(),\n-          fileScanTask ->\n-              EncryptedFiles.encryptedInput(\n-                  this.fileIo.newInputFile(fileScanTask.file().path().toString()),\n-                  fileScanTask.file().keyMetadata())));\n-      ImmutableMap.Builder<String, InputFile> inputFileBuilder = ImmutableMap.builder();\n-      decryptedFiles.forEach(decrypted -> inputFileBuilder.put(decrypted.location(), decrypted));\n-      this.inputFiles = inputFileBuilder.build();\n-      // open last because the schemas and fileIo must be set\n-      this.currentIterator = open(tasks.next());\n-      this.caseSensitive = caseSensitive;\n-    }\n+  /**\n+   * Organizes input data into [InputPartition]s for Vectorized [ColumnarBatch] reads\n+   */\n+  private static class ColumnarBatchReadTask extends BaseReadTask implements InputPartition<ColumnarBatch> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 357}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTA2MDc4OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDo0NDoyMFrOFzG5CA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDo0NDoyMFrOFzG5CA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzNDYwMA==", "bodyText": "This should be enableBatchRead().", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389134600", "createdAt": "2020-03-06T20:44:20Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -305,61 +342,74 @@ public Statistics estimateStatistics() {\n   @Override\n   public String toString() {\n     return String.format(\n-        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive);\n+        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 220}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTA3ODQ3OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDo1MTowMVrOFzHEJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQyMzoyNjo0MVrOF0kgRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzNzQ0NA==", "bodyText": "Looks like the only differences between the batch and row tasks is the locality and batch size. Locality should probably be supported by both. If that's done, then do we need 3 classes? What about using one and using batchSize=1 for row-based tasks? Then we'd be able to have just one class for both cases.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389137444", "createdAt": "2020-03-06T20:51:01Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -305,61 +342,74 @@ public Statistics estimateStatistics() {\n   @Override\n   public String toString() {\n     return String.format(\n-        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive);\n+        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead);\n   }\n \n-  private static class ReadTask implements InputPartition<InternalRow>, Serializable {\n-    private final CombinedScanTask task;\n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+  private static class BaseReadTask implements Serializable {\n+    final CombinedScanTask task;\n     private final String tableSchemaString;\n     private final String expectedSchemaString;\n-    private final Broadcast<FileIO> io;\n-    private final Broadcast<EncryptionManager> encryptionManager;\n-    private final boolean caseSensitive;\n-    private final boolean localityPreferred;\n+    final Broadcast<FileIO> io;\n+    final Broadcast<EncryptionManager> encryptionManager;\n+    final boolean caseSensitive;\n \n     private transient Schema tableSchema = null;\n     private transient Schema expectedSchema = null;\n-    private transient String[] preferredLocations;\n \n-    private ReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-                     Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-                     boolean caseSensitive, boolean localityPreferred) {\n+    BaseReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 245}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDYzNTAyNA==", "bodyText": "I think 3 classes are still needed since we create different InputPartitionReader for each version.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r390635024", "createdAt": "2020-03-10T21:57:11Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -305,61 +342,74 @@ public Statistics estimateStatistics() {\n   @Override\n   public String toString() {\n     return String.format(\n-        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive);\n+        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead);\n   }\n \n-  private static class ReadTask implements InputPartition<InternalRow>, Serializable {\n-    private final CombinedScanTask task;\n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+  private static class BaseReadTask implements Serializable {\n+    final CombinedScanTask task;\n     private final String tableSchemaString;\n     private final String expectedSchemaString;\n-    private final Broadcast<FileIO> io;\n-    private final Broadcast<EncryptionManager> encryptionManager;\n-    private final boolean caseSensitive;\n-    private final boolean localityPreferred;\n+    final Broadcast<FileIO> io;\n+    final Broadcast<EncryptionManager> encryptionManager;\n+    final boolean caseSensitive;\n \n     private transient Schema tableSchema = null;\n     private transient Schema expectedSchema = null;\n-    private transient String[] preferredLocations;\n \n-    private ReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-                     Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-                     boolean caseSensitive, boolean localityPreferred) {\n+    BaseReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzNzQ0NA=="}, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 245}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY2ODM1OA==", "bodyText": "You're right. That was changed in a later version of the API. What about using anonymous classes and an abstract BaseReadTask? That might be clean, but up to you.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r390668358", "createdAt": "2020-03-10T23:26:41Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -305,61 +342,74 @@ public Statistics estimateStatistics() {\n   @Override\n   public String toString() {\n     return String.format(\n-        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive);\n+        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead);\n   }\n \n-  private static class ReadTask implements InputPartition<InternalRow>, Serializable {\n-    private final CombinedScanTask task;\n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+  private static class BaseReadTask implements Serializable {\n+    final CombinedScanTask task;\n     private final String tableSchemaString;\n     private final String expectedSchemaString;\n-    private final Broadcast<FileIO> io;\n-    private final Broadcast<EncryptionManager> encryptionManager;\n-    private final boolean caseSensitive;\n-    private final boolean localityPreferred;\n+    final Broadcast<FileIO> io;\n+    final Broadcast<EncryptionManager> encryptionManager;\n+    final boolean caseSensitive;\n \n     private transient Schema tableSchema = null;\n     private transient Schema expectedSchema = null;\n-    private transient String[] preferredLocations;\n \n-    private ReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,\n-                     Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-                     boolean caseSensitive, boolean localityPreferred) {\n+    BaseReadTask(CombinedScanTask task, String tableSchemaString, String expectedSchemaString,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzNzQ0NA=="}, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 245}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTMyNDAyOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMjozMjo0NFrOFzJbUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xOVQyMTo0NjoxNFrOF5BgVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE3NjE0NA==", "bodyText": "Could this implement VectorHolder and get passed into the read method, rather than allocating a VectorHolder and copying its contents?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389176144", "createdAt": "2020-03-06T22:32:44Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTMzNzgxNQ==", "bodyText": "Not sure I follow, @rdblue.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r395337815", "createdAt": "2020-03-19T21:46:14Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE3NjE0NA=="}, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTMyNjQ3OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMjozMzo1MVrOFzJc6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMjozMzo1MVrOFzJc6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE3NjU1Mw==", "bodyText": "Is this necessary if we always eagerly decode ints?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389176553", "createdAt": "2020-03-06T22:33:51Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private ArrowVectorAccessor getVectorAccessor(ColumnDescriptor desc, ValueVector vector) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector);\n+          case INT32:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof TinyIntVector) {\n+        return new ByteAccessor((TinyIntVector) vector);\n+      } else if (vector instanceof SmallIntVector) {\n+        return new ShortAccessor((SmallIntVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        ArrowVectorAccessor structAccessor = new StructAccessor(structVector);\n+        childColumns = new ArrowColumnVector[structVector.size()];\n+        for (int i = 0; i < childColumns.length; ++i) {\n+          childColumns[i] = new ArrowColumnVector(structVector.getVectorById(i));\n+        }\n+        return structAccessor;\n+      }\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  private class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private class ByteAccessor extends ArrowVectorAccessor {\n+\n+    private final TinyIntVector vector;\n+\n+    ByteAccessor(TinyIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte getByte(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class ShortAccessor extends ArrowVectorAccessor {\n+\n+    private final SmallIntVector vector;\n+\n+    ShortAccessor(SmallIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final short getShort(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryIntAccessor extends ArrowVectorAccessor {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 409}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTMzNjU5OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMjozODo0NFrOFzJjFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQyMzozMzo0OFrOF0kojg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE3ODEzMg==", "bodyText": "Have we tested using IntVector instead of DateDayVector?\nIt would simplify the decoding logic to always use an IntVector, and I'm not sure what the benefit of using a DateDayVector is. Isn't the underlying buffer identical?\nSame logic applies for TimeStampMicroTZVector and LongVector.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389178132", "createdAt": "2020-03-06T22:38:44Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private ArrowVectorAccessor getVectorAccessor(ColumnDescriptor desc, ValueVector vector) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector);\n+          case INT32:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof TinyIntVector) {\n+        return new ByteAccessor((TinyIntVector) vector);\n+      } else if (vector instanceof SmallIntVector) {\n+        return new ShortAccessor((SmallIntVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        ArrowVectorAccessor structAccessor = new StructAccessor(structVector);\n+        childColumns = new ArrowColumnVector[structVector.size()];\n+        for (int i = 0; i < childColumns.length; ++i) {\n+          childColumns[i] = new ArrowColumnVector(structVector.getVectorById(i));\n+        }\n+        return structAccessor;\n+      }\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  private class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private class ByteAccessor extends ArrowVectorAccessor {\n+\n+    private final TinyIntVector vector;\n+\n+    ByteAccessor(TinyIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte getByte(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class ShortAccessor extends ArrowVectorAccessor {\n+\n+    private final SmallIntVector vector;\n+\n+    ShortAccessor(SmallIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final short getShort(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryIntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryIntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return dictionary.decodeToInt(vector.get(rowId));\n+    }\n+  }\n+\n+  private class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryLongAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return dictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryFloatAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return dictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryDoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryDoubleAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return dictionary.decodeToDouble(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DecimalAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.DecimalArrowVector vector;\n+\n+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      return Decimal.apply(vector.getObject(rowId), precision, scale);\n+    }\n+  }\n+\n+  private class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.VarcharArrowVector vector;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      vector.get(rowId, stringResult);\n+      if (stringResult.isSet == 0) {\n+        return null;\n+      } else {\n+        return UTF8String.fromAddress(\n+            null,\n+            stringResult.buffer.memoryAddress() + stringResult.start,\n+            stringResult.end - stringResult.start);\n+      }\n+    }\n+  }\n+\n+  private class DictionaryStringAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryStringAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      Binary binary = dictionary.decodeToBinary(vector.get(rowId));\n+      return UTF8String.fromBytes(binary.getBytesUnsafe());\n+    }\n+  }\n+\n+  private class FixedSizeBinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final FixedSizeBinaryVector vector;\n+\n+    FixedSizeBinaryAccessor(FixedSizeBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private class BinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final VarBinaryVector vector;\n+\n+    BinaryAccessor(VarBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private class DictionaryBinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryBinaryAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      Binary binary = dictionary.decodeToBinary(vector.get(rowId));\n+      return binary.getBytesUnsafe();\n+    }\n+  }\n+\n+  private class DateAccessor extends ArrowVectorAccessor {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 621}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY3MDQ3OA==", "bodyText": "Filed #834 to follow up on this.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r390670478", "createdAt": "2020-03-10T23:33:48Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private ArrowVectorAccessor getVectorAccessor(ColumnDescriptor desc, ValueVector vector) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector);\n+          case INT32:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof TinyIntVector) {\n+        return new ByteAccessor((TinyIntVector) vector);\n+      } else if (vector instanceof SmallIntVector) {\n+        return new ShortAccessor((SmallIntVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        ArrowVectorAccessor structAccessor = new StructAccessor(structVector);\n+        childColumns = new ArrowColumnVector[structVector.size()];\n+        for (int i = 0; i < childColumns.length; ++i) {\n+          childColumns[i] = new ArrowColumnVector(structVector.getVectorById(i));\n+        }\n+        return structAccessor;\n+      }\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  private class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private class ByteAccessor extends ArrowVectorAccessor {\n+\n+    private final TinyIntVector vector;\n+\n+    ByteAccessor(TinyIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte getByte(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class ShortAccessor extends ArrowVectorAccessor {\n+\n+    private final SmallIntVector vector;\n+\n+    ShortAccessor(SmallIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final short getShort(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryIntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryIntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return dictionary.decodeToInt(vector.get(rowId));\n+    }\n+  }\n+\n+  private class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryLongAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return dictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryFloatAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return dictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryDoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryDoubleAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return dictionary.decodeToDouble(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DecimalAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.DecimalArrowVector vector;\n+\n+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      return Decimal.apply(vector.getObject(rowId), precision, scale);\n+    }\n+  }\n+\n+  private class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.VarcharArrowVector vector;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      vector.get(rowId, stringResult);\n+      if (stringResult.isSet == 0) {\n+        return null;\n+      } else {\n+        return UTF8String.fromAddress(\n+            null,\n+            stringResult.buffer.memoryAddress() + stringResult.start,\n+            stringResult.end - stringResult.start);\n+      }\n+    }\n+  }\n+\n+  private class DictionaryStringAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryStringAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      Binary binary = dictionary.decodeToBinary(vector.get(rowId));\n+      return UTF8String.fromBytes(binary.getBytesUnsafe());\n+    }\n+  }\n+\n+  private class FixedSizeBinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final FixedSizeBinaryVector vector;\n+\n+    FixedSizeBinaryAccessor(FixedSizeBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private class BinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final VarBinaryVector vector;\n+\n+    BinaryAccessor(VarBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private class DictionaryBinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryBinaryAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      Binary binary = dictionary.decodeToBinary(vector.get(rowId));\n+      return binary.getBytesUnsafe();\n+    }\n+  }\n+\n+  private class DateAccessor extends ArrowVectorAccessor {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE3ODEzMg=="}, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 621}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTM0ODkxOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMjo0NDo1NFrOFzJqnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMFQyMzozODo1OFrOF0kueg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE4MDA2MA==", "bodyText": "Does NullableVarCharHolder make a copy?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389180060", "createdAt": "2020-03-06T22:44:54Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private ArrowVectorAccessor getVectorAccessor(ColumnDescriptor desc, ValueVector vector) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector);\n+          case INT32:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof TinyIntVector) {\n+        return new ByteAccessor((TinyIntVector) vector);\n+      } else if (vector instanceof SmallIntVector) {\n+        return new ShortAccessor((SmallIntVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        ArrowVectorAccessor structAccessor = new StructAccessor(structVector);\n+        childColumns = new ArrowColumnVector[structVector.size()];\n+        for (int i = 0; i < childColumns.length; ++i) {\n+          childColumns[i] = new ArrowColumnVector(structVector.getVectorById(i));\n+        }\n+        return structAccessor;\n+      }\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  private class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private class ByteAccessor extends ArrowVectorAccessor {\n+\n+    private final TinyIntVector vector;\n+\n+    ByteAccessor(TinyIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte getByte(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class ShortAccessor extends ArrowVectorAccessor {\n+\n+    private final SmallIntVector vector;\n+\n+    ShortAccessor(SmallIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final short getShort(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryIntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryIntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return dictionary.decodeToInt(vector.get(rowId));\n+    }\n+  }\n+\n+  private class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryLongAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return dictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryFloatAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return dictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryDoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryDoubleAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return dictionary.decodeToDouble(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DecimalAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.DecimalArrowVector vector;\n+\n+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      return Decimal.apply(vector.getObject(rowId), precision, scale);\n+    }\n+  }\n+\n+  private class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.VarcharArrowVector vector;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      vector.get(rowId, stringResult);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 544}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDY3MTk5NA==", "bodyText": "No, it uses the underlying buffer and sets the start and end positions to get the variable width data out of it.\nhttps://github.com/apache/arrow/blob/maint-0.14.x/java/vector/src/main/java/org/apache/arrow/vector/VarCharVector.java#L134", "url": "https://github.com/apache/iceberg/pull/828#discussion_r390671994", "createdAt": "2020-03-10T23:38:58Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private ArrowVectorAccessor getVectorAccessor(ColumnDescriptor desc, ValueVector vector) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector);\n+          case INT32:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof TinyIntVector) {\n+        return new ByteAccessor((TinyIntVector) vector);\n+      } else if (vector instanceof SmallIntVector) {\n+        return new ShortAccessor((SmallIntVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        ArrowVectorAccessor structAccessor = new StructAccessor(structVector);\n+        childColumns = new ArrowColumnVector[structVector.size()];\n+        for (int i = 0; i < childColumns.length; ++i) {\n+          childColumns[i] = new ArrowColumnVector(structVector.getVectorById(i));\n+        }\n+        return structAccessor;\n+      }\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  private class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private class ByteAccessor extends ArrowVectorAccessor {\n+\n+    private final TinyIntVector vector;\n+\n+    ByteAccessor(TinyIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte getByte(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class ShortAccessor extends ArrowVectorAccessor {\n+\n+    private final SmallIntVector vector;\n+\n+    ShortAccessor(SmallIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final short getShort(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryIntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryIntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return dictionary.decodeToInt(vector.get(rowId));\n+    }\n+  }\n+\n+  private class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryLongAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return dictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryFloatAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return dictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryDoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryDoubleAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return dictionary.decodeToDouble(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DecimalAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.DecimalArrowVector vector;\n+\n+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      return Decimal.apply(vector.getObject(rowId), precision, scale);\n+    }\n+  }\n+\n+  private class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.VarcharArrowVector vector;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      vector.get(rowId, stringResult);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE4MDA2MA=="}, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 544}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTM1NzMwOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMjo0ODo0N1rOFzJvoA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMjo0ODo0N1rOFzJvoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE4MTM0NA==", "bodyText": "This looks concerning to me. This variant of UTF8String uses Java's Unsafe to work with bytes underneath an object. By setting this to null, this is using an absolute address, which I assume should be off-heap. Does our allocator use off-heap memory?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r389181344", "createdAt": "2020-03-06T22:48:47Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,753 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.SmallIntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.TinyIntVector;\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.arrow.ArrowUtils;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.DecimalMetadata;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+  private final Dictionary dictionary;\n+  private final boolean isVectorDictEncoded;\n+  private ArrowColumnVector[] childColumns;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(ArrowUtils.instance().fromArrowField(holder.vector().getField()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.dictionary = holder.dictionary();\n+    this.isVectorDictEncoded = holder.isDictionaryEncoded();\n+    this.accessor = getVectorAccessor(holder.descriptor(), holder.vector());\n+  }\n+\n+  @Override\n+  public void close() {\n+    if (childColumns != null) {\n+      for (int i = 0; i < childColumns.length; i++) {\n+        childColumns[i].close();\n+        childColumns[i] = null;\n+      }\n+      childColumns = null;\n+    }\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    return accessor.getByte(rowId);\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    return accessor.getShort(rowId);\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    return childColumns[ordinal];\n+  }\n+\n+  private abstract class ArrowVectorAccessor {\n+\n+    private final ValueVector vector;\n+\n+    ArrowVectorAccessor(ValueVector vector) {\n+      this.vector = vector;\n+    }\n+\n+    final boolean isNullAt(int rowId) {\n+      return nullabilityHolder.isNullAt(rowId) == 1;\n+    }\n+\n+    final void close() {\n+      vector.close();\n+    }\n+\n+    boolean getBoolean(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte getByte(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    short getShort(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    int getInt(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    long getLong(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    float getFloat(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    double getDouble(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    Decimal getDecimal(int rowId, int precision, int scale) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    UTF8String getUTF8String(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    byte[] getBinary(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+\n+    ColumnarArray getArray(int rowId) {\n+      throw new UnsupportedOperationException();\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private ArrowVectorAccessor getVectorAccessor(ColumnDescriptor desc, ValueVector vector) {\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DECIMAL:\n+            DecimalMetadata decimal = primitive.getDecimalMetadata();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    decimal.getPrecision(),\n+                    decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector);\n+          case INT32:\n+            return new DictionaryIntAccessor((IntVector) vector);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof TinyIntVector) {\n+        return new ByteAccessor((TinyIntVector) vector);\n+      } else if (vector instanceof SmallIntVector) {\n+        return new ShortAccessor((SmallIntVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        ArrowVectorAccessor structAccessor = new StructAccessor(structVector);\n+        childColumns = new ArrowColumnVector[structVector.size()];\n+        for (int i = 0; i < childColumns.length; ++i) {\n+          childColumns[i] = new ArrowColumnVector(structVector.getVectorById(i));\n+        }\n+        return structAccessor;\n+      }\n+    }\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  private class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private class ByteAccessor extends ArrowVectorAccessor {\n+\n+    private final TinyIntVector vector;\n+\n+    ByteAccessor(TinyIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte getByte(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class ShortAccessor extends ArrowVectorAccessor {\n+\n+    private final SmallIntVector vector;\n+\n+    ShortAccessor(SmallIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final short getShort(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryIntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryIntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return dictionary.decodeToInt(vector.get(rowId));\n+    }\n+  }\n+\n+  private class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryLongAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return dictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryFloatAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return dictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private class DictionaryDoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryDoubleAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return dictionary.decodeToDouble(vector.get(rowId));\n+    }\n+  }\n+\n+  private class DecimalAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.DecimalArrowVector vector;\n+\n+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      if (isNullAt(rowId)) {\n+        return null;\n+      }\n+      return Decimal.apply(vector.getObject(rowId), precision, scale);\n+    }\n+  }\n+\n+  private class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.VarcharArrowVector vector;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      vector.get(rowId, stringResult);\n+      if (stringResult.isSet == 0) {\n+        return null;\n+      } else {\n+        return UTF8String.fromAddress(\n+            null,\n+            stringResult.buffer.memoryAddress() + stringResult.start,\n+            stringResult.end - stringResult.start);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07dd61a5ff849116b8f7234cad9a90fc5fbc2743"}, "originalPosition": 551}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODc1NDUxOnYy", "diffSide": "RIGHT", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMzowMTozNFrOGHfZTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMzowMTozNFrOGHfZTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUwNzU5Nw==", "bodyText": "Nit: indentation is off.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410507597", "createdAt": "2020-04-17T23:01:34Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java", "diffHunk": "@@ -111,7 +111,8 @@ void readBatchOfDictionaryEncodedLongs(FieldVector vector, int startOffset, int\n   }\n \n   void readBatchOfDictionaryEncodedTimestampMillis(FieldVector vector, int startOffset, int numValuesToRead,\n-                                                   Dictionary dict, NullabilityHolder nullabilityHolder) {\n+                                                   Dictionary dict, NullabilityHolder nullabilityHolder,\n+      int typeWidth) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODc3MDMzOnYy", "diffSide": "RIGHT", "path": "spark/src/jmh/java/org/apache/iceberg/spark/source/IcebergSourceBenchmark.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMzoxMjoxNlrOGHfiwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQwODo1ODoxNVrOGRJ5XA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMDAxNw==", "bodyText": "Does this make Spark start faster? Should we use it in other tests?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410510017", "createdAt": "2020-04-17T23:12:16Z", "author": {"login": "rdblue"}, "path": "spark/src/jmh/java/org/apache/iceberg/spark/source/IcebergSourceBenchmark.java", "diffHunk": "@@ -92,15 +93,24 @@ protected void cleanupFiles() throws IOException {\n     }\n   }\n \n-  protected void setupSpark() {\n-    spark = SparkSession.builder()\n-        .config(\"spark.ui.enabled\", false)\n-        .master(\"local\")\n-        .getOrCreate();\n+  protected void setupSpark(boolean enableDictionaryEncoding) {\n+    SparkSession.Builder builder = SparkSession.builder()\n+            .config(\"spark.ui.enabled\", false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDY0MTExNg==", "bodyText": "I am actually not sure. But it does look like a lot of tests within Spark turn off the UI.\nFor ex- https://github.com/apache/spark/blob/a222644e1df907d0aba19634a166e146dfb4f551/core/src/test/scala/org/apache/spark/deploy/SparkSubmitSuite.scala#L268", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420641116", "createdAt": "2020-05-06T08:58:15Z", "author": {"login": "samarthjain"}, "path": "spark/src/jmh/java/org/apache/iceberg/spark/source/IcebergSourceBenchmark.java", "diffHunk": "@@ -92,15 +93,24 @@ protected void cleanupFiles() throws IOException {\n     }\n   }\n \n-  protected void setupSpark() {\n-    spark = SparkSession.builder()\n-        .config(\"spark.ui.enabled\", false)\n-        .master(\"local\")\n-        .getOrCreate();\n+  protected void setupSpark(boolean enableDictionaryEncoding) {\n+    SparkSession.Builder builder = SparkSession.builder()\n+            .config(\"spark.ui.enabled\", false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMDAxNw=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODc3MzAwOnYy", "diffSide": "RIGHT", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetDefinitionLevelReader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMzoxMzo1N1rOGHfkVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQyMzo1NDowNFrOGOVFYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMDQyMg==", "bodyText": "What about the call in case PACKED just below? Does that also need to use the typeWidth?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410510422", "createdAt": "2020-04-17T23:13:57Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetDefinitionLevelReader.java", "diffHunk": "@@ -193,7 +193,7 @@ public void readBatchOfDictionaryEncodedLongs(\n         case RLE:\n           if (currentValue == maxDefLevel) {\n             dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedLongs(vector,\n-                idx, numValues, dict, nullabilityHolder);\n+                idx, numValues, dict, nullabilityHolder, typeWidth);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3ODY4OQ==", "bodyText": "Good catch, looks like I missed it. Will fix here and other places.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r417678689", "createdAt": "2020-04-29T23:54:04Z", "author": {"login": "samarthjain"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedParquetDefinitionLevelReader.java", "diffHunk": "@@ -193,7 +193,7 @@ public void readBatchOfDictionaryEncodedLongs(\n         case RLE:\n           if (currentValue == maxDefLevel) {\n             dictionaryEncodedValuesReader.readBatchOfDictionaryEncodedLongs(vector,\n-                idx, numValues, dict, nullabilityHolder);\n+                idx, numValues, dict, nullabilityHolder, typeWidth);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMDQyMg=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODc3NjQ0OnYy", "diffSide": "RIGHT", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMzoxNjozM1rOGHfmWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQwODo1MjowNFrOGRJrnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMDkzOA==", "bodyText": "These changes look concerning. It looks like the old offset (only index) must not have been correct. If so, there are places where getDataBuffer().setLong(...) and similar methods are called but aren't updated like these. Are those cases bugs as well?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410510938", "createdAt": "2020-04-17T23:16:33Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java", "diffHunk": "@@ -72,7 +72,7 @@ void readBatchOfDictionaryIds(IntVector intVector, int startOffset, int numValue\n   }\n \n   void readBatchOfDictionaryEncodedLongs(FieldVector vector, int startOffset, int numValuesToRead, Dictionary dict,\n-                                         NullabilityHolder nullabilityHolder) {\n+                                         NullabilityHolder nullabilityHolder, int typeWidth) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3ODg3Mw==", "bodyText": "Good catch, looks like I missed it. Will fix here and other places.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r417678873", "createdAt": "2020-04-29T23:54:35Z", "author": {"login": "samarthjain"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java", "diffHunk": "@@ -72,7 +72,7 @@ void readBatchOfDictionaryIds(IntVector intVector, int startOffset, int numValue\n   }\n \n   void readBatchOfDictionaryEncodedLongs(FieldVector vector, int startOffset, int numValuesToRead, Dictionary dict,\n-                                         NullabilityHolder nullabilityHolder) {\n+                                         NullabilityHolder nullabilityHolder, int typeWidth) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMDkzOA=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYzNzU5Nw==", "bodyText": "Looks like the test TestSparkParquetFallbackToDictionaryEncodingForVectorizedReader wasn't adequately testing the fallback to plain encoding behavior for all the datatypes. Will tweak it.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420637597", "createdAt": "2020-05-06T08:52:04Z", "author": {"login": "samarthjain"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java", "diffHunk": "@@ -72,7 +72,7 @@ void readBatchOfDictionaryIds(IntVector intVector, int startOffset, int numValue\n   }\n \n   void readBatchOfDictionaryEncodedLongs(FieldVector vector, int startOffset, int numValuesToRead, Dictionary dict,\n-                                         NullabilityHolder nullabilityHolder) {\n+                                         NullabilityHolder nullabilityHolder, int typeWidth) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMDkzOA=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODc3NzE4OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowAllocation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMzoxNzoxMVrOGHfmzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yOVQyMzo1NjoyMlrOGOVIOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMTA1NQ==", "bodyText": "Why is this in the Spark package? Is it specific to Spark?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410511055", "createdAt": "2020-04-17T23:17:11Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowAllocation.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.arrow;\n+\n+import org.apache.arrow.memory.RootAllocator;\n+\n+public class ArrowAllocation {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNzY3OTQxNg==", "bodyText": "This can be moved to iceberg-arrow.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r417679416", "createdAt": "2020-04-29T23:56:22Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/arrow/ArrowAllocation.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.arrow;\n+\n+import org.apache.arrow.memory.RootAllocator;\n+\n+public class ArrowAllocation {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMTA1NQ=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODc4MDMzOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMzoxOToxMVrOGHfooA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMzozMzoxM1rOGHf0hg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMTUyMA==", "bodyText": "It looks like this could easily be refactored into separate methods: getDictionaryVectorAccessor(VectorHolder) and getPlainVectorAccessor(VectorHolder).", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410511520", "createdAt": "2020-04-17T23:19:11Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java", "diffHunk": "@@ -0,0 +1,495 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class ArrowVectorAccessors {\n+\n+  private ArrowVectorAccessors() {}\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  static ArrowVectorAccessor getVectorAccessor(VectorHolder holder) {\n+    Dictionary dictionary = holder.dictionary();\n+    boolean isVectorDictEncoded = holder.isDictionaryEncoded();\n+    ColumnDescriptor desc = holder.descriptor();\n+    FieldVector vector = holder.vector();\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxNDU2Ng==", "bodyText": "This would probably address the complexity problem above without suppressing it.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410514566", "createdAt": "2020-04-17T23:33:13Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java", "diffHunk": "@@ -0,0 +1,495 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class ArrowVectorAccessors {\n+\n+  private ArrowVectorAccessors() {}\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  static ArrowVectorAccessor getVectorAccessor(VectorHolder holder) {\n+    Dictionary dictionary = holder.dictionary();\n+    boolean isVectorDictEncoded = holder.isDictionaryEncoded();\n+    ColumnDescriptor desc = holder.descriptor();\n+    FieldVector vector = holder.vector();\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxMTUyMA=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODgxNjI2OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMzo0NjoxM1rOGHf-5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMzo0NjoxM1rOGHf-5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxNzIyMA==", "bodyText": "I think that the dictionary should be eagerly decoded, for a few reasons:\n\ngetBytesUnsafe will make a copy of the bytes when the binary is backed by a ByteBuffer or an array slice. It may also cache values and perform checks.\ngetBytesUnsafe should only be used when the bytes are immediately consumed, but UTF8String keeps a reference, so even if there isn't a copy this could result in a correctness problem\ndecodeToBinary should incur a dynamic dispatch cost (this also applies to the other dictionary readers)\n\nIt's fairly easy to eagerly decode:\n  private static class DictionaryStringAccessor extends ArrowVectorAccessor {\n    private IntVector offsetVector;\n    private UTF8String[] decodedDictionary;\n\n    DictionaryStringAccessor(IntVector vector, Dictionary dictionary) {\n      super(vector);\n      this.offsetVector = vector;\n      this.decodedDictionary = IntStream.rangeClosed(0, dictionary.getMaxId())\n          .mapToObj(dictionary::decodeToBinary)\n          .map(binary -> UTF8String.fromBytes(binary.getBytes()))\n          .toArray(UTF8String[]::new);\n    }\n\n    @Override\n    final UTF8String getUTF8String(int rowId) {\n      int offset = offsetVector.get(rowId);\n      return decodedDictionary[offset];\n    }\n  }\nAlso, there's no need for the DictionaryArrowVectorAccessor, since it only tracks one column that is also tracked by its parent.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410517220", "createdAt": "2020-04-17T23:46:13Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java", "diffHunk": "@@ -0,0 +1,495 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class ArrowVectorAccessors {\n+\n+  private ArrowVectorAccessors() {}\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  static ArrowVectorAccessor getVectorAccessor(VectorHolder holder) {\n+    Dictionary dictionary = holder.dictionary();\n+    boolean isVectorDictEncoded = holder.isDictionaryEncoded();\n+    ColumnDescriptor desc = holder.descriptor();\n+    FieldVector vector = holder.vector();\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector, dictionary);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+          case DECIMAL:\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector, dictionary);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector, dictionary);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector, dictionary);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        return new StructAccessor(structVector);\n+      }\n+    }\n+    throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+  }\n+\n+  private static class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private static class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryLongAccessor extends DictionaryArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return parquetDictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private static class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryFloatAccessor extends DictionaryArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return parquetDictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private static class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryDoubleAccessor extends DictionaryArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryDoubleAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return parquetDictionary.decodeToDouble(vector.get(rowId));\n+    }\n+  }\n+\n+  private static class DecimalAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.DecimalArrowVector vector;\n+\n+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      return Decimal.apply(vector.getObject(rowId), precision, scale);\n+    }\n+  }\n+\n+  private static class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.VarcharArrowVector vector;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      vector.get(rowId, stringResult);\n+      if (stringResult.isSet == 0) {\n+        return null;\n+      } else {\n+        return UTF8String.fromAddress(\n+            null,\n+            stringResult.buffer.memoryAddress() + stringResult.start,\n+            stringResult.end - stringResult.start);\n+      }\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+  private abstract static class DictionaryArrowVectorAccessor extends ArrowVectorAccessor {\n+    final Dictionary parquetDictionary;\n+    final IntVector dictionaryVector;\n+\n+    private DictionaryArrowVectorAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector);\n+      this.dictionaryVector = vector;\n+      this.parquetDictionary = dictionary;\n+    }\n+  }\n+\n+  private static class DictionaryStringAccessor extends DictionaryArrowVectorAccessor {\n+\n+    DictionaryStringAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 317}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODgyMjkxOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMzo1MjoyM1rOGHgC6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMzo1MjoyM1rOGHgC6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxODI0OA==", "bodyText": "Similar to the string dictionary accessor, I think we can avoid extra work and make this simpler by eagerly decoding here:\n  private static class DictionaryDoubleAccessor extends ArrowVectorAccessor {\n    private final IntVector vector;\n    private final double[] decodedDictionary;\n\n    DictionaryDoubleAccessor(IntVector vector, Dictionary dictionary) {\n      super(vector);\n      this.vector = vector;\n      this.decodedDictionary = IntStream.rangeClosed(0, dictionary.getMaxId())\n          .mapToDouble(dictionary::decodeToDouble)\n          .toArray();\n    }\n\n    @Override\n    final double getDouble(int rowId) {\n      return decodedDictionary[vector.get(rowId)];\n    }\n  }\nParquet decodes into a double[] as well, but this avoids dynamic dispatch costs for decodeToDouble and is easier to see what's happening.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410518248", "createdAt": "2020-04-17T23:52:23Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java", "diffHunk": "@@ -0,0 +1,495 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class ArrowVectorAccessors {\n+\n+  private ArrowVectorAccessors() {}\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  static ArrowVectorAccessor getVectorAccessor(VectorHolder holder) {\n+    Dictionary dictionary = holder.dictionary();\n+    boolean isVectorDictEncoded = holder.isDictionaryEncoded();\n+    ColumnDescriptor desc = holder.descriptor();\n+    FieldVector vector = holder.vector();\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector, dictionary);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+          case DECIMAL:\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector, dictionary);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector, dictionary);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector, dictionary);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        return new StructAccessor(structVector);\n+      }\n+    }\n+    throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+  }\n+\n+  private static class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private static class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryLongAccessor extends DictionaryArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return parquetDictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private static class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryFloatAccessor extends DictionaryArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return parquetDictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private static class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryDoubleAccessor extends DictionaryArrowVectorAccessor {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 248}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODgyODQwOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMzo1Njo1MVrOGHgGDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMzo1Njo1MVrOGHgGDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxOTA1Mw==", "bodyText": "Nit: why is the class name plural? Shouldn't it be ColumnarBatchReader?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410519053", "createdAt": "2020-04-17T23:56:51Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODgyOTE4OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xN1QyMzo1NzozMlrOGHgGfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQwNzoxNzoxMVrOGPDyeQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxOTE2NA==", "bodyText": "Why not new VectorizedArrowReader[readers.size()]?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410519164", "createdAt": "2020-04-17T23:57:32Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxOTU3Nw==", "bodyText": "Or better, this is where streams are really useful:\n    this.readers = readers.stream()\n        .map(VectorizedArrowReader.class::cast)\n        .toArray(VectorizedArrowReader[]::new);", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410519577", "createdAt": "2020-04-17T23:59:37Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxOTE2NA=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODQ0Mzg5Nw==", "bodyText": "Good tip on using streams. Makes it so much more concise.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r418443897", "createdAt": "2020-05-01T07:17:11Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUxOTE2NA=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODg1NDg1OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDoxOTo1NVrOGHgVPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNzo0NDozOFrOGQ0n_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyMjk0Mw==", "bodyText": "This doesn't use projectedIcebergSchema. Can you remove it?\nAlso, I think it would be possible to use just the expected schema instead of the table schema. In primitive, the table schema is used to find the Iceberg type. But if the Iceberg type is missing from the projection, then it can't be projected and will be skipped later. So it should be safe to do this:\n      ...\n      ColumnDescriptor desc = parquetSchema.getColumnDescription(currentPath());\n      // Nested types not yet supported for vectorized reads\n      if (desc.getMaxRepetitionLevel() > 0) {\n        return null;\n      }\n\n      Types.NestedField icebergField = icebergSchema.findField(parquetFieldId);\n      if (icebergField == null) {\n        return null;\n      }\n\n      return new VectorizedArrowReader(\n          desc, icebergField, rootAllocator, batchSize, false /* setArrowValidityVector */);\nIf that works, then you wouldn't need to pass the file schema all the way in.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410522943", "createdAt": "2020-04-18T00:19:55Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.spark.arrow.ArrowAllocation;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedSparkParquetReaders {\n+\n+  private VectorizedSparkParquetReaders() {\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  public static ColumnarBatchReaders buildReader(\n+      Schema tableSchema,\n+      Schema expectedSchema,\n+      MessageType fileSchema,\n+      Integer recordsPerBatch) {\n+    return (ColumnarBatchReaders)\n+        TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+            new VectorizedReaderBuilder(tableSchema, expectedSchema, fileSchema, recordsPerBatch));\n+  }\n+\n+  private static class VectorizedReaderBuilder extends TypeWithSchemaVisitor<VectorizedReader> {\n+    private final MessageType parquetSchema;\n+    private final Schema tableIcebergSchema;\n+    private final BufferAllocator rootAllocator;\n+    private final int batchSize;\n+\n+    VectorizedReaderBuilder(\n+        Schema tableSchema,\n+        Schema projectedIcebergSchema,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTc2NzU2OQ==", "bodyText": "Will change. However, we still need the file schema(parquetSchema) as it is used to get hold of the column descriptor.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r419767569", "createdAt": "2020-05-04T22:31:50Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.spark.arrow.ArrowAllocation;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedSparkParquetReaders {\n+\n+  private VectorizedSparkParquetReaders() {\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  public static ColumnarBatchReaders buildReader(\n+      Schema tableSchema,\n+      Schema expectedSchema,\n+      MessageType fileSchema,\n+      Integer recordsPerBatch) {\n+    return (ColumnarBatchReaders)\n+        TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+            new VectorizedReaderBuilder(tableSchema, expectedSchema, fileSchema, recordsPerBatch));\n+  }\n+\n+  private static class VectorizedReaderBuilder extends TypeWithSchemaVisitor<VectorizedReader> {\n+    private final MessageType parquetSchema;\n+    private final Schema tableIcebergSchema;\n+    private final BufferAllocator rootAllocator;\n+    private final int batchSize;\n+\n+    VectorizedReaderBuilder(\n+        Schema tableSchema,\n+        Schema projectedIcebergSchema,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyMjk0Mw=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDI5MjYwNg==", "bodyText": "I think I meant table schema, not file schema.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420292606", "createdAt": "2020-05-05T17:44:38Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.spark.arrow.ArrowAllocation;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedSparkParquetReaders {\n+\n+  private VectorizedSparkParquetReaders() {\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  public static ColumnarBatchReaders buildReader(\n+      Schema tableSchema,\n+      Schema expectedSchema,\n+      MessageType fileSchema,\n+      Integer recordsPerBatch) {\n+    return (ColumnarBatchReaders)\n+        TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+            new VectorizedReaderBuilder(tableSchema, expectedSchema, fileSchema, recordsPerBatch));\n+  }\n+\n+  private static class VectorizedReaderBuilder extends TypeWithSchemaVisitor<VectorizedReader> {\n+    private final MessageType parquetSchema;\n+    private final Schema tableIcebergSchema;\n+    private final BufferAllocator rootAllocator;\n+    private final int batchSize;\n+\n+    VectorizedReaderBuilder(\n+        Schema tableSchema,\n+        Schema projectedIcebergSchema,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyMjk0Mw=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODg1NTY1OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDoyMDoyOFrOGHgVpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDoyMDoyOFrOGHgVpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyMzA0Nw==", "bodyText": "This is another case where IntStream can make this a bit simpler:\n      IntStream.range(0, fields.size())\n          .forEach(pos -> readersById.put(fields.get(pos).getId().intValue(), fieldReaders.get(pos)));", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410523047", "createdAt": "2020-04-18T00:20:28Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.spark.arrow.ArrowAllocation;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedSparkParquetReaders {\n+\n+  private VectorizedSparkParquetReaders() {\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  public static ColumnarBatchReaders buildReader(\n+      Schema tableSchema,\n+      Schema expectedSchema,\n+      MessageType fileSchema,\n+      Integer recordsPerBatch) {\n+    return (ColumnarBatchReaders)\n+        TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+            new VectorizedReaderBuilder(tableSchema, expectedSchema, fileSchema, recordsPerBatch));\n+  }\n+\n+  private static class VectorizedReaderBuilder extends TypeWithSchemaVisitor<VectorizedReader> {\n+    private final MessageType parquetSchema;\n+    private final Schema tableIcebergSchema;\n+    private final BufferAllocator rootAllocator;\n+    private final int batchSize;\n+\n+    VectorizedReaderBuilder(\n+        Schema tableSchema,\n+        Schema projectedIcebergSchema,\n+        MessageType parquetSchema,\n+        int bSize) {\n+      this.parquetSchema = parquetSchema;\n+      this.tableIcebergSchema = tableSchema;\n+      this.batchSize = bSize;\n+      this.rootAllocator = ArrowAllocation.rootAllocator()\n+          .newChildAllocator(\"VectorizedReadBuilder\", 0, Long.MAX_VALUE);\n+    }\n+\n+    @Override\n+    public VectorizedReader message(\n+            Types.StructType expected, MessageType message,\n+            List<VectorizedReader> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public VectorizedReader struct(\n+            Types.StructType expected, GroupType struct,\n+            List<VectorizedReader> fieldReaders) {\n+\n+      Map<Integer, VectorizedReader> readersById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+\n+      for (int i = 0; i < fields.size(); i += 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODg2MzAzOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDoyNzoyMlrOGHgZ1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDoyNzoyMlrOGHgZ1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNDExNw==", "bodyText": "Preconditions accept a format string and arguments to avoid string creation every time the method is called, which is what happens when you use concatenation like this. Can you update the check like this?\nPreconditions.checkArgument(numRowsToRead > 0, \"Invalid number of rows to read: %s\", numRowsToRead);", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410524117", "createdAt": "2020-04-18T00:27:22Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());\n+    int idx = 0;\n+    for (VectorizedReader reader : readers) {\n+      this.readers[idx] = (VectorizedArrowReader) reader;\n+      idx++;\n+    }\n+  }\n+\n+  @Override\n+  public final void setRowGroupInfo(PageReadStore pageStore, Map<ColumnPath, ColumnChunkMetaData> metaData) {\n+    for (int i = 0; i < readers.length; i += 1) {\n+      if (readers[i] != null) {\n+        readers[i].setRowGroupInfo(pageStore, metaData);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void reuseContainers(boolean reuse) {\n+    for (VectorizedReader reader : readers) {\n+      reader.reuseContainers(reuse);\n+    }\n+  }\n+\n+  @Override\n+  public final ColumnarBatch read(int numRowsToRead) {\n+    Preconditions.checkArgument(numRowsToRead > 0, \"Invalid value: \" + numRowsToRead);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODg2NDA3OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDoyODoyNFrOGHgaZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDoyODoyNFrOGHgaZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNDI2Mg==", "bodyText": "I'm seeing a lot of parameterized types without parameters. Can you make sure you add <?> here and check for that in other places?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410524262", "createdAt": "2020-04-18T00:28:24Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());\n+    int idx = 0;\n+    for (VectorizedReader reader : readers) {\n+      this.readers[idx] = (VectorizedArrowReader) reader;\n+      idx++;\n+    }\n+  }\n+\n+  @Override\n+  public final void setRowGroupInfo(PageReadStore pageStore, Map<ColumnPath, ColumnChunkMetaData> metaData) {\n+    for (int i = 0; i < readers.length; i += 1) {\n+      if (readers[i] != null) {\n+        readers[i].setRowGroupInfo(pageStore, metaData);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void reuseContainers(boolean reuse) {\n+    for (VectorizedReader reader : readers) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODg2NDc4OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDoyODo1MlrOGHgawQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDoyODo1MlrOGHgawQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNDM1Mw==", "bodyText": "This can be a for-each loop:\n    for (VectorizedArrowReader reader : readers) {\n      if (reader != null) {\n        reader.setRowGroupInfo(pageStore, metaData);\n      }\n    }", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410524353", "createdAt": "2020-04-18T00:28:52Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());\n+    int idx = 0;\n+    for (VectorizedReader reader : readers) {\n+      this.readers[idx] = (VectorizedArrowReader) reader;\n+      idx++;\n+    }\n+  }\n+\n+  @Override\n+  public final void setRowGroupInfo(PageReadStore pageStore, Map<ColumnPath, ColumnChunkMetaData> metaData) {\n+    for (int i = 0; i < readers.length; i += 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODg2NzE0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDozMToxOFrOGHgcEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDozMToxOFrOGHgcEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNDY5MA==", "bodyText": "This should also use a format string.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410524690", "createdAt": "2020-04-18T00:31:18Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());\n+    int idx = 0;\n+    for (VectorizedReader reader : readers) {\n+      this.readers[idx] = (VectorizedArrowReader) reader;\n+      idx++;\n+    }\n+  }\n+\n+  @Override\n+  public final void setRowGroupInfo(PageReadStore pageStore, Map<ColumnPath, ColumnChunkMetaData> metaData) {\n+    for (int i = 0; i < readers.length; i += 1) {\n+      if (readers[i] != null) {\n+        readers[i].setRowGroupInfo(pageStore, metaData);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void reuseContainers(boolean reuse) {\n+    for (VectorizedReader reader : readers) {\n+      reader.reuseContainers(reuse);\n+    }\n+  }\n+\n+  @Override\n+  public final ColumnarBatch read(int numRowsToRead) {\n+    Preconditions.checkArgument(numRowsToRead > 0, \"Invalid value: \" + numRowsToRead);\n+    ColumnVector[] arrowColumnVectors = new ColumnVector[readers.length];\n+    int prevNum = 0;\n+    for (int i = 0; i < readers.length; i += 1) {\n+      VectorHolder holder = readers[i].read(numRowsToRead);\n+      int numRowsInVector = holder.numValues();\n+      Preconditions.checkState(\n+          numRowsInVector == numRowsToRead,\n+          \"Number of rows in the vector \" + numRowsInVector + \" didn't match expected \" +\n+              numRowsToRead);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODg2NzYxOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDozMTo1NFrOGHgcXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wMVQyMjoyNzo0OFrOGPXT-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNDc2Nw==", "bodyText": "If this is always expected to return the requested number of rows, then there is no need for the prevNum check because this would fail before that check would.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410524767", "createdAt": "2020-04-18T00:31:54Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());\n+    int idx = 0;\n+    for (VectorizedReader reader : readers) {\n+      this.readers[idx] = (VectorizedArrowReader) reader;\n+      idx++;\n+    }\n+  }\n+\n+  @Override\n+  public final void setRowGroupInfo(PageReadStore pageStore, Map<ColumnPath, ColumnChunkMetaData> metaData) {\n+    for (int i = 0; i < readers.length; i += 1) {\n+      if (readers[i] != null) {\n+        readers[i].setRowGroupInfo(pageStore, metaData);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void reuseContainers(boolean reuse) {\n+    for (VectorizedReader reader : readers) {\n+      reader.reuseContainers(reuse);\n+    }\n+  }\n+\n+  @Override\n+  public final ColumnarBatch read(int numRowsToRead) {\n+    Preconditions.checkArgument(numRowsToRead > 0, \"Invalid value: \" + numRowsToRead);\n+    ColumnVector[] arrowColumnVectors = new ColumnVector[readers.length];\n+    int prevNum = 0;\n+    for (int i = 0; i < readers.length; i += 1) {\n+      VectorHolder holder = readers[i].read(numRowsToRead);\n+      int numRowsInVector = holder.numValues();\n+      Preconditions.checkState(\n+          numRowsInVector == numRowsToRead,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc2Mzc2OA==", "bodyText": "Good point. Will remove the check.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r418763768", "createdAt": "2020-05-01T22:27:48Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ColumnarBatchReaders.java", "diffHunk": "@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import java.lang.reflect.Array;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+/**\n+ * {@link VectorizedReader} that returns Spark's {@link ColumnarBatch} to support Spark's vectorized read path. The\n+ * {@link ColumnarBatch} returned is created by passing in the Arrow vectors populated via delegated read calls to\n+ * {@linkplain VectorizedArrowReader VectorReader(s)}.\n+ */\n+public class ColumnarBatchReaders implements VectorizedReader<ColumnarBatch> {\n+  private final VectorizedArrowReader[] readers;\n+\n+  public ColumnarBatchReaders(List<VectorizedReader> readers) {\n+    this.readers = (VectorizedArrowReader[]) Array.newInstance(\n+        VectorizedArrowReader.class, readers.size());\n+    int idx = 0;\n+    for (VectorizedReader reader : readers) {\n+      this.readers[idx] = (VectorizedArrowReader) reader;\n+      idx++;\n+    }\n+  }\n+\n+  @Override\n+  public final void setRowGroupInfo(PageReadStore pageStore, Map<ColumnPath, ColumnChunkMetaData> metaData) {\n+    for (int i = 0; i < readers.length; i += 1) {\n+      if (readers[i] != null) {\n+        readers[i].setRowGroupInfo(pageStore, metaData);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void reuseContainers(boolean reuse) {\n+    for (VectorizedReader reader : readers) {\n+      reader.reuseContainers(reuse);\n+    }\n+  }\n+\n+  @Override\n+  public final ColumnarBatch read(int numRowsToRead) {\n+    Preconditions.checkArgument(numRowsToRead > 0, \"Invalid value: \" + numRowsToRead);\n+    ColumnVector[] arrowColumnVectors = new ColumnVector[readers.length];\n+    int prevNum = 0;\n+    for (int i = 0; i < readers.length; i += 1) {\n+      VectorHolder holder = readers[i].read(numRowsToRead);\n+      int numRowsInVector = holder.numValues();\n+      Preconditions.checkState(\n+          numRowsInVector == numRowsToRead,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNDc2Nw=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODg3MTAzOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDozNDo0NlrOGHgeOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNzo0Mzo0OFrOGQ0mSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNTI0Mg==", "bodyText": "Are these types rejected when we create Iceberg tables?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410525242", "createdAt": "2020-04-18T00:34:46Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.parquet.Preconditions;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(SparkSchemaUtil.convert(holder.icebergType()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.accessor = ArrowVectorAccessors.getVectorAccessor(holder);\n+  }\n+\n+  @Override\n+  public void close() {\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - byte\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc2NDY2Mg==", "bodyText": "I don't see BYTE and SHORT type in https://github.com/apache/incubator-iceberg/blob/master/api/src/main/java/org/apache/iceberg/types/Type.java\nSo I would assume so?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r418764662", "createdAt": "2020-05-01T22:31:13Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.parquet.Preconditions;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(SparkSchemaUtil.convert(holder.icebergType()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.accessor = ArrowVectorAccessors.getVectorAccessor(holder);\n+  }\n+\n+  @Override\n+  public void close() {\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - byte\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNTI0Mg=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDI5MjE3MQ==", "bodyText": "Looks good to me.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420292171", "createdAt": "2020-05-05T17:43:48Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.parquet.Preconditions;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(SparkSchemaUtil.convert(holder.icebergType()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.accessor = ArrowVectorAccessors.getVectorAccessor(holder);\n+  }\n+\n+  @Override\n+  public void close() {\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - byte\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNTI0Mg=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODg3NjcxOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDo0MDoyMFrOGHghQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDo0MDoyMFrOGHghQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNjAxNg==", "bodyText": "This is the only place that uses childColumns and it seems really strange to only set them in a sub-class, even though they are available in the constructor.\nCould you make the superclass childColumns a private final field instead, and pass the child columns through a second constructor? Like this:\n    StructAccessor(StructVector structVector) {\n      super(structVector,\n          IntStream.range(0, structVector.size())\n              .mapToObj(structVector::getVectorById)\n              .map(ArrowColumnVector::new)\n              .toArray(ArrowColumnVector[]::new));\n    }", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410526016", "createdAt": "2020-04-18T00:40:20Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java", "diffHunk": "@@ -0,0 +1,495 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.FixedSizeBinaryVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class ArrowVectorAccessors {\n+\n+  private ArrowVectorAccessors() {}\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  static ArrowVectorAccessor getVectorAccessor(VectorHolder holder) {\n+    Dictionary dictionary = holder.dictionary();\n+    boolean isVectorDictEncoded = holder.isDictionaryEncoded();\n+    ColumnDescriptor desc = holder.descriptor();\n+    FieldVector vector = holder.vector();\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+      if (primitive.getOriginalType() != null) {\n+        switch (desc.getPrimitiveType().getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+          case BSON:\n+            return new DictionaryStringAccessor((IntVector) vector, dictionary);\n+          case INT_64:\n+          case TIMESTAMP_MILLIS:\n+          case TIMESTAMP_MICROS:\n+            return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+          case DECIMAL:\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new DictionaryDecimalBinaryAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              case INT64:\n+                return new DictionaryDecimalLongAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              case INT32:\n+                return new DictionaryDecimalIntAccessor(\n+                    (IntVector) vector,\n+                    dictionary);\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      } else {\n+        switch (primitive.getPrimitiveTypeName()) {\n+          case FIXED_LEN_BYTE_ARRAY:\n+          case BINARY:\n+            return new DictionaryBinaryAccessor((IntVector) vector, dictionary);\n+          case FLOAT:\n+            return new DictionaryFloatAccessor((IntVector) vector, dictionary);\n+          case INT64:\n+            return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+          case DOUBLE:\n+            return new DictionaryDoubleAccessor((IntVector) vector, dictionary);\n+          default:\n+            throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+        }\n+      }\n+    } else {\n+      if (vector instanceof BitVector) {\n+        return new BooleanAccessor((BitVector) vector);\n+      } else if (vector instanceof IntVector) {\n+        return new IntAccessor((IntVector) vector);\n+      } else if (vector instanceof BigIntVector) {\n+        return new LongAccessor((BigIntVector) vector);\n+      } else if (vector instanceof Float4Vector) {\n+        return new FloatAccessor((Float4Vector) vector);\n+      } else if (vector instanceof Float8Vector) {\n+        return new DoubleAccessor((Float8Vector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+        return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+        return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+      } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+        return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+      } else if (vector instanceof DateDayVector) {\n+        return new DateAccessor((DateDayVector) vector);\n+      } else if (vector instanceof TimeStampMicroTZVector) {\n+        return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+      } else if (vector instanceof ListVector) {\n+        ListVector listVector = (ListVector) vector;\n+        return new ArrayAccessor(listVector);\n+      } else if (vector instanceof StructVector) {\n+        StructVector structVector = (StructVector) vector;\n+        return new StructAccessor(structVector);\n+      }\n+    }\n+    throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+  }\n+\n+  private static class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private static class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryLongAccessor extends DictionaryArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryLongAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return parquetDictionary.decodeToLong(vector.get(rowId));\n+    }\n+  }\n+\n+  private static class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryFloatAccessor extends DictionaryArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryFloatAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return parquetDictionary.decodeToFloat(vector.get(rowId));\n+    }\n+  }\n+\n+  private static class DoubleAccessor extends ArrowVectorAccessor {\n+\n+    private final Float8Vector vector;\n+\n+    DoubleAccessor(Float8Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryDoubleAccessor extends DictionaryArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    DictionaryDoubleAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final double getDouble(int rowId) {\n+      return parquetDictionary.decodeToDouble(vector.get(rowId));\n+    }\n+  }\n+\n+  private static class DecimalAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.DecimalArrowVector vector;\n+\n+    DecimalAccessor(IcebergArrowVectors.DecimalArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final Decimal getDecimal(int rowId, int precision, int scale) {\n+      return Decimal.apply(vector.getObject(rowId), precision, scale);\n+    }\n+  }\n+\n+  private static class StringAccessor extends ArrowVectorAccessor {\n+\n+    private final IcebergArrowVectors.VarcharArrowVector vector;\n+    private final NullableVarCharHolder stringResult = new NullableVarCharHolder();\n+\n+    StringAccessor(IcebergArrowVectors.VarcharArrowVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      vector.get(rowId, stringResult);\n+      if (stringResult.isSet == 0) {\n+        return null;\n+      } else {\n+        return UTF8String.fromAddress(\n+            null,\n+            stringResult.buffer.memoryAddress() + stringResult.start,\n+            stringResult.end - stringResult.start);\n+      }\n+    }\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+  private abstract static class DictionaryArrowVectorAccessor extends ArrowVectorAccessor {\n+    final Dictionary parquetDictionary;\n+    final IntVector dictionaryVector;\n+\n+    private DictionaryArrowVectorAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector);\n+      this.dictionaryVector = vector;\n+      this.parquetDictionary = dictionary;\n+    }\n+  }\n+\n+  private static class DictionaryStringAccessor extends DictionaryArrowVectorAccessor {\n+\n+    DictionaryStringAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+    }\n+\n+    @Override\n+    final UTF8String getUTF8String(int rowId) {\n+      Binary binary = parquetDictionary.decodeToBinary(dictionaryVector.get(rowId));\n+      return UTF8String.fromBytes(binary.getBytesUnsafe());\n+    }\n+  }\n+\n+  private static class FixedSizeBinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final FixedSizeBinaryVector vector;\n+\n+    FixedSizeBinaryAccessor(FixedSizeBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private static class BinaryAccessor extends ArrowVectorAccessor {\n+\n+    private final VarBinaryVector vector;\n+\n+    BinaryAccessor(VarBinaryVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      return vector.getObject(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryBinaryAccessor extends DictionaryArrowVectorAccessor {\n+\n+    DictionaryBinaryAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector, dictionary);\n+    }\n+\n+    @Override\n+    final byte[] getBinary(int rowId) {\n+      Binary binary = parquetDictionary.decodeToBinary(dictionaryVector.get(rowId));\n+      return binary.getBytesUnsafe();\n+    }\n+  }\n+\n+  private static class DateAccessor extends ArrowVectorAccessor {\n+\n+    private final DateDayVector vector;\n+\n+    DateAccessor(DateDayVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class TimestampAccessor extends ArrowVectorAccessor {\n+\n+    private final TimeStampMicroTZVector vector;\n+\n+    TimestampAccessor(TimeStampMicroTZVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class ArrayAccessor extends ArrowVectorAccessor {\n+\n+    private final ListVector vector;\n+    private final ArrowColumnVector arrayData;\n+\n+    ArrayAccessor(ListVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+      this.arrayData = new ArrowColumnVector(vector.getDataVector());\n+    }\n+\n+    @Override\n+    final ColumnarArray getArray(int rowId) {\n+      ArrowBuf offsets = vector.getOffsetBuffer();\n+      int index = rowId * ListVector.OFFSET_WIDTH;\n+      int start = offsets.getInt(index);\n+      int end = offsets.getInt(index + ListVector.OFFSET_WIDTH);\n+      return new ColumnarArray(arrayData, start, end - start);\n+    }\n+  }\n+\n+  /**\n+   * Use {@link IcebergArrowColumnVector#getChild(int)} to get hold of the {@link ArrowColumnVector} vectors holding the\n+   * struct values.\n+   */\n+  private static class StructAccessor extends ArrowVectorAccessor {\n+    StructAccessor(StructVector structVector) {\n+      super(structVector);\n+      childColumns = new ArrowColumnVector[structVector.size()];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 428}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODg3Nzg3OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDo0MTo0MFrOGHgh5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNlQwODo0ODo0NVrOGRJkVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNjE4MA==", "bodyText": "Did I already ask why this is necessary?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410526180", "createdAt": "2020-04-18T00:41:40Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.parquet.Preconditions;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(SparkSchemaUtil.convert(holder.icebergType()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.accessor = ArrowVectorAccessors.getVectorAccessor(holder);\n+  }\n+\n+  @Override\n+  public void close() {\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - byte\");\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - short\");\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - map\");\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1MTMxNA==", "bodyText": "This was copied over from Spark's ArrowColumnVector\nhttps://github.com/apache/spark/blob/master/sql/catalyst/src/main/java/org/apache/spark/sql/vectorized/ArrowColumnVector.java#L108\nThe difference from Spark's implementation is that our isNullAt(rowId) goes to the NullabilityHolder where as Spark's uses vector.isNull(rowId);\nFor spark, we don't set the validity bits in the validity vector since we track that piece of information in NullabilityHolder.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r418751314", "createdAt": "2020-05-01T21:44:45Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.parquet.Preconditions;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(SparkSchemaUtil.convert(holder.icebergType()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.accessor = ArrowVectorAccessors.getVectorAccessor(holder);\n+  }\n+\n+  @Override\n+  public void close() {\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - byte\");\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - short\");\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - map\");\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNjE4MA=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDI5MTg4MA==", "bodyText": "But doesn't Spark already check nullability before calling this method?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420291880", "createdAt": "2020-05-05T17:43:21Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.parquet.Preconditions;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(SparkSchemaUtil.convert(holder.icebergType()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.accessor = ArrowVectorAccessors.getVectorAccessor(holder);\n+  }\n+\n+  @Override\n+  public void close() {\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - byte\");\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - short\");\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - map\");\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNjE4MA=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYyMDA0Mg==", "bodyText": "No, spark doesn't. See - https://github.com/apache/spark/blob/branch-2.4/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/MutableColumnarRow.java#L135", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420620042", "createdAt": "2020-05-06T08:20:56Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.parquet.Preconditions;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(SparkSchemaUtil.convert(holder.icebergType()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.accessor = ArrowVectorAccessors.getVectorAccessor(holder);\n+  }\n+\n+  @Override\n+  public void close() {\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - byte\");\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - short\");\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - map\");\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNjE4MA=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDYzNTczNA==", "bodyText": "Also, depending on the accessor, it may or may not have the nullability information. For ex- for DictionaryStringAccessor,\n    final UTF8String getUTF8String(int rowId) {\n      int offset = offsetVector.get(rowId);\n      return decodedDictionary[offset];\n    }\n\nwithout the null check, the offsetVector could return a wrong value when the value at the rowId was actually null.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420635734", "createdAt": "2020-05-06T08:48:45Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.parquet.Preconditions;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(SparkSchemaUtil.convert(holder.icebergType()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.accessor = ArrowVectorAccessors.getVectorAccessor(holder);\n+  }\n+\n+  @Override\n+  public void close() {\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - byte\");\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - short\");\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - map\");\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNjE4MA=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODg3ODcwOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDo0MjozNVrOGHgiag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDo0MjozNVrOGHgiag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNjMxNA==", "bodyText": "Instead of childColumns() and then accessing an ordinal, why not change the method to accessor.childColumn(int pos)?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410526314", "createdAt": "2020-04-18T00:42:35Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.parquet.Preconditions;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(SparkSchemaUtil.convert(holder.icebergType()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.accessor = ArrowVectorAccessors.getVectorAccessor(holder);\n+  }\n+\n+  @Override\n+  public void close() {\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - byte\");\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - short\");\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - map\");\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    ArrowColumnVector[] childColumns = accessor.childColumns();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 143}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODg3OTc1OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDo0MzozM1rOGHgi_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDo0MzozM1rOGHgi_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNjQ2MA==", "bodyText": "Let's remove this Precondition. An invalid ordinal will cause an automatic exception and there is no need to check twice.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410526460", "createdAt": "2020-04-18T00:43:33Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/IcebergArrowColumnVector.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import org.apache.iceberg.arrow.vectorized.NullabilityHolder;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.parquet.Preconditions;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.sql.vectorized.ColumnarMap;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+/**\n+ * Implementation of Spark's {@link ColumnVector} interface. The code for this class is heavily inspired from Spark's\n+ * {@link ArrowColumnVector} The main difference is in how nullability checks are made in this class by relying on\n+ * {@link NullabilityHolder} instead of the validity vector in the Arrow vector.\n+ */\n+public class IcebergArrowColumnVector extends ColumnVector {\n+\n+  private final ArrowVectorAccessor accessor;\n+  private final NullabilityHolder nullabilityHolder;\n+\n+  public IcebergArrowColumnVector(VectorHolder holder) {\n+    super(SparkSchemaUtil.convert(holder.icebergType()));\n+    this.nullabilityHolder = holder.nullabilityHolder();\n+    this.accessor = ArrowVectorAccessors.getVectorAccessor(holder);\n+  }\n+\n+  @Override\n+  public void close() {\n+    accessor.close();\n+  }\n+\n+  @Override\n+  public boolean hasNull() {\n+    return nullabilityHolder.hasNulls();\n+  }\n+\n+  @Override\n+  public int numNulls() {\n+    return nullabilityHolder.numNulls();\n+  }\n+\n+  @Override\n+  public boolean isNullAt(int rowId) {\n+    return nullabilityHolder.isNullAt(rowId) == 1;\n+  }\n+\n+  @Override\n+  public boolean getBoolean(int rowId) {\n+    return accessor.getBoolean(rowId);\n+  }\n+\n+  @Override\n+  public byte getByte(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - byte\");\n+  }\n+\n+  @Override\n+  public short getShort(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - short\");\n+  }\n+\n+  @Override\n+  public int getInt(int rowId) {\n+    return accessor.getInt(rowId);\n+  }\n+\n+  @Override\n+  public long getLong(int rowId) {\n+    return accessor.getLong(rowId);\n+  }\n+\n+  @Override\n+  public float getFloat(int rowId) {\n+    return accessor.getFloat(rowId);\n+  }\n+\n+  @Override\n+  public double getDouble(int rowId) {\n+    return accessor.getDouble(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarArray getArray(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getArray(rowId);\n+  }\n+\n+  @Override\n+  public ColumnarMap getMap(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type - map\");\n+  }\n+\n+  @Override\n+  public Decimal getDecimal(int rowId, int precision, int scale) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getDecimal(rowId, precision, scale);\n+  }\n+\n+  @Override\n+  public UTF8String getUTF8String(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getUTF8String(rowId);\n+  }\n+\n+  @Override\n+  public byte[] getBinary(int rowId) {\n+    if (isNullAt(rowId)) {\n+      return null;\n+    }\n+    return accessor.getBinary(rowId);\n+  }\n+\n+  @Override\n+  public ArrowColumnVector getChild(int ordinal) {\n+    ArrowColumnVector[] childColumns = accessor.childColumns();\n+    Preconditions.checkArgument(childColumns != null && ordinal < childColumns.length, \"Invalid call for getChild() \" +\n+        \"with ordinal \" + ordinal);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODg4NTQ1OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDo0ODo0OVrOGHgmFQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxODo0NDoyN1rOGQ21gg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNzI1Mw==", "bodyText": "We need to make sure the extra filter columns are added in the reader and returned to Spark (so Spark expects the projection). This can't return a different set of columns than expected because columns are accessed by ordinal.\nAnother option is to do the ColumnarBatch projection here (remove filter columns that have been used), but I would prefer to just return everything to Spark. That simplifies both row and batch read paths.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410527253", "createdAt": "2020-04-18T00:48:49Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Iterator;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+class BatchDataReader extends BaseDataReader<ColumnarBatch> {\n+  private final Schema tableSchema;\n+  private final Schema expectedSchema;\n+  private final boolean caseSensitive;\n+  private final int batchSize;\n+\n+  BatchDataReader(\n+      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n+      EncryptionManager encryptionManager, boolean caseSensitive, int size) {\n+    super(task, fileIo, encryptionManager);\n+    this.tableSchema = tableSchema;\n+    this.expectedSchema = expectedSchema;\n+    this.caseSensitive = caseSensitive;\n+    this.batchSize = size;\n+  }\n+\n+  @Override\n+  Iterator<ColumnarBatch> open(FileScanTask task) {\n+    // schema or rows returned by readers\n+    Schema finalSchema = expectedSchema;\n+    // schema needed for the projection and filtering\n+    StructType sparkType = SparkSchemaUtil.convert(finalSchema);\n+    Schema requiredSchema = SparkSchemaUtil.prune(tableSchema, sparkType, task.residual(), caseSensitive);\n+    boolean hasExtraFilterColumns = requiredSchema.columns().size() != finalSchema.columns().size();\n+    Iterator<ColumnarBatch> iter;\n+    if (hasExtraFilterColumns) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc4MjU2Nw==", "bodyText": "Upon looking closer, I think BaseDataReader and RowDataReader classes can be merged. The only difference between BatchDataReader and RowDataReader is how in the former case we don't currently handle identity partition columns. I will spin up a PR for getting that refactoring into master. Once that is in, the only method that we would need to override in BatchDataReader would be Iterator<ColumnarBatch> open(FileScanTask task, Schema readSchema)", "url": "https://github.com/apache/iceberg/pull/828#discussion_r418782567", "createdAt": "2020-05-01T23:51:56Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Iterator;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+class BatchDataReader extends BaseDataReader<ColumnarBatch> {\n+  private final Schema tableSchema;\n+  private final Schema expectedSchema;\n+  private final boolean caseSensitive;\n+  private final int batchSize;\n+\n+  BatchDataReader(\n+      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n+      EncryptionManager encryptionManager, boolean caseSensitive, int size) {\n+    super(task, fileIo, encryptionManager);\n+    this.tableSchema = tableSchema;\n+    this.expectedSchema = expectedSchema;\n+    this.caseSensitive = caseSensitive;\n+    this.batchSize = size;\n+  }\n+\n+  @Override\n+  Iterator<ColumnarBatch> open(FileScanTask task) {\n+    // schema or rows returned by readers\n+    Schema finalSchema = expectedSchema;\n+    // schema needed for the projection and filtering\n+    StructType sparkType = SparkSchemaUtil.convert(finalSchema);\n+    Schema requiredSchema = SparkSchemaUtil.prune(tableSchema, sparkType, task.residual(), caseSensitive);\n+    boolean hasExtraFilterColumns = requiredSchema.columns().size() != finalSchema.columns().size();\n+    Iterator<ColumnarBatch> iter;\n+    if (hasExtraFilterColumns) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNzI1Mw=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxOTcxNzU3Mw==", "bodyText": "Turns out, I think it makes sense to not merge the two classes. However, a lot of the functionality can be moved to the BaseDataReader class with override for specialized handling of identity partition columns. PR for refactoring - #1000", "url": "https://github.com/apache/iceberg/pull/828#discussion_r419717573", "createdAt": "2020-05-04T20:46:14Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Iterator;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+class BatchDataReader extends BaseDataReader<ColumnarBatch> {\n+  private final Schema tableSchema;\n+  private final Schema expectedSchema;\n+  private final boolean caseSensitive;\n+  private final int batchSize;\n+\n+  BatchDataReader(\n+      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n+      EncryptionManager encryptionManager, boolean caseSensitive, int size) {\n+    super(task, fileIo, encryptionManager);\n+    this.tableSchema = tableSchema;\n+    this.expectedSchema = expectedSchema;\n+    this.caseSensitive = caseSensitive;\n+    this.batchSize = size;\n+  }\n+\n+  @Override\n+  Iterator<ColumnarBatch> open(FileScanTask task) {\n+    // schema or rows returned by readers\n+    Schema finalSchema = expectedSchema;\n+    // schema needed for the projection and filtering\n+    StructType sparkType = SparkSchemaUtil.convert(finalSchema);\n+    Schema requiredSchema = SparkSchemaUtil.prune(tableSchema, sparkType, task.residual(), caseSensitive);\n+    boolean hasExtraFilterColumns = requiredSchema.columns().size() != finalSchema.columns().size();\n+    Iterator<ColumnarBatch> iter;\n+    if (hasExtraFilterColumns) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNzI1Mw=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDI5MDY3Ng==", "bodyText": "I opened #1004 to remove the special handling for extra filter columns. When that's merged, we won't need to handle it. That's what I was talking about with this comment. Can you see how that changes the need for #1000?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420290676", "createdAt": "2020-05-05T17:41:26Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Iterator;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+class BatchDataReader extends BaseDataReader<ColumnarBatch> {\n+  private final Schema tableSchema;\n+  private final Schema expectedSchema;\n+  private final boolean caseSensitive;\n+  private final int batchSize;\n+\n+  BatchDataReader(\n+      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n+      EncryptionManager encryptionManager, boolean caseSensitive, int size) {\n+    super(task, fileIo, encryptionManager);\n+    this.tableSchema = tableSchema;\n+    this.expectedSchema = expectedSchema;\n+    this.caseSensitive = caseSensitive;\n+    this.batchSize = size;\n+  }\n+\n+  @Override\n+  Iterator<ColumnarBatch> open(FileScanTask task) {\n+    // schema or rows returned by readers\n+    Schema finalSchema = expectedSchema;\n+    // schema needed for the projection and filtering\n+    StructType sparkType = SparkSchemaUtil.convert(finalSchema);\n+    Schema requiredSchema = SparkSchemaUtil.prune(tableSchema, sparkType, task.residual(), caseSensitive);\n+    boolean hasExtraFilterColumns = requiredSchema.columns().size() != finalSchema.columns().size();\n+    Iterator<ColumnarBatch> iter;\n+    if (hasExtraFilterColumns) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNzI1Mw=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDMyODgzNA==", "bodyText": "So the idea behind the need for #1000 was to provide only those methods that need to be customized for the two readers. In turn, it would have taken care of the special handling for extra filter columns and other common functionality that would have been introduced in the future.\nI think #1000 is still relevant. It will need to change though if your PR goes in first :)", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420328834", "createdAt": "2020-05-05T18:44:27Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Iterator;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+class BatchDataReader extends BaseDataReader<ColumnarBatch> {\n+  private final Schema tableSchema;\n+  private final Schema expectedSchema;\n+  private final boolean caseSensitive;\n+  private final int batchSize;\n+\n+  BatchDataReader(\n+      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n+      EncryptionManager encryptionManager, boolean caseSensitive, int size) {\n+    super(task, fileIo, encryptionManager);\n+    this.tableSchema = tableSchema;\n+    this.expectedSchema = expectedSchema;\n+    this.caseSensitive = caseSensitive;\n+    this.batchSize = size;\n+  }\n+\n+  @Override\n+  Iterator<ColumnarBatch> open(FileScanTask task) {\n+    // schema or rows returned by readers\n+    Schema finalSchema = expectedSchema;\n+    // schema needed for the projection and filtering\n+    StructType sparkType = SparkSchemaUtil.convert(finalSchema);\n+    Schema requiredSchema = SparkSchemaUtil.prune(tableSchema, sparkType, task.residual(), caseSensitive);\n+    boolean hasExtraFilterColumns = requiredSchema.columns().size() != finalSchema.columns().size();\n+    Iterator<ColumnarBatch> iter;\n+    if (hasExtraFilterColumns) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyNzI1Mw=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjU0ODg5NTMyOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xOFQwMDo1NzozNFrOGHgrQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wNVQxNzo0Mjo0M1rOGQ0jvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyODU3OQ==", "bodyText": "I think this method would be simpler by not exiting early. Something like this:\n  if (enableBatchRead == null) {\n    boolean batchReadEnabled = ...;\n    boolean allParquetFileScanTasks = ...;\n    boolean atLeastOneColumn = lazySchema().columsn().size() > 0;\n    boolean hasNoIdentityProjections = ...;\n    enableBatchRead = batchReadEnabled && allParquetFileScanTasks && atLeastOneColumn && hasNoIdentityProjections;\n  }\n  return enableBatchRead;", "url": "https://github.com/apache/iceberg/pull/828#discussion_r410528579", "createdAt": "2020-04-18T00:57:34Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -238,6 +279,46 @@ public Statistics estimateStatistics() {\n     return new Stats(sizeInBytes, numRows);\n   }\n \n+  @Override\n+  public boolean enableBatchRead() {\n+    return lazyCheckEnableBatchRead();\n+  }\n+\n+  private boolean lazyCheckEnableBatchRead() {\n+    if (enableBatchRead == null) {\n+      boolean allParquetFileScanTasks =\n+          tasks().stream()\n+              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n+                  .stream()\n+                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n+                      FileFormat.PARQUET)));\n+      if (!allParquetFileScanTasks) {\n+        this.enableBatchRead = false;\n+        return false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxODc1ODA5Ng==", "bodyText": "While it makes it cleaner, it is less efficient than exiting early. For ex-\nboolean allParquetFileScanTasks =\n          tasks().stream()\n              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n                  .stream()\n                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n                      FileFormat.PARQUET)));\n\nThe above is probably a somewhat expensive operation since it is iterating through all the files. We don't want to execute this is batch read has been disabled by config.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r418758096", "createdAt": "2020-05-01T22:06:45Z", "author": {"login": "samarthjain"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -238,6 +279,46 @@ public Statistics estimateStatistics() {\n     return new Stats(sizeInBytes, numRows);\n   }\n \n+  @Override\n+  public boolean enableBatchRead() {\n+    return lazyCheckEnableBatchRead();\n+  }\n+\n+  private boolean lazyCheckEnableBatchRead() {\n+    if (enableBatchRead == null) {\n+      boolean allParquetFileScanTasks =\n+          tasks().stream()\n+              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n+                  .stream()\n+                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n+                      FileFormat.PARQUET)));\n+      if (!allParquetFileScanTasks) {\n+        this.enableBatchRead = false;\n+        return false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyODU3OQ=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 144}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMDI5MTUxOQ==", "bodyText": "Why does this need to be efficient? I think that the primary concern should be readability here because this is executed at planning time and the result is cached.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r420291519", "createdAt": "2020-05-05T17:42:43Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -238,6 +279,46 @@ public Statistics estimateStatistics() {\n     return new Stats(sizeInBytes, numRows);\n   }\n \n+  @Override\n+  public boolean enableBatchRead() {\n+    return lazyCheckEnableBatchRead();\n+  }\n+\n+  private boolean lazyCheckEnableBatchRead() {\n+    if (enableBatchRead == null) {\n+      boolean allParquetFileScanTasks =\n+          tasks().stream()\n+              .allMatch(combinedScanTask -> !combinedScanTask.isDataTask() && combinedScanTask.files()\n+                  .stream()\n+                  .allMatch(fileScanTask -> fileScanTask.file().format().equals(\n+                      FileFormat.PARQUET)));\n+      if (!allParquetFileScanTasks) {\n+        this.enableBatchRead = false;\n+        return false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDUyODU3OQ=="}, "originalCommit": {"oid": "714c943b3812c8c65309e8161e2801cecafb4dbb"}, "originalPosition": 144}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NDg3ODY4OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMDoxNToxM1rOGZmS3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMDoxNToxM1rOGZmS3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NTAwNA==", "bodyText": "This has a warning that it may produce NullPointerException. It would be better to initialize childColumns to new ArrowColumnVector[0] so that this throws an IndexOutOfBoundsException instead. That would also avoid needing to check if it is null in close.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r429495004", "createdAt": "2020-05-23T00:15:13Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessor.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import org.apache.arrow.vector.ValueVector;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+abstract class ArrowVectorAccessor {\n+\n+  private final ValueVector vector;\n+  private final ArrowColumnVector[] childColumns;\n+\n+  ArrowVectorAccessor(ValueVector vector) {\n+    this.vector = vector;\n+    this.childColumns = null;\n+  }\n+\n+  ArrowVectorAccessor(ValueVector vector, ArrowColumnVector[] children) {\n+    this.vector = vector;\n+    this.childColumns = children;\n+  }\n+\n+  final void close() {\n+    if (childColumns != null) {\n+      for (ArrowColumnVector column : childColumns) {\n+        // Closing an ArrowColumnVector is expected to not throw any exception\n+        column.close();\n+      }\n+    }\n+    vector.close();\n+  }\n+\n+  boolean getBoolean(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type: boolean\");\n+  }\n+\n+  int getInt(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type: int\");\n+  }\n+\n+  long getLong(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type: long\");\n+  }\n+\n+  float getFloat(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type: float\");\n+  }\n+\n+  double getDouble(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type: double\");\n+  }\n+\n+  Decimal getDecimal(int rowId, int precision, int scale) {\n+    throw new UnsupportedOperationException(\"Unsupported type: decimal\");\n+  }\n+\n+  UTF8String getUTF8String(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type: UTF8String\");\n+  }\n+\n+  byte[] getBinary(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type: binary\");\n+  }\n+\n+  ColumnarArray getArray(int rowId) {\n+    throw new UnsupportedOperationException(\"Unsupported type: array\");\n+  }\n+\n+  ArrowColumnVector childColumn(int pos) {\n+    return childColumns[pos];", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53c920b24e2599ee70a5e9ce96a5a839dcb4753a"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NDg4MjE0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMDoxOTo1MlrOGZmUuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMDoxOTo1MlrOGZmUuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NTQ4MA==", "bodyText": "This updated DictionaryDoubleAccessor to decode into an array, but the other types weren't similarly updated. I think they should be for consistency and so it is clear what's happening.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r429495480", "createdAt": "2020-05-23T00:19:52Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/ArrowVectorAccessors.java", "diffHunk": "@@ -0,0 +1,505 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import io.netty.buffer.ArrowBuf;\n+import java.math.BigInteger;\n+import java.util.stream.IntStream;\n+import org.apache.arrow.vector.BigIntVector;\n+import org.apache.arrow.vector.BitVector;\n+import org.apache.arrow.vector.DateDayVector;\n+import org.apache.arrow.vector.FieldVector;\n+import org.apache.arrow.vector.Float4Vector;\n+import org.apache.arrow.vector.Float8Vector;\n+import org.apache.arrow.vector.IntVector;\n+import org.apache.arrow.vector.TimeStampMicroTZVector;\n+import org.apache.arrow.vector.VarBinaryVector;\n+import org.apache.arrow.vector.complex.ListVector;\n+import org.apache.arrow.vector.complex.StructVector;\n+import org.apache.arrow.vector.holders.NullableVarCharHolder;\n+import org.apache.iceberg.arrow.vectorized.IcebergArrowVectors;\n+import org.apache.iceberg.arrow.vectorized.VectorHolder;\n+import org.apache.parquet.Preconditions;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.Dictionary;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.vectorized.ArrowColumnVector;\n+import org.apache.spark.sql.vectorized.ColumnarArray;\n+import org.apache.spark.unsafe.types.UTF8String;\n+import org.jetbrains.annotations.NotNull;\n+\n+public class ArrowVectorAccessors {\n+\n+  private ArrowVectorAccessors() {}\n+\n+  static ArrowVectorAccessor getVectorAccessor(VectorHolder holder) {\n+    Dictionary dictionary = holder.dictionary();\n+    boolean isVectorDictEncoded = holder.isDictionaryEncoded();\n+    ColumnDescriptor desc = holder.descriptor();\n+    FieldVector vector = holder.vector();\n+    PrimitiveType primitive = desc.getPrimitiveType();\n+    if (isVectorDictEncoded) {\n+      return getDictionaryVectorAccessor(dictionary, desc, vector, primitive);\n+    } else {\n+      return getPlainVectorAccessor(vector);\n+    }\n+  }\n+\n+  @NotNull\n+  private static ArrowVectorAccessor getDictionaryVectorAccessor(\n+      Dictionary dictionary,\n+      ColumnDescriptor desc,\n+      FieldVector vector, PrimitiveType primitive) {\n+    Preconditions.checkState(vector instanceof IntVector, \"Dictionary ids should be stored in IntVectors only\");\n+    if (primitive.getOriginalType() != null) {\n+      switch (desc.getPrimitiveType().getOriginalType()) {\n+        case ENUM:\n+        case JSON:\n+        case UTF8:\n+        case BSON:\n+          return new DictionaryStringAccessor((IntVector) vector, dictionary);\n+        case INT_64:\n+        case TIMESTAMP_MILLIS:\n+        case TIMESTAMP_MICROS:\n+          return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+        case DECIMAL:\n+          switch (primitive.getPrimitiveTypeName()) {\n+            case BINARY:\n+            case FIXED_LEN_BYTE_ARRAY:\n+              return new DictionaryDecimalBinaryAccessor(\n+                  (IntVector) vector,\n+                  dictionary);\n+            case INT64:\n+              return new DictionaryDecimalLongAccessor(\n+                  (IntVector) vector,\n+                  dictionary);\n+            case INT32:\n+              return new DictionaryDecimalIntAccessor(\n+                  (IntVector) vector,\n+                  dictionary);\n+            default:\n+              throw new UnsupportedOperationException(\n+                  \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+          }\n+        default:\n+          throw new UnsupportedOperationException(\n+              \"Unsupported logical type: \" + primitive.getOriginalType());\n+      }\n+    } else {\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new DictionaryBinaryAccessor((IntVector) vector, dictionary);\n+        case FLOAT:\n+          return new DictionaryFloatAccessor((IntVector) vector, dictionary);\n+        case INT64:\n+          return new DictionaryLongAccessor((IntVector) vector, dictionary);\n+        case DOUBLE:\n+          return new DictionaryDoubleAccessor((IntVector) vector, dictionary);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  @NotNull\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private static ArrowVectorAccessor getPlainVectorAccessor(FieldVector vector) {\n+    if (vector instanceof BitVector) {\n+      return new BooleanAccessor((BitVector) vector);\n+    } else if (vector instanceof IntVector) {\n+      return new IntAccessor((IntVector) vector);\n+    } else if (vector instanceof BigIntVector) {\n+      return new LongAccessor((BigIntVector) vector);\n+    } else if (vector instanceof Float4Vector) {\n+      return new FloatAccessor((Float4Vector) vector);\n+    } else if (vector instanceof Float8Vector) {\n+      return new DoubleAccessor((Float8Vector) vector);\n+    } else if (vector instanceof IcebergArrowVectors.DecimalArrowVector) {\n+      return new DecimalAccessor((IcebergArrowVectors.DecimalArrowVector) vector);\n+    } else if (vector instanceof IcebergArrowVectors.VarcharArrowVector) {\n+      return new StringAccessor((IcebergArrowVectors.VarcharArrowVector) vector);\n+    } else if (vector instanceof IcebergArrowVectors.VarBinaryArrowVector) {\n+      return new BinaryAccessor((IcebergArrowVectors.VarBinaryArrowVector) vector);\n+    } else if (vector instanceof DateDayVector) {\n+      return new DateAccessor((DateDayVector) vector);\n+    } else if (vector instanceof TimeStampMicroTZVector) {\n+      return new TimestampAccessor((TimeStampMicroTZVector) vector);\n+    } else if (vector instanceof ListVector) {\n+      ListVector listVector = (ListVector) vector;\n+      return new ArrayAccessor(listVector);\n+    } else if (vector instanceof StructVector) {\n+      StructVector structVector = (StructVector) vector;\n+      return new StructAccessor(structVector);\n+    }\n+    throw new UnsupportedOperationException(\"Unsupported vector: \" + vector.getClass());\n+  }\n+\n+  private static class BooleanAccessor extends ArrowVectorAccessor {\n+\n+    private final BitVector vector;\n+\n+    BooleanAccessor(BitVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final boolean getBoolean(int rowId) {\n+      return vector.get(rowId) == 1;\n+    }\n+  }\n+\n+  private static class IntAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector vector;\n+\n+    IntAccessor(IntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final int getInt(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class LongAccessor extends ArrowVectorAccessor {\n+\n+    private final BigIntVector vector;\n+\n+    LongAccessor(BigIntVector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryLongAccessor extends ArrowVectorAccessor {\n+\n+    private final Dictionary parquetDictionary;\n+    private final IntVector offsetVector;\n+\n+    DictionaryLongAccessor(IntVector vector, Dictionary dictionary) {\n+      super(vector);\n+      this.offsetVector = vector;\n+      this.parquetDictionary = dictionary;\n+    }\n+\n+    @Override\n+    final long getLong(int rowId) {\n+      return parquetDictionary.decodeToLong(offsetVector.get(rowId));\n+    }\n+  }\n+\n+  private static class FloatAccessor extends ArrowVectorAccessor {\n+\n+    private final Float4Vector vector;\n+\n+    FloatAccessor(Float4Vector vector) {\n+      super(vector);\n+      this.vector = vector;\n+    }\n+\n+    @Override\n+    final float getFloat(int rowId) {\n+      return vector.get(rowId);\n+    }\n+  }\n+\n+  private static class DictionaryFloatAccessor extends ArrowVectorAccessor {\n+\n+    private final IntVector offsetVector;\n+    private final Dictionary parquetDictionary;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53c920b24e2599ee70a5e9ce96a5a839dcb4753a"}, "originalPosition": 236}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NDg5MDc2OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMDozMToxM1rOGZmZsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMDozMToxM1rOGZmZsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5Njc1Mw==", "bodyText": "Is this needed since you've updated build.gradle?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r429496753", "createdAt": "2020-05-23T00:31:13Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java", "diffHunk": "@@ -54,6 +56,11 @@ protected abstract Record writeAndRead(String desc,\n   @Rule\n   public TemporaryFolder temp = new TemporaryFolder();\n \n+  @BeforeClass\n+  public static void beforeClass() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53c920b24e2599ee70a5e9ce96a5a839dcb4753a"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NDg5ODI0OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMDo0MTo1M1rOGZmd2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMDo0MTo1M1rOGZmd2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NzgxOQ==", "bodyText": "This is missing the type parameter for VectorizedReader.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r429497819", "createdAt": "2020-05-23T00:41:53Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.NullCheckingForGet;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.ArrowAllocation;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedSparkParquetReaders {\n+\n+  private VectorizedSparkParquetReaders() {\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  public static ColumnarBatchReader buildReader(\n+      Schema expectedSchema,\n+      MessageType fileSchema,\n+      Integer recordsPerBatch) {\n+    return (ColumnarBatchReader)\n+        TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+            new VectorizedReaderBuilder(expectedSchema, fileSchema, recordsPerBatch));\n+  }\n+\n+  private static class VectorizedReaderBuilder extends TypeWithSchemaVisitor<VectorizedReader<?>> {\n+    private final MessageType parquetSchema;\n+    private final Schema icebergSchema;\n+    private final BufferAllocator rootAllocator;\n+    private final int batchSize;\n+\n+    VectorizedReaderBuilder(\n+        Schema expectedSchema,\n+        MessageType parquetSchema,\n+        int bSize) {\n+      this.parquetSchema = parquetSchema;\n+      this.icebergSchema = expectedSchema;\n+      this.batchSize = bSize;\n+      this.rootAllocator = ArrowAllocation.rootAllocator()\n+          .newChildAllocator(\"VectorizedReadBuilder\", 0, Long.MAX_VALUE);\n+    }\n+\n+    @Override\n+    public VectorizedReader message(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53c920b24e2599ee70a5e9ce96a5a839dcb4753a"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NDg5ODY4OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMDo0MjozMVrOGZmeGg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMDo0MjozMVrOGZmeGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5Nzg4Mg==", "bodyText": "IntelliJ is telling me that this isn't needed?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r429497882", "createdAt": "2020-05-23T00:42:31Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.NullCheckingForGet;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.ArrowAllocation;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedSparkParquetReaders {\n+\n+  private VectorizedSparkParquetReaders() {\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53c920b24e2599ee70a5e9ce96a5a839dcb4753a"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY3NDg5OTg4OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMDo0NDozMlrOGZmezg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yM1QwMDo0NDozMlrOGZmezg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5ODA2Mg==", "bodyText": "Now that #1004 is in, this can be removed. Now, we guarantee that the expected schema has all of the necessary columns. No need to create requiredSchema and handle it differently.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r429498062", "createdAt": "2020-05-23T00:44:32Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import java.util.Iterator;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+\n+class BatchDataReader extends BaseDataReader<ColumnarBatch> {\n+  private final Schema tableSchema;\n+  private final Schema expectedSchema;\n+  private final boolean caseSensitive;\n+  private final int batchSize;\n+\n+  BatchDataReader(\n+      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n+      EncryptionManager encryptionManager, boolean caseSensitive, int size) {\n+    super(task, fileIo, encryptionManager);\n+    this.tableSchema = tableSchema;\n+    this.expectedSchema = expectedSchema;\n+    this.caseSensitive = caseSensitive;\n+    this.batchSize = size;\n+  }\n+\n+  @Override\n+  Iterator<ColumnarBatch> open(FileScanTask task) {\n+    // schema or rows returned by readers\n+    Schema finalSchema = expectedSchema;\n+    // schema needed for the projection and filtering\n+    StructType sparkType = SparkSchemaUtil.convert(finalSchema);\n+    Schema requiredSchema = SparkSchemaUtil.prune(tableSchema, sparkType, task.residual(), caseSensitive);\n+    boolean hasExtraFilterColumns = requiredSchema.columns().size() != finalSchema.columns().size();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "53c920b24e2599ee70a5e9ce96a5a839dcb4753a"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODYwOTc1OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMDo0MTo0MFrOGeslRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzo1Mjo1N1rOGexTBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg0MDkwMQ==", "bodyText": "This method isn't used, and the main difference between it and assertArrowVectors is that the other one asserts that each vector is an IcebergArrowColumnVector, which I don't think is actually necessary. Is there a reason to require a specific type instead of just validating the data in each row?\nAlso, since the ColumnarBatch provides access to an InternalRow that is already supported, I was able to use the existing assertEqualsUnsafe(StructType, Record, InternalRow) in a loop, like this:\n  public static void assertEqualsBatch(Types.StructType struct, List<Record> expected, ColumnarBatch batch) {\n    for (int r = 0; r < batch.numRows(); r++) {\n      assertEqualsUnsafe(struct, expected.get(r), batch.getRow(r));\n    }\n  }\nThat required fixing null handling in assertEqualsUnsafe, which wasn't calling isNullAt, but once that was updated everything works without these two fairly large methods. I'd prefer to move to using assertEqualsBatch instead unless you think that introduces a problem.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434840901", "createdAt": "2020-06-03T20:41:40Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java", "diffHunk": "@@ -78,6 +81,48 @@ public static void assertEqualsSafe(Types.StructType struct, Record rec, Row row\n     }\n   }\n \n+  public static void assertEqualsUnsafe(Types.StructType struct, List<Record> expected, ColumnarBatch batch) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxODE1MQ==", "bodyText": "Fixed in my PR.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434918151", "createdAt": "2020-06-03T23:52:57Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java", "diffHunk": "@@ -78,6 +81,48 @@ public static void assertEqualsSafe(Types.StructType struct, Record rec, Row row\n     }\n   }\n \n+  public static void assertEqualsUnsafe(Types.StructType struct, List<Record> expected, ColumnarBatch batch) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg0MDkwMQ=="}, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODYxNzg2OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMDo0NDoyMlrOGesqYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMDo0NDoyMlrOGesqYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg0MjIwOQ==", "bodyText": "Can we use Arrays.fill instead of leaving some bytes uninitialized?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434842209", "createdAt": "2020-06-03T20:44:22Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java", "diffHunk": "@@ -442,4 +474,226 @@ private static BigInteger randomUnscaled(int precision, Random random) {\n \n     return new BigInteger(sb.toString());\n   }\n+\n+  private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {\n+\n+    private DictionaryEncodedDataGenerator(Schema schema, long seed) {\n+      super(schema, seed);\n+    }\n+\n+    @Override\n+    public Object primitive(Type.PrimitiveType primitive) {\n+      Object result = generateDictionaryEncodablePrimitive(primitive, random);\n+      return super.getPrimitive(primitive, result);\n+    }\n+\n+    @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+    private static Object generateDictionaryEncodablePrimitive(Type.PrimitiveType primitive, Random random) {\n+      // 3 choices\n+      int choice = random.nextInt(3);\n+      switch (primitive.typeId()) {\n+        case BOOLEAN:\n+          return true; // doesn't really matter for booleans since they are not dictionary encoded\n+\n+        case INTEGER:\n+          switch (choice) {\n+            case 0:\n+              return 0;\n+            case 1:\n+              return 1;\n+            case 2:\n+              return 2;\n+          }\n+\n+        case LONG:\n+          switch (choice) {\n+            case 0:\n+              return 0L;\n+            case 1:\n+              return 1L;\n+            case 2:\n+              return 2L;\n+          }\n+\n+        case FLOAT:\n+          switch (choice) {\n+            case 0:\n+              return 0.0f;\n+            case 1:\n+              return 1.0f;\n+            case 2:\n+              return 2.0f;\n+          }\n+\n+        case DOUBLE:\n+          switch (choice) {\n+            case 0:\n+              return 0.0d;\n+            case 1:\n+              return 1.0d;\n+            case 2:\n+              return 2.0d;\n+          }\n+\n+        case DATE:\n+          switch (choice) {\n+            case 0:\n+              return 0;\n+            case 1:\n+              return 1;\n+            case 2:\n+              return 2;\n+          }\n+\n+        case TIME:\n+          switch (choice) {\n+            case 0:\n+              return 0L;\n+            case 1:\n+              return 1L;\n+            case 2:\n+              return 2L;\n+          }\n+\n+        case TIMESTAMP:\n+          switch (choice) {\n+            case 0:\n+              return 0L;\n+            case 1:\n+              return 1L;\n+            case 2:\n+              return 2L;\n+          }\n+\n+        case STRING:\n+          switch (choice) {\n+            case 0:\n+              return UTF8String.fromString(\"0\");\n+            case 1:\n+              return UTF8String.fromString(\"1\");\n+            case 2:\n+              return UTF8String.fromString(\"2\");\n+          }\n+\n+        case FIXED:\n+          byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n+          switch (choice) {\n+            case 0:\n+              fixed[0] = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 159}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODY3NzY3OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMTowMTowNVrOGetPQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzo1Mjo0NlrOGexSyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg1MTY1MQ==", "bodyText": "I think the way that this extends the base class is awkward. It overrides primitive, but then needs to call getPrimitive. What getPrimitive is doing is not obvious in the child class, and the Random variable needs to be directly accessible causing the need to override checkstyle.\nInstead, the base class should add a method to generate a value for a primitive, like randomValue and pass a Random into it. Then random can stay private and we don't need to override checkstyle. Also, the conversion only needs to happen in one place, the implementation of primitive.\n    @Override\n    public Object primitive(Type.PrimitiveType primitive) {\n      Object result = randomValue(primitive, random);\n      // For the primitives that Avro needs a different type than Spark, fix\n      // them here.\n      switch (primitive.typeId()) {\n        ...\n      }\n    }\n\n    protected Object randomValue(Type.PrimitiveType primitive, Random rand) {\n      return generatePrimitive(primitive, rand);\n    }", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434851651", "createdAt": "2020-06-03T21:01:05Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java", "diffHunk": "@@ -442,4 +474,226 @@ private static BigInteger randomUnscaled(int precision, Random random) {\n \n     return new BigInteger(sb.toString());\n   }\n+\n+  private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxODA4OQ==", "bodyText": "Included in my PR.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434918089", "createdAt": "2020-06-03T23:52:46Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java", "diffHunk": "@@ -442,4 +474,226 @@ private static BigInteger randomUnscaled(int precision, Random random) {\n \n     return new BigInteger(sb.toString());\n   }\n+\n+  private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg1MTY1MQ=="}, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODY5MjI1OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMTowNDowMlrOGetYeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzo1MjozNlrOGexSnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg1NDAwOA==", "bodyText": "This implementation seems over-complicated with a lot of unnecessary switch statements. I think checkstyle was right.\nI think you can simplify most of the implementations for types by just converting the choice variable (maybe name it better?)\n    @Override\n    protected Object randomValue(Type.PrimitiveType primitive, Random random) {\n      // 3 choices\n      int choice = random.nextInt(3);\n      switch (primitive.typeId()) {\n        case BOOLEAN:\n          return true; // doesn't really matter for booleans since they are not dictionary encoded\n        case INTEGER:\n        case DATE:\n          return choice;\n        case FLOAT:\n          return (float) choice;\n        case DOUBLE:\n          return (double) choice;\n        case LONG:\n        case TIME:\n        case TIMESTAMP:\n          return (long) choice;\n        case STRING:\n          return UTF8String.fromString(String.valueOf(choice));\n        case FIXED:\n          byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n          Arrays.fill(fixed, (byte) choice);\n          return fixed;\n        case BINARY:\n          byte[] binary = new byte[choice + 1];\n          Arrays.fill(binary, (byte) choice);\n          return binary;\n        case DECIMAL:\n          Types.DecimalType type = (Types.DecimalType) primitive;\n          BigInteger unscaled = new BigInteger(String.valueOf(choice + 1));\n          return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n        default:\n          throw new IllegalArgumentException(\n              \"Cannot generate random value for unknown type: \" + primitive);\n      }\n    }\nThis also uses Arrays.fill like I suggested below and updates binary to test different lengths.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434854008", "createdAt": "2020-06-03T21:04:02Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java", "diffHunk": "@@ -442,4 +474,226 @@ private static BigInteger randomUnscaled(int precision, Random random) {\n \n     return new BigInteger(sb.toString());\n   }\n+\n+  private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {\n+\n+    private DictionaryEncodedDataGenerator(Schema schema, long seed) {\n+      super(schema, seed);\n+    }\n+\n+    @Override\n+    public Object primitive(Type.PrimitiveType primitive) {\n+      Object result = generateDictionaryEncodablePrimitive(primitive, random);\n+      return super.getPrimitive(primitive, result);\n+    }\n+\n+    @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxODA0NA==", "bodyText": "Included in my PR.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434918044", "createdAt": "2020-06-03T23:52:36Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java", "diffHunk": "@@ -442,4 +474,226 @@ private static BigInteger randomUnscaled(int precision, Random random) {\n \n     return new BigInteger(sb.toString());\n   }\n+\n+  private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {\n+\n+    private DictionaryEncodedDataGenerator(Schema schema, long seed) {\n+      super(schema, seed);\n+    }\n+\n+    @Override\n+    public Object primitive(Type.PrimitiveType primitive) {\n+      Object result = generateDictionaryEncodablePrimitive(primitive, random);\n+      return super.getPrimitive(primitive, result);\n+    }\n+\n+    @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg1NDAwOA=="}, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODczNDEwOnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMToxMjo1M1rOGetylA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzo1MjoyN1rOGexSdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2MDY5Mg==", "bodyText": "I think it would be easier to understand if this were a combination of the normal generator and the dictionary-encoded generator and used just some number of records before falling back. Then you would only need the two existing ways to generate primitives. Something like this, where generateDictionaryEncodablePrimitive() is what I pasted for DictionaryEncodedDataGenerator above:\n  private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {\n    private DictionaryEncodedDataGenerator(Schema schema, long seed) {\n      super(schema, seed);\n    }\n\n    @Override\n    protected Object randomValue(Type.PrimitiveType primitive, Random random) {\n      return generateDictionaryEncodablePrimitive(primitive, random);\n    }\n  }\n\n  private static class FallbackDataGenerator extends RandomDataGenerator {\n    private final long dictionaryEncodedRows;\n    private long rowCount = 0;\n\n    private FallbackDataGenerator(Schema schema, long seed, long numDictionaryEncoded) {\n      super(schema, seed);\n      this.dictionaryEncodedRows = numDictionaryEncoded;\n    }\n\n    @Override\n    protected Object randomValue(Type.PrimitiveType primitive, Random rand) {\n      this.rowCount += 1;\n      if (rowCount > dictionaryEncodedRows) {\n        return generatePrimitive(primitive, rand);\n      } else {\n        return generateDictionaryEncodablePrimitive(primitive, rand);\n      }\n    }\n  }", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434860692", "createdAt": "2020-06-03T21:12:53Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java", "diffHunk": "@@ -442,4 +474,226 @@ private static BigInteger randomUnscaled(int precision, Random random) {\n \n     return new BigInteger(sb.toString());\n   }\n+\n+  private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {\n+\n+    private DictionaryEncodedDataGenerator(Schema schema, long seed) {\n+      super(schema, seed);\n+    }\n+\n+    @Override\n+    public Object primitive(Type.PrimitiveType primitive) {\n+      Object result = generateDictionaryEncodablePrimitive(primitive, random);\n+      return super.getPrimitive(primitive, result);\n+    }\n+\n+    @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+    private static Object generateDictionaryEncodablePrimitive(Type.PrimitiveType primitive, Random random) {\n+      // 3 choices\n+      int choice = random.nextInt(3);\n+      switch (primitive.typeId()) {\n+        case BOOLEAN:\n+          return true; // doesn't really matter for booleans since they are not dictionary encoded\n+\n+        case INTEGER:\n+          switch (choice) {\n+            case 0:\n+              return 0;\n+            case 1:\n+              return 1;\n+            case 2:\n+              return 2;\n+          }\n+\n+        case LONG:\n+          switch (choice) {\n+            case 0:\n+              return 0L;\n+            case 1:\n+              return 1L;\n+            case 2:\n+              return 2L;\n+          }\n+\n+        case FLOAT:\n+          switch (choice) {\n+            case 0:\n+              return 0.0f;\n+            case 1:\n+              return 1.0f;\n+            case 2:\n+              return 2.0f;\n+          }\n+\n+        case DOUBLE:\n+          switch (choice) {\n+            case 0:\n+              return 0.0d;\n+            case 1:\n+              return 1.0d;\n+            case 2:\n+              return 2.0d;\n+          }\n+\n+        case DATE:\n+          switch (choice) {\n+            case 0:\n+              return 0;\n+            case 1:\n+              return 1;\n+            case 2:\n+              return 2;\n+          }\n+\n+        case TIME:\n+          switch (choice) {\n+            case 0:\n+              return 0L;\n+            case 1:\n+              return 1L;\n+            case 2:\n+              return 2L;\n+          }\n+\n+        case TIMESTAMP:\n+          switch (choice) {\n+            case 0:\n+              return 0L;\n+            case 1:\n+              return 1L;\n+            case 2:\n+              return 2L;\n+          }\n+\n+        case STRING:\n+          switch (choice) {\n+            case 0:\n+              return UTF8String.fromString(\"0\");\n+            case 1:\n+              return UTF8String.fromString(\"1\");\n+            case 2:\n+              return UTF8String.fromString(\"2\");\n+          }\n+\n+        case FIXED:\n+          byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n+          switch (choice) {\n+            case 0:\n+              fixed[0] = 0;\n+              return fixed;\n+            case 1:\n+              fixed[0] = 1;\n+              return fixed;\n+            case 2:\n+              fixed[0] = 2;\n+              return fixed;\n+          }\n+\n+        case BINARY:\n+          byte[] binary = new byte[4];\n+          switch (choice) {\n+            case 0:\n+              binary[0] = 0;\n+              return binary;\n+            case 1:\n+              binary[0] = 1;\n+              return binary;\n+            case 2:\n+              binary[0] = 2;\n+              return binary;\n+          }\n+\n+        case DECIMAL:\n+          Types.DecimalType type = (Types.DecimalType) primitive;\n+          switch (choice) {\n+            case 0:\n+              BigInteger unscaled = new BigInteger(\"1\");\n+              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n+            case 1:\n+              unscaled = new BigInteger(\"2\");\n+              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n+            case 2:\n+              unscaled = new BigInteger(\"3\");\n+              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n+          }\n+\n+        default:\n+          throw new IllegalArgumentException(\n+              \"Cannot generate random value for unknown type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class DictionaryFallbackToPlainEncodingDataGenerator extends RandomDataGenerator {\n+    private final long numValues;\n+    private final float fraction;\n+    private int current;\n+\n+    private DictionaryFallbackToPlainEncodingDataGenerator(Schema schema, long seed, int numRecords, float fraction) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 209}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxODAwNw==", "bodyText": "Included in my PR.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434918007", "createdAt": "2020-06-03T23:52:27Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java", "diffHunk": "@@ -442,4 +474,226 @@ private static BigInteger randomUnscaled(int precision, Random random) {\n \n     return new BigInteger(sb.toString());\n   }\n+\n+  private static class DictionaryEncodedDataGenerator extends RandomDataGenerator {\n+\n+    private DictionaryEncodedDataGenerator(Schema schema, long seed) {\n+      super(schema, seed);\n+    }\n+\n+    @Override\n+    public Object primitive(Type.PrimitiveType primitive) {\n+      Object result = generateDictionaryEncodablePrimitive(primitive, random);\n+      return super.getPrimitive(primitive, result);\n+    }\n+\n+    @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+    private static Object generateDictionaryEncodablePrimitive(Type.PrimitiveType primitive, Random random) {\n+      // 3 choices\n+      int choice = random.nextInt(3);\n+      switch (primitive.typeId()) {\n+        case BOOLEAN:\n+          return true; // doesn't really matter for booleans since they are not dictionary encoded\n+\n+        case INTEGER:\n+          switch (choice) {\n+            case 0:\n+              return 0;\n+            case 1:\n+              return 1;\n+            case 2:\n+              return 2;\n+          }\n+\n+        case LONG:\n+          switch (choice) {\n+            case 0:\n+              return 0L;\n+            case 1:\n+              return 1L;\n+            case 2:\n+              return 2L;\n+          }\n+\n+        case FLOAT:\n+          switch (choice) {\n+            case 0:\n+              return 0.0f;\n+            case 1:\n+              return 1.0f;\n+            case 2:\n+              return 2.0f;\n+          }\n+\n+        case DOUBLE:\n+          switch (choice) {\n+            case 0:\n+              return 0.0d;\n+            case 1:\n+              return 1.0d;\n+            case 2:\n+              return 2.0d;\n+          }\n+\n+        case DATE:\n+          switch (choice) {\n+            case 0:\n+              return 0;\n+            case 1:\n+              return 1;\n+            case 2:\n+              return 2;\n+          }\n+\n+        case TIME:\n+          switch (choice) {\n+            case 0:\n+              return 0L;\n+            case 1:\n+              return 1L;\n+            case 2:\n+              return 2L;\n+          }\n+\n+        case TIMESTAMP:\n+          switch (choice) {\n+            case 0:\n+              return 0L;\n+            case 1:\n+              return 1L;\n+            case 2:\n+              return 2L;\n+          }\n+\n+        case STRING:\n+          switch (choice) {\n+            case 0:\n+              return UTF8String.fromString(\"0\");\n+            case 1:\n+              return UTF8String.fromString(\"1\");\n+            case 2:\n+              return UTF8String.fromString(\"2\");\n+          }\n+\n+        case FIXED:\n+          byte[] fixed = new byte[((Types.FixedType) primitive).length()];\n+          switch (choice) {\n+            case 0:\n+              fixed[0] = 0;\n+              return fixed;\n+            case 1:\n+              fixed[0] = 1;\n+              return fixed;\n+            case 2:\n+              fixed[0] = 2;\n+              return fixed;\n+          }\n+\n+        case BINARY:\n+          byte[] binary = new byte[4];\n+          switch (choice) {\n+            case 0:\n+              binary[0] = 0;\n+              return binary;\n+            case 1:\n+              binary[0] = 1;\n+              return binary;\n+            case 2:\n+              binary[0] = 2;\n+              return binary;\n+          }\n+\n+        case DECIMAL:\n+          Types.DecimalType type = (Types.DecimalType) primitive;\n+          switch (choice) {\n+            case 0:\n+              BigInteger unscaled = new BigInteger(\"1\");\n+              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n+            case 1:\n+              unscaled = new BigInteger(\"2\");\n+              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n+            case 2:\n+              unscaled = new BigInteger(\"3\");\n+              return Decimal.apply(new BigDecimal(unscaled, type.scale()));\n+          }\n+\n+        default:\n+          throw new IllegalArgumentException(\n+              \"Cannot generate random value for unknown type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class DictionaryFallbackToPlainEncodingDataGenerator extends RandomDataGenerator {\n+    private final long numValues;\n+    private final float fraction;\n+    private int current;\n+\n+    private DictionaryFallbackToPlainEncodingDataGenerator(Schema schema, long seed, int numRecords, float fraction) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2MDY5Mg=="}, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 209}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODc1MDk5OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/data/AvroDataTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMToxODoxNVrOGet9IQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzo1NDo0NVrOGexU9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2MzM5Mw==", "bodyText": "Why were these converted to optional?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434863393", "createdAt": "2020-06-03T21:18:15Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/AvroDataTest.java", "diffHunk": "@@ -44,18 +44,18 @@\n       optional(101, \"data\", Types.StringType.get()),\n       required(102, \"b\", Types.BooleanType.get()),\n       optional(103, \"i\", Types.IntegerType.get()),\n-      required(104, \"l\", LongType.get()),\n+      optional(104, \"l\", LongType.get()),\n       optional(105, \"f\", Types.FloatType.get()),\n-      required(106, \"d\", Types.DoubleType.get()),\n+      optional(106, \"d\", Types.DoubleType.get()),\n       optional(107, \"date\", Types.DateType.get()),\n-      required(108, \"ts\", Types.TimestampType.withZone()),\n-      required(110, \"s\", Types.StringType.get()),\n+      optional(108, \"ts\", Types.TimestampType.withZone()),\n+      optional(110, \"s\", Types.StringType.get()),\n       //required(111, \"uuid\", Types.UUIDType.get()),\n-      required(112, \"fixed\", Types.FixedType.ofLength(7)),\n+      optional(112, \"fixed\", Types.FixedType.ofLength(7)),\n       optional(113, \"bytes\", Types.BinaryType.get()),\n-      required(114, \"dec_9_0\", Types.DecimalType.of(9, 0)),\n-      required(115, \"dec_11_2\", Types.DecimalType.of(11, 2)),\n-      required(116, \"dec_38_10\", Types.DecimalType.of(38, 10)) // spark's maximum precision\n+      optional(114, \"dec_9_0\", Types.DecimalType.of(9, 0)),\n+      optional(115, \"dec_11_2\", Types.DecimalType.of(11, 2)),\n+      optional(116, \"dec_38_10\", Types.DecimalType.of(38, 10)) // spark's maximum precision", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxODY0Ng==", "bodyText": "Reverted this change in my PR.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434918646", "createdAt": "2020-06-03T23:54:45Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/AvroDataTest.java", "diffHunk": "@@ -44,18 +44,18 @@\n       optional(101, \"data\", Types.StringType.get()),\n       required(102, \"b\", Types.BooleanType.get()),\n       optional(103, \"i\", Types.IntegerType.get()),\n-      required(104, \"l\", LongType.get()),\n+      optional(104, \"l\", LongType.get()),\n       optional(105, \"f\", Types.FloatType.get()),\n-      required(106, \"d\", Types.DoubleType.get()),\n+      optional(106, \"d\", Types.DoubleType.get()),\n       optional(107, \"date\", Types.DateType.get()),\n-      required(108, \"ts\", Types.TimestampType.withZone()),\n-      required(110, \"s\", Types.StringType.get()),\n+      optional(108, \"ts\", Types.TimestampType.withZone()),\n+      optional(110, \"s\", Types.StringType.get()),\n       //required(111, \"uuid\", Types.UUIDType.get()),\n-      required(112, \"fixed\", Types.FixedType.ofLength(7)),\n+      optional(112, \"fixed\", Types.FixedType.ofLength(7)),\n       optional(113, \"bytes\", Types.BinaryType.get()),\n-      required(114, \"dec_9_0\", Types.DecimalType.of(9, 0)),\n-      required(115, \"dec_11_2\", Types.DecimalType.of(11, 2)),\n-      required(116, \"dec_38_10\", Types.DecimalType.of(38, 10)) // spark's maximum precision\n+      optional(114, \"dec_9_0\", Types.DecimalType.of(9, 0)),\n+      optional(115, \"dec_11_2\", Types.DecimalType.of(11, 2)),\n+      optional(116, \"dec_38_10\", Types.DecimalType.of(38, 10)) // spark's maximum precision", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2MzM5Mw=="}, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODc3Mzk2OnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMToyNTo1M1rOGeuLOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzo1MjoxMlrOGexSLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2NzAwMQ==", "bodyText": "I think this should create a different base class instead of using AvroDataTest. This removes quite a few methods from AvroDataTest and also makes changes to it, like making all of the fields optional. Rather than doing that, I think it would make sense to just copy those test cases into a new VectorizedDataTest, since they are just schemas. And you could also add other cases, like these:\n  @Test\n  public void testStructWithRequiredFields() throws IOException {\n    writeAndValidate(TypeUtil.assignIncreasingFreshIds(new Schema(\n        Lists.transform(SUPPORTED_PRIMITIVES.fields(), Types.NestedField::asRequired))));\n  }\n\n  @Test\n  public void testStructWithOptionalFields() throws IOException {\n    writeAndValidate(TypeUtil.assignIncreasingFreshIds(new Schema(\n        Lists.transform(SUPPORTED_PRIMITIVES.fields(), Types.NestedField::asOptional))));\n  }\n\n  @Test\n  public void testNestedStruct() throws IOException {\n    writeAndValidate(TypeUtil.assignIncreasingFreshIds(new Schema(required(1, \"struct\", SUPPORTED_PRIMITIVES))));\n  }", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434867001", "createdAt": "2020-06-03T21:25:53Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.parquet.vectorized;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.data.AvroDataTest;\n+import org.apache.iceberg.spark.data.RandomData;\n+import org.apache.iceberg.spark.data.TestHelpers;\n+import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+public class TestParquetVectorizedReads extends AvroDataTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNzkzNQ==", "bodyText": "Included in my PR.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434917935", "createdAt": "2020-06-03T23:52:12Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java", "diffHunk": "@@ -0,0 +1,135 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.parquet.vectorized;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Iterator;\n+import java.util.List;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.data.AvroDataTest;\n+import org.apache.iceberg.spark.data.RandomData;\n+import org.apache.iceberg.spark.data.TestHelpers;\n+import org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.vectorized.ColumnarBatch;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+public class TestParquetVectorizedReads extends AvroDataTest {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2NzAwMQ=="}, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODc3NzE1OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMToyNzowM1rOGeuNJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzo1MDowOVrOGexP1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2NzQ5NQ==", "bodyText": "All this method does is call another method. Do we need both or can we move the body of lazyCheckEnableBatchRead here?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434867495", "createdAt": "2020-06-03T21:27:03Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -249,6 +290,34 @@ public Statistics estimateStatistics() {\n     return new Stats(sizeInBytes, numRows);\n   }\n \n+  @Override\n+  public boolean enableBatchRead() {\n+    return lazyCheckEnableBatchRead();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNzMzMw==", "bodyText": "Done in my PR.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434917333", "createdAt": "2020-06-03T23:50:09Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -249,6 +290,34 @@ public Statistics estimateStatistics() {\n     return new Stats(sizeInBytes, numRows);\n   }\n \n+  @Override\n+  public boolean enableBatchRead() {\n+    return lazyCheckEnableBatchRead();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2NzQ5NQ=="}, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODc4MzYzOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMToyOTozMVrOGeuRdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzo1MDo0MVrOGexQZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2ODU5Nw==", "bodyText": "This method isn't used and can be removed.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434868597", "createdAt": "2020-06-03T21:29:31Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.NullCheckingForGet;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.ArrowAllocation;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedSparkParquetReaders {\n+\n+  private VectorizedSparkParquetReaders() {\n+  }\n+\n+  public static ColumnarBatchReader buildReader(\n+      Schema expectedSchema,\n+      MessageType fileSchema,\n+      Integer recordsPerBatch) {\n+    return (ColumnarBatchReader)\n+        TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+            new VectorizedReaderBuilder(expectedSchema, fileSchema, recordsPerBatch));\n+  }\n+\n+  private static class VectorizedReaderBuilder extends TypeWithSchemaVisitor<VectorizedReader<?>> {\n+    private final MessageType parquetSchema;\n+    private final Schema icebergSchema;\n+    private final BufferAllocator rootAllocator;\n+    private final int batchSize;\n+\n+    VectorizedReaderBuilder(\n+        Schema expectedSchema,\n+        MessageType parquetSchema,\n+        int bSize) {\n+      this.parquetSchema = parquetSchema;\n+      this.icebergSchema = expectedSchema;\n+      this.batchSize = bSize;\n+      this.rootAllocator = ArrowAllocation.rootAllocator()\n+          .newChildAllocator(\"VectorizedReadBuilder\", 0, Long.MAX_VALUE);\n+    }\n+\n+    @Override\n+    public VectorizedReader<?> message(\n+            Types.StructType expected, MessageType message,\n+            List<VectorizedReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public VectorizedReader<?> struct(\n+            Types.StructType expected, GroupType struct,\n+            List<VectorizedReader<?>> fieldReaders) {\n+\n+      Map<Integer, VectorizedReader<?>> readersById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+\n+      IntStream.range(0, fields.size())\n+          .forEach(pos -> readersById.put(fields.get(pos).getId().intValue(), fieldReaders.get(pos)));\n+\n+      List<Types.NestedField> icebergFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+\n+      List<VectorizedReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          icebergFields.size());\n+\n+      for (Types.NestedField field : icebergFields) {\n+        int id = field.fieldId();\n+        VectorizedReader<?> reader = readersById.get(id);\n+        if (reader != null) {\n+          reorderedFields.add(reader);\n+        } else {\n+          reorderedFields.add(VectorizedArrowReader.nulls());\n+        }\n+      }\n+      return new ColumnarBatchReader(reorderedFields);\n+    }\n+\n+    @Override\n+    public VectorizedReader<?> primitive(\n+        org.apache.iceberg.types.Type.PrimitiveType expected,\n+        PrimitiveType primitive) {\n+\n+      // Create arrow vector for this field\n+      int parquetFieldId = primitive.getId().intValue();\n+      ColumnDescriptor desc = parquetSchema.getColumnDescription(currentPath());\n+      // Nested types not yet supported for vectorized reads\n+      if (desc.getMaxRepetitionLevel() > 0) {\n+        return null;\n+      }\n+      Types.NestedField icebergField = icebergSchema.findField(parquetFieldId);\n+      if (icebergField == null) {\n+        return null;\n+      }\n+      // Set the validity buffer if null checking is enabled in arrow\n+      return new VectorizedArrowReader(desc, icebergField, rootAllocator,\n+          batchSize, /* setArrowValidityVector */ NullCheckingForGet.NULL_CHECKING_ENABLED);\n+    }\n+\n+    protected MessageType type() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNzQ3OA==", "bodyText": "Done in my PR.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434917478", "createdAt": "2020-06-03T23:50:41Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.NullCheckingForGet;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.ArrowAllocation;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedSparkParquetReaders {\n+\n+  private VectorizedSparkParquetReaders() {\n+  }\n+\n+  public static ColumnarBatchReader buildReader(\n+      Schema expectedSchema,\n+      MessageType fileSchema,\n+      Integer recordsPerBatch) {\n+    return (ColumnarBatchReader)\n+        TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+            new VectorizedReaderBuilder(expectedSchema, fileSchema, recordsPerBatch));\n+  }\n+\n+  private static class VectorizedReaderBuilder extends TypeWithSchemaVisitor<VectorizedReader<?>> {\n+    private final MessageType parquetSchema;\n+    private final Schema icebergSchema;\n+    private final BufferAllocator rootAllocator;\n+    private final int batchSize;\n+\n+    VectorizedReaderBuilder(\n+        Schema expectedSchema,\n+        MessageType parquetSchema,\n+        int bSize) {\n+      this.parquetSchema = parquetSchema;\n+      this.icebergSchema = expectedSchema;\n+      this.batchSize = bSize;\n+      this.rootAllocator = ArrowAllocation.rootAllocator()\n+          .newChildAllocator(\"VectorizedReadBuilder\", 0, Long.MAX_VALUE);\n+    }\n+\n+    @Override\n+    public VectorizedReader<?> message(\n+            Types.StructType expected, MessageType message,\n+            List<VectorizedReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public VectorizedReader<?> struct(\n+            Types.StructType expected, GroupType struct,\n+            List<VectorizedReader<?>> fieldReaders) {\n+\n+      Map<Integer, VectorizedReader<?>> readersById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+\n+      IntStream.range(0, fields.size())\n+          .forEach(pos -> readersById.put(fields.get(pos).getId().intValue(), fieldReaders.get(pos)));\n+\n+      List<Types.NestedField> icebergFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+\n+      List<VectorizedReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          icebergFields.size());\n+\n+      for (Types.NestedField field : icebergFields) {\n+        int id = field.fieldId();\n+        VectorizedReader<?> reader = readersById.get(id);\n+        if (reader != null) {\n+          reorderedFields.add(reader);\n+        } else {\n+          reorderedFields.add(VectorizedArrowReader.nulls());\n+        }\n+      }\n+      return new ColumnarBatchReader(reorderedFields);\n+    }\n+\n+    @Override\n+    public VectorizedReader<?> primitive(\n+        org.apache.iceberg.types.Type.PrimitiveType expected,\n+        PrimitiveType primitive) {\n+\n+      // Create arrow vector for this field\n+      int parquetFieldId = primitive.getId().intValue();\n+      ColumnDescriptor desc = parquetSchema.getColumnDescription(currentPath());\n+      // Nested types not yet supported for vectorized reads\n+      if (desc.getMaxRepetitionLevel() > 0) {\n+        return null;\n+      }\n+      Types.NestedField icebergField = icebergSchema.findField(parquetFieldId);\n+      if (icebergField == null) {\n+        return null;\n+      }\n+      // Set the validity buffer if null checking is enabled in arrow\n+      return new VectorizedArrowReader(desc, icebergField, rootAllocator,\n+          batchSize, /* setArrowValidityVector */ NullCheckingForGet.NULL_CHECKING_ENABLED);\n+    }\n+\n+    protected MessageType type() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2ODU5Nw=="}, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODc5MDQzOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMTozMTo0NVrOGeuVvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzo1MTo1MFrOGexRqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2OTY5NQ==", "bodyText": "Coverage shows that this branch is never taken because there are no tests for read projection that use the vectorized path. Can you add tests based on TestReadProjection like the tests you added using AvroDataTest?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434869695", "createdAt": "2020-06-03T21:31:45Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.NullCheckingForGet;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.ArrowAllocation;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedSparkParquetReaders {\n+\n+  private VectorizedSparkParquetReaders() {\n+  }\n+\n+  public static ColumnarBatchReader buildReader(\n+      Schema expectedSchema,\n+      MessageType fileSchema,\n+      Integer recordsPerBatch) {\n+    return (ColumnarBatchReader)\n+        TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+            new VectorizedReaderBuilder(expectedSchema, fileSchema, recordsPerBatch));\n+  }\n+\n+  private static class VectorizedReaderBuilder extends TypeWithSchemaVisitor<VectorizedReader<?>> {\n+    private final MessageType parquetSchema;\n+    private final Schema icebergSchema;\n+    private final BufferAllocator rootAllocator;\n+    private final int batchSize;\n+\n+    VectorizedReaderBuilder(\n+        Schema expectedSchema,\n+        MessageType parquetSchema,\n+        int bSize) {\n+      this.parquetSchema = parquetSchema;\n+      this.icebergSchema = expectedSchema;\n+      this.batchSize = bSize;\n+      this.rootAllocator = ArrowAllocation.rootAllocator()\n+          .newChildAllocator(\"VectorizedReadBuilder\", 0, Long.MAX_VALUE);\n+    }\n+\n+    @Override\n+    public VectorizedReader<?> message(\n+            Types.StructType expected, MessageType message,\n+            List<VectorizedReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public VectorizedReader<?> struct(\n+            Types.StructType expected, GroupType struct,\n+            List<VectorizedReader<?>> fieldReaders) {\n+\n+      Map<Integer, VectorizedReader<?>> readersById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+\n+      IntStream.range(0, fields.size())\n+          .forEach(pos -> readersById.put(fields.get(pos).getId().intValue(), fieldReaders.get(pos)));\n+\n+      List<Types.NestedField> icebergFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+\n+      List<VectorizedReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          icebergFields.size());\n+\n+      for (Types.NestedField field : icebergFields) {\n+        int id = field.fieldId();\n+        VectorizedReader<?> reader = readersById.get(id);\n+        if (reader != null) {\n+          reorderedFields.add(reader);\n+        } else {\n+          reorderedFields.add(VectorizedArrowReader.nulls());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNzgwMw==", "bodyText": "Coverage shows that this is now tested with the update in my PR.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434917803", "createdAt": "2020-06-03T23:51:50Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.IntStream;\n+import org.apache.arrow.memory.BufferAllocator;\n+import org.apache.arrow.vector.NullCheckingForGet;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.arrow.ArrowAllocation;\n+import org.apache.iceberg.arrow.vectorized.VectorizedArrowReader;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class VectorizedSparkParquetReaders {\n+\n+  private VectorizedSparkParquetReaders() {\n+  }\n+\n+  public static ColumnarBatchReader buildReader(\n+      Schema expectedSchema,\n+      MessageType fileSchema,\n+      Integer recordsPerBatch) {\n+    return (ColumnarBatchReader)\n+        TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+            new VectorizedReaderBuilder(expectedSchema, fileSchema, recordsPerBatch));\n+  }\n+\n+  private static class VectorizedReaderBuilder extends TypeWithSchemaVisitor<VectorizedReader<?>> {\n+    private final MessageType parquetSchema;\n+    private final Schema icebergSchema;\n+    private final BufferAllocator rootAllocator;\n+    private final int batchSize;\n+\n+    VectorizedReaderBuilder(\n+        Schema expectedSchema,\n+        MessageType parquetSchema,\n+        int bSize) {\n+      this.parquetSchema = parquetSchema;\n+      this.icebergSchema = expectedSchema;\n+      this.batchSize = bSize;\n+      this.rootAllocator = ArrowAllocation.rootAllocator()\n+          .newChildAllocator(\"VectorizedReadBuilder\", 0, Long.MAX_VALUE);\n+    }\n+\n+    @Override\n+    public VectorizedReader<?> message(\n+            Types.StructType expected, MessageType message,\n+            List<VectorizedReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public VectorizedReader<?> struct(\n+            Types.StructType expected, GroupType struct,\n+            List<VectorizedReader<?>> fieldReaders) {\n+\n+      Map<Integer, VectorizedReader<?>> readersById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+\n+      IntStream.range(0, fields.size())\n+          .forEach(pos -> readersById.put(fields.get(pos).getId().intValue(), fieldReaders.get(pos)));\n+\n+      List<Types.NestedField> icebergFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+\n+      List<VectorizedReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          icebergFields.size());\n+\n+      for (Types.NestedField field : icebergFields) {\n+        int id = field.fieldId();\n+        VectorizedReader<?> reader = readersById.get(id);\n+        if (reader != null) {\n+          reorderedFields.add(reader);\n+        } else {\n+          reorderedFields.add(VectorizedArrowReader.nulls());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg2OTY5NQ=="}, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODgxMTMzOnYy", "diffSide": "RIGHT", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMTozOToyN1rOGeujNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMTo0ODozNlrOGeuyeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg3MzE0MQ==", "bodyText": "Coverage shows that this branch isn't taken -- line 213 is never used by the existing tests. The equivalent line for doubles is taken, though. Can you find out what's happening?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434873141", "createdAt": "2020-06-03T21:39:27Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java", "diffHunk": "@@ -202,7 +209,7 @@ public int nextBatchFloats(\n     if (actualBatchSize <= 0) {\n       return 0;\n     }\n-    if (eagerDecodeDictionary) {\n+    if (dictionaryDecodeMode == DictionaryDecodeMode.EAGER) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg3NzA1MA==", "bodyText": "Actually, this was in my local copy. When I reverted the changes I see that the tests are hitting this line. But the decimal line below is not showing up in coverage.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434877050", "createdAt": "2020-06-03T21:48:36Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java", "diffHunk": "@@ -202,7 +209,7 @@ public int nextBatchFloats(\n     if (actualBatchSize <= 0) {\n       return 0;\n     }\n-    if (eagerDecodeDictionary) {\n+    if (dictionaryDecodeMode == DictionaryDecodeMode.EAGER) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg3MzE0MQ=="}, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODgxMzQ2OnYy", "diffSide": "RIGHT", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMTo0MDoxMlrOGeukhQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMTo0MTowOFrOGeul_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg3MzQ3Nw==", "bodyText": "Coverage shows that this path isn't taken either.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434873477", "createdAt": "2020-06-03T21:40:12Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java", "diffHunk": "@@ -312,7 +319,7 @@ public int nextBatchFixedLengthDecimal(\n     if (actualBatchSize <= 0) {\n       return 0;\n     }\n-    if (eagerDecodeDictionary) {\n+    if (dictionaryDecodeMode == DictionaryDecodeMode.EAGER) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg3Mzg1NA==", "bodyText": "Looks like DictionaryDecimalBinaryAccessor is also not used by tests. That's probably related. Maybe decimals are not getting dictionary encoded?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434873854", "createdAt": "2020-06-03T21:41:08Z", "author": {"login": "rdblue"}, "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedPageIterator.java", "diffHunk": "@@ -312,7 +319,7 @@ public int nextBatchFixedLengthDecimal(\n     if (actualBatchSize <= 0) {\n       return 0;\n     }\n-    if (eagerDecodeDictionary) {\n+    if (dictionaryDecodeMode == DictionaryDecodeMode.EAGER) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg3MzQ3Nw=="}, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwOTAwNDk5OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzowMDo1OVrOGewZ4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzo0NzoxN1rOGexMuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwMzUyMw==", "bodyText": "I think this is correct. We don't need to have multiple task instances, especially since this will go away in 3.0.\nInstead, it's cleaner if we pass a reader factory into a single read task and call that factory in createPartitionReader:\n  private interface ReaderFactory<T> {\n    InputPartitionReader<T> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO io,\n                                   EncryptionManager encryptionManager, boolean caseSensitive);\n  }\n\n  private static class InternalRowReaderFactory implements ReaderFactory<InternalRow> {\n    private static final InternalRowReaderFactory INSTANCE = new InternalRowReaderFactory();\n\n    private InternalRowReaderFactory() {\n    }\n\n    @Override\n    public InputPartitionReader<InternalRow> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema,\n                                                    FileIO io, EncryptionManager encryptionManager,\n                                                    boolean caseSensitive) {\n      return new RowDataReader(task, tableSchema, expectedSchema, io, encryptionManager, caseSensitive);\n    }\n  }\n\n  private static class BatchReaderFactory implements ReaderFactory<ColumnarBatch> {\n    private final int batchSize;\n\n    BatchReaderFactory(int batchSize) {\n      this.batchSize = batchSize;\n    }\n\n    @Override\n    public InputPartitionReader<ColumnarBatch> create(CombinedScanTask task, Schema tableSchema, Schema expectedSchema,\n                                                    FileIO io, EncryptionManager encryptionManager,\n                                                    boolean caseSensitive) {\n      return new BatchDataReader(task, expectedSchema, io, encryptionManager, caseSensitive, batchSize);\n    }\n  }", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434903523", "createdAt": "2020-06-03T23:00:59Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -310,26 +379,27 @@ private static void mergeIcebergHadoopConfs(\n   @Override\n   public String toString() {\n     return String.format(\n-        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive);\n+        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead());\n   }\n \n-  private static class ReadTask implements InputPartition<InternalRow>, Serializable {\n-    private final CombinedScanTask task;\n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwMzczNA==", "bodyText": "Then the planBatchInputPartitions method is updated like this:\n      readTasks.add(new ReadTask<>(\n          task, tableSchemaString, expectedSchemaString, io, encryptionManager, caseSensitive, localityPreferred,\n          new BatchReaderFactory(batchSize)));", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434903734", "createdAt": "2020-06-03T23:01:54Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -310,26 +379,27 @@ private static void mergeIcebergHadoopConfs(\n   @Override\n   public String toString() {\n     return String.format(\n-        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive);\n+        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead());\n   }\n \n-  private static class ReadTask implements InputPartition<InternalRow>, Serializable {\n-    private final CombinedScanTask task;\n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwMzUyMw=="}, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNjUzNg==", "bodyText": "This is in the PR against your branch.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434916536", "createdAt": "2020-06-03T23:47:17Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -310,26 +379,27 @@ private static void mergeIcebergHadoopConfs(\n   @Override\n   public String toString() {\n     return String.format(\n-        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s)\",\n-        table, lazySchema().asStruct(), filterExpressions, caseSensitive);\n+        \"IcebergScan(table=%s, type=%s, filters=%s, caseSensitive=%s, batchedReads=%s)\",\n+        table, lazySchema().asStruct(), filterExpressions, caseSensitive, enableBatchRead());\n   }\n \n-  private static class ReadTask implements InputPartition<InternalRow>, Serializable {\n-    private final CombinedScanTask task;\n+  @SuppressWarnings(\"checkstyle:VisibilityModifier\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwMzUyMw=="}, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 177}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwOTAwODkzOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzowMzoxNlrOGewcUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzowMzoxNlrOGewcUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwNDE0Ng==", "bodyText": "I don't think this needed to be reformatted. Can you revert this change?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434904146", "createdAt": "2020-06-03T23:03:16Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -87,14 +93,17 @@\n   private List<Expression> filterExpressions = null;\n   private Filter[] pushedFilters = NO_FILTERS;\n   private final boolean localityPreferred;\n+  private final int batchSize;\n \n   // lazy variables\n   private Schema schema = null;\n   private StructType type = null; // cached because Spark accesses it multiple times\n   private List<CombinedScanTask> tasks = null; // lazy cache of tasks\n+  private Boolean enableBatchRead = null; // cache variable for enabling batched reads\n \n-  Reader(Table table, Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n-         boolean caseSensitive, DataSourceOptions options) {\n+  Reader(\n+      Table table, Broadcast<FileIO> io, Broadcast<EncryptionManager> encryptionManager,\n+      boolean caseSensitive, DataSourceOptions options) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwOTAxMjg3OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzowNToxMlrOGewemA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzo0Njo1MlrOGexMUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwNDcyOA==", "bodyText": "I don't see a test for this. Can you update some of the Spark tests to run both vectorized and non-vectorized? We can also do this in a follow-up, but we need to make sure that this code path is being tested as thoroughly as the non-vectorized code path.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434904728", "createdAt": "2020-06-03T23:05:12Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -178,6 +195,30 @@ public StructType readSchema() {\n     return lazyType();\n   }\n \n+  /**\n+   * This is called in the Spark Driver when data is to be materialized into {@link ColumnarBatch}\n+   */\n+  @Override\n+  public List<InputPartition<ColumnarBatch>> planBatchInputPartitions() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNjQzMg==", "bodyText": "I added cases to TestSparkReadProjection that use this path in the PR against your branch.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434916432", "createdAt": "2020-06-03T23:46:52Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -178,6 +195,30 @@ public StructType readSchema() {\n     return lazyType();\n   }\n \n+  /**\n+   * This is called in the Spark Driver when data is to be materialized into {@link ColumnarBatch}\n+   */\n+  @Override\n+  public List<InputPartition<ColumnarBatch>> planBatchInputPartitions() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwNDcyOA=="}, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwOTAyNTYyOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzoxMjoyM1rOGewmiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMzo0NjoyOFrOGexLzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwNjc2MA==", "bodyText": "Options passed into the read here should be short options because they come from the DataFrameReader interface:\nspark.read.format(\"iceberg\").config(\"snapshot-id\", snapId).load(\"db.table\");\nThe long option names like the ones you have here are what we use for table properties, which are tracked in TableProperties and documented. These options should default to the table property value, but a DataFrameReader option should override.\nFor short names, how about vectorization-enabled and batch-size? We should also add constants in TableProperties for the properties you have here. Let's turn off vectorization by default.\nAlso, we can drop \"iceberg\" from the start of the properties.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434906760", "createdAt": "2020-06-03T23:12:23Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -145,6 +154,14 @@\n     this.io = io;\n     this.encryptionManager = encryptionManager;\n     this.caseSensitive = caseSensitive;\n+\n+    boolean enableBatchReadsConfig =\n+        options.get(\"iceberg.read.parquet-vectorization.enabled\").map(Boolean::parseBoolean).orElse(true);\n+    if (!enableBatchReadsConfig) {\n+      enableBatchRead = Boolean.FALSE;\n+    }\n+    Optional<String> numRecordsPerBatchOpt = options.get(\"iceberg.read.parquet-vectorization.batch-size\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkxNjMwMQ==", "bodyText": "I included this change in the PR against your branch.", "url": "https://github.com/apache/iceberg/pull/828#discussion_r434916301", "createdAt": "2020-06-03T23:46:28Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/spark/source/Reader.java", "diffHunk": "@@ -145,6 +154,14 @@\n     this.io = io;\n     this.encryptionManager = encryptionManager;\n     this.caseSensitive = caseSensitive;\n+\n+    boolean enableBatchReadsConfig =\n+        options.get(\"iceberg.read.parquet-vectorization.enabled\").map(Boolean::parseBoolean).orElse(true);\n+    if (!enableBatchReadsConfig) {\n+      enableBatchRead = Boolean.FALSE;\n+    }\n+    Optional<String> numRecordsPerBatchOpt = options.get(\"iceberg.read.parquet-vectorization.batch-size\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDkwNjc2MA=="}, "originalCommit": {"oid": "032c8c12c972614ee5514b7c586021331c8bf002"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczOTAxNzkxOnYy", "diffSide": "RIGHT", "path": "spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQyMzozOTo0NVrOGjT_LA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQyMzo0NDo1M1rOGjUCVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY4MDgxMg==", "bodyText": "Why make this a method? So it can be overridden?", "url": "https://github.com/apache/iceberg/pull/828#discussion_r439680812", "createdAt": "2020-06-12T23:39:45Z", "author": {"login": "rdblue"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java", "diffHunk": "@@ -48,11 +48,15 @@\n import static org.apache.iceberg.types.Types.NestedField.required;\n \n public class TestParquetVectorizedReads extends AvroDataTest {\n-  private static final int NUM_ROWS = 1_000_000;\n+  private static final int NUM_ROWS = 200_000;\n \n   @Override\n   protected void writeAndValidate(Schema schema) throws IOException {\n-    writeAndValidate(schema, NUM_ROWS, 0L, RandomData.DEFAULT_NULL_PERCENTAGE, false, true);\n+    writeAndValidate(schema, getNumRows(), 0L, RandomData.DEFAULT_NULL_PERCENTAGE, false, true);\n+  }\n+\n+  protected int getNumRows() {\n+    return NUM_ROWS;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8893379821bf6532a2b7001b574e65a98f8a699a"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY4MTYyMg==", "bodyText": "Yes", "url": "https://github.com/apache/iceberg/pull/828#discussion_r439681622", "createdAt": "2020-06-12T23:44:53Z", "author": {"login": "samarthjain"}, "path": "spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java", "diffHunk": "@@ -48,11 +48,15 @@\n import static org.apache.iceberg.types.Types.NestedField.required;\n \n public class TestParquetVectorizedReads extends AvroDataTest {\n-  private static final int NUM_ROWS = 1_000_000;\n+  private static final int NUM_ROWS = 200_000;\n \n   @Override\n   protected void writeAndValidate(Schema schema) throws IOException {\n-    writeAndValidate(schema, NUM_ROWS, 0L, RandomData.DEFAULT_NULL_PERCENTAGE, false, true);\n+    writeAndValidate(schema, getNumRows(), 0L, RandomData.DEFAULT_NULL_PERCENTAGE, false, true);\n+  }\n+\n+  protected int getNumRows() {\n+    return NUM_ROWS;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY4MDgxMg=="}, "originalCommit": {"oid": "8893379821bf6532a2b7001b574e65a98f8a699a"}, "originalPosition": 14}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2830, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}