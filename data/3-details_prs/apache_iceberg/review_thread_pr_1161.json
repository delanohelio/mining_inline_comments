{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQzMzUwNDMx", "number": 1161, "reviewThreads": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwODo1MjowNFrOEKzoRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNFQyMjowMzowNlrOELaQxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5NzY3MTEwOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwODo1MjowNFrOGsDo_A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQwODo1MjowNFrOGsDo_A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg1MDE3Mg==", "bodyText": "Is possible to provide a unit test addressing your changes ?\nThanks.", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r448850172", "createdAt": "2020-07-02T08:52:04Z", "author": {"login": "openinx"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -30,8 +31,11 @@ protected String metadataTableName(MetadataTableType type) {\n     String tableName = table().toString();\n     if (tableName.contains(\"/\")) {\n       return tableName + \"#\" + type;\n-    } else if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\")) {\n-      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // HadoopCatalog tableName style is 'hadoop.ns.tb'\n+      return ((BaseTable) table()).operations().current().location() + \"#\" + type;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "6608675f429e50f78605c7b41e3e922ae2578c25"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwMjc5NjAyOnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjozNzozMFrOGs0x1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNFQyMjowMzoyOVrOGs-J6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NTI1NA==", "bodyText": "I don't think this comment is correct because the solution is to convert to a path table reference.\nIf i understand correctly, the problem in Spark 2.4 is that IcebergSource uses the Hive catalog or it uses HadoopTables to load a path. The table you're passing in was loaded by HadoopCatalog, so it doesn't work because that catalog is not available. I agree that the solution is to convert a Hadoop table to a path reference, but then this comment should explain what's happening: a HadoopCatalog was used to load the table, so convert to the path.", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449655254", "createdAt": "2020-07-03T16:37:30Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -30,8 +31,11 @@ protected String metadataTableName(MetadataTableType type) {\n     String tableName = table().toString();\n     if (tableName.contains(\"/\")) {\n       return tableName + \"#\" + type;\n-    } else if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\")) {\n-      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // HadoopCatalog tableName style is 'hadoop.ns.tb'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTc0Nzg2NA==", "bodyText": "Yes, just as you described.I have modified the comment, I wonder if you have a better one.", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449747864", "createdAt": "2020-07-04T07:11:56Z", "author": {"login": "zhangdove"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -30,8 +31,11 @@ protected String metadataTableName(MetadataTableType type) {\n     String tableName = table().toString();\n     if (tableName.contains(\"/\")) {\n       return tableName + \"#\" + type;\n-    } else if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\")) {\n-      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // HadoopCatalog tableName style is 'hadoop.ns.tb'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NTI1NA=="}, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTgwODg3NQ==", "bodyText": "I made a suggestion. Thanks!", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449808875", "createdAt": "2020-07-04T22:03:29Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -30,8 +31,11 @@ protected String metadataTableName(MetadataTableType type) {\n     String tableName = table().toString();\n     if (tableName.contains(\"/\")) {\n       return tableName + \"#\" + type;\n-    } else if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\")) {\n-      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // HadoopCatalog tableName style is 'hadoop.ns.tb'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NTI1NA=="}, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwMjc5Njk4OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjozODowNVrOGs0ybA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNFQwNjowNjowM1rOGs6Ltg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NTQwNA==", "bodyText": "Why not use table().location() instead? There's no need to cast to BaseTable and access TableOperations.", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449655404", "createdAt": "2020-07-03T16:38:05Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -30,8 +31,11 @@ protected String metadataTableName(MetadataTableType type) {\n     String tableName = table().toString();\n     if (tableName.contains(\"/\")) {\n       return tableName + \"#\" + type;\n-    } else if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\")) {\n-      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // HadoopCatalog tableName style is 'hadoop.ns.tb'\n+      return ((BaseTable) table()).operations().current().location() + \"#\" + type;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTc0Mzc5OA==", "bodyText": "That sounds like a good idea to me. I'll fix it.", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449743798", "createdAt": "2020-07-04T06:06:03Z", "author": {"login": "zhangdove"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -30,8 +31,11 @@ protected String metadataTableName(MetadataTableType type) {\n     String tableName = table().toString();\n     if (tableName.contains(\"/\")) {\n       return tableName + \"#\" + type;\n-    } else if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\")) {\n-      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // HadoopCatalog tableName style is 'hadoop.ns.tb'\n+      return ((BaseTable) table()).operations().current().location() + \"#\" + type;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NTQwNA=="}, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwMjc5OTM0OnYy", "diffSide": "RIGHT", "path": "spark2/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjozOTozNFrOGs0z2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjozOTozNFrOGs0z2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NTc3MQ==", "bodyText": "Nit: Indentation is off. It should be 2 indents (4 spaces) from the indent of the line that is being continued, df.select(...).", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449655771", "createdAt": "2020-07-03T16:39:34Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java", "diffHunk": "@@ -542,4 +545,41 @@ public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException, Inte\n     Assert.assertEquals(\"Action should find 1 file\", invalidFiles, result);\n     Assert.assertTrue(\"Invalid file should be present\", fs.exists(new Path(invalidFiles.get(0))));\n   }\n+\n+  @Test\n+  public void testRemoveOrphanFilesWithHadoopCatalog() throws InterruptedException {\n+    HadoopCatalog catalog = new HadoopCatalog(new Configuration(), tableLocation);\n+    String namespaceName = \"testDb\";\n+    String tableName = \"testTb\";\n+\n+    Namespace namespace = Namespace.of(namespaceName);\n+    TableIdentifier tableIdentifier = TableIdentifier.of(namespace, tableName);\n+    Table table = catalog.createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap());\n+\n+    List<ThreeColumnRecord> records = Lists.newArrayList(\n+            new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n+    );\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+\n+    String tableFileSystemPath = tableLocation + \"/\" + namespaceName + \"/\" + tableName;\n+    df.select(\"c1\", \"c2\", \"c3\")\n+            .write()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwMjgwMzA1OnYy", "diffSide": "RIGHT", "path": "spark2/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjo0MTo0MVrOGs016g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjo0MTo0MVrOGs016g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NjI5OA==", "bodyText": "Can you add the comment that explains this line from the other test cases?", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449656298", "createdAt": "2020-07-03T16:41:41Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java", "diffHunk": "@@ -542,4 +545,41 @@ public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException, Inte\n     Assert.assertEquals(\"Action should find 1 file\", invalidFiles, result);\n     Assert.assertTrue(\"Invalid file should be present\", fs.exists(new Path(invalidFiles.get(0))));\n   }\n+\n+  @Test\n+  public void testRemoveOrphanFilesWithHadoopCatalog() throws InterruptedException {\n+    HadoopCatalog catalog = new HadoopCatalog(new Configuration(), tableLocation);\n+    String namespaceName = \"testDb\";\n+    String tableName = \"testTb\";\n+\n+    Namespace namespace = Namespace.of(namespaceName);\n+    TableIdentifier tableIdentifier = TableIdentifier.of(namespace, tableName);\n+    Table table = catalog.createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap());\n+\n+    List<ThreeColumnRecord> records = Lists.newArrayList(\n+            new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n+    );\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+\n+    String tableFileSystemPath = tableLocation + \"/\" + namespaceName + \"/\" + tableName;\n+    df.select(\"c1\", \"c2\", \"c3\")\n+            .write()\n+            .format(\"iceberg\")\n+            .mode(\"append\")\n+            .save(tableFileSystemPath);\n+\n+    df.write().mode(\"append\").parquet(tableFileSystemPath + \"/data\");\n+\n+    Thread.sleep(1000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwMjgwMzc1OnYy", "diffSide": "RIGHT", "path": "spark2/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjo0MjowOFrOGs02Wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjo0MjowOFrOGs02Wg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NjQxMA==", "bodyText": "You can combine this with the previous line:\nList<String> deletedFiles = Actions.forTable(table)\n    .removeOrphanFiles()\n    .olderThan(timestamp)\n    .execute();", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449656410", "createdAt": "2020-07-03T16:42:08Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java", "diffHunk": "@@ -542,4 +545,41 @@ public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException, Inte\n     Assert.assertEquals(\"Action should find 1 file\", invalidFiles, result);\n     Assert.assertTrue(\"Invalid file should be present\", fs.exists(new Path(invalidFiles.get(0))));\n   }\n+\n+  @Test\n+  public void testRemoveOrphanFilesWithHadoopCatalog() throws InterruptedException {\n+    HadoopCatalog catalog = new HadoopCatalog(new Configuration(), tableLocation);\n+    String namespaceName = \"testDb\";\n+    String tableName = \"testTb\";\n+\n+    Namespace namespace = Namespace.of(namespaceName);\n+    TableIdentifier tableIdentifier = TableIdentifier.of(namespace, tableName);\n+    Table table = catalog.createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap());\n+\n+    List<ThreeColumnRecord> records = Lists.newArrayList(\n+            new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n+    );\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+\n+    String tableFileSystemPath = tableLocation + \"/\" + namespaceName + \"/\" + tableName;\n+    df.select(\"c1\", \"c2\", \"c3\")\n+            .write()\n+            .format(\"iceberg\")\n+            .mode(\"append\")\n+            .save(tableFileSystemPath);\n+\n+    df.write().mode(\"append\").parquet(tableFileSystemPath + \"/data\");\n+\n+    Thread.sleep(1000);\n+\n+    long timestamp = System.currentTimeMillis();\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    List<String> result = actions.removeOrphanFiles()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwMjgwNTAwOnYy", "diffSide": "RIGHT", "path": "spark2/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjo0Mjo0OFrOGs03Gw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjo0Mjo0OFrOGs03Gw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NjYwMw==", "bodyText": "Minor: The other tests don't use a separate variable for this. Could this be embedded in the olderThan call?", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449656603", "createdAt": "2020-07-03T16:42:48Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java", "diffHunk": "@@ -542,4 +545,41 @@ public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException, Inte\n     Assert.assertEquals(\"Action should find 1 file\", invalidFiles, result);\n     Assert.assertTrue(\"Invalid file should be present\", fs.exists(new Path(invalidFiles.get(0))));\n   }\n+\n+  @Test\n+  public void testRemoveOrphanFilesWithHadoopCatalog() throws InterruptedException {\n+    HadoopCatalog catalog = new HadoopCatalog(new Configuration(), tableLocation);\n+    String namespaceName = \"testDb\";\n+    String tableName = \"testTb\";\n+\n+    Namespace namespace = Namespace.of(namespaceName);\n+    TableIdentifier tableIdentifier = TableIdentifier.of(namespace, tableName);\n+    Table table = catalog.createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap());\n+\n+    List<ThreeColumnRecord> records = Lists.newArrayList(\n+            new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n+    );\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+\n+    String tableFileSystemPath = tableLocation + \"/\" + namespaceName + \"/\" + tableName;\n+    df.select(\"c1\", \"c2\", \"c3\")\n+            .write()\n+            .format(\"iceberg\")\n+            .mode(\"append\")\n+            .save(tableFileSystemPath);\n+\n+    df.write().mode(\"append\").parquet(tableFileSystemPath + \"/data\");\n+\n+    Thread.sleep(1000);\n+\n+    long timestamp = System.currentTimeMillis();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwMjgwNjI2OnYy", "diffSide": "RIGHT", "path": "spark2/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjo0MzozOVrOGs036g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wM1QxNjo0MzozOVrOGs036g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTY1NjgxMA==", "bodyText": "Does this work using table.location instead of building the path here?", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449656810", "createdAt": "2020-07-03T16:43:39Z", "author": {"login": "rdblue"}, "path": "spark2/src/test/java/org/apache/iceberg/actions/TestRemoveOrphanFilesAction.java", "diffHunk": "@@ -542,4 +545,41 @@ public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException, Inte\n     Assert.assertEquals(\"Action should find 1 file\", invalidFiles, result);\n     Assert.assertTrue(\"Invalid file should be present\", fs.exists(new Path(invalidFiles.get(0))));\n   }\n+\n+  @Test\n+  public void testRemoveOrphanFilesWithHadoopCatalog() throws InterruptedException {\n+    HadoopCatalog catalog = new HadoopCatalog(new Configuration(), tableLocation);\n+    String namespaceName = \"testDb\";\n+    String tableName = \"testTb\";\n+\n+    Namespace namespace = Namespace.of(namespaceName);\n+    TableIdentifier tableIdentifier = TableIdentifier.of(namespace, tableName);\n+    Table table = catalog.createTable(tableIdentifier, SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap());\n+\n+    List<ThreeColumnRecord> records = Lists.newArrayList(\n+            new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\")\n+    );\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+\n+    String tableFileSystemPath = tableLocation + \"/\" + namespaceName + \"/\" + tableName;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "527bb60c067a12463688a99a2a05d67c86fb92ec"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwMzk5ODg1OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNFQyMTo1OTo0OVrOGs-JFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNFQyMTo1OTo0OVrOGs-JFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTgwODY2Mg==", "bodyText": "Looks like this needs to be updated. There is no need to remove hadoop. if Hadoop tables don't use this code path.", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449808662", "createdAt": "2020-07-04T21:59:49Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -30,8 +30,11 @@ protected String metadataTableName(MetadataTableType type) {\n     String tableName = table().toString();\n     if (tableName.contains(\"/\")) {\n       return tableName + \"#\" + type;\n-    } else if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\")) {\n-      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // Load a path by HadoopCatalog or HadoopTables\n+      return table().location() + \"#\" + type;\n+    } else if (tableName.startsWith(\"hive.\")) {\n+      // HiveCatalog prepend a logical name which we need to drop for Spark 2.4\n       return tableName.replaceFirst(\"(hadoop\\\\.)|(hive\\\\.)\", \"\") + \".\" + type;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86569e3a95525e77ed681ffcf49107dcb0dbe3b9"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgwNDAwMDY4OnYy", "diffSide": "RIGHT", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNFQyMjowMzowNlrOGs-J4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNFQyMjowMzowNlrOGs-J4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTgwODg2NA==", "bodyText": "How about \"for HadoopCatalog tables, use the table location to load the metadata table because IcebergCatalog uses HiveCatalog when the table is identified by name\".", "url": "https://github.com/apache/iceberg/pull/1161#discussion_r449808864", "createdAt": "2020-07-04T22:03:06Z", "author": {"login": "rdblue"}, "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -30,8 +30,11 @@ protected String metadataTableName(MetadataTableType type) {\n     String tableName = table().toString();\n     if (tableName.contains(\"/\")) {\n       return tableName + \"#\" + type;\n-    } else if (tableName.startsWith(\"hadoop.\") || tableName.startsWith(\"hive.\")) {\n-      // HiveCatalog and HadoopCatalog prepend a logical name which we need to drop for Spark 2.4\n+    } else if (tableName.startsWith(\"hadoop.\")) {\n+      // Load a path by HadoopCatalog or HadoopTables", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "86569e3a95525e77ed681ffcf49107dcb0dbe3b9"}, "originalPosition": 7}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3939, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}